# 1 "/root/yjw/HANS/HANS-NPU/hans_merge.cpp"
# 1 "<built-in>" 1
# 1 "<built-in>" 3
# 406 "<built-in>" 3
# 1 "<command line>" 1
# 1 "<built-in>" 2
# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 1 3
# 17 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 3
# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 1 3
# 35 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 3
typedef long int ptrdiff_t;
# 46 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 3
typedef long unsigned int size_t;
# 102 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 3
# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__stddef_max_align_t.h" 1 3
# 19 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__stddef_max_align_t.h" 3
typedef struct {
  long long __clang_max_align_nonce1
      __attribute__((__aligned__(__alignof__(long long))));
  long double __clang_max_align_nonce2
      __attribute__((__aligned__(__alignof__(long double))));
} max_align_t;
# 103 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 2 3
# 18 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3
# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdint.h" 1 3
# 52 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdint.h" 3
# 1 "/usr/include/stdint.h" 1 3 4
# 26 "/usr/include/stdint.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/libc-header-start.h" 1 3 4
# 33 "/usr/include/aarch64-linux-gnu/bits/libc-header-start.h" 3 4
# 1 "/usr/include/features.h" 1 3 4
# 392 "/usr/include/features.h" 3 4
# 1 "/usr/include/features-time64.h" 1 3 4
# 20 "/usr/include/features-time64.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/wordsize.h" 1 3 4
# 21 "/usr/include/features-time64.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/timesize.h" 1 3 4
# 22 "/usr/include/features-time64.h" 2 3 4
# 393 "/usr/include/features.h" 2 3 4
# 464 "/usr/include/features.h" 3 4
# 1 "/usr/include/stdc-predef.h" 1 3 4
# 465 "/usr/include/features.h" 2 3 4
# 486 "/usr/include/features.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/sys/cdefs.h" 1 3 4
# 559 "/usr/include/aarch64-linux-gnu/sys/cdefs.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/wordsize.h" 1 3 4
# 560 "/usr/include/aarch64-linux-gnu/sys/cdefs.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/long-double.h" 1 3 4
# 561 "/usr/include/aarch64-linux-gnu/sys/cdefs.h" 2 3 4
# 487 "/usr/include/features.h" 2 3 4
# 510 "/usr/include/features.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/gnu/stubs.h" 1 3 4




# 1 "/usr/include/aarch64-linux-gnu/bits/wordsize.h" 1 3 4
# 6 "/usr/include/aarch64-linux-gnu/gnu/stubs.h" 2 3 4


# 1 "/usr/include/aarch64-linux-gnu/gnu/stubs-lp64.h" 1 3 4
# 9 "/usr/include/aarch64-linux-gnu/gnu/stubs.h" 2 3 4
# 511 "/usr/include/features.h" 2 3 4
# 34 "/usr/include/aarch64-linux-gnu/bits/libc-header-start.h" 2 3 4
# 27 "/usr/include/stdint.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/types.h" 1 3 4
# 27 "/usr/include/aarch64-linux-gnu/bits/types.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/wordsize.h" 1 3 4
# 28 "/usr/include/aarch64-linux-gnu/bits/types.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/timesize.h" 1 3 4
# 29 "/usr/include/aarch64-linux-gnu/bits/types.h" 2 3 4


typedef unsigned char __u_char;
typedef unsigned short int __u_short;
typedef unsigned int __u_int;
typedef unsigned long int __u_long;


typedef signed char __int8_t;
typedef unsigned char __uint8_t;
typedef signed short int __int16_t;
typedef unsigned short int __uint16_t;
typedef signed int __int32_t;
typedef unsigned int __uint32_t;

typedef signed long int __int64_t;
typedef unsigned long int __uint64_t;






typedef __int8_t __int_least8_t;
typedef __uint8_t __uint_least8_t;
typedef __int16_t __int_least16_t;
typedef __uint16_t __uint_least16_t;
typedef __int32_t __int_least32_t;
typedef __uint32_t __uint_least32_t;
typedef __int64_t __int_least64_t;
typedef __uint64_t __uint_least64_t;



typedef long int __quad_t;
typedef unsigned long int __u_quad_t;







typedef long int __intmax_t;
typedef unsigned long int __uintmax_t;
# 141 "/usr/include/aarch64-linux-gnu/bits/types.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/typesizes.h" 1 3 4
# 142 "/usr/include/aarch64-linux-gnu/bits/types.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/time64.h" 1 3 4
# 143 "/usr/include/aarch64-linux-gnu/bits/types.h" 2 3 4


typedef unsigned long int __dev_t;
typedef unsigned int __uid_t;
typedef unsigned int __gid_t;
typedef unsigned long int __ino_t;
typedef unsigned long int __ino64_t;
typedef unsigned int __mode_t;
typedef unsigned int __nlink_t;
typedef long int __off_t;
typedef long int __off64_t;
typedef int __pid_t;
typedef struct { int __val[2]; } __fsid_t;
typedef long int __clock_t;
typedef unsigned long int __rlim_t;
typedef unsigned long int __rlim64_t;
typedef unsigned int __id_t;
typedef long int __time_t;
typedef unsigned int __useconds_t;
typedef long int __suseconds_t;
typedef long int __suseconds64_t;

typedef int __daddr_t;
typedef int __key_t;


typedef int __clockid_t;


typedef void * __timer_t;


typedef int __blksize_t;




typedef long int __blkcnt_t;
typedef long int __blkcnt64_t;


typedef unsigned long int __fsblkcnt_t;
typedef unsigned long int __fsblkcnt64_t;


typedef unsigned long int __fsfilcnt_t;
typedef unsigned long int __fsfilcnt64_t;


typedef long int __fsword_t;

typedef long int __ssize_t;


typedef long int __syscall_slong_t;

typedef unsigned long int __syscall_ulong_t;



typedef __off64_t __loff_t;
typedef char *__caddr_t;


typedef long int __intptr_t;


typedef unsigned int __socklen_t;




typedef int __sig_atomic_t;
# 28 "/usr/include/stdint.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/wchar.h" 1 3 4
# 29 "/usr/include/stdint.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/wordsize.h" 1 3 4
# 30 "/usr/include/stdint.h" 2 3 4




# 1 "/usr/include/aarch64-linux-gnu/bits/stdint-intn.h" 1 3 4
# 24 "/usr/include/aarch64-linux-gnu/bits/stdint-intn.h" 3 4
typedef __int8_t int8_t;
typedef __int16_t int16_t;
typedef __int32_t int32_t;
typedef __int64_t int64_t;
# 35 "/usr/include/stdint.h" 2 3 4


# 1 "/usr/include/aarch64-linux-gnu/bits/stdint-uintn.h" 1 3 4
# 24 "/usr/include/aarch64-linux-gnu/bits/stdint-uintn.h" 3 4
typedef __uint8_t uint8_t;
typedef __uint16_t uint16_t;
typedef __uint32_t uint32_t;
typedef __uint64_t uint64_t;
# 38 "/usr/include/stdint.h" 2 3 4





typedef __int_least8_t int_least8_t;
typedef __int_least16_t int_least16_t;
typedef __int_least32_t int_least32_t;
typedef __int_least64_t int_least64_t;


typedef __uint_least8_t uint_least8_t;
typedef __uint_least16_t uint_least16_t;
typedef __uint_least32_t uint_least32_t;
typedef __uint_least64_t uint_least64_t;





typedef signed char int_fast8_t;

typedef long int int_fast16_t;
typedef long int int_fast32_t;
typedef long int int_fast64_t;
# 71 "/usr/include/stdint.h" 3 4
typedef unsigned char uint_fast8_t;

typedef unsigned long int uint_fast16_t;
typedef unsigned long int uint_fast32_t;
typedef unsigned long int uint_fast64_t;
# 87 "/usr/include/stdint.h" 3 4
typedef long int intptr_t;


typedef unsigned long int uintptr_t;
# 101 "/usr/include/stdint.h" 3 4
typedef __intmax_t intmax_t;
typedef __uintmax_t uintmax_t;
# 53 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdint.h" 2 3
# 19 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3






# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 1 3
# 10 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_defines.h" 1 3
# 11 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 2 3
# 26 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
typedef __bf16 bfloat16_t;
# 54 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
namespace bisheng {
struct [[bisheng::mangling_hint(BFloat16)]] __BISHENG_BUILTIN_TYPE_PROXY_BFloat16 { __BISHENG_BUILTIN_TYPE_PROXY_BFloat16() = delete; __BISHENG_BUILTIN_TYPE_PROXY_BFloat16 (const __BISHENG_BUILTIN_TYPE_PROXY_BFloat16 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_BFloat16 (__BISHENG_BUILTIN_TYPE_PROXY_BFloat16 &&) = delete; private: char Storage[2]; };
struct [[bisheng::mangling_hint(HFloat8)]] __BISHENG_BUILTIN_TYPE_PROXY_HFloat8 { __BISHENG_BUILTIN_TYPE_PROXY_HFloat8() = delete; __BISHENG_BUILTIN_TYPE_PROXY_HFloat8 (const __BISHENG_BUILTIN_TYPE_PROXY_HFloat8 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_HFloat8 (__BISHENG_BUILTIN_TYPE_PROXY_HFloat8 &&) = delete; private: char Storage[1]; };
struct [[bisheng::mangling_hint(HFloat4x2)]] __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2 { __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2() = delete; __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2 (const __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2 (__BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2 &&) = delete; private: char Storage[1]; };
struct [[bisheng::mangling_hint(Float8_E4M3)]] __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3 { __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3() = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3 (const __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3 (__BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3 &&) = delete; private: char Storage[1]; };
struct [[bisheng::mangling_hint(Float8_E5M2)]] __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2 { __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2() = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2 (const __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2 (__BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2 &&) = delete; private: char Storage[1]; };
struct [[bisheng::mangling_hint(Float4_E2M1)]] __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1 { __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1() = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1 (const __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1 (__BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1 &&) = delete; private: char Storage[1]; };
struct [[bisheng::mangling_hint(Float4_E1M2)]] __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2 { __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2() = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2 (const __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2 (__BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2 &&) = delete; private: char Storage[1]; };
}
# 112 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/cce_aicore_intrinsics.h" 1 3




typedef enum {

  CRFMODE_NONE = 0,
  CRFMODE_DEQSCALE_VDEQ8 = 8,
  CRFMODE_DEQSCALE_DEQ8 = 9,
  CRFMODE_DEQSCALE_VDEQ16 = 10,
  CRFMODE_DEQSCALE_DEQ16 = 11,
  CRFMODE_DEQSCALE_VDEQS16 = 12,
  CRFMODE_DEQSCALE_DEQS16 = 13,
  CRFMODE_DEQSCALE_VDEQ2S16 = 14,
  CRFMODE_DEQSCALE_DEQ2S16 = 15,

} ConvReluFix_t;

typedef enum {


  CRMODE_NONE = 0,
  CRMODE_F32toF16_NONE = 1,
  CRMODE_F32toF16_RELU = 2,
  CRMODE_S32toF16_NONE = 3,
  CRMODE_F16toF32_NONE = 4,
  CRMODE_NONE_RELU = 5,
  CRMODE_F16_MUL = 6,
  CRMODE_S32toF16_DEQSCALE_SPR = 7,
  CRMODE_DEQSCALE_VDEQ8 = 8,
  CRMODE_DEQSCALE_DEQ8 = 9,
  CRMODE_DEQSCALE_VDEQ16 = 10,
  CRMODE_DEQSCALE_DEQ16 = 11,
  CRMODE_DEQSCALE_VDEQS16 = 12,
  CRMODE_DEQSCALE_DEQS16 = 13,

} ConvRelu_t;

typedef enum {

  NoConversion = 0,
  CvtMode1 = 1,
  CvtMode2 = 2,

} CvtMode_t;

typedef enum {

  DI_featuremap = 0,
  DI_others = 1,

} DI_t;

typedef enum {


  DUAL_MODE0 = 0,
  DUAL_MODE1 = 1,
  DUAL_MODE2 = 2,
  DUAL_MODE3 = 3,
  DUAL_MODE4 = 4,

} DualMode_t;

typedef enum {

    VALUE_INDEX = 0,
    INDEX_VALUE = 1,
    ONLY_VALUE = 2,
    ONLY_INDEX = 3,

} Order_t;

typedef enum {
# 88 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/cce_aicore_intrinsics.h" 3
} PadFuncMode_t;

typedef enum {


  NoPooling = 0,
  AVGPooling = 1,
  MAXPooling = 2,
  GAVGPooling = 3,

} Pool_t;

typedef enum {
# 116 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/cce_aicore_intrinsics.h" 3
} QMode_t;

typedef enum {

  NoConv= 0,
  VQS162B8_POST = 1,
  QS162B8_POST = 2,
  VQF162B8_POST = 3,
  QF162B8_POST = 4,
  VQS162S4_POST = 5,
  QS162S4_POST = 6,
  VQF162S4_POST = 7,
  QF162S4_POST = 8,
  VQS162S16_POST = 9,
  QS162S16_POST = 10,
  VQF162S16_POST = 11,
  QF162S16_POST = 12,
  VSHIFT2S4_POST = 13,
  SHIFT2S4_POST = 14,
  VSHIFT2S8_POST = 15,
  SHIFT2S8_POST = 16,
  VSHIFT2S16_POST = 17,
  SHIFT2S16_POST = 18,

} QuantMode_post;

typedef enum {

  NoQuant= 0,
# 153 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/cce_aicore_intrinsics.h" 3
  F322F16 = 1,
  VQF322HIF8_PRE = 2,
  QF322HIF8_PRE = 3,
  VQF322HIF8_PRE_HYBRID = 4,
  QF322HIF8_PRE_HYBRID = 5,

  AttachF16Mul = 6,




  VREQ8 = 8,
  REQ8 = 9,
  VDEQF16 = 10,
  DEQF16 = 11,






  VSHIFTS322S16 = 12,
  SHIFTS322S16 = 13,

  F322BF16 = 16,
  VQF162B8_PRE = 17,
  QF162B8_PRE = 18,
  VQF162S4_PRE = 19,
  QF162S4_PRE = 20,
  VREQ4 = 21,
  REQ4 = 22,
  VQF322B8_PRE = 23,
  QF322B8_PRE = 24,
  VQF322S4_PRE = 25,
  QF322S4_PRE = 26,
  VDEQS16 = 27,
  DEQS16 = 28,
  VQF162S16_PRE = 29,
  QF162S16_PRE = 30,
  VQF322F16_PRE = 31,
  QF322F16_PRE = 32,
  VQF322BF16_PRE = 33,
  QF322BF16_PRE = 34,
  VQS322BF16_PRE = 35,
  QS322BF16_PRE = 36,


} QuantMode_t;

typedef enum {

  NoRelu = 0,
  NormalRelu = 1,
  ScalarRelu = 2,
  VectorRelu = 3,
  LUTActivation = 4,

} ReluMode_t;

typedef enum {


  NoRELU = 0,
  NormalRELU = 1,
  LeakyRELU = 2,
  PRELU = 3,

} Relu_t;

typedef enum {


  NoREQ = 0,
  REQ = 1,
  VREQ = 2,

} Req_t;

typedef enum {

    MODE0 = 0,
    MODE1 = 1,
    MODE2 = 2,

} VSEL_mode_t;

typedef enum {




  DATA_EXP_0 = 48,
  DATA_EXP_1 = 49,
  DATA_EXP_2 = 50,
  DATA_EXP_3 = 51,

} VSPR_t;

typedef enum {

  inc = 0,
  dec = 1,

} addr_cal_mode_t;

typedef enum {

  ATOMIC_SUM = 0,


} atomic_op_t;

typedef enum {

  ATOMIC_NONE = 0,
  ATOMIC_F32 = 1,
  ATOMIC_F16 = 2,
  ATOMIC_S16 = 3,
  ATOMIC_S32 = 4,
  ATOMIC_S8 = 5,
  ATOMIC_BF16 = 6,

} atomic_type_t;

typedef enum {


  BM_DISABLE = 0,
  BM_ENABLE = 1,

} bm_t;

typedef enum {

  SINGLE_CACHE_LINE = 0,
  ENTIRE_DATA_CACHE,

} cache_line_t;

typedef enum {

  CSIZE0 = 0,
  CSIZE1 = 1,

} csize_t;

typedef enum {

  CACHELINE_ALL = 0,

  CACHELINE_UB,

  CACHELINE_OUT = 2,

  CACHELINE_ATOMIC,


} dcci_dst_t;

typedef enum {

  e0 = 0,
  e2 = 2,
  e4 = 4,
  e6 = 6,

} even0_7_t;

typedef enum {

  EVENT_ID0 = 0,
  EVENT_ID1,
  EVENT_ID2,
  EVENT_ID3,

  EVENT_ID4,
  EVENT_ID5,
  EVENT_ID6,
  EVENT_ID7,


} event_t;

typedef enum {

  DSB_ALL = 0,
  DSB_DDR,
  DSB_UB,
  DSB_SEQ,

} mem_dsb_t;

typedef enum {

  L1 = 0,
  L0A,
  L0B,
  L0C,
  UB,
  BT,

} mem_t;

typedef enum {


  PAD_NONE = 0,
  PAD_MODE1 = 1,
  PAD_MODE2 = 2,
  PAD_MODE3 = 3,
  PAD_MODE4 = 4,
  PAD_MODE5 = 5,
  PAD_MODE6 = 6,
  PAD_MODE7 = 7,
  PAD_MODE8 = 8,

} pad_t;

typedef enum {

  PIPE_S = 0,
  PIPE_V,
  PIPE_M,
  PIPE_MTE1,
  PIPE_MTE2,
  PIPE_MTE3,
  PIPE_ALL,

  PIPE_MTE4 = 7,
  PIPE_MTE5 = 8,

  PIPE_V2 = 9,



  PIPE_FIX = 10,



} pipe_t;

typedef enum {

  CROSS_CORE = 0,
  INTRA_BLOCK,
  BUFFER_ID,
  RESERVED,

} sync_mode_t;

typedef enum {







  VA0 = 0,
  VA1,
  VA2,
  VA3,
  VA4,
  VA5,
  VA6,
  VA7,


} ub_addr8_t;

typedef enum {

  UFMode0 = 0,
  Reserved,
  UFMode2,
  UFMode3,

} unit_flag_t;

typedef enum {

  L128 = 0,
  H128,

} vpart_t;

typedef enum {

  b8 = 0,
  b16 = 1,
  b32 = 2,
  s8 = 3,
  s32 = 4,
  f16 = 5,
  fmix = 6,
  f32 = 7,

} vtype_t;

typedef enum {

  W_3 = 0,
  W_5,

} w_size_t;
# 113 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 2 3
# 26 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_builtin_vars.h" 1 3
# 13 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_builtin_vars.h" 3
struct ulong_2;
# 52 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_builtin_vars.h" 3
struct __cce_builtin_block_t {
  __declspec( property(get = __fetch_builtin_idx)) unsigned long long idx; static inline __attribute__((cce_builtin_api, always_inline))[aicore] unsigned long long __fetch_builtin_idx(void) { return get_block_idx(); };
  __declspec( property(get = __fetch_builtin_num)) unsigned long long num; static inline __attribute__((cce_builtin_api, always_inline))[aicore] unsigned long long __fetch_builtin_num(void) { return get_block_num(); };

private:
  [aicore] __cce_builtin_block_t() = delete; [aicore] __cce_builtin_block_t(const __cce_builtin_block_t &) = delete; [aicore] void operator=(const __cce_builtin_block_t &) const = delete; [aicore] __cce_builtin_block_t *operator&() const = delete;
};


extern const[aicore] __attribute__((weak)) __cce_builtin_block_t block;







struct ulong_2 {
  unsigned long long idx, num;
};

typedef struct ulong_2 ulong_2;
# 28 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime.h" 1 3
# 15 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime.h" 3
extern "C" {



unsigned int rtConfigureCall(uint32_t numBlocks, void *smDesc = nullptr, void *stream = nullptr);





}
# 30 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_intrinsics.h" 1 3
# 131 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_intrinsics.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void trap(uint64_t err_code) {
  __builtin_cce_trap_mov(err_code);
}
# 32 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 1 3
# 1756 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
namespace mmad_t {
struct shape_t {
  uint16_t shape_M;
  uint16_t shape_K;
  uint16_t shape_N;
  [aicore] __attribute__((cce_builtin_api, always_inline)) shape_t(uint16_t shape_M = 16, uint16_t shape_K = 16,
                            uint16_t shape_N = 16)
      : shape_M(shape_M), shape_K(shape_K), shape_N(shape_N) {}
};

struct control_t {
  uint8_t unit_flag_ctrl;
  bool gemv_ctrl;
  bool BTbuf_ctrl;
  bool zero_Cmatrix_ctrl;
  [aicore] __attribute__((cce_builtin_api, always_inline)) control_t(uint8_t unit_flag_ctrl = 0, bool gemv_ctrl = 0,
                              bool BTbuf_ctrl = 0, bool zero_Cmatrix_ctrl = 0)
      : unit_flag_ctrl(unit_flag_ctrl), gemv_ctrl(gemv_ctrl),
        BTbuf_ctrl(BTbuf_ctrl), zero_Cmatrix_ctrl(zero_Cmatrix_ctrl) {}
};
}
# 2366 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void
copy_data_align64(uint8_t *dst, __attribute__((cce_unif_buff)) uint8_t *src, uint64_t size) {
  __builtin_cce_copy_from_ub_align64(dst, src, size);
}
# 2385 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data_align64(uint8_t *dst, __attribute__((cce_global)) uint8_t *src,
                                             uint64_t size) {
  __builtin_cce_copy_from_gm_align64(dst, src, size);
}
# 2401 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data_align64(uint8_t *dst, uint8_t *src,
                                             uint64_t size) {
  __builtin_cce_copy_from_stack_align64(dst, src, size);
}
# 2417 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data(uint8_t *dst, __attribute__((cce_unif_buff)) uint8_t *src,
                                     uint64_t size) {
  __builtin_cce_copy_from_ub(dst, src, size);
}
# 2435 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data(uint8_t *dst, __attribute__((cce_global)) uint8_t *src,
                                     uint64_t size) {
  __builtin_cce_copy_from_gm(dst, src, size);
}
# 2454 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data(uint8_t *dst, uint8_t *src,
                                     uint64_t size) {
  __builtin_cce_copy_from_stack(dst, src, size);
}
# 2480 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __cce_set_flag(pipe_t p, pipe_t tp, event_t n) {
  set_flag(PIPE_M, PIPE_V, n);
  (void)p;
  (void)tp;
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __cce_wait_flag(pipe_t p, pipe_t tp, event_t n) {
  wait_flag(PIPE_M, PIPE_V, n);
  (void)p;
  (void)tp;
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __cce_pipe_barrier(pipe_t p) {
  pipe_barrier(PIPE_V);
  (void)p;
}






static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long long abs(long long in) { return __builtin_cce_llabs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long abs(long in) { return __builtin_cce_labs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int abs(int in) { return __builtin_cce_abs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] short abs(short in) { return __builtin_cce_abs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] char abs(char in) { return __builtin_cce_abs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float abs(float in) { return __builtin_cce_fabsf(in); };
# 2519 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(long long in) { return SQRT_s64(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(long in) { return SQRT_s64(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(int in) { return SQRT_s64(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(short in) { return SQRT_s64(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(char in) { return SQRT_s64(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float sqrt(float in) { return __builtin_cce_sqrtf(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] __cce_half sqrt(__cce_half in) { return __builtin_cce_sqrtf16(in); }


static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_S)))[aicore] void __dummy_pipe_s() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_M)))[aicore] void __dummy_pipe_m() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void __dummy_pipe_v() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_MTE1)))[aicore] void __dummy_pipe_mte1() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_MTE2)))[aicore] void __dummy_pipe_mte2() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_MTE3)))[aicore] void __dummy_pipe_mte3() {}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long long max(long long in1, long long in2) {
  return __builtin_cce_llsmax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long max(long in1, long in2) {
  return __builtin_cce_lsmax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int max(int in1, int in2) {
  return __builtin_cce_smax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] short max(short in1, short in2) {
  return __builtin_cce_smax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] char max(char in1, char in2) {
  return __builtin_cce_smax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long long max(unsigned long long in1,
                                             unsigned long long in2) {
  return __builtin_cce_llumax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long max(unsigned long in1, unsigned long in2) {
  return __builtin_cce_lumax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned int max(unsigned int in1, unsigned int in2) {
  return __builtin_cce_umax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned short max(unsigned short in1,
                                         unsigned short in2) {
  return __builtin_cce_umax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned char max(unsigned char in1, unsigned char in2) {
  return __builtin_cce_umax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float max(float in1, float in2) {
  return __builtin_cce_fmaxf(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] __cce_half max(__cce_half in1, __cce_half in2) {
  return __builtin_cce_fmaxf16(in1, in2);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long long min(long long in1, long long in2) {
  return __builtin_cce_llsmin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long min(long in1, long in2) {
  return __builtin_cce_lsmin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int min(int in1, int in2) {
  return __builtin_cce_smin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] short min(short in1, short in2) {
  return __builtin_cce_smin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] char min(char in1, char in2) {
  return __builtin_cce_smin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long long min(unsigned long long in1,
                                             unsigned long long in2) {
  return __builtin_cce_llumin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long min(unsigned long in1, unsigned long in2) {
  return __builtin_cce_lumin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned int min(unsigned int in1, unsigned int in2) {
  return __builtin_cce_umin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned short min(unsigned short in1,
                                         unsigned short in2) {
  return __builtin_cce_umin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned char min(unsigned char in1, unsigned char in2) {
  return __builtin_cce_umin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float min(float in1, float in2) {
  return __builtin_cce_fminf(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] __cce_half min(__cce_half in1, __cce_half in2) {
  return __builtin_cce_fminf16(in1, in2);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_ubuf_to_sbuf(void *dst, __attribute__((cce_unif_buff)) void *src,
                                             uint64_t size, int64_t inc) {
  MOV_UB_TO_SB(dst, src, size, inc);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_sbuf_to_ubuf(__attribute__((cce_unif_buff)) void *dst, void *src,
                                             uint64_t size, int64_t inc) {
  MOV_SB_TO_UB(dst, src, size, inc);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VBS16_f16(dst, src, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VBS16_f32(dst, src, config);
}
# 2692 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src0,
                                    __attribute__((cce_unif_buff)) unsigned int *src1,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VBS32_f16(dst, src0, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src0,
                                    __attribute__((cce_unif_buff)) unsigned int *src1,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VBS32_f32(dst, src0, src1, config);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vaadd(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src0,
                                 __attribute__((cce_unif_buff)) __cce_half *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VAADD_f16(dst, src0, src1, config);
}
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vaadd(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src0,
                                 __attribute__((cce_unif_buff)) float *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VAADD_f32(dst, src0, src1, config);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vmergech(__attribute__((cce_unif_buff)) uint8_t *dst,
                                    __attribute__((cce_unif_buff)) uint8_t *src, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VMERGECH_b8(dst, src, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vmergech(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VMERGECH_f16(dst, src, config);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vrpac(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src,
                                 uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VRPAC_f16(dst, src, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vrpac(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src,
                                 uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VRPAC_f32(dst, src, config);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void viou(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src0,
                                __attribute__((cce_unif_buff)) __cce_half *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VIOU_f16(dst, src0, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void viou(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src0,
                                __attribute__((cce_unif_buff)) float *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VIOU_f32(dst, src0, src1, config);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] inline void icache_preload(int64_t n) {
  preload((const void *)get_pc(), n);
}
# 2873 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((cce_builtin_api, always_inline))[aicore] uint64_t
get_vmrgsort_real_addr(__attribute__((cce_unif_buff)) uint64_t *src, unsigned shift,
                       uint64_t config = 0x0F00ULL) {
  uint64_t realAddr = 0;
  realAddr =
      (config & 0x0100ULL) ? (((uint64_t)src[0] >> shift) & 0xFFFFULL) : 0;
  realAddr |=
      ((config & 0x0200ULL) ? (((uint64_t)src[1] >> shift) & 0xFFFFULL) : 0)
      << 16;
  realAddr |=
      ((config & 0x0400ULL) ? (((uint64_t)src[2] >> shift) & 0xFFFFULL) : 0)
      << 32;
  realAddr |=
      ((config & 0x0800ULL) ? (((uint64_t)src[3] >> shift) & 0xFFFFULL) : 0)
      << 48;
  return realAddr;
}

static __attribute__((cce_builtin_api, always_inline))[aicore] uint64_t
get_vmrgsort_real_addr(__attribute__((cce_unif_buff)) __cce_half *src[4], unsigned shift,
                       uint64_t config = 0x0F00ULL) {
  uint64_t realAddr = 0;
  realAddr =
      (config & 0x0100ULL) ? (((uint64_t)src[0] >> shift) & 0xFFFFULL) : 0;
  realAddr |=
      ((config & 0x0200ULL) ? (((uint64_t)src[1] >> shift) & 0xFFFFULL) : 0)
      << 16;
  realAddr |=
      ((config & 0x0400ULL) ? (((uint64_t)src[2] >> shift) & 0xFFFFULL) : 0)
      << 32;
  realAddr |=
      ((config & 0x0800ULL) ? (((uint64_t)src[3] >> shift) & 0xFFFFULL) : 0)
      << 48;
  return realAddr;
}

static __attribute__((cce_builtin_api, always_inline))[aicore] uint64_t
get_vmrgsort_real_addr(__attribute__((cce_unif_buff)) float *src[4], unsigned shift,
                       uint64_t config = 0x0F00ULL) {
  uint64_t realAddr = 0;
  realAddr =
      (config & 0x0100ULL) ? (((uint64_t)src[0] >> shift) & 0xFFFFULL) : 0;
  realAddr |=
      ((config & 0x0200ULL) ? (((uint64_t)src[1] >> shift) & 0xFFFFULL) : 0)
      << 16;
  realAddr |=
      ((config & 0x0400ULL) ? (((uint64_t)src[2] >> shift) & 0xFFFFULL) : 0)
      << 32;
  realAddr |=
      ((config & 0x0800ULL) ? (((uint64_t)src[3] >> shift) & 0xFFFFULL) : 0)
      << 48;
  return realAddr;
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) __cce_half *dst,
                                                     __attribute__((cce_unif_buff)) uint64_t *src0,
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) __cce_half *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) uint64_t *src, uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask) __attribute__((cce_range_check("VMRGSORT_f16_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) __cce_half *)realAddr, repeat, list0, list1, list2, list3,
            enable_exh_sus, mask);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) __cce_half *dst,
                                                     __attribute__((cce_unif_buff)) __cce_half *src0[4],
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) __cce_half *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src[4], uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask) __attribute__((cce_range_check("VMRGSORT_f16_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) __cce_half *)realAddr, repeat, list0, list1, list2, list3,
            enable_exh_sus, mask);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) float *dst,
                                                     __attribute__((cce_unif_buff)) uint64_t *src0,
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) float *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) uint64_t *src, uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask) __attribute__((cce_range_check("VMRGSORT_f32_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) float *)realAddr, repeat, list0, list1, list2, list3,
            enable_exh_sus, mask);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) float *dst,
                                                     __attribute__((cce_unif_buff)) float *src0[4],
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) float *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src[4], uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask) __attribute__((cce_range_check("VMRGSORT_f32_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) float *)realAddr, repeat, list0, list1, list2, list3,
            enable_exh_sus, mask);
}
# 3002 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_l0_set_value(uint32_t value) { set_l0_set_value_ui(value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_l0_set_value(__cce_half value) { set_l0_set_value_h(value); };



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_l0_set_value(bfloat16_t value) { set_l0_set_value_bf16(value); };
# 3021 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_ca_matrix( __attribute__((cce_cube_a)) bfloat16_t *dst, int64_t repeat, __cce_half value) { create_ca_matrix_h(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_ca_matrix( __attribute__((cce_cube_a)) bfloat16_t *dst, int64_t repeat, uint32_t value) { create_ca_matrix_ui(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_ca_matrix( __attribute__((cce_cube_a)) bfloat16_t *dst, int64_t repeat, bfloat16_t value) { create_ca_matrix_bf16(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cb_matrix( __attribute__((cce_cube_b)) bfloat16_t *dst, int64_t repeat, __cce_half value) { create_cb_matrix_h(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cb_matrix( __attribute__((cce_cube_b)) bfloat16_t *dst, int64_t repeat, uint32_t value) { create_cb_matrix_ui(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cb_matrix( __attribute__((cce_cube_b)) bfloat16_t *dst, int64_t repeat, bfloat16_t value) { create_cb_matrix_bf16(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cbuf_matrix( __attribute__((cce_cube_buff)) bfloat16_t *dst, int64_t repeat, __cce_half value) { create_cbuf_matrix_h(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cbuf_matrix( __attribute__((cce_cube_buff)) bfloat16_t *dst, int64_t repeat, uint32_t value) { create_cbuf_matrix_ui(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cbuf_matrix( __attribute__((cce_cube_buff)) bfloat16_t *dst, int64_t repeat, bfloat16_t value) { create_cbuf_matrix_bf16(dst, repeat, value); };
# 3279 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_va_reg_sb(ub_addr8_t addr, uint64_t *array) {
  switch (addr) {
  case VA0:
    MOVEVA((VA0), 0, (array[0]), (array[1]));
    MOVEVA((VA0), 2, (array[2]), (array[3]));
    MOVEVA((VA0), 4, (array[4]), (array[5]));
    MOVEVA((VA0), 6, (array[6]), (array[7]));
    break;
  case VA1:
    MOVEVA((VA1), 0, (array[0]), (array[1]));
    MOVEVA((VA1), 2, (array[2]), (array[3]));
    MOVEVA((VA1), 4, (array[4]), (array[5]));
    MOVEVA((VA1), 6, (array[6]), (array[7]));
    break;
  case VA2:
    MOVEVA((VA2), 0, (array[0]), (array[1]));
    MOVEVA((VA2), 2, (array[2]), (array[3]));
    MOVEVA((VA2), 4, (array[4]), (array[5]));
    MOVEVA((VA2), 6, (array[6]), (array[7]));
    break;
  case VA3:
    MOVEVA((VA3), 0, (array[0]), (array[1]));
    MOVEVA((VA3), 2, (array[2]), (array[3]));
    MOVEVA((VA3), 4, (array[4]), (array[5]));
    MOVEVA((VA3), 6, (array[6]), (array[7]));
    break;
  case VA4:
    MOVEVA((VA4), 0, (array[0]), (array[1]));
    MOVEVA((VA4), 2, (array[2]), (array[3]));
    MOVEVA((VA4), 4, (array[4]), (array[5]));
    MOVEVA((VA4), 6, (array[6]), (array[7]));
    break;
  case VA5:
    MOVEVA((VA5), 0, (array[0]), (array[1]));
    MOVEVA((VA5), 2, (array[2]), (array[3]));
    MOVEVA((VA5), 4, (array[4]), (array[5]));
    MOVEVA((VA5), 6, (array[6]), (array[7]));
    break;
  case VA6:
    MOVEVA((VA6), 0, (array[0]), (array[1]));
    MOVEVA((VA6), 2, (array[2]), (array[3]));
    MOVEVA((VA6), 4, (array[4]), (array[5]));
    MOVEVA((VA6), 6, (array[6]), (array[7]));
    break;
  case VA7:
    MOVEVA((VA7), 0, (array[0]), (array[1]));
    MOVEVA((VA7), 2, (array[2]), (array[3]));
    MOVEVA((VA7), 4, (array[4]), (array[5]));
    MOVEVA((VA7), 6, (array[6]), (array[7]));
    break;
  default:
    break;
  }
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_va_reg(ub_addr8_t addr,
                                      __attribute__((cce_unif_buff)) uint64_t *array) {
  switch (addr) {
  case VA0:
    MOVEVA((VA0), 0, (array[0]), (array[1]));
    MOVEVA((VA0), 2, (array[2]), (array[3]));
    MOVEVA((VA0), 4, (array[4]), (array[5]));
    MOVEVA((VA0), 6, (array[6]), (array[7]));
    break;
  case VA1:
    MOVEVA((VA1), 0, (array[0]), (array[1]));
    MOVEVA((VA1), 2, (array[2]), (array[3]));
    MOVEVA((VA1), 4, (array[4]), (array[5]));
    MOVEVA((VA1), 6, (array[6]), (array[7]));
    break;
  case VA2:
    MOVEVA((VA2), 0, (array[0]), (array[1]));
    MOVEVA((VA2), 2, (array[2]), (array[3]));
    MOVEVA((VA2), 4, (array[4]), (array[5]));
    MOVEVA((VA2), 6, (array[6]), (array[7]));
    break;
  case VA3:
    MOVEVA((VA3), 0, (array[0]), (array[1]));
    MOVEVA((VA3), 2, (array[2]), (array[3]));
    MOVEVA((VA3), 4, (array[4]), (array[5]));
    MOVEVA((VA3), 6, (array[6]), (array[7]));
    break;
  case VA4:
    MOVEVA((VA4), 0, (array[0]), (array[1]));
    MOVEVA((VA4), 2, (array[2]), (array[3]));
    MOVEVA((VA4), 4, (array[4]), (array[5]));
    MOVEVA((VA4), 6, (array[6]), (array[7]));
    break;
  case VA5:
    MOVEVA((VA5), 0, (array[0]), (array[1]));
    MOVEVA((VA5), 2, (array[2]), (array[3]));
    MOVEVA((VA5), 4, (array[4]), (array[5]));
    MOVEVA((VA5), 6, (array[6]), (array[7]));
    break;
  case VA6:
    MOVEVA((VA6), 0, (array[0]), (array[1]));
    MOVEVA((VA6), 2, (array[2]), (array[3]));
    MOVEVA((VA6), 4, (array[4]), (array[5]));
    MOVEVA((VA6), 6, (array[6]), (array[7]));
    break;
  case VA7:
    MOVEVA((VA7), 0, (array[0]), (array[1]));
    MOVEVA((VA7), 2, (array[2]), (array[3]));
    MOVEVA((VA7), 4, (array[4]), (array[5]));
    MOVEVA((VA7), 6, (array[6]), (array[7]));
    break;
  default:
    break;
  }
}
# 4062 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void clear_overflow_status() {
  uint64_t a = fake_overflow_status_1();
  uint64_t b = fake_overflow_status_2();
  CLEAR_OVERFLOW_STATUS(a, b);
}
# 4323 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
# 4338 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) __cce_half *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) __cce_half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) __cce_half *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) __cce_half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) __cce_half *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) __cce_half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, uint64_t addr, uint64_t config) { mad_s4((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint64_t config) { mad_tf322f32((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad_s4((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad_s4(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad_tf322f32((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad_tf322f32(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad_s4((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad_s4(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad_tf322f32((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad_tf322f32(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_sp(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint64_t config) { mad_sp((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_sp( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad_sp((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, cmatrixSource, cmatrixInitVal); } else { mad_sp(c, a, b, m, k, n, unitFlag, cmatrixSource, cmatrixInitVal); } };
# 4442 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) int8_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) int8_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) uint8_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) uint8_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) int16_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) int16_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) uint16_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) uint16_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) __cce_half *
get_ub_virtual_address(__attribute__((cce_unif_buff)) __cce_half *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) int32_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) int32_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) uint32_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) uint32_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) float *
get_ub_virtual_address(__attribute__((cce_unif_buff)) float *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] uint64_t get_ub_virtual_address(uint64_t addr) {
  return addr;
}
# 4519 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
template <typename T>
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void st_atomic(int64_t value, __attribute__((cce_global)) T *addr) {
  __atomic_store_n(addr, value, 0);
}
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void st_atomic(float value, __attribute__((cce_global)) float *addr) {
  typedef union {
    int32_t i;
    float f;
  } val;
  val tmp;
  tmp.f = value;
  __atomic_store_n(reinterpret_cast<__attribute__((cce_global)) int32_t *>(addr), tmp.i,
                   0);
}
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void st_atomic(__cce_half value, __attribute__((cce_global)) __cce_half *addr) {
  typedef union {
    int16_t i;
    __cce_half f;
  } val;
  val tmp;
  tmp.f = value;
  __atomic_store_n(reinterpret_cast<__attribute__((cce_global)) int16_t *>(addr), tmp.i,
                   0);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void wait_flag_dev(int64_t flagID) {
  __BUILTIN_CCE_wait_flag_dev(flagID);
}


namespace bisheng {
namespace cce {



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void metrics_prof_start() {
  pipe_barrier(PIPE_ALL);
  set_ctrl(sbitset1(get_ctrl(), 0));
  pipe_barrier(PIPE_ALL);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void metrics_prof_stop() {
  pipe_barrier(PIPE_ALL);
  set_ctrl(sbitset0(get_ctrl(), 0));
  pipe_barrier(PIPE_ALL);
}
# 4590 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicpu_neon.h" 1 3
# 71 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicpu_neon.h" 3
typedef float float32_t;
typedef __cce_half float16_t;
typedef double float64_t;
typedef int8_t poly8_t;
typedef int16_t poly16_t;
typedef uint64_t poly64_t;
typedef __uint128_t poly128_t;

typedef __attribute__((invalid_vector_type(8))) int8_t int8x8_t;
typedef __attribute__((invalid_vector_type(16))) int8_t int8x16_t;
typedef __attribute__((invalid_vector_type(4))) int16_t int16x4_t;
typedef __attribute__((invalid_vector_type(8))) int16_t int16x8_t;
typedef __attribute__((invalid_vector_type(2))) int32_t int32x2_t;
typedef __attribute__((invalid_vector_type(4))) int32_t int32x4_t;
typedef __attribute__((invalid_vector_type(1))) int64_t int64x1_t;
typedef __attribute__((invalid_vector_type(2))) int64_t int64x2_t;
typedef __attribute__((invalid_vector_type(8))) uint8_t uint8x8_t;
typedef __attribute__((invalid_vector_type(16))) uint8_t uint8x16_t;
typedef __attribute__((invalid_vector_type(4))) uint16_t uint16x4_t;
typedef __attribute__((invalid_vector_type(8))) uint16_t uint16x8_t;
typedef __attribute__((invalid_vector_type(2))) uint32_t uint32x2_t;
typedef __attribute__((invalid_vector_type(4))) uint32_t uint32x4_t;
typedef __attribute__((invalid_vector_type(1))) uint64_t uint64x1_t;
typedef __attribute__((invalid_vector_type(2))) uint64_t uint64x2_t;
typedef __attribute__((invalid_vector_type(4))) float16_t float16x4_t;
typedef __attribute__((invalid_vector_type(8))) float16_t float16x8_t;
typedef __attribute__((invalid_vector_type(2))) float32_t float32x2_t;
typedef __attribute__((invalid_vector_type(4))) float32_t float32x4_t;
typedef __attribute__((invalid_vector_type(1))) float64_t float64x1_t;
typedef __attribute__((invalid_vector_type(2))) float64_t float64x2_t;
typedef __attribute__((invalid_vector_type(8))) poly8_t poly8x8_t;
typedef __attribute__((invalid_vector_type(16))) poly8_t poly8x16_t;
typedef __attribute__((invalid_vector_type(4))) poly16_t poly16x4_t;
typedef __attribute__((invalid_vector_type(8))) poly16_t poly16x8_t;
typedef __attribute__((invalid_vector_type(1))) poly64_t poly64x1_t;
typedef __attribute__((invalid_vector_type(2))) poly64_t poly64x2_t;
# 36 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3



# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_vector_types.h" 1 3
# 62 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_vector_types.h" 3
typedef long long vector_s64
    __attribute__((ext_vector_type(32), ));
typedef int vector_s32
    __attribute__((ext_vector_type(64), ));
typedef short vector_s16
    __attribute__((ext_vector_type(128), ));
typedef char vector_s8
    __attribute__((ext_vector_type(256), ));
typedef vector_s8 vector_s4x2;

typedef unsigned long long vector_u64
    __attribute__((ext_vector_type(32), ));
typedef unsigned int vector_u32
    __attribute__((ext_vector_type(64), ));
typedef unsigned short vector_u16
    __attribute__((ext_vector_type(128), ));
typedef unsigned char vector_u8
    __attribute__((ext_vector_type(256), ));
typedef float vector_f32
    __attribute__((ext_vector_type(64), ));
typedef __cce_half vector_f16
    __attribute__((ext_vector_type(128), ));
# 99 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_vector_types.h" 3
typedef bfloat16_t vector_bf16 __attribute__((
    ext_vector_type(128), ));
# 119 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_vector_types.h" 3
typedef long long vector_bool
    __attribute__((ext_vector_type(4), ));


typedef char vector_align_data __attribute__((ext_vector_type(32)));





typedef struct vector_align {
  [aicore] __attribute__((cce_builtin_api, always_inline)) vector_align() {
    Data = __builtin_cce_init_vector_align_data();
  }

  [aicore] __attribute__((cce_builtin_api, always_inline))
  operator vector_align_data &() {
    return Data;
  }

  [aicore] __attribute__((cce_builtin_api, always_inline)) vector_align &
  operator=(vector_align_data alignData) {
    Data = alignData;
    return *this;
  }
  vector_align_data Data;
} vector_align;

typedef uint32_t vector_address __attribute__((ext_vector_type(1)));




typedef int wvector_s24
    __attribute__((ext_vector_type(256), ));
typedef long long wvector_s48
    __attribute__((ext_vector_type(128), ));
typedef signed __int128 wvector_s64
    __attribute__((ext_vector_type(64), ));






typedef struct vector_s64x2_t {
  vector_s64 val[2];
} vector_s64x2_t;

typedef struct vector_u64x2_t {
  vector_u64 val[2];
} vector_u64x2_t;

typedef struct vector_s32x2_t {
  vector_s32 val[2];
} vector_s32x2_t;

typedef struct vector_u32x2_t {
  vector_u32 val[2];
} vector_u32x2_t;

typedef struct vector_s16x2_t {
  vector_s16 val[2];
} vector_s16x2_t;

typedef struct vector_u16x2_t {
  vector_u16 val[2];
} vector_u16x2_t;

typedef struct vector_s8x2_t {
  vector_s8 val[2];
} vector_s8x2_t;

typedef struct vector_u8x2_t {
  vector_u8 val[2];
} vector_u8x2_t;

typedef struct vector_f32x2_t {
  vector_f32 val[2];
} vector_f32x2_t;

typedef struct vector_f16x2_t {
  vector_f16 val[2];
} vector_f16x2_t;

typedef struct vector_boolx2_t {
  vector_bool val[2];
} vector_boolx2_t;
# 40 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3
# 88 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 3
extern "C" {

inline __attribute__((alway_inline)) int __cce_getOrSetBlockNum(int value,
                                                                int type) {
  static thread_local int local = 0;
  if (type == 0)
    local = value;
  return local;
}

}

inline __attribute__((alway_inline)) unsigned int
__cce_rtConfigureCall(unsigned int numBlocks, void *smDesc = nullptr,
                      void *stream = nullptr) {
  __cce_getOrSetBlockNum(numBlocks, 0);
  return rtConfigureCall(numBlocks, smDesc, stream);
}
# 2 "<built-in>" 2
# 1 "/usr/include/stdio.h" 1 3 4
# 27 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/libc-header-start.h" 1 3 4
# 28 "/usr/include/stdio.h" 2 3 4

extern "C" {



# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 1 3 4
# 34 "/usr/include/stdio.h" 2 3 4


# 1 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdarg.h" 1 3 4
# 14 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdarg.h" 3 4
typedef __builtin_va_list va_list;
# 32 "/usr/local/Ascend/ascend-toolkit/8.2.RC1.alpha002/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdarg.h" 3 4
typedef __builtin_va_list __gnuc_va_list;
# 37 "/usr/include/stdio.h" 2 3 4


# 1 "/usr/include/aarch64-linux-gnu/bits/types/__fpos_t.h" 1 3 4




# 1 "/usr/include/aarch64-linux-gnu/bits/types/__mbstate_t.h" 1 3 4
# 13 "/usr/include/aarch64-linux-gnu/bits/types/__mbstate_t.h" 3 4
typedef struct
{
  int __count;
  union
  {
    int __wch;
    char __wchb[4];
  } __value;
} __mbstate_t;
# 6 "/usr/include/aarch64-linux-gnu/bits/types/__fpos_t.h" 2 3 4




typedef struct _G_fpos_t
{
  __off_t __pos;
  __mbstate_t __state;
} __fpos_t;
# 40 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/types/__fpos64_t.h" 1 3 4
# 10 "/usr/include/aarch64-linux-gnu/bits/types/__fpos64_t.h" 3 4
typedef struct _G_fpos64_t
{
  __off64_t __pos;
  __mbstate_t __state;
} __fpos64_t;
# 41 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/types/__FILE.h" 1 3 4



struct _IO_FILE;
typedef struct _IO_FILE __FILE;
# 42 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/types/FILE.h" 1 3 4



struct _IO_FILE;


typedef struct _IO_FILE FILE;
# 43 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/types/struct_FILE.h" 1 3 4
# 35 "/usr/include/aarch64-linux-gnu/bits/types/struct_FILE.h" 3 4
struct _IO_FILE;
struct _IO_marker;
struct _IO_codecvt;
struct _IO_wide_data;




typedef void _IO_lock_t;





struct _IO_FILE
{
  int _flags;


  char *_IO_read_ptr;
  char *_IO_read_end;
  char *_IO_read_base;
  char *_IO_write_base;
  char *_IO_write_ptr;
  char *_IO_write_end;
  char *_IO_buf_base;
  char *_IO_buf_end;


  char *_IO_save_base;
  char *_IO_backup_base;
  char *_IO_save_end;

  struct _IO_marker *_markers;

  struct _IO_FILE *_chain;

  int _fileno;
  int _flags2;
  __off_t _old_offset;


  unsigned short _cur_column;
  signed char _vtable_offset;
  char _shortbuf[1];

  _IO_lock_t *_lock;







  __off64_t _offset;

  struct _IO_codecvt *_codecvt;
  struct _IO_wide_data *_wide_data;
  struct _IO_FILE *_freeres_list;
  void *_freeres_buf;
  size_t __pad5;
  int _mode;

  char _unused2[15 * sizeof (int) - 4 * sizeof (void *) - sizeof (size_t)];
};
# 44 "/usr/include/stdio.h" 2 3 4


# 1 "/usr/include/aarch64-linux-gnu/bits/types/cookie_io_functions_t.h" 1 3 4
# 27 "/usr/include/aarch64-linux-gnu/bits/types/cookie_io_functions_t.h" 3 4
typedef __ssize_t cookie_read_function_t (void *__cookie, char *__buf,
                                          size_t __nbytes);







typedef __ssize_t cookie_write_function_t (void *__cookie, const char *__buf,
                                           size_t __nbytes);







typedef int cookie_seek_function_t (void *__cookie, __off64_t *__pos, int __w);


typedef int cookie_close_function_t (void *__cookie);






typedef struct _IO_cookie_io_functions_t
{
  cookie_read_function_t *read;
  cookie_write_function_t *write;
  cookie_seek_function_t *seek;
  cookie_close_function_t *close;
} cookie_io_functions_t;
# 47 "/usr/include/stdio.h" 2 3 4





typedef __gnuc_va_list va_list;
# 63 "/usr/include/stdio.h" 3 4
typedef __off_t off_t;






typedef __off64_t off64_t;






typedef __ssize_t ssize_t;






typedef __fpos_t fpos_t;




typedef __fpos64_t fpos64_t;
# 133 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/stdio_lim.h" 1 3 4
# 134 "/usr/include/stdio.h" 2 3 4
# 143 "/usr/include/stdio.h" 3 4
extern FILE *stdin;
extern FILE *stdout;
extern FILE *stderr;






extern int remove (const char *__filename) noexcept (true);

extern int rename (const char *__old, const char *__new) noexcept (true);



extern int renameat (int __oldfd, const char *__old, int __newfd,
       const char *__new) noexcept (true);
# 170 "/usr/include/stdio.h" 3 4
extern int renameat2 (int __oldfd, const char *__old, int __newfd,
        const char *__new, unsigned int __flags) noexcept (true);






extern int fclose (FILE *__stream);
# 188 "/usr/include/stdio.h" 3 4
extern FILE *tmpfile (void)
  __attribute__ ((__malloc__)) ;
# 200 "/usr/include/stdio.h" 3 4
extern FILE *tmpfile64 (void)
   __attribute__ ((__malloc__)) ;



extern char *tmpnam (char[20]) noexcept (true) ;




extern char *tmpnam_r (char __s[20]) noexcept (true) ;
# 222 "/usr/include/stdio.h" 3 4
extern char *tempnam (const char *__dir, const char *__pfx)
   noexcept (true) __attribute__ ((__malloc__)) ;






extern int fflush (FILE *__stream);
# 239 "/usr/include/stdio.h" 3 4
extern int fflush_unlocked (FILE *__stream);
# 249 "/usr/include/stdio.h" 3 4
extern int fcloseall (void);
# 258 "/usr/include/stdio.h" 3 4
extern FILE *fopen (const char *__restrict __filename,
      const char *__restrict __modes)
  __attribute__ ((__malloc__)) ;




extern FILE *freopen (const char *__restrict __filename,
        const char *__restrict __modes,
        FILE *__restrict __stream) ;
# 283 "/usr/include/stdio.h" 3 4
extern FILE *fopen64 (const char *__restrict __filename,
        const char *__restrict __modes)
  __attribute__ ((__malloc__)) ;
extern FILE *freopen64 (const char *__restrict __filename,
   const char *__restrict __modes,
   FILE *__restrict __stream) ;




extern FILE *fdopen (int __fd, const char *__modes) noexcept (true)
  __attribute__ ((__malloc__)) ;





extern FILE *fopencookie (void *__restrict __magic_cookie,
     const char *__restrict __modes,
     cookie_io_functions_t __io_funcs) noexcept (true)
  __attribute__ ((__malloc__)) ;




extern FILE *fmemopen (void *__s, size_t __len, const char *__modes)
  noexcept (true) __attribute__ ((__malloc__)) ;




extern FILE *open_memstream (char **__bufloc, size_t *__sizeloc) noexcept (true)
  __attribute__ ((__malloc__)) ;
# 328 "/usr/include/stdio.h" 3 4
extern void setbuf (FILE *__restrict __stream, char *__restrict __buf) noexcept (true);



extern int setvbuf (FILE *__restrict __stream, char *__restrict __buf,
      int __modes, size_t __n) noexcept (true);




extern void setbuffer (FILE *__restrict __stream, char *__restrict __buf,
         size_t __size) noexcept (true);


extern void setlinebuf (FILE *__stream) noexcept (true);







extern int fprintf (FILE *__restrict __stream,
      const char *__restrict __format, ...);




extern int printf (const char *__restrict __format, ...);

extern int sprintf (char *__restrict __s,
      const char *__restrict __format, ...) noexcept (true);





extern int vfprintf (FILE *__restrict __s, const char *__restrict __format,
       __gnuc_va_list __arg);




extern int vprintf (const char *__restrict __format, __gnuc_va_list __arg);

extern int vsprintf (char *__restrict __s, const char *__restrict __format,
       __gnuc_va_list __arg) noexcept (true);



extern int snprintf (char *__restrict __s, size_t __maxlen,
       const char *__restrict __format, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 3, 4)));

extern int vsnprintf (char *__restrict __s, size_t __maxlen,
        const char *__restrict __format, __gnuc_va_list __arg)
     noexcept (true) __attribute__ ((__format__ (__printf__, 3, 0)));





extern int vasprintf (char **__restrict __ptr, const char *__restrict __f,
        __gnuc_va_list __arg)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 0))) ;
extern int __asprintf (char **__restrict __ptr,
         const char *__restrict __fmt, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 3))) ;
extern int asprintf (char **__restrict __ptr,
       const char *__restrict __fmt, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 3))) ;




extern int vdprintf (int __fd, const char *__restrict __fmt,
       __gnuc_va_list __arg)
     __attribute__ ((__format__ (__printf__, 2, 0)));
extern int dprintf (int __fd, const char *__restrict __fmt, ...)
     __attribute__ ((__format__ (__printf__, 2, 3)));







extern int fscanf (FILE *__restrict __stream,
     const char *__restrict __format, ...) ;




extern int scanf (const char *__restrict __format, ...) ;

extern int sscanf (const char *__restrict __s,
     const char *__restrict __format, ...) noexcept (true);





# 1 "/usr/include/aarch64-linux-gnu/bits/floatn.h" 1 3 4
# 23 "/usr/include/aarch64-linux-gnu/bits/floatn.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/long-double.h" 1 3 4
# 24 "/usr/include/aarch64-linux-gnu/bits/floatn.h" 2 3 4
# 80 "/usr/include/aarch64-linux-gnu/bits/floatn.h" 3 4
typedef long double _Float128;
# 95 "/usr/include/aarch64-linux-gnu/bits/floatn.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/floatn-common.h" 1 3 4
# 24 "/usr/include/aarch64-linux-gnu/bits/floatn-common.h" 3 4
# 1 "/usr/include/aarch64-linux-gnu/bits/long-double.h" 1 3 4
# 25 "/usr/include/aarch64-linux-gnu/bits/floatn-common.h" 2 3 4
# 214 "/usr/include/aarch64-linux-gnu/bits/floatn-common.h" 3 4
typedef float _Float32;
# 251 "/usr/include/aarch64-linux-gnu/bits/floatn-common.h" 3 4
typedef double _Float64;
# 268 "/usr/include/aarch64-linux-gnu/bits/floatn-common.h" 3 4
typedef double _Float32x;
# 285 "/usr/include/aarch64-linux-gnu/bits/floatn-common.h" 3 4
typedef long double _Float64x;
# 96 "/usr/include/aarch64-linux-gnu/bits/floatn.h" 2 3 4
# 431 "/usr/include/stdio.h" 2 3 4



extern int fscanf (FILE *__restrict __stream, const char *__restrict __format, ...) __asm__ ("" "__isoc99_fscanf") ;


extern int scanf (const char *__restrict __format, ...) __asm__ ("" "__isoc99_scanf") ;

extern int sscanf (const char *__restrict __s, const char *__restrict __format, ...) noexcept (true) __asm__ ("" "__isoc99_sscanf");
# 459 "/usr/include/stdio.h" 3 4
extern int vfscanf (FILE *__restrict __s, const char *__restrict __format,
      __gnuc_va_list __arg)
     __attribute__ ((__format__ (__scanf__, 2, 0))) ;





extern int vscanf (const char *__restrict __format, __gnuc_va_list __arg)
     __attribute__ ((__format__ (__scanf__, 1, 0))) ;


extern int vsscanf (const char *__restrict __s,
      const char *__restrict __format, __gnuc_va_list __arg)
     noexcept (true) __attribute__ ((__format__ (__scanf__, 2, 0)));





extern int vfscanf (FILE *__restrict __s, const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc99_vfscanf")



     __attribute__ ((__format__ (__scanf__, 2, 0))) ;
extern int vscanf (const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc99_vscanf")

     __attribute__ ((__format__ (__scanf__, 1, 0))) ;
extern int vsscanf (const char *__restrict __s, const char *__restrict __format, __gnuc_va_list __arg) noexcept (true) __asm__ ("" "__isoc99_vsscanf")



     __attribute__ ((__format__ (__scanf__, 2, 0)));
# 513 "/usr/include/stdio.h" 3 4
extern int fgetc (FILE *__stream);
extern int getc (FILE *__stream);





extern int getchar (void);






extern int getc_unlocked (FILE *__stream);
extern int getchar_unlocked (void);
# 538 "/usr/include/stdio.h" 3 4
extern int fgetc_unlocked (FILE *__stream);
# 549 "/usr/include/stdio.h" 3 4
extern int fputc (int __c, FILE *__stream);
extern int putc (int __c, FILE *__stream);





extern int putchar (int __c);
# 565 "/usr/include/stdio.h" 3 4
extern int fputc_unlocked (int __c, FILE *__stream);







extern int putc_unlocked (int __c, FILE *__stream);
extern int putchar_unlocked (int __c);






extern int getw (FILE *__stream);


extern int putw (int __w, FILE *__stream);







extern char *fgets (char *__restrict __s, int __n, FILE *__restrict __stream)
                                                         ;
# 615 "/usr/include/stdio.h" 3 4
extern char *fgets_unlocked (char *__restrict __s, int __n,
        FILE *__restrict __stream)
                                                  ;
# 632 "/usr/include/stdio.h" 3 4
extern __ssize_t __getdelim (char **__restrict __lineptr,
                             size_t *__restrict __n, int __delimiter,
                             FILE *__restrict __stream) ;
extern __ssize_t getdelim (char **__restrict __lineptr,
                           size_t *__restrict __n, int __delimiter,
                           FILE *__restrict __stream) ;







extern __ssize_t getline (char **__restrict __lineptr,
                          size_t *__restrict __n,
                          FILE *__restrict __stream) ;







extern int fputs (const char *__restrict __s, FILE *__restrict __stream);





extern int puts (const char *__s);






extern int ungetc (int __c, FILE *__stream);






extern size_t fread (void *__restrict __ptr, size_t __size,
       size_t __n, FILE *__restrict __stream) ;




extern size_t fwrite (const void *__restrict __ptr, size_t __size,
        size_t __n, FILE *__restrict __s);
# 691 "/usr/include/stdio.h" 3 4
extern int fputs_unlocked (const char *__restrict __s,
      FILE *__restrict __stream);
# 702 "/usr/include/stdio.h" 3 4
extern size_t fread_unlocked (void *__restrict __ptr, size_t __size,
         size_t __n, FILE *__restrict __stream) ;
extern size_t fwrite_unlocked (const void *__restrict __ptr, size_t __size,
          size_t __n, FILE *__restrict __stream);







extern int fseek (FILE *__stream, long int __off, int __whence);




extern long int ftell (FILE *__stream) ;




extern void rewind (FILE *__stream);
# 736 "/usr/include/stdio.h" 3 4
extern int fseeko (FILE *__stream, __off_t __off, int __whence);




extern __off_t ftello (FILE *__stream) ;
# 760 "/usr/include/stdio.h" 3 4
extern int fgetpos (FILE *__restrict __stream, fpos_t *__restrict __pos);




extern int fsetpos (FILE *__stream, const fpos_t *__pos);
# 779 "/usr/include/stdio.h" 3 4
extern int fseeko64 (FILE *__stream, __off64_t __off, int __whence);
extern __off64_t ftello64 (FILE *__stream) ;
extern int fgetpos64 (FILE *__restrict __stream, fpos64_t *__restrict __pos);
extern int fsetpos64 (FILE *__stream, const fpos64_t *__pos);



extern void clearerr (FILE *__stream) noexcept (true);

extern int feof (FILE *__stream) noexcept (true) ;

extern int ferror (FILE *__stream) noexcept (true) ;



extern void clearerr_unlocked (FILE *__stream) noexcept (true);
extern int feof_unlocked (FILE *__stream) noexcept (true) ;
extern int ferror_unlocked (FILE *__stream) noexcept (true) ;







extern void perror (const char *__s);




extern int fileno (FILE *__stream) noexcept (true) ;




extern int fileno_unlocked (FILE *__stream) noexcept (true) ;
# 823 "/usr/include/stdio.h" 3 4
extern int pclose (FILE *__stream);





extern FILE *popen (const char *__command, const char *__modes)
  __attribute__ ((__malloc__)) ;






extern char *ctermid (char *__s) noexcept (true)
                                     ;





extern char *cuserid (char *__s)
                                     ;




struct obstack;


extern int obstack_printf (struct obstack *__restrict __obstack,
      const char *__restrict __format, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 3)));
extern int obstack_vprintf (struct obstack *__restrict __obstack,
       const char *__restrict __format,
       __gnuc_va_list __args)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 0)));







extern void flockfile (FILE *__stream) noexcept (true);



extern int ftrylockfile (FILE *__stream) noexcept (true) ;


extern void funlockfile (FILE *__stream) noexcept (true);
# 885 "/usr/include/stdio.h" 3 4
extern int __uflow (FILE *);
extern int __overflow (FILE *, int);




# 1 "/usr/include/aarch64-linux-gnu/bits/stdio.h" 1 3 4
# 38 "/usr/include/aarch64-linux-gnu/bits/stdio.h" 3 4
extern __inline __attribute__ ((__gnu_inline__)) int
vprintf (const char *__restrict __fmt, __gnuc_va_list __arg)
{
  return vfprintf (stdout, __fmt, __arg);
}



extern __inline __attribute__ ((__gnu_inline__)) int
getchar (void)
{
  return getc (stdin);
}




extern __inline __attribute__ ((__gnu_inline__)) int
fgetc_unlocked (FILE *__fp)
{
  return (__builtin_expect (((__fp)->_IO_read_ptr >= (__fp)->_IO_read_end), 0) ? __uflow (__fp) : *(unsigned char *) (__fp)->_IO_read_ptr++);
}





extern __inline __attribute__ ((__gnu_inline__)) int
getc_unlocked (FILE *__fp)
{
  return (__builtin_expect (((__fp)->_IO_read_ptr >= (__fp)->_IO_read_end), 0) ? __uflow (__fp) : *(unsigned char *) (__fp)->_IO_read_ptr++);
}


extern __inline __attribute__ ((__gnu_inline__)) int
getchar_unlocked (void)
{
  return (__builtin_expect (((stdin)->_IO_read_ptr >= (stdin)->_IO_read_end), 0) ? __uflow (stdin) : *(unsigned char *) (stdin)->_IO_read_ptr++);
}




extern __inline __attribute__ ((__gnu_inline__)) int
putchar (int __c)
{
  return putc (__c, stdout);
}




extern __inline __attribute__ ((__gnu_inline__)) int
fputc_unlocked (int __c, FILE *__stream)
{
  return (__builtin_expect (((__stream)->_IO_write_ptr >= (__stream)->_IO_write_end), 0) ? __overflow (__stream, (unsigned char) (__c)) : (unsigned char) (*(__stream)->_IO_write_ptr++ = (__c)));
}





extern __inline __attribute__ ((__gnu_inline__)) int
putc_unlocked (int __c, FILE *__stream)
{
  return (__builtin_expect (((__stream)->_IO_write_ptr >= (__stream)->_IO_write_end), 0) ? __overflow (__stream, (unsigned char) (__c)) : (unsigned char) (*(__stream)->_IO_write_ptr++ = (__c)));
}


extern __inline __attribute__ ((__gnu_inline__)) int
putchar_unlocked (int __c)
{
  return (__builtin_expect (((stdout)->_IO_write_ptr >= (stdout)->_IO_write_end), 0) ? __overflow (stdout, (unsigned char) (__c)) : (unsigned char) (*(stdout)->_IO_write_ptr++ = (__c)));
}





extern __inline __attribute__ ((__gnu_inline__)) __ssize_t
getline (char **__lineptr, size_t *__n, FILE *__stream)
{
  return __getdelim (__lineptr, __n, '\n', __stream);
}





extern __inline __attribute__ ((__gnu_inline__)) int
 feof_unlocked (FILE *__stream) noexcept (true)
{
  return (((__stream)->_flags & 0x0010) != 0);
}


extern __inline __attribute__ ((__gnu_inline__)) int
 ferror_unlocked (FILE *__stream) noexcept (true)
{
  return (((__stream)->_flags & 0x0020) != 0);
}
# 892 "/usr/include/stdio.h" 2 3 4
# 902 "/usr/include/stdio.h" 3 4
}
# 3 "<built-in>" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/../include/version/cann_version.h" 1
# 4 "<built-in>" 2
# 1 "/root/yjw/HANS/HANS-NPU/hans_merge.cpp" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tpipe.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tpipe.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_macros.h" 1
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_macros.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_macros.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_macros.h"
# 1 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/cstdint" 1 3
# 33 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/cstdint" 3





# 1 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11/bits/c++config.h" 1 3
# 278 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11/bits/c++config.h" 3
namespace std
{
  typedef long unsigned int size_t;
  typedef long int ptrdiff_t;


  typedef decltype(nullptr) nullptr_t;

}
# 300 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11/bits/c++config.h" 3
namespace std
{
  inline namespace __cxx11 __attribute__((__abi_tag__ ("cxx11"))) { }
}
namespace __gnu_cxx
{
  inline namespace __cxx11 __attribute__((__abi_tag__ ("cxx11"))) { }
}
# 586 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11/bits/c++config.h" 3
# 1 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11/bits/os_defines.h" 1 3
# 587 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11/bits/c++config.h" 2 3


# 1 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11/bits/cpu_defines.h" 1 3
# 590 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11/bits/c++config.h" 2 3
# 777 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11/bits/c++config.h" 3
# 1 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/pstl/pstl_config.h" 1 3
# 778 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11/bits/c++config.h" 2 3
# 39 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/cstdint" 2 3





namespace std
{

  using ::int8_t;
  using ::int16_t;
  using ::int32_t;
  using ::int64_t;

  using ::int_fast8_t;
  using ::int_fast16_t;
  using ::int_fast32_t;
  using ::int_fast64_t;

  using ::int_least8_t;
  using ::int_least16_t;
  using ::int_least32_t;
  using ::int_least64_t;

  using ::intmax_t;
  using ::intptr_t;

  using ::uint8_t;
  using ::uint16_t;
  using ::uint32_t;
  using ::uint64_t;

  using ::uint_fast8_t;
  using ::uint_fast16_t;
  using ::uint_fast32_t;
  using ::uint_fast64_t;

  using ::uint_least8_t;
  using ::uint_least16_t;
  using ::uint_least32_t;
  using ::uint_least64_t;

  using ::uintmax_t;
  using ::uintptr_t;





}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_macros.h" 2
# 127 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_macros.h"
namespace AscendC {

constexpr int32_t QUE_MAX_EVENT = 8;



constexpr int32_t HF32_MODE_BIT = 46;
constexpr int32_t HF32_TRANS_MODE_BIT = 47;
constexpr int32_t MM_LAYOUT_MODE_BIT = 51;
constexpr int32_t LEAKY_RELU_MODE_BIT = 50;
constexpr int32_t CAST_MODE_BIT = 59;
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_macros.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_log.h" 1
# 258 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_log.h"
namespace AscendC {
template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void AssertImpl(__attribute__((cce_global)) const char* fmt, Args&&... args);
# 345 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_log.h"
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_macros.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h" 1
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
namespace AscendC {
enum class TPosition : uint8_t {
    GM,
    A1,
    A2,
    B1,
    B2,
    C1,
    C2,
    CO1,
    CO2,
    VECIN,
    VECOUT,
    VECCALC,
    LCM = VECCALC,
    SPM,
    SHM = SPM,
    TSCM,
    C2PIPE2GM,
    C2PIPE2LOCAL,
    MAX,
};

using QuePosition = TPosition;
enum class Hardware : uint8_t { GM, UB, L1, L0A, L0B, L0C, BIAS, FIXBUF, MAX };

enum class HardEvent : uint8_t {

    MTE2_MTE1,
    MTE1_MTE2,
    MTE1_M,
    M_MTE1,
    MTE2_V,
    V_MTE2,
    MTE3_V,
    V_MTE3,
    M_V,
    V_M,
    V_V,
    MTE3_MTE1,
    MTE1_MTE3,
    MTE1_V,
    MTE2_M,
    M_MTE2,
    V_MTE1,
    M_FIX,
    FIX_M,
    MTE3_MTE2,
    MTE2_MTE3,
    S_V,
    V_S,
    S_MTE2,
    MTE2_S,
    S_MTE3,
    MTE3_S,
    MTE2_FIX,
    FIX_MTE2,
    FIX_S,
    M_S,
    FIX_MTE3,
    MTE1_FIX,
    FIX_MTE1,
    FIX_FIX,
    MAX,
};

enum class HardEventAic : uint8_t {

    MTE2_MTE1,
    MTE1_MTE2,
    MTE1_M,
    M_MTE1,
    MTE3_MTE1,
    MTE1_MTE3,
    MTE2_M,
    M_MTE2,
    M_FIX,
    FIX_M,
    MTE3_MTE2,
    MTE2_MTE3,
    S_MTE2,
    MTE2_S,
    S_MTE3,
    MTE3_S,
    MTE2_FIX,
    FIX_MTE2,
    FIX_S,
    M_S,
    FIX_MTE3,
    MTE1_FIX,
    FIX_MTE1,
    FIX_FIX,
    MAX,
};

enum class HardEventAiv : uint8_t {

    MTE2_V,
    V_MTE2,
    MTE3_V,
    V_MTE3,
    V_V,
    MTE3_MTE2,
    MTE2_MTE3,
    S_V,
    V_S,
    S_MTE2,
    MTE2_S,
    S_MTE3,
    MTE3_S,
    MAX,
};

enum class MemoryT : uint8_t { L1 = 0, L0A, L0B, L0C, UB, BIAS };

enum class MemDsbT : uint8_t { ALL = 0, DDR, UB, SEQ };







constexpr uint8_t EVENT_NUM = static_cast<uint8_t>(HardEventAiv::MAX);



[aicore] constexpr uint8_t EventToIndexAic(HardEvent evt)
{


    if (evt == HardEvent::MTE2_MTE1) {
        return static_cast<uint8_t>(HardEventAic::MTE2_MTE1);
    } else if (evt == HardEvent::MTE1_MTE2) {
        return static_cast<uint8_t>(HardEventAic::MTE1_MTE2);
    } else if (evt == HardEvent::MTE1_M) {
        return static_cast<uint8_t>(HardEventAic::MTE1_M);
    } else if (evt == HardEvent::M_MTE1) {
        return static_cast<uint8_t>(HardEventAic::M_MTE1);
    } else if (evt == HardEvent::MTE3_MTE1) {
        return static_cast<uint8_t>(HardEventAic::MTE3_MTE1);
    } else if (evt == HardEvent::MTE1_MTE3) {
        return static_cast<uint8_t>(HardEventAic::MTE1_MTE3);
    } else if (evt == HardEvent::MTE2_M) {
        return static_cast<uint8_t>(HardEventAic::MTE2_M);
    } else if (evt == HardEvent::M_MTE2) {
        return static_cast<uint8_t>(HardEventAic::M_MTE2);
    } else if (evt == HardEvent::M_FIX) {
        return static_cast<uint8_t>(HardEventAic::M_FIX);
    } else if (evt == HardEvent::FIX_M) {
        return static_cast<uint8_t>(HardEventAic::FIX_M);
    } else if (evt == HardEvent::MTE3_MTE2) {
        return static_cast<uint8_t>(HardEventAic::MTE3_MTE2);
    } else if (evt == HardEvent::MTE2_MTE3) {
        return static_cast<uint8_t>(HardEventAic::MTE2_MTE3);
    } else if (evt == HardEvent::S_MTE2) {
        return static_cast<uint8_t>(HardEventAic::S_MTE2);
    } else if (evt == HardEvent::MTE2_S) {
        return static_cast<uint8_t>(HardEventAic::MTE2_S);
    } else if (evt == HardEvent::S_MTE3) {
        return static_cast<uint8_t>(HardEventAic::S_MTE3);
    } else if (evt == HardEvent::MTE3_S) {
        return static_cast<uint8_t>(HardEventAic::MTE3_S);
    } else if (evt == HardEvent::MTE2_FIX) {
        return static_cast<uint8_t>(HardEventAic::MTE2_FIX);
    } else if (evt == HardEvent::FIX_MTE2) {
        return static_cast<uint8_t>(HardEventAic::FIX_MTE2);
    } else if (evt == HardEvent::FIX_S) {
        return static_cast<uint8_t>(HardEventAic::FIX_S);
    } else if (evt == HardEvent::M_S) {
        return static_cast<uint8_t>(HardEventAic::M_S);
    } else if (evt == HardEvent::FIX_MTE3) {
        return static_cast<uint8_t>(HardEventAic::FIX_MTE3);
    } else if (evt == HardEvent::FIX_MTE1) {
        return static_cast<uint8_t>(HardEventAic::FIX_MTE1);
    } else if (evt == HardEvent::MTE1_FIX) {
        return static_cast<uint8_t>(HardEventAic::MTE1_FIX);
    } else if (evt == HardEvent::FIX_FIX) {
        return static_cast<uint8_t>(HardEventAic::FIX_FIX);
    } else {
        return static_cast<uint8_t>(HardEventAic::MAX);
    }
}

[aicore] constexpr uint8_t EventToIndexAiv(HardEvent evt)
{


    if (evt == HardEvent::MTE2_V) {
        return static_cast<uint8_t>(HardEventAiv::MTE2_V);
    } else if (evt == HardEvent::V_MTE2) {
        return static_cast<uint8_t>(HardEventAiv::V_MTE2);
    } else if (evt == HardEvent::MTE3_V) {
        return static_cast<uint8_t>(HardEventAiv::MTE3_V);
    } else if (evt == HardEvent::V_MTE3) {
        return static_cast<uint8_t>(HardEventAiv::V_MTE3);
    } else if (evt == HardEvent::V_V) {
        return static_cast<uint8_t>(HardEventAiv::V_V);
    } else if (evt == HardEvent::MTE3_MTE2) {
        return static_cast<uint8_t>(HardEventAiv::MTE3_MTE2);
    } else if (evt == HardEvent::MTE2_MTE3) {
        return static_cast<uint8_t>(HardEventAiv::MTE2_MTE3);
    } else if (evt == HardEvent::S_V) {
        return static_cast<uint8_t>(HardEventAiv::S_V);
    } else if (evt == HardEvent::V_S) {
        return static_cast<uint8_t>(HardEventAiv::V_S);
    } else if (evt == HardEvent::S_MTE2) {
        return static_cast<uint8_t>(HardEventAiv::S_MTE2);
    } else if (evt == HardEvent::MTE2_S) {
        return static_cast<uint8_t>(HardEventAiv::MTE2_S);
    } else if (evt == HardEvent::S_MTE3) {
        return static_cast<uint8_t>(HardEventAiv::S_MTE3);
    } else if (evt == HardEvent::MTE3_S) {
        return static_cast<uint8_t>(HardEventAiv::MTE3_S);
    } else {
        return static_cast<uint8_t>(HardEventAiv::MAX);
    }
}

[aicore] constexpr uint8_t EventToIndex(HardEvent evt)
{





    return EventToIndexAiv(evt);

}





constexpr int32_t PIPE_NUM = 7;
constexpr pipe_t SUPPORTED_PIPE[PIPE_NUM] = { PIPE_S, PIPE_V, PIPE_M, PIPE_MTE1, PIPE_MTE2, PIPE_MTE3, PIPE_FIX };


[aicore] constexpr bool IsSupportedPipe(pipe_t pipe)
{
    for (int i = 0; i < PIPE_NUM; i++) {
        if (pipe == SUPPORTED_PIPE[i]) {
            return true;
        }
    }
    return false;
}

[aicore] constexpr Hardware GetPhyType(TPosition pos)
{
                                 ;
    Hardware hard = Hardware::UB;
    if (pos == TPosition::GM) {
        hard = Hardware::GM;
    } else if (pos == TPosition::A1) {
        hard = Hardware::L1;
    } else if (pos == TPosition::A2) {
        hard = Hardware::L0A;
    } else if (pos == TPosition::B1) {
        hard = Hardware::L1;
    } else if (pos == TPosition::B2) {
        hard = Hardware::L0B;
# 302 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
    } else if (pos == TPosition::C1) {
        hard = Hardware::L1;
    } else if (pos == TPosition::C2) {
        hard = Hardware::BIAS;
    } else if (pos == TPosition::CO2) {
        hard = Hardware::GM;
    } else if (pos == TPosition::C2PIPE2GM) {
        hard = Hardware::FIXBUF;
# 323 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
    } else if (pos == TPosition::CO1) {
        hard = Hardware::L0C;
    } else if (pos == TPosition::SHM) {
        hard = Hardware::L1;
    } else if (pos == TPosition::TSCM) {
        hard = Hardware::L1;
    }
    return hard;
}

[aicore] constexpr TPosition GetPosition(TPosition srcPos, TPosition dstPos)
{

                                                                       ;
                                                                          ;





    if ((dstPos == TPosition::GM) || (dstPos == TPosition::CO2)) {
        return srcPos;
    }

    return dstPos;
}

[aicore] constexpr Hardware GetBufferPos(TPosition srcPos, TPosition dstPos)
{

                                                                       ;
                                                                          ;





    if ((dstPos == TPosition::GM) || (dstPos == TPosition::CO2)) {
        return GetPhyType(srcPos);
    }

    return GetPhyType(dstPos);
}

[aicore] constexpr TPosition GetBufferLogicPos(TPosition pos, bool isSrc)
{
                                ;
                                     ;
                                 ;
    if (pos == TPosition::A1) {
        return isSrc ? TPosition::GM : TPosition::A1;
    } else if (pos == TPosition::B1) {
        return isSrc ? TPosition::GM : TPosition::B1;
    } else if (pos == TPosition::C1) {
        return isSrc ? TPosition::GM : TPosition::C1;
    } else if (pos == TPosition::A2) {
        return isSrc ? TPosition::A1 : TPosition::A2;
    } else if (pos == TPosition::B2) {
        return isSrc ? TPosition::B1 : TPosition::B2;
    } else if (pos == TPosition::C2) {
        return isSrc ? TPosition::C1 : TPosition::C2;
    } else if (pos == TPosition::CO1) {
        return isSrc ? TPosition::CO1 : TPosition::CO2;
    } else if (pos == TPosition::CO2) {
        return isSrc ? TPosition::CO2 : TPosition::GM;
    } else if (pos == TPosition::VECIN) {
        return isSrc ? TPosition::GM : TPosition::VECIN;
    } else if (pos == TPosition::VECOUT) {
        return isSrc ? TPosition::VECOUT : TPosition::GM;
    } else if (pos == TPosition::SPM) {
        return isSrc ? TPosition::VECOUT : TPosition::GM;
    } else if (pos == TPosition::C2PIPE2GM) {
        return isSrc ? TPosition::B1 : TPosition::C2PIPE2GM;
    }
    return TPosition::MAX;
}

[aicore] constexpr HardEvent GetQueEvt(Hardware src, Hardware dst, bool fwdDirect, bool nd2nz = false,
                                         bool nz2nd = false)
{
    (void)(nz2nd);

                                                            ;
                                ;
                                ;
    if (src == Hardware::GM) {
                                   ;
                                    ;
                                     ;
                                       ;
        if (dst == Hardware::UB) {
            return fwdDirect ? HardEvent::MTE2_V : HardEvent::V_MTE2;
        } else if (dst == Hardware::L1) {






            (void)(nd2nz);

            return fwdDirect ? HardEvent::MTE2_MTE1 : HardEvent::MTE1_MTE2;
        } else if (dst == Hardware::L0A) {
            return fwdDirect ? HardEvent::MTE2_M : HardEvent::M_MTE2;
        } else if (dst == Hardware::L0B) {
            return fwdDirect ? HardEvent::MTE2_M : HardEvent::M_MTE2;
        }
    } else if (src == Hardware::UB) {
                                    ;
                                    ;
                                     ;
                                       ;
        if (dst == Hardware::GM) {
            return fwdDirect ? HardEvent::V_MTE3 : HardEvent::MTE3_V;
        } else if (dst == Hardware::L1) {
            return fwdDirect ? HardEvent::MTE3_MTE1 : HardEvent::MTE1_MTE3;
        } else if (dst == Hardware::L0C) {
            return fwdDirect ? HardEvent::V_V : HardEvent::MAX;
        } else if (dst == Hardware::UB) {
            return fwdDirect ? HardEvent::MTE2_MTE3 : HardEvent::MTE3_MTE2;
        }
    } else if (src == Hardware::L1) {
                                   ;
                                   ;
                                    ;




        if (dst == Hardware::UB) {
            return fwdDirect ? HardEvent::MTE1_V : HardEvent::V_MTE1;
        } else if (dst == Hardware::L0A) {
            return fwdDirect ? HardEvent::MTE1_M : HardEvent::M_MTE1;
        } else if (dst == Hardware::L0B) {
            return fwdDirect ? HardEvent::MTE1_M : HardEvent::M_MTE1;
        } else if (dst == Hardware::FIXBUF) {
            return fwdDirect ? HardEvent::MTE1_FIX : HardEvent::FIX_MTE1;
        } else if (dst == Hardware::BIAS) {
            return fwdDirect ? HardEvent::MTE1_M : HardEvent::M_MTE1;
        }
    } else if (src == Hardware::L0A) {
                                    ;
        return fwdDirect ? HardEvent::M_V : HardEvent::V_M;
    } else if (src == Hardware::L0B) {
                                    ;
        return fwdDirect ? HardEvent::M_V : HardEvent::V_M;






    } else if (src == Hardware::L0C) {
                                   ;
        return fwdDirect ? HardEvent::M_FIX : HardEvent::FIX_M;
    }
# 492 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
    return HardEvent::MAX;
}


template <MemDsbT arg>
[aicore] __inline__ __attribute__((always_inline)) void DataSyncBarrierImpl()
{
    dsb((mem_dsb_t)arg);
}

template <HardEvent event, MemoryT memT, bool isVirtual>
[aicore] __inline__ __attribute__((always_inline)) void HSetFlagImpl(int32_t eventID)
{

                                                                                                                     ;
    static_assert(((int32_t)memT >= 0 && memT <= MemoryT::BIAS && memT != MemoryT::UB && memT != MemoryT::L1),
        "For HSetFlag, memT only support L0A, L0B, L0C, BIAS.");

    event_t e = static_cast<event_t>(eventID);

    switch (event) {
        case HardEvent::MTE1_M:

                                                                         ;
            hset_flag(PIPE_MTE1, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_MTE1:

                                                                         ;
            hset_flag(PIPE_M, PIPE_MTE1, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_FIX:
                                                                                     ;
            hset_flag(PIPE_M, PIPE_FIX, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::FIX_M:
                                                                                     ;
            hset_flag(PIPE_FIX, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        default:
                                                                                                           ;
            break;
    }
}

template <HardEvent event, MemoryT memT, bool isVirtual>
[aicore] __inline__ __attribute__((always_inline)) void HWaitFlagImpl(int32_t eventID)
{

                                                                                                                                 ;
    static_assert(((int32_t)memT >= 0 && memT <= MemoryT::BIAS && memT != MemoryT::UB && memT != MemoryT::L1),
                  "For HWaitFlag, memT only support L0A, L0B, L0C, BIAS.");

    event_t e = static_cast<event_t>(eventID);

    switch (event) {
        case HardEvent::MTE1_M:

                                                                         ;
            hwait_flag(PIPE_MTE1, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_MTE1:

                                                                         ;
            hwait_flag(PIPE_M, PIPE_MTE1, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_FIX:
                                                                                     ;
            hwait_flag(PIPE_M, PIPE_FIX, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::FIX_M:
                                                                                     ;
            hwait_flag(PIPE_FIX, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        default:
                                                                                                           ;
            break;
    }
}


template <HardEvent event>
[aicore] __inline__ __attribute__((always_inline)) void SetFlagImpl(int32_t eventID)
{

                                                                                                                  ;
    event_t e = static_cast<event_t>(eventID);
    switch (event) {
        case HardEvent::MTE2_MTE1:
            set_flag(PIPE_MTE2, PIPE_MTE1, e);
            break;
        case HardEvent::MTE1_MTE2:
            set_flag(PIPE_MTE1, PIPE_MTE2, e);
            break;
        case HardEvent::MTE2_MTE3:
            set_flag(PIPE_MTE2, PIPE_MTE3, e);
            break;
        case HardEvent::MTE3_MTE2:
            set_flag(PIPE_MTE3, PIPE_MTE2, e);
            break;
        case HardEvent::MTE1_M:
            set_flag(PIPE_MTE1, PIPE_M, e);
            break;
        case HardEvent::M_MTE1:
            set_flag(PIPE_M, PIPE_MTE1, e);
            break;
        case HardEvent::MTE2_V:
            set_flag(PIPE_MTE2, PIPE_V, e);
            break;
        case HardEvent::V_MTE2:
            set_flag(PIPE_V, PIPE_MTE2, e);
            break;
        case HardEvent::MTE3_V:
            set_flag(PIPE_MTE3, PIPE_V, e);
            break;
        case HardEvent::V_MTE3:
            set_flag(PIPE_V, PIPE_MTE3, e);
            break;
        case HardEvent::M_V:
            set_flag(PIPE_M, PIPE_V, e);
            break;
        case HardEvent::M_S:
            set_flag(PIPE_M, PIPE_S, e);
            break;
        case HardEvent::V_M:
            set_flag(PIPE_V, PIPE_M, e);
            break;
        case HardEvent::S_V:
            set_flag(PIPE_S, PIPE_V, e);
            break;
        case HardEvent::V_S:
            set_flag(PIPE_V, PIPE_S, e);
            break;

        case HardEvent::V_V:
            pipe_barrier(PIPE_V);
            return;

        case HardEvent::MTE3_MTE1:
            set_flag(PIPE_MTE3, PIPE_MTE1, e);
            break;
        case HardEvent::MTE1_MTE3:
            set_flag(PIPE_MTE1, PIPE_MTE3, e);
            break;
        case HardEvent::MTE1_V:
            set_flag(PIPE_MTE1, PIPE_V, e);
            break;
        case HardEvent::MTE2_M:
            set_flag(PIPE_MTE2, PIPE_M, e);
            break;
        case HardEvent::M_MTE2:
            set_flag(PIPE_M, PIPE_MTE2, e);
            break;
        case HardEvent::S_MTE2:
            set_flag(PIPE_S, PIPE_MTE2, e);
            break;
        case HardEvent::MTE2_S:
            set_flag(PIPE_MTE2, PIPE_S, e);
            break;
        case HardEvent::V_MTE1:
            set_flag(PIPE_V, PIPE_MTE1, e);
            break;
        case HardEvent::S_MTE3:
            set_flag(PIPE_S, PIPE_MTE3, e);
            break;
        case HardEvent::MTE3_S:
            set_flag(PIPE_MTE3, PIPE_S, e);
            break;

        case HardEvent::M_FIX:
            set_flag(PIPE_M, PIPE_FIX, e);
            break;
        case HardEvent::FIX_M:
            set_flag(PIPE_FIX, PIPE_M, e);
            break;
        case HardEvent::FIX_MTE3:
            set_flag(PIPE_FIX, PIPE_MTE3, e);
            break;
        case HardEvent::FIX_MTE2:
            set_flag(PIPE_FIX, PIPE_MTE2, e);
            break;
        case HardEvent::MTE2_FIX:
            set_flag(PIPE_MTE2, PIPE_FIX, e);
            break;
        case HardEvent::FIX_S:
            set_flag(PIPE_FIX, PIPE_S, e);
            break;
        case HardEvent::MTE1_FIX:
            set_flag(PIPE_MTE1, PIPE_FIX, e);
            break;
        case HardEvent::FIX_MTE1:
            set_flag(PIPE_FIX, PIPE_MTE1, e);
            break;
        case HardEvent::FIX_FIX:
            pipe_barrier(PIPE_FIX);
            break;

        case HardEvent::MAX:
            break;
        default:
                                                                                                               ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void WaitFlagImpl(const HardEvent event, int32_t eventID)
{

                                                                                                                  ;
    event_t e = static_cast<event_t>(eventID);
    switch (event) {
# 730 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
        case HardEvent::MTE2_V:
            wait_flag(PIPE_MTE2, PIPE_V, e);
            break;
        case HardEvent::V_MTE2:
            wait_flag(PIPE_V, PIPE_MTE2, e);
            break;
        case HardEvent::MTE3_V:
            wait_flag(PIPE_MTE3, PIPE_V, e);
            break;
        case HardEvent::V_MTE3:
            wait_flag(PIPE_V, PIPE_MTE3, e);
            break;
# 758 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
        case HardEvent::FIX_M:
            wait_flag(PIPE_FIX, PIPE_M, e);
            break;
        case HardEvent::M_FIX:
            wait_flag(PIPE_M, PIPE_FIX, e);
            break;
        case HardEvent::MTE2_FIX:
            wait_flag(PIPE_MTE2, PIPE_FIX, e);
            break;
        case HardEvent::FIX_MTE2:
            wait_flag(PIPE_FIX, PIPE_MTE2, e);
            break;
        case HardEvent::FIX_S:
            wait_flag(PIPE_FIX, PIPE_S, e);
            break;
        case HardEvent::FIX_MTE3:
            wait_flag(PIPE_FIX, PIPE_MTE3, e);
            break;
        case HardEvent::MTE1_FIX:
            wait_flag(PIPE_MTE1, PIPE_FIX, e);
            break;
        case HardEvent::FIX_MTE1:
            wait_flag(PIPE_FIX, PIPE_MTE1, e);
            break;
        case HardEvent::FIX_FIX:
            pipe_barrier(PIPE_FIX);
            break;

        case HardEvent::MTE3_MTE2:
            wait_flag(PIPE_MTE3, PIPE_MTE2, e);
            break;
        case HardEvent::MTE2_MTE3:
            wait_flag(PIPE_MTE2, PIPE_MTE3, e);
            break;
        case HardEvent::S_MTE2:
            wait_flag(PIPE_S, PIPE_MTE2, e);
            break;
        case HardEvent::MTE2_S:
            wait_flag(PIPE_MTE2, PIPE_S, e);
            break;
        case HardEvent::S_MTE3:
            wait_flag(PIPE_S, PIPE_MTE3, e);
            break;
        case HardEvent::MTE3_S:
            wait_flag(PIPE_MTE3, PIPE_S, e);
            break;
        case HardEvent::M_S:
            wait_flag(PIPE_M, PIPE_S, e);
            break;
        case HardEvent::S_V:
            wait_flag(PIPE_S, PIPE_V, e);
            break;
        case HardEvent::V_S:
            wait_flag(PIPE_V, PIPE_S, e);
            break;
        case HardEvent::V_V:
            return;
        case HardEvent::MAX:
            break;
        default:
            break;
    }
    return;
}
}
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_macros.h" 2
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_macros.h"
enum KernelMetaType : uint8_t {
    KERNEL_TYPE_AIV_ONLY,
    KERNEL_TYPE_AIC_ONLY,
    KERNEL_TYPE_MIX_AIV_1_0,
    KERNEL_TYPE_MIX_AIC_1_0,
    KERNEL_TYPE_MIX_AIC_1_1,
    KERNEL_TYPE_MIX_AIC_1_2,
    KERNEL_TYPE_AICORE,
    KERNEL_TYPE_VECTORCORE,
    KERNEL_TYPE_MIX_AICORE,
    KERNEL_TYPE_MIX_VECTOR_CORE,
    KERNEL_TYPE_MAX,
};

enum KernelType {
    K_TYPE_AICORE = 1,
    K_TYPE_AIC = 2,
    K_TYPE_AIV = 3,
    K_TYPE_MIX_AIC_MAIN = 4,
    K_TYPE_MIX_AIV_MAIN = 5,
    K_TYPE_AIC_ROLLBACK = 6,
    K_TYPE_AIV_ROLLBACK = 7,
    K_TYPE_MAX
};

struct BaseTlv {
    unsigned short type;
    unsigned short len;
};

enum FuncMetaType {
    F_TYPE_KTYPE = 1,
    F_TYPE_CROSS_CORE_SYNC = 2,
    F_TYPE_MIX_TASK_RATION = 3,
    F_TYPE_L0_EXCEPTION_DFX = 4,
    F_TYPE_L0_EXCEPTION_DFX_ARGSINFO = 5,
    F_TYPE_L0_EXCEPTION_DFX_IS_TIK = 6,
    F_TYPE_MAX
};

enum CrossCoreSyncType {
    C_TYPE_USE_SYNC = 1,
    C_TYPE_MAX
};

struct OpSystemRunCfg {
    uint64_t l2Cacheoffset;
};





[aicore] __inline__ __attribute__((always_inline)) void GetCannVersion(__attribute__((cce_global)) char*& versionStr, uint64_t& version, uint64_t& timeStamp)
{

    versionStr = const_cast<__attribute__((cce_global)) char*>("8.2.RC1.alpha002");





    timeStamp = static_cast<uint64_t>(20250522125202340);





    version = static_cast<uint64_t>(((9 * 100000000) + (3 * 1000000) + ((2 * 100) + 5000) + 2));



}


namespace AscendC {
template <typename U>
[aicore] __inline__ __attribute__((always_inline)) static auto IsLite(int) -> typename U::LiteType;
template <typename U>
[aicore] __inline__ __attribute__((always_inline)) static auto IsLite(void*) -> U;

template <typename T>
using PrimT = decltype(IsLite<T>(0));

enum class CacheMode {
    CACHE_MODE_DISABLE = 0,
    CACHE_MODE_NORMAL = 1,
    CACHE_MODE_LAST = 2,
    CACHE_MODE_PERSISTENT = 4
};

enum class CacheRwMode {
    READ = 1,
    WRITE = 2,
    RW = 3
};

template<class T, CacheRwMode rwMode = CacheRwMode::RW>
[aicore] __inline__ __attribute__((cce_global)) T* L2CacheAlter(__attribute__((cce_global)) T* addr, CacheMode mode)
{






    return addr;
}
}

struct FunMetaKType {
    BaseTlv head;
    unsigned int ktype;
};

struct FunMetaCrossCoreType {
    BaseTlv head;
    unsigned int usedCrossCoreSync;
};

struct FunMetaMixCoreType {
    BaseTlv head;
    unsigned short taskRation0;
    unsigned short taskRation1;
};

struct FunLevelKType {
    struct FunMetaKType ktypeMeta;
};

struct FunLevelCrossCoreType {
    struct FunMetaKType ktypeMeta;
    struct FunMetaCrossCoreType crossCoreType;
};

struct FunLevelMixCoreType {
    struct FunMetaKType ktypeMeta;
    struct FunMetaMixCoreType mixCoreType;
};
# 220 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_macros.h"
namespace AscendC {
constexpr int32_t MIX = 0;
constexpr int32_t AIC = 1;
constexpr int32_t AIV = 2;
constexpr size_t DUMP_UINTSIZE = (1024 * 1024);
}
# 237 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_macros.h"
constexpr int32_t g_coreType = AscendC::AIV;
# 274 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_macros.h"
constexpr bool g_gm_overflow_check = false;
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_ceil_oom_que.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_ceil_oom_que.h"
namespace AscendC {
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_ceil_oom_que.h"
[aicore] __inline__ __attribute__((always_inline)) uint32_t DivCeil(uint32_t a, uint32_t b)
{
    return (a + b - 1) / b;
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t AlignUp(uint32_t a, uint32_t b)
{
    return DivCeil(a, b) * b;
}

[aicore] constexpr __inline__ __attribute__((always_inline)) uint32_t ConstCeil(uint32_t a, uint32_t b)
{
    return (a + b - 1) / b;
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t Ceil(uint32_t a, uint32_t b)
{
    return (a + b - 1) / b;
}

[aicore] __inline__ __attribute__((always_inline)) int32_t CeilDivision(int32_t num1, int32_t num2)
{
    if (num2 == 0) {
        return 0;
    }
    return (num1 + num2 - 1) / num2;
}


[aicore] __inline__ __attribute__((always_inline)) void WriteBackOverflow(__attribute__((cce_global)) uint8_t* overflowStatus)
{
    (void)overflowStatus;
# 100 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_ceil_oom_que.h"
}

template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void OOMCheckTensorListRange(__attribute__((cce_global)) T *gmInputAddr, const int inputSize)
{
# 115 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_ceil_oom_que.h"
}

[aicore] static __inline__ __attribute__((always_inline)) bool OOMCheckAddrInTensorList(uint64_t index, uintptr_t gmAddrConvert,
    uintptr_t& inputOutputAddr, uint64_t& inputOutputLen)
{
# 150 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_ceil_oom_que.h"
    (void)index;
    (void)gmAddrConvert;
    (void)inputOutputAddr;
    (void)inputOutputLen;
    return false;
}

template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void OOMCheckAddrRange(__attribute__((cce_global)) T* gmAddr, const uint64_t gmSize)
{
# 170 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_ceil_oom_que.h"
}

template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void OOMAddAddrForL2Cache(__attribute__((cce_global)) T* gmAddr, __attribute__((cce_global)) T* oriAddr)
{
# 198 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_ceil_oom_que.h"
}

[aicore] static __inline__ __attribute__((always_inline)) void OOMInit()
{



}

struct TQueConfig {
    bool nd2nz = false;
    bool nz2nd = false;
    bool scmBlockGroup = false;
    uint32_t bufferLen = 0;
    uint32_t bufferNumber = 0;
    uint32_t consumerSize = 0;
    TPosition consumer[8] = {};
    bool enableStaticEvtId = false;
    bool enableLoopQueue = false;
};

[aicore] constexpr TQueConfig GetTQueConfig(bool nd2nzIn, bool nz2ndIn, bool scmBlockGroupIn,
    uint32_t bufferLenIn, uint32_t bufferNumberIn, uint32_t consumerSizeIn,
    const TPosition consumerIn[], bool enableStaticEvtIdIn, bool enableLoopQueueIn)
{
    return {
        .nd2nz = nd2nzIn,
        .nz2nd = nz2ndIn,
        .scmBlockGroup = scmBlockGroupIn,
        .bufferLen = bufferLenIn,
        .bufferNumber = bufferNumberIn,
        .consumerSize = consumerSizeIn,
        .consumer = {consumerIn[0], consumerIn[1], consumerIn[2], consumerIn[3],
            consumerIn[4], consumerIn[5], consumerIn[6], consumerIn[7]},
        .enableStaticEvtId = enableStaticEvtIdIn,
        .enableLoopQueue = enableLoopQueueIn
    };
}

[aicore] constexpr TQueConfig GetTQueConfig(const int32_t mask)
{
    return {
        .nd2nz = static_cast<bool>(static_cast<uint32_t>(mask) & 0x1u),
        .nz2nd = static_cast<bool>((static_cast<uint32_t>(mask) & 0x2u) >> 1),
        .scmBlockGroup = static_cast<bool>((static_cast<uint32_t>(mask) & 0x4u) >> 2),
        .bufferLen = 0,
        .bufferNumber = 0,
        .consumerSize = 0,
        .consumer = {TPosition::MAX, TPosition::MAX, TPosition::MAX, TPosition::MAX,
            TPosition::MAX, TPosition::MAX, TPosition::MAX, TPosition::MAX},
        .enableStaticEvtId = false,
        .enableLoopQueue = false
    };
}

[aicore] constexpr TQueConfig GetTQueConfig(const TQueConfig* conf)
{
    return {
        .nd2nz = conf->nd2nz,
        .nz2nd = conf->nz2nd,
        .scmBlockGroup = conf->scmBlockGroup,
        .bufferLen = conf->bufferLen,
        .bufferNumber = conf->bufferNumber,
        .consumerSize = conf->consumerSize,
        .consumer = {conf->consumer[0], conf->consumer[1], conf->consumer[2], conf->consumer[3],
            conf->consumer[4], conf->consumer[5], conf->consumer[6], conf->consumer[7]},
        .enableStaticEvtId = conf->enableStaticEvtId,
        .enableLoopQueue = conf->enableLoopQueue
    };
}

template <bool b> struct BoolInst {
    using Type = BoolInst<b>;
    static constexpr bool value = b;
};

using TrueType = BoolInst<true>;
using FalseType = BoolInst<false>;

template <typename T, typename U> struct IsSameType : public FalseType {};

template <typename T> struct IsSameType<T, T> : public TrueType {};

template <typename... Arg>
struct Tuple {};

template <typename T, typename U, typename... Args>
[aicore] constexpr bool SupportType()
{
    if constexpr (sizeof...(Args) > 0) {
        return IsSameType<T, U>::value || SupportType<T, Args...>();
    }
    return IsSameType<T, U>::value;
}

template <typename T, int U, int... Args> [aicore] constexpr bool SupportBytes()
{
    if constexpr (sizeof...(Args) > 0) {
        return sizeof(T) == U || SupportBytes<T, Args...>();
    }
    return sizeof(T) == U;
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_constants.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_constants.h"
namespace AscendC {
const int32_t DEFAULT_BLK_NUM = 8;
const int32_t POWER_MASK_NUM = 8;
const int32_t HALF_FACTOR = 2;
const int32_t DOUBLE_FACTOR = 2;
const int32_t DEFAULT_BLK_STRIDE = 1;
const uint8_t DEFAULT_REPEAT_STRIDE = 8;
const uint8_t HALF_DEFAULT_REPEAT_STRIDE = 4;
const uint8_t ONE_FOURTH_DEFAULT_REPEAT_STRIDE = 2;
const uint64_t FULL_MASK = 0xffffffffffffffff;
const uint64_t CONST_MASK_VALUE = 0x8000000000000000;
const uint16_t MAX_HALF_MASK_LEN = 64;
const int32_t DEFAULT_C0_SIZE = 32;
const int32_t DEFAULT_BLOCK_SIZE = 256;
const int32_t MAX_REPEAT_TIMES = 255;
const int32_t MIN_REPEAT_TIMES = 0;
const bool DEFAULT_REPEAT_STRIDE_MODE = 0;
const bool STRIDE_SIZE_MODE = 0;
const int32_t ONE_BYTE_BIT_SIZE = 8;
const int32_t ONE_DUMP_BACKUP_SIZE = 1024;
const int32_t DUMP_UB_SIZE = 256;
const int32_t DUMP_EXC_FLAG = 7;
const uint32_t TOTAL_L0A_SIZE = 64 * 1024;
const uint32_t TOTAL_L0B_SIZE = 64 * 1024;
const uint32_t TMP_UB_SIZE = 8 * 1024;
const uint32_t MAX_SLICE_SIZE = 6 * 256;
const uint32_t F32_INF = 0x7f800000;
const uint32_t F32_NEG_INF = 0xff800000;
const uint32_t F32_NAN = 0x7fc00000;

const uint16_t VALUE_512 = 512;
const uint16_t UINT12_MAX = 4095;
const uint16_t UINT15_MAX = 32767;


const uint32_t BLOCK_INFO_LEN_POS = 0;
const uint32_t BLOCK_INFO_CORE_POS = 1;
const uint32_t BLOCK_INFO_BLOCKNUM_POS = 2;
const uint32_t BLOCK_INFO_DUMPOFFSET_POS = 3;
const uint32_t BLOCK_INFO_MAGIC_POS = 4;
const uint32_t BLOCK_INFO_RSV_POS = 5;
const uint32_t BLOCK_INFO_DUMP_ADDR = 6;
const uint32_t BLOCK_INFO_MAGIC_NUM = 0x5aa5bccd;

const uint32_t DUMP_META_TYPE_POS = 0;
const uint32_t DUMP_META_LEN_POS = 4;
const uint16_t DUMP_META_BLOCK_DIM_POS = 8;
const uint8_t DUMP_META_CORE_TYPE_POS = 10;
const uint8_t DUMP_META_TASK_RATION = 11;
const uint32_t DUMP_META_RSV_POS = 12;

const uint32_t DUMP_MESSAGE_HEAD_TYPE_POS = 0;
const uint32_t DUMP_MESSAGE_HEAD_LEN_POS = 1;
const uint32_t DUMP_MESSAGE_HEAD_ADDR_POS = 2;
const uint32_t DUMP_MESSAGE_HEAD_DATA_TYPE_POS = 3;
const uint32_t DUMP_MESSAGE_HEAD_DESC_POS = 4;
const uint32_t DUMP_MESSAGE_HEAD_BUFFERID_POS = 5;
const uint32_t DUMP_MESSAGE_HEAD_POSITION_POS = 6;
const uint32_t DUMP_MESSAGE_HEAD_RSV_POS = 7;
const uint32_t DUMP_SCALAR_POS = 8;
const uint32_t DUMP_CORE_COUNT = 75;
const uint32_t DUMP_WORKSPACE_SIZE = DUMP_CORE_COUNT * (1024 * 1024);

const uint32_t DUMP_SHAPE_MESSAGE_HEAD_TYPE_POS = 0;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_LEN_POS = 1;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_DIM_POS = 2;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_SHAPE_START_POS = 3;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_RSV_POS = 11;
const uint32_t DUMP_SHAPE_MESSAGE_TL_LEN = 8;


const uint32_t DUMP_TIME_STAMP_LEN = 24;
const uint32_t DUMP_TIME_STAMP_TOTAL_LEN = 32;
const uint32_t DUMP_TIME_STAMP_LEN_POS = 1;
const uint32_t DUMP_TIME_STAMP_ID_POS = 2;
const uint32_t DUMP_TIME_STAMP_CYCLE_POS = 4;
const uint32_t DUMP_TIME_STAMP_PTR_POS = 6;

constexpr int32_t CTRL_46_BIT = 46;
constexpr int32_t CTRL_47_BIT = 47;
constexpr int32_t CTRL_48_BIT = 48;
constexpr int32_t CTRL_53_BIT = 53;


constexpr uint32_t TENSOR_TENSOR_FLOAT_POWER_FACTOR = 4;
constexpr uint32_t TENSOR_TENSOR_INT_POWER_FACTOR = 6;
constexpr uint32_t TENSOR_TENSOR_HALF_POWER_FACTOR = 7;
constexpr uint32_t TENSOR_SCALAR_FLOAT_POWER_FACTOR = 5;
constexpr uint32_t TENSOR_SCALAR_INT_POWER_FACTOR = 7;
constexpr uint32_t TENSOR_SCALAR_HALF_POWER_FACTOR = 7;
constexpr uint32_t POWER_TWO = 2;
constexpr uint32_t POWER_THREE = 3;
constexpr uint32_t POWER_INT32_BITS = 32;


constexpr uint32_t INT4_TWO = 2;
constexpr uint32_t INT4_BIT_NUM = 4;


constexpr int32_t DEQ_SHIFT_LEFT_17_BIT = 131072;
constexpr float DEQ_SHIFT_RIGHT_17_BIT = 1.0 / DEQ_SHIFT_LEFT_17_BIT;
constexpr int8_t ADDDEQRELU_MASK_MODE_ONE = 1;
constexpr int8_t ADDDEQRELU_MASK_MODE_TWO = 2;
# 137 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_constants.h"
const int32_t TOTAL_VEC_LOCAL_SIZE = 184 * 1024;
const uint32_t TOTAL_UB_SIZE = 192 * 1024;
const uint32_t TMP_UB_OFFSET = 184 * 1024;



const uint32_t TOTAL_L1_SIZE = 512 * 1024 - 128;
const uint32_t SINGLE_MSG_SIZE = 64;
const uint32_t CACHE_LINE_SIZE = 64;
const uint32_t TOTAL_L0C_SIZE = 128 * 1024;
# 171 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_constants.h"
const int32_t BLOCK_CUBE = 16;




const uint16_t ONE_BLK_SIZE = 32;




const int32_t CUBE_MAX_SIZE = 256;
# 202 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_constants.h"
const uint8_t PAD_SIZE = 4;
const uint8_t MRG_SORT_ELEMENT_LEN = 4;
const uint8_t DEFAULT_DATA_COPY_NBURST = 1;
const uint8_t DEFAULT_DATA_COPY_STRIDE = 0;
const int32_t BYTE_PER_FRACTAL = 512;
const int32_t SRC_BURST_LEN_SIZE_ELE = 16;
const int32_t SRC_GAP_SIZE_BYTE = 32;
const int32_t DST_BURST_LEN_SIZE_ELE = 256;
const int32_t VREDUCE_PER_REP_OUTPUT = 2;
const uint16_t ONE_PARAM_SIZE = 8;
const uint16_t AIV_CORE_NUM = 50;
const uint16_t DUMP_MSG_HEAD_SIZE = 24;
const int32_t ONE_REPEAT_BYTE_SIZE = 256;
const int32_t FULL_MASK_LEN = 128;
const int32_t HLAF_MASK_LEN = 64;
const int32_t DEFAULT_REDUCE_DST_REP_SRIDE = 1;
const uint8_t B64_BYTE_SIZE = 8;
const uint8_t B32_BYTE_SIZE = 4;
const uint8_t B16_BYTE_SIZE = 2;
const uint8_t B8_BYTE_SIZE = 1;
const uint8_t B32_DATA_NUM_PER_BLOCK = 8;
const uint8_t B16_DATA_NUM_PER_BLOCK = 16;
const int32_t B16_DATA_NUM_PER_REPEAT = 128;
const int32_t B32_DATA_NUM_PER_REPEAT = 64;

const int32_t BLOCK_STRIDE_POS_IN_SM = 16;
const int32_t PLD_BUFFER_SIZE = 2;
const uint8_t FIXPIPE_DEQ_TENSOR_SIZE = 16;
const uint8_t SET_DATA_EXP_ZERO = 0;
const uint8_t SET_DATA_EXP_ONE = 1;
const uint8_t SET_DATA_EXP_TWO = 2;
const uint8_t SET_DATA_EXP_THREE = 3;
const uint8_t VDEQ_TENSOR_SIZE = 16;






constexpr size_t RESERVED_WORKSPACE = 16 * 1024 * 1024;







const int32_t NCHW_CONV_ADDR_LIST_SIZE = 16;
const int32_t VA_REG_ARRAY_LEN = 8;
const uint8_t CONV2D_IMG_SIZE = 2;
const uint8_t CONV2D_KERNEL_SIZE = 2;
const uint8_t CONV2D_STRIDE = 2;
const uint8_t CONV2D_PAD = 4;
const uint8_t CONV2D_DILATION = 2;
const int32_t K_MAX_DIM = 8;

const uint32_t TWO_OF_STACK_BUFFER = 2;
const uint32_t THREE_OF_STACK_BUFFER = 3;
const uint32_t HALF_REPEAT_SIZE = ONE_REPEAT_BYTE_SIZE / B16_BYTE_SIZE;
const uint32_t FLOAT_REPEAT_SIZE = ONE_REPEAT_BYTE_SIZE / B32_BYTE_SIZE;
const uint32_t ONE_REPEAT_FLOAT_SIZE = ONE_REPEAT_BYTE_SIZE / B32_BYTE_SIZE;
const uint32_t ONE_REPEAT_HALF_SIZE = ONE_REPEAT_BYTE_SIZE / B16_BYTE_SIZE;
const uint32_t MAX_REPEAT_FLOAT_SIZE = ONE_REPEAT_FLOAT_SIZE * MAX_REPEAT_TIMES;
const uint32_t MAX_REPEAT_HALF_SIZE = ONE_REPEAT_HALF_SIZE * MAX_REPEAT_TIMES;
const uint32_t ONE_BLK_HALF_NUM = ONE_BLK_SIZE / B16_BYTE_SIZE;
const uint32_t ONE_BLK_FLOAT_NUM = ONE_BLK_SIZE / B32_BYTE_SIZE;

const uint32_t BRCB_BROADCAST_NUMBER = 8;
const uint32_t BRCB_MAX_REPEAT_SIZE = BRCB_BROADCAST_NUMBER * MAX_REPEAT_TIMES;
const int32_t MIN_BLOCK_LEN = 1;
const uint32_t PAIR_REDUCE_REPEAT_STRIDE_LEN = 128;
const uint32_t PAIR_REDUCE_SUM_MERGES = 2;
const uint32_t TWO_HUNDRED_FIFTY_TWO_REPEAT = 252;
const uint32_t TWO_HUNDRED_FIFTY_TWO_REPEAT_BYTE_SIZE = TWO_HUNDRED_FIFTY_TWO_REPEAT * ONE_REPEAT_BYTE_SIZE;
const uint32_t REDUCEV2_MODE_SEVEN = 7;
const uint32_t DROPOUT_MODE_BYTE_MISALIGN = 1;
const uint32_t DROPOUT_MODE_BYTE_ALIGN = 2;
const uint32_t DROPOUT_MODE_BIT_ALIGN = 3;
const uint32_t DROPOUT_MODE_BIT_MISALIGN = 4;
const uint32_t REDUCEV2_MODE_ONE = 1;
const uint32_t REDUCEV2_MODE_TWO = 2;
const uint32_t REDUCEV2_MODE_THREE = 3;


const int32_t B8_TMP_ELE_LEN = 1024;
const int32_t B16_TMP_ELE_LEN = 256;
const int32_t B32_TMP_ELE_LEN = 128;
const int32_t B8_TRANS_LEN = 1024;
const int32_t B8_TRANS_FRACTAL = 512;
const int32_t B8_TRANS_ROW = 32;
const int32_t B8_COPY_COL = 32;


const uint64_t LOAD_M_START_POSITION = 48;
const uint64_t LOAD_K_START_POSITION = 32;
const uint64_t LOAD_M_EXTENSION = 16;
const uint64_t LOAD_DILATION_FILTER_H = 40;
const uint64_t LOAD_DILATION_FILTER_W = 32;
const uint64_t LOAD_FILTER_H = 24;
const uint64_t LOAD_FILTER_W = 16;
const uint64_t LOAD_STRIDE_H = 8;
# 416 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_constants.h"
template <bool condition, class T1, class T2>
struct Conditional {
    using type = T1;
};

template <class T1, class T2>
struct Conditional<false, T1, T2> {
    using type = T2;
};

template <bool condition1, bool condition2, class T1, class T2, class T3, class T4> struct ConditionalMulti {
    using type = typename Conditional<condition1, typename Conditional<condition2, T1, T2>::type,
        typename Conditional<condition2, T3, T4>::type>::type;
};

template <int bitNum, bool sign = true>
struct IntegerSubType {
    static int const kBits = bitNum;
    static bool const kSigned = sign;

    using T = typename Conditional<kSigned, int8_t, uint8_t>::type;
    using Storage = uint8_t;

    static Storage const mask = Storage(((static_cast<uint64_t>(1)) << static_cast<uint32_t>(kBits)) - 1);
    Storage storage;
    [aicore] __inline__ __attribute__((always_inline)) IntegerSubType() = default;

    [aicore] __inline__ __attribute__((always_inline)) IntegerSubType(uint32_t value)
        : storage(reinterpret_cast<Storage const &>(value) & mask) {}

    [aicore] __inline__ __attribute__((always_inline)) IntegerSubType(int32_t value)
        : storage(reinterpret_cast<Storage const &>(value) & mask) {}

    [aicore] __inline__ __attribute__((always_inline)) operator T() const
    {
        if (kSigned && ((storage & Storage(static_cast<uint64_t>(1) << static_cast<uint32_t>(kBits - 1))) != 0)) {

            return T(storage) | ~T(mask);
        }
        return T(storage);
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator == (IntegerSubType const &rhs) const
    {
        return storage == rhs.storage;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator != (IntegerSubType const &rhs) const
    {
        return storage != rhs.storage;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator > (IntegerSubType const &rhs) const
    {
        bool lhsIsNeg = (this->storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(this->kBits - 1)));
        bool rhsIsNeg = (rhs.storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(rhs.kBits - 1)));
        if (kSigned && (lhsIsNeg != rhsIsNeg)) {
            return (!lhsIsNeg) && rhsIsNeg;
        }
        return this->storage > rhs.storage;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator >= (IntegerSubType const &rhs) const
    {
        bool lhsIsNeg = (this->storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(this->kBits - 1)));
        bool rhsIsNeg = (rhs.storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(rhs.kBits - 1)));
        if (kSigned && (lhsIsNeg != rhsIsNeg)) {
            return (!lhsIsNeg) && rhsIsNeg;
        }
        return storage >= rhs.storage;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator < (IntegerSubType const &rhs) const
    {
        return !(*this >= rhs);
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator <= (IntegerSubType const &rhs) const
    {
        return !(*this > rhs);
    }
};

using int4b_t = IntegerSubType<INT4_BIT_NUM, true>;
using float8_e8m0_t = uint8_t;

template <typename T>
[aicore] constexpr bool IsHalfByteDataType()
{
    return IsSameType<T, int4b_t>::value;
}

template <typename T> struct SizeOfBits {};

template <>
struct SizeOfBits<int4b_t> {
    static int const value = INT4_BIT_NUM;
};

[aicore] __inline__ __attribute__((always_inline)) bool CheckCastOverlappingHigh(const uint64_t dstAddr, const uint64_t srcAddr,
    const uint32_t dstTypeSize, const uint32_t srcTypeSize, const uint32_t calCount)
{
    uint64_t addrLow = dstAddr > srcAddr ? srcAddr : dstAddr;
    uint64_t addrHigh = dstAddr > srcAddr ? dstAddr : srcAddr;
    uint64_t needSizeLow = dstAddr > srcAddr ? calCount * srcTypeSize : calCount * dstTypeSize;

    if ((srcTypeSize < dstTypeSize) && (srcAddr >= AlignUp(dstAddr + calCount * srcTypeSize, ONE_BLK_SIZE))) {
        return true;
    }
    if (dstTypeSize > srcTypeSize && srcAddr == dstAddr) {
        return false;
    }
    if ((needSizeLow > static_cast<uint64_t>(ONE_REPEAT_BYTE_SIZE)) && (srcAddr != dstAddr) &&
        ((addrLow + needSizeLow > addrHigh))) {
        return false;
    }
    return true;
}
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_mode.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_mode.h"
namespace AscendC {
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_mode.h"
class MaskSetter {
public:
    static MaskSetter& Instance()
    {
        static MaskSetter instance;
        return instance;
    };

    void SetMask(bool setMask)
    {
        isSetMask = setMask;
    }

    bool GetMask() const
    {
        return isSetMask;
    }

private:
    MaskSetter(){};
    ~MaskSetter(){};
    bool isSetMask = true;
};

class Int4Setter {
public:
    static Int4Setter& Instance()
    {
        static Int4Setter instance;
        return instance;
    };

    void SetInt4()
    {
        isInt4 = true;
    }

    void SetDstInt4()
    {
        isDstInt4 = true;
    }

    void SetSrcInt4()
    {
        isSrcInt4 = true;
    }

    void ResetInt4()
    {
        isInt4 = false;
    }

    void ResetDstSrcInt4()
    {
        isDstInt4 = false;
        isSrcInt4 = false;
    }

    bool GetInt4() const
    {
        return isInt4;
    }

    bool GetDstInt4() const
    {
        return isDstInt4;
    }

    bool GetSrcInt4() const
    {
        return isSrcInt4;
    }

private:
    Int4Setter(){};
    ~Int4Setter(){};

    bool isInt4 = false;
    bool isDstInt4 = false;
    bool isSrcInt4 = false;
};

union NotNumUnion {
    [aicore] NotNumUnion() {}
    float f;
    uint32_t i;
};

enum class TShapeType : uint8_t {
    DEFAULT,
    NHWC,
    NC1HWC0,
    NHC,
    NCHT,
    ND,
    FRACTAL_NZ,
    HNC,
    HCNT,
    NDHWC,
    FRACTAL_Z_3D,
    NDHC,
    DCHNT,
    FRACTAL_Z,
    NCDHW,
    NDC1HWC0,
    NCDH,
    NDCHT,
    NCHW,
    NCH,
    HWCN,
    HCN,
    CHNT,
    DHWCN,
    DHCN
};

enum class RoundMode : uint8_t {
    CAST_NONE = 0,
    CAST_RINT,
    CAST_FLOOR,
    CAST_CEIL,
    CAST_ROUND,
    CAST_TRUNC,
    CAST_ODD,
};

enum class CMPMODE : uint8_t {
    LT = 0,
    GT,
    EQ,
    LE,
    GE,
    NE,
};

enum class SELMODE : uint8_t {
    VSEL_CMPMASK_SPR = 0,
    VSEL_TENSOR_SCALAR_MODE,
    VSEL_TENSOR_TENSOR_MODE,
};

enum class BlockMode : uint8_t {
    BLOCK_MODE_NORMAL = 0,
    BLOCK_MODE_MATRIX,
    BLOCK_MODE_VECTOR,
    BLOCK_MODE_SMALL_CHANNEL,
    BLOCK_MODE_DEPTHWISE,
};

enum class DeqScale : uint8_t {
    DEQ_NONE = 0,
    DEQ,
    VDEQ,
    DEQ8,
    VDEQ8,
    DEQ16,
    VDEQ16,
};

enum class ReduceMode : uint8_t {
    REDUCE_MAX = 0,
    REDUCE_MIN,
    REDUCE_SUM,
};

enum class ReduceOrder : uint8_t {
    ORDER_VALUE_INDEX = 0,
    ORDER_INDEX_VALUE,
    ORDER_ONLY_VALUE,
    ORDER_ONLY_INDEX,
};

enum class DumpType : uint8_t {
    DUMP_DEFAULT = 0,
    DUMP_SCALAR,
    DUMP_TENSOR,
    DUMP_SHAPE,
    DUMP_ASSERT,
    DUMP_META,
    DUMP_TIME_STAMP,
};

enum class CLAMPMODE {
    CLAMP_MAX = 0,
    CLAMP_MIN,
};

enum class PcieCtrl : uint64_t {
    WR = 0,
    RD
};

enum class DeQuantMode : uint8_t {
    DEQUANT_WITH_SINGLE_ROW = 0,
    DEQUANT_WITH_MULTI_ROW,
};
# 453 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_mode.h"
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_struct_confusion_pad.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_struct_confusion_pad.h"
namespace AscendC {
struct ConfusionTranspose2NZ012NTiling {
    [aicore] ConfusionTranspose2NZ012NTiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeN = 0;
        hnDiv = 0;
        blockNum = 0;
        shapeH = 0;
        hBlockNum = 0;
        sBlockNum = 0;
        alignH = 0;
        alignS = 0;
        hnDivBlockNum = 0;
        alignHnDiv = 0;
        gap = 0;
        alignsBlockCube = 0;
        prehBlockNum = 0;
        dstBatchOffset = 0;
        srcBatchOffset = 0;
    }

    [aicore] ConfusionTranspose2NZ012NTiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeNIn,
        uint32_t hnDivIn, uint32_t blockNumIn, uint32_t shapeHIn, uint32_t hBlockNumIn, uint32_t sBlockNumIn,
        uint32_t alignHIn, uint32_t alignSIn, uint32_t hnDivBlockNumIn, uint32_t alignHnDivIn, uint32_t gapIn,
        uint32_t alignsBlockCubeIn, uint32_t prehBlockNumIn, uint32_t dstBatchOffsetIn, uint32_t srcBatchOffsetIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeN = shapeNIn;
        hnDiv = hnDivIn;
        blockNum = blockNumIn;
        shapeH = shapeHIn;
        hBlockNum = hBlockNumIn;
        sBlockNum = sBlockNumIn;
        alignH = alignHIn;
        alignS = alignSIn;
        hnDivBlockNum = hnDivBlockNumIn;
        alignHnDiv = alignHnDivIn;
        gap = gapIn;
        alignsBlockCube = alignsBlockCubeIn;
        prehBlockNum = prehBlockNumIn;
        dstBatchOffset = dstBatchOffsetIn;
        srcBatchOffset = srcBatchOffsetIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeN = 0;
    uint32_t hnDiv = 0;
    uint32_t blockNum = 0;
    uint32_t shapeH = 0;
    uint32_t hBlockNum = 0;
    uint32_t sBlockNum = 0;
    uint32_t alignH = 0;
    uint32_t alignS = 0;
    uint32_t hnDivBlockNum = 0;
    uint32_t alignHnDiv = 0;
    uint32_t gap = 0;
    uint32_t alignsBlockCube = 0;
    uint32_t prehBlockNum = 0;
    uint32_t dstBatchOffset = 0;
    uint32_t srcBatchOffset = 0;
};

struct ConfusionTranspose2ND012NTiling {
    [aicore] ConfusionTranspose2ND012NTiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeN = 0;
        hnDiv = 0;
        shapeH = 0;
        hBlockNum = 0;
        sBlockNum = 0;
        hnDivBlockNum = 0;
        alignHnDiv = 0;
        gap = 0;
        alignsCube = 0;
        prehBlockNum = 0;
        alignsMulAlignHnDiv = 0;
        alignHnDivCube = 0;
        alignHnDivBlockSize = 0;
        dstBatchOffset = 0;
        srcBatchOffset = 0;
        blockNum = 0;
    }

    [aicore] ConfusionTranspose2ND012NTiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeNIn,
        uint32_t hnDivIn, uint32_t shapeHIn, uint32_t hBlockNumIn, uint32_t sBlockNumIn, uint32_t hnDivBlockNumIn,
        uint32_t alignHnDivIn, uint32_t gapIn, uint32_t alignsCubeIn, uint32_t prehBlockNumIn,
        uint32_t alignsMulAlignHnDivIn, uint32_t alignHnDivCubeIn, uint32_t alignHnDivBlockSizeIn,
        uint32_t dstBatchOffsetIn, uint32_t srcBatchOffsetIn, uint32_t blockNumIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeN = shapeNIn;
        hnDiv = hnDivIn;
        shapeH = shapeHIn;
        hBlockNum = hBlockNumIn;
        sBlockNum = sBlockNumIn;
        hnDivBlockNum = hnDivBlockNumIn;
        alignHnDiv = alignHnDivIn;
        gap = gapIn;
        alignsCube = alignsCubeIn;
        prehBlockNum = prehBlockNumIn;
        alignsMulAlignHnDiv = alignsMulAlignHnDivIn;
        alignHnDivCube = alignHnDivCubeIn;
        alignHnDivBlockSize = alignHnDivBlockSizeIn;
        dstBatchOffset = dstBatchOffsetIn;
        srcBatchOffset = srcBatchOffsetIn;
        blockNum = blockNumIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeN = 0;
    uint32_t hnDiv = 0;
    uint32_t shapeH = 0;
    uint32_t hBlockNum = 0;
    uint32_t sBlockNum = 0;
    uint32_t hnDivBlockNum = 0;
    uint32_t alignHnDiv = 0;
    uint32_t gap = 0;
    uint32_t alignsCube = 0;
    uint32_t prehBlockNum = 0;
    uint32_t alignsMulAlignHnDiv = 0;
    uint32_t alignHnDivCube = 0;
    uint32_t alignHnDivBlockSize = 0;
    uint32_t dstBatchOffset = 0;
    uint32_t srcBatchOffset = 0;
    uint32_t blockNum = 0;
};

struct ConfusionTranspose012Tiling {
    [aicore] ConfusionTranspose012Tiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeN = 0;
        hnDiv = 0;
        shapeH = 0;
        hBlockNum = 0;
        sBlockNum = 0;
        hnDivBlockNum = 0;
        alignH = 0;
        alignsCube = 0;
        alignhBlockCube = 0;
        blockSizeMulAlignH = 0;
        srcBatchOffset = 0;
        dstBatchOffset = 0;
        blockNum = 0;
    }

    [aicore] ConfusionTranspose012Tiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeNIn, uint32_t hnDivIn,
        uint32_t shapeHIn, uint32_t hBlockNumIn, uint32_t sBlockNumIn, uint32_t hnDivBlockNumIn, uint32_t alignHIn,
        uint32_t alignsCubeIn, uint32_t alignhBlockCubeIn, uint32_t blockSizeMulAlignHIn, uint32_t srcBatchOffsetIn,
        uint32_t dstBatchOffsetIn, uint32_t blockNumIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeN = shapeNIn;
        hnDiv = hnDivIn;
        shapeH = shapeHIn;
        hBlockNum = hBlockNumIn;
        sBlockNum = sBlockNumIn;
        hnDivBlockNum = hnDivBlockNumIn;
        alignH = alignHIn;
        alignsCube = alignsCubeIn;
        alignhBlockCube = alignhBlockCubeIn;
        blockSizeMulAlignH = blockSizeMulAlignHIn;
        srcBatchOffset = srcBatchOffsetIn;
        dstBatchOffset = dstBatchOffsetIn;
        blockNum = blockNumIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeN = 0;
    uint32_t hnDiv = 0;
    uint32_t shapeH = 0;
    uint32_t hBlockNum = 0;
    uint32_t sBlockNum = 0;
    uint32_t hnDivBlockNum = 0;
    uint32_t alignH = 0;
    uint32_t alignsCube = 0;
    uint32_t alignhBlockCube = 0;
    uint32_t blockSizeMulAlignH = 0;
    uint32_t srcBatchOffset = 0;
    uint32_t dstBatchOffset = 0;
    uint32_t blockNum = 0;
};

struct ConfusionTransposeOnlyTiling {
    [aicore] ConfusionTransposeOnlyTiling()
    {
        blockSize = 0;
        height = 0;
        width = 0;
        highBlock = 0;
        stride = 0;
        repeat = 0;
    }

    [aicore] ConfusionTransposeOnlyTiling(uint32_t blockSizeIn, uint32_t heightIn, uint32_t widthIn,
        uint32_t highBlockIn, uint32_t strideIn, uint32_t repeatIn)
    {
        blockSize = blockSizeIn;
        height = heightIn;
        width = widthIn;
        highBlock = highBlockIn;
        stride = strideIn;
        repeat = repeatIn;
    }
    uint32_t blockSize = 0;
    uint32_t height = 0;
    uint32_t width = 0;
    uint32_t highBlock = 0;
    uint32_t stride = 0;
    uint32_t repeat = 0;
};

struct ConfusionTranspose0213Tiling {
    [aicore] ConfusionTranspose0213Tiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeA1 = 0;
        alignA3 = 0;
        alignA2 = 0;
        widthTiling = 0;
        newPopSize = 0;
        newPopH = 0;
        needSize = 0;
        mainBlocks = 0;
        tailSize = 0;
        alignA2MulAlignA3 = 0;
        batchOffset = 0;
        alignA3MulA1 = 0;
        shapeA1BlockCube = 0;
        mainOffset = 0;
    }

    [aicore] ConfusionTranspose0213Tiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeA1In,
        uint32_t alignA3In, uint32_t alignA2In, uint32_t widthTilingIn, uint32_t newPopSizeIn, uint32_t newPopHIn,
        uint32_t needSizeIn, uint32_t mainBlocksIn, uint32_t tailSizeIn, uint32_t alignA2MulAlignA3In,
        uint32_t batchOffsetIn, uint32_t alignA3MulA1In, uint32_t shapeA1BlockCubeIn, uint32_t mainOffsetIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeA1 = shapeA1In;
        alignA3 = alignA3In;
        alignA2 = alignA2In;
        widthTiling = widthTilingIn;
        newPopSize = newPopSizeIn;
        newPopH = newPopHIn;
        needSize = needSizeIn;
        mainBlocks = mainBlocksIn;
        tailSize = tailSizeIn;
        alignA2MulAlignA3 = alignA2MulAlignA3In;
        batchOffset = batchOffsetIn;
        alignA3MulA1 = alignA3MulA1In;
        shapeA1BlockCube = shapeA1BlockCubeIn;
        mainOffset = mainOffsetIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeA1 = 0;
    uint32_t alignA3 = 0;
    uint32_t alignA2 = 0;
    uint32_t widthTiling = 0;
    uint32_t newPopSize = 0;
    uint32_t newPopH = 0;
    uint32_t needSize = 0;
    uint32_t mainBlocks = 0;
    uint32_t tailSize = 0;
    uint32_t alignA2MulAlignA3 = 0;
    uint32_t batchOffset = 0;
    uint32_t alignA3MulA1 = 0;
    uint32_t shapeA1BlockCube = 0;
    uint32_t mainOffset = 0;
};

struct IntriInfo {
    uint32_t c0Count{ 0 };
    uint32_t repeat{ 0 };
    uint32_t repeatRounding{ 0 };
    uint32_t repeatRemaining{ 0 };
    uint32_t tail{ 0 };
};

enum class DataFormat : uint8_t {
    ND = 0,
    NZ,
    NCHW,
    NC1HWC0,
    NHWC,
};

struct PadParams {
    [aicore] PadParams()
    {
        leftPad = 0;
        rightPad = 0;
        padValue = 0;
    }

    [aicore] PadParams(const uint16_t leftPadIn, const uint16_t rightPadIn, const int32_t padValueIn)
    {
        leftPad = leftPadIn;
        rightPad = rightPadIn;
        padValue = padValueIn;
    }

    uint16_t leftPad = 0;
    uint16_t rightPad = 0;
    int32_t padValue = 0;
};

struct UnPadParams {
    [aicore] UnPadParams()
    {
        leftPad = 0;
        rightPad = 0;
    }

    [aicore] UnPadParams(const uint16_t leftPadIn, const uint16_t rightPadIn)
    {
        leftPad = leftPadIn;
        rightPad = rightPadIn;
    }

    uint16_t leftPad = 0;
    uint16_t rightPad = 0;
};



constexpr int32_t AIPP_OFFSET_CSC_ENABLE = 63;
constexpr int32_t AIPP_OFFSET_CH1 = 16;
constexpr int32_t AIPP_OFFSET_CH2 = 32;
constexpr int32_t AIPP_OFFSET_CH3 = 48;
constexpr int32_t AIPP_OFFSET_SWAP_RB = 16;
constexpr int32_t AIPP_OFFSET_SWAP_UV = 17;
constexpr int32_t AIPP_OFFSET_SWAP_AX = 18;
constexpr int32_t AIPP_OFFSET_FORMAT = 19;
constexpr int32_t AIPP_OFFSET_SINGLE_LINE = 24;
constexpr int32_t AIPP_OFFSET_PADDING_MODE = 27;
constexpr int32_t AIPP_OFFSET_CPADDING_MODE = 40;
constexpr int32_t AIPP_OFFSET_CSC_OUT_CH0 = 16;
constexpr int32_t AIPP_OFFSET_CSC_OUT_CH1 = 24;
constexpr int32_t AIPP_OFFSET_CSC_OUT_CH2 = 32;
constexpr int32_t AIPP_OFFSET_CSC_IN_CH0 = 40;
constexpr int32_t AIPP_OFFSET_CSC_IN_CH1 = 48;
constexpr int32_t AIPP_OFFSET_CSC_IN_CH2 = 56;
constexpr int32_t AIPP_OFFSET_DTC_CH1 = 32;
constexpr int32_t AIPP_OFFSET_DTC_ROUND_MODE = 34;


}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_struct_dma_params.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_struct_dma_params.h"
namespace AscendC {
struct QuantParams {
    [aicore] QuantParams() {}
    [aicore] QuantParams(const QuantMode_t quantPreIn) : quantPre(quantPreIn) {}
    [aicore] QuantParams(const QuantMode_t quantPreIn, const uint64_t deqScalarIn)
        : quantPre(quantPreIn), deqScalar(deqScalarIn) {}
    QuantMode_t quantPre = QuantMode_t::NoQuant;
    uint64_t deqScalar;
};

struct Nz2NdParams {
    [aicore] Nz2NdParams()
    {
        nz2ndEn = false;
        ndNum = 1;
        srcNdStride = 0;
        dstNdStride = 0;
        originalNSize = 0;
    }

    [aicore] Nz2NdParams(const bool nz2ndEnIn, const uint16_t ndNumIn, const uint16_t srcNdStrideIn,
        const uint16_t dstNdStrideIn, const uint16_t originalNSizeIn)
    {
        nz2ndEn = nz2ndEnIn;
        ndNum = ndNumIn;
        srcNdStride = srcNdStrideIn;
        dstNdStride = dstNdStrideIn;
        originalNSize = originalNSizeIn;
    }

    bool nz2ndEn = false;
    uint16_t ndNum = 1;
    uint16_t srcNdStride = 0;
    uint16_t dstNdStride = 0;
    uint16_t originalNSize = 0;
};

template <typename src_T = int32_t>
struct FixpipeParams {
    [aicore] FixpipeParams()
    {
        cburstNum = DEFAULT_DATA_COPY_NBURST;
        burstLen = 1;
        srcStride = DEFAULT_DATA_COPY_STRIDE;
        dstStride = DEFAULT_DATA_COPY_STRIDE;
        reluEn = false;
        unitFlag = 0;
    }

    [aicore] FixpipeParams(const uint16_t count, const uint16_t len, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn)
    {
        cburstNum = count;
        burstLen = len;
        dstStride = dstStrideIn;
        srcStride = srcStrideIn;
    }

    uint16_t cburstNum = 0;
    uint16_t burstLen = 0;
    uint32_t dstStride = 0;
    uint16_t srcStride = 0;

    QuantParams quantParams;
    bool reluEn = false;
    Nz2NdParams nz2ndParams;
    uint8_t unitFlag = 0;
};
}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_struct_norm_sort.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_struct_norm_sort.h"
namespace AscendC {
struct MatMulInfo {
    const uint16_t m{ 0 };
    const uint16_t n{ 0 };
    const uint16_t k{ 0 };
    const bool isInitOut{ false };
    const bool isBias{ false };
};

struct DropOutShapeInfo {
    [aicore] DropOutShapeInfo(){};
    uint32_t firstAxis = 0;
    uint32_t srcLastAxis = 0;
    uint32_t maskLastAxis = 0;
};

struct SelectWithBytesMaskShapeInfo {
    [aicore] SelectWithBytesMaskShapeInfo(){};
    uint32_t firstAxis = 0;
    uint32_t srcLastAxis = 0;
    uint32_t maskLastAxis = 0;
};

template <typename T> class LocalTensor;
template <typename T> class GlobalTensor;

template <typename T> struct LayerNormParams {
    [aicore] LayerNormParams(){};
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

template <typename T> struct BatchNormParams {
    [aicore] BatchNormParams(){};
    float firstDimValueBack = 1.0;
    uint8_t srcRepeatStride = 1;
    uint32_t srcOffset = 1;
    uint32_t basicLoop = 0;
    uint32_t brcRepeatTimes = 0;
    uint32_t oriBloop = 0;
    uint32_t oriBTail = 0;
    uint32_t oriBTmpLoopOffset = 0;
    uint32_t oriBTmpTailOffset = 0;
    uint32_t oriBOutLoopOffset = 0;
    uint32_t oriBOutTailOffset = 0;
    uint32_t reduceAddLoop = 0;
    uint32_t reduceAddTail = 0;
    uint32_t reduceAddTailOffset = 0;

    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

template <typename T> struct DeepNormParams {
    [aicore] DeepNormParams(){};
    float lastDimValueBack = 1.0;

    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

template <typename T> struct ExpParams {
    [aicore] ExpParams() {};
    uint32_t inputSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t fourthTmpStartPos = 0;
    uint32_t loopNum = 0;
    uint32_t tailSize = 0;
    uint32_t tailPos = 0;
    uint32_t curDataLength = 0;
    uint32_t expandLevel = 0;

    LocalTensor<T> tempTensorFloorX;
    LocalTensor<T> tempTensorFloorXPow;
    LocalTensor<T> tempTensorRes;
    LocalTensor<T> tempTensorIntPart;
};

template <typename T> struct AntiquantParams {
    [aicore] AntiquantParams() {};
    LocalTensor<T> tempTensorOffset;
    LocalTensor<T> tempTensorScale;
    LocalTensor<T> tempTensorInput;
};

template <typename T, typename U>struct DropOutParams {
    [aicore] DropOutParams() {};
    uint32_t dataSize = 0;
    uint32_t stackBufferSize = 0;
    uint32_t repeatTimes = 1;

    uint32_t maxRepeatSize = 0;
    uint32_t oneRepeatSize = 0;

    uint32_t currentSize = 0;
    uint32_t repeatRounding = 0;
    uint32_t repeatRemaining = 0;
    uint32_t repeatTail = 0;

    LocalTensor<T> firstLocal;
    LocalTensor<U> secondLocal;
};

template <typename T, typename U> struct PowerFParams {
    [aicore] PowerFParams(){};
    LocalTensor<T> tmpTensor1;
    LocalTensor<T> tmpTensor2;
    LocalTensor<T> tmpTensor3;
    LocalTensor<U> tmpMask1;
    LocalTensor<U> tmpMask2;
    LocalTensor<U> tmpMask3;
    LocalTensor<U> finiteIntegerYMask;
};

template <typename T, typename U> struct PowerIParams {
    [aicore] PowerIParams(){};
    float expIterateSum;

    LocalTensor<T> expUBIterate;
    LocalTensor<T> oriAbsExp;
    LocalTensor<T> recordExpNode;
    LocalTensor<T> tmpTensor1;
    LocalTensor<T> tmpTensor2;
    LocalTensor<U> negMask;
    LocalTensor<U> mask;
    LocalTensor<U> tmpScalar;
};

template <typename T> struct GeluParams {
    [aicore] GeluParams(){};
    uint32_t repeatTimes = 1;

    uint32_t currentSize = 0;
    uint32_t repeatRounding = 0;
    uint32_t repeatRemaining = 0;
    uint32_t tail = 0;

    uint32_t maxRepeatSize = 0;
    uint32_t oneRepeatSize = 0;

    uint32_t dataSize = 0;
    uint32_t stackSize = 0;
    uint32_t tmpBufferSize = 0;
    LocalTensor<T> sharedTmpBuffer;

    LocalTensor<T> tempTensorConv;
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
};

template <typename T> struct TanhParams {
    [aicore] TanhParams(){};
    uint32_t repeatTimes = 1;
    uint32_t calCount = 0;
    uint32_t stackSize = 0;
    uint32_t tmpBufferSize = 0;
    LocalTensor<T> sharedTmpBuffer;

    LocalTensor<T> tempTensorConv;
    LocalTensor<T> tmpClip;
};

template <typename T> struct AscendDequantParams {
    [aicore] AscendDequantParams(){};
    uint64_t tmpSize;

    LocalTensor<T> tmpAddrA;
    LocalTensor<T> tmpAddrB;
};

template <typename T> [aicore] __inline__ __attribute__((always_inline)) uint64_t GetScalarBitcodeValue(T scalarValue)
{
    union ScalarBitcode {
        [aicore] ScalarBitcode() {}
        T input;
        uint64_t output;
    } data;

    data.input = scalarValue;
    return data.output;
}

template <typename T, typename U> [aicore] __inline__ __attribute__((always_inline)) U GetScalarBitcodeValue(T scalarValue)
{
    union ScalarBitcode {
        [aicore] ScalarBitcode() {}
        T input;
        U output;
    } data;

    data.input = scalarValue;
    return static_cast<U>(data.output);
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) __cce_half GetScalarBitcodeToHalf(T scalarValue)
{
    union ScalarBitcode {
        [aicore] ScalarBitcode() {}
        T input;
        __cce_half output;
    } data;

    data.input = scalarValue;
    return data.output;
}
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_struct_param.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/utils/kernel_utils_struct_param.h"
namespace AscendC {
struct ReduceRepeatParams {
    [aicore] ReduceRepeatParams()
    {
        highMask = FULL_MASK;
        lowMask = FULL_MASK;
        repeatTimes = 0;
        dstRepStride = DEFAULT_REDUCE_DST_REP_SRIDE;
        srcBlkStride = DEFAULT_BLK_STRIDE;
        srcRepStride = DEFAULT_REPEAT_STRIDE;
    }

    [aicore] ReduceRepeatParams(const int32_t mask, const int32_t repeatTimesIn, const int32_t dstRepStrideIn,
        const int32_t srcBlkStrideIn, const int32_t srcRepStrideIn)
    {




        if (mask == HLAF_MASK_LEN) {
            highMask = 0;
            lowMask = FULL_MASK;
        } else if (mask == HLAF_MASK_LEN * DOUBLE_FACTOR) {
            highMask = FULL_MASK;
            lowMask = FULL_MASK;
        } else {
            highMask = (mask > HLAF_MASK_LEN) ?
                (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(mask - HLAF_MASK_LEN)) - 1) :
                0;
            lowMask =
                (mask > HLAF_MASK_LEN) ? FULL_MASK : (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(mask)) - 1);
        }

        repeatTimes = repeatTimesIn;
        dstRepStride = dstRepStrideIn;
        srcBlkStride = srcBlkStrideIn;
        srcRepStride = srcRepStrideIn;
    }

    [aicore] ReduceRepeatParams(const uint64_t mask[2], const int32_t repeatTimesIn, const int32_t dstRepStrideIn,
        const int32_t srcBlkStrideIn, const int32_t srcRepStrideIn)
    {




        highMask = mask[1];
        lowMask = mask[0];

        repeatTimes = repeatTimesIn;
        dstRepStride = dstRepStrideIn;
        srcBlkStride = srcBlkStrideIn;
        srcRepStride = srcRepStrideIn;
    }

    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    uint64_t bitMask[2] = {0, 0};
    int32_t normalMask = 0;
    int32_t maskMode = 0;
    int32_t repeatTimes = 0;
    int32_t dstRepStride = 0;
    int32_t srcBlkStride = 0;
    int32_t srcRepStride = 0;
};

struct DumpMessageHead {
    [aicore] DumpMessageHead()
    {
        type = 0;
        lenth = 0;
        addr = 0;
        dataType = 0;
        desc = 0;
        bufferId = 0;
        position = 0;
        rsv = 0;
    }

    [aicore] DumpMessageHead(uint32_t typeIn, uint32_t lenthIn, uint32_t addrIn, uint32_t dataTypeIn, uint32_t descIn,
        uint32_t bufferIdIn, uint32_t positionIn, uint32_t rsvIn)
    {
        type = typeIn;
        lenth = lenthIn;
        addr = addrIn;
        dataType = dataTypeIn;
        desc = descIn;
        bufferId = bufferIdIn;
        position = positionIn;
        rsv = rsvIn;
    }

    uint32_t type = 0;
    uint32_t lenth = 0;
    uint32_t addr = 0;
    uint32_t dataType = 0;
    uint32_t desc = 0;
    uint32_t bufferId = 0;
    uint32_t position = 0;
    uint32_t rsv = 0;
};

struct DumpShapeMessageHead {
    [aicore] DumpShapeMessageHead()
    {
        dim = 0;
        rsv = 0;
        for (uint32_t idx = 0; idx < 8; ++idx) {
            shape[idx] = 0;
        }
    }

    [aicore] DumpShapeMessageHead(uint32_t dimIn, uint32_t shapeIn[], uint32_t rsvIn = 0)
    {


          ;
        dim = dimIn;
        rsv = rsvIn;
        for (uint32_t idx = 0; idx < 8; ++idx) {
            if (idx < dim) {
                shape[idx] = shapeIn[idx];
            } else {
                shape[idx] = 0;
            }
        }
    }

    uint32_t dim = 0;
    uint32_t shape[8];
    uint32_t rsv = 0;
};

struct ProposalIntriParams {
    [aicore] ProposalIntriParams()
    {
        repeat = 0;
        modeNumber = 0;
    }

    [aicore] ProposalIntriParams(const int32_t repeatTimes, const int32_t modeNumberIn)
    {
        repeat = repeatTimes;
        modeNumber = modeNumberIn;
    }

    int32_t repeat = 0;
    int32_t modeNumber = 0;
};

struct BlockInfo {
    [aicore] BlockInfo()
    {
        len = 0;
        core = 0;
        blockNum = 0;
        dumpOffset = 0;
        magic = 0;
        rsv = 0;
        dumpAddr = 0;
    }
    [aicore] BlockInfo(uint64_t dumpAddrIn, uint32_t lenIn, uint32_t coreIn, uint32_t blockNumIn,
        uint32_t dumpOffsetIn, uint32_t magicIn, uint32_t rsvIn)
    {
        len = lenIn;
        core = coreIn;
        blockNum = blockNumIn;
        dumpOffset = dumpOffsetIn;
        magic = magicIn;
        rsv = rsvIn;
        dumpAddr = dumpAddrIn;
    }
    uint32_t len = 0;
    uint32_t core = 0;
    uint32_t blockNum = 0;
    uint32_t dumpOffset = 0;
    uint32_t magic = 0;
    uint32_t rsv = 0;
    uint64_t dumpAddr = 0;
};

struct DumpMeta {
    uint32_t typeId = static_cast<uint32_t>(DumpType::DUMP_META);
    uint32_t len = 8;
    uint16_t blockDim = 0;
    uint8_t coreType = 0;
    uint8_t taskRation = 0;
    uint32_t rsv = 0;
};

struct DumpTimeStamp {
    uint32_t typeId = static_cast<uint32_t>(DumpType::DUMP_TIME_STAMP);
    uint32_t len = 24;
    uint32_t descId = 0;
    uint32_t rsv = 0;
    uint64_t systemCycle = 0;
    uint64_t pcPtr = 0;
};
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_data_copy.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_data_copy.h"
namespace AscendC {
struct DataCopyParams {
    [aicore] DataCopyParams() {}

    [aicore] DataCopyParams(const uint16_t count, const uint16_t len, const uint16_t srcStrideIn,
        const uint16_t dstStrideIn)
        : blockCount(count),
          blockLen(len),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn)
    {}

    uint16_t blockCount = DEFAULT_DATA_COPY_NBURST;
    uint16_t blockLen = 0;
    uint16_t srcStride = DEFAULT_DATA_COPY_STRIDE;
    uint16_t dstStride = DEFAULT_DATA_COPY_STRIDE;
};

struct DataCopyEnhancedParams {
    [aicore] DataCopyEnhancedParams() {}

    [aicore] DataCopyEnhancedParams(const BlockMode blockModeIn, const DeqScale deqScaleIn, const uint64_t deqValueIn,
        const uint8_t sidStoreModeIn, const bool isReluIn, const pad_t padModeIn, const uint64_t padValueIn)
        : blockMode(blockModeIn),
          deqScale(deqScaleIn),
          deqValue(deqValueIn),
          sidStoreMode(sidStoreModeIn),
          isRelu(isReluIn),
          padMode(padModeIn),
          padValue(padValueIn)
    {}

    BlockMode blockMode = BlockMode::BLOCK_MODE_NORMAL;
    DeqScale deqScale = DeqScale::DEQ_NONE;
    uint64_t deqValue = 0;
    uint8_t sidStoreMode = 0;
    bool isRelu = false;
    pad_t padMode = pad_t::PAD_NONE;
    uint64_t padValue = 0;
    uint64_t deqTensorAddr = 0;
};

struct DataCopyCO12DstParams {
    [aicore] DataCopyCO12DstParams() {}

    [aicore] DataCopyCO12DstParams(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint32_t dstStrideIn,
        const uint16_t srcStrideIn, const QuantMode_t quantPreIn, const uint8_t reluPreIn, const bool channelSplitIn,
        const bool nz2ndEnIn)
        : nSize(nSizeIn),
          mSize(mSizeIn),
          dstStride(dstStrideIn),
          srcStride(srcStrideIn),
          quantPre(quantPreIn),
          reluPre(reluPreIn),
          channelSplit(channelSplitIn),
          nz2ndEn(nz2ndEnIn)
    {}

    uint8_t sid = 0;
    uint16_t nSize = 0;
    uint16_t mSize = 0;
    uint32_t dstStride = DEFAULT_DATA_COPY_STRIDE;
    uint16_t srcStride = DEFAULT_DATA_COPY_STRIDE;
    uint8_t unitFlag = 0;
    uint8_t clipReluPre = 0;
    uint8_t eltWiseOp = 0;
    QuantMode_t quantPre = QuantMode_t::NoQuant;
    uint8_t reluPre = 0;
    bool channelSplit = false;
    bool nz2ndEn = false;
};

struct DataCopyPadParams {
    [aicore] DataCopyPadParams() {}

    [aicore] DataCopyPadParams(const bool isPadValue, const uint8_t leftPadValue, const uint8_t rightPadValue,
        const uint64_t padValue)
        : isPad(isPadValue),
          leftPadding(leftPadValue),
          rightPadding(rightPadValue),
          paddingValue(padValue)
    {}

    bool isPad = false;
    uint8_t leftPadding = 0;
    uint8_t rightPadding = 0;
    uint64_t paddingValue = 0;
};

struct DataCopyExtParams {
    [aicore] DataCopyExtParams() {}

    [aicore] DataCopyExtParams(const uint16_t count, const uint32_t len, const uint32_t srcStrideIn,
        const uint32_t dstStrideIn, const uint32_t rsvIn)
        : blockCount(count),
          blockLen(len),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn),
          rsv(rsvIn)
    {}

    uint16_t blockCount = DEFAULT_DATA_COPY_NBURST;
    uint32_t blockLen = 0;
    uint32_t srcStride = DEFAULT_DATA_COPY_STRIDE;
    uint32_t dstStride = DEFAULT_DATA_COPY_STRIDE;
    uint32_t rsv = 0;
};

template <typename T>
struct DataCopyPadExtParams {
    [aicore] DataCopyPadExtParams() {}

    [aicore] DataCopyPadExtParams(const bool isPadValue, const uint8_t leftPadValue, const uint8_t rightPadValue,
        T padValue)
        : isPad(isPadValue),
          leftPadding(leftPadValue),
          rightPadding(rightPadValue),
          paddingValue(padValue)
    {}

    bool isPad = false;
    uint8_t leftPadding = 0;
    uint8_t rightPadding = 0;
    T paddingValue = 0;
};

struct Nd2NzParams {
    [aicore] Nd2NzParams() {}

    [aicore] Nd2NzParams(const uint16_t ndNumIn, const uint16_t nValueIn, const uint16_t dValueIn,
        const uint16_t srcNdMatrixStrideIn, const uint16_t srcDValueIn, const uint16_t dstNzC0StrideIn,
        const uint16_t dstNzNStrideIn, const uint16_t dstNzMatrixStrideIn)
        : ndNum(ndNumIn),
          nValue(nValueIn),
          dValue(dValueIn),
          srcNdMatrixStride(srcNdMatrixStrideIn),
          srcDValue(srcDValueIn),
          dstNzC0Stride(dstNzC0StrideIn),
          dstNzNStride(dstNzNStrideIn),
          dstNzMatrixStride(dstNzMatrixStrideIn)
    {}

    uint16_t ndNum = 0;
    uint16_t nValue = 0;
    uint16_t dValue = 0;
    uint16_t srcNdMatrixStride = 0;
    uint16_t srcDValue = 0;
    uint16_t dstNzC0Stride = 0;
    uint16_t dstNzNStride = 0;
    uint16_t dstNzMatrixStride = 0;
};

struct Nz2NdParamsFull {
    [aicore] Nz2NdParamsFull() {}

    [aicore] Nz2NdParamsFull(const uint16_t ndNumIn, const uint16_t nValueIn, const uint16_t dValueIn,
        const uint16_t srcNdMatrixStrideIn, const uint16_t srcNStrideIn, const uint16_t dstDStrideIn,
        const uint16_t dstNdMatrixStrideIn)
        : ndNum(ndNumIn),
          nValue(nValueIn),
          dValue(dValueIn),
          srcNdMatrixStride(srcNdMatrixStrideIn),
          srcNStride(srcNStrideIn),
          dstDStride(dstDStrideIn),
          dstNdMatrixStride(dstNdMatrixStrideIn)
    {}

    uint16_t ndNum = 1;
    uint16_t nValue = 0;
    uint16_t dValue = 0;
    uint16_t srcNdMatrixStride = 1;
    uint16_t srcNStride = 0;
    uint16_t dstDStride = 0;
    uint16_t dstNdMatrixStride = 1;
};

struct SliceInfo {
    [aicore] SliceInfo() {}

    [aicore] SliceInfo(const uint32_t startIndexIn, const uint32_t endIndexIn, const uint32_t strideIn,
        const uint32_t burstLenIn, const uint32_t shapeValueIn = 0)
        : startIndex(startIndexIn),
          endIndex(endIndexIn),
          stride(strideIn),
          burstLen(burstLenIn),
          shapeValue(shapeValueIn)
    {}

    uint32_t startIndex = 0;
    uint32_t endIndex = ONE_BLK_SIZE - 1;
    uint32_t stride = 0;
    uint32_t burstLen = ONE_BLK_SIZE;
    uint32_t shapeValue = 0;
};

struct CopyRepeatParams {
    [aicore] CopyRepeatParams() {}

    [aicore] CopyRepeatParams(const uint16_t dstStrideIn, const uint16_t srcStrideIn, uint16_t dstRepeatSizeIn,
        uint16_t srcRepeatSizeIn)
        : dstStride(dstStrideIn),
          srcStride(srcStrideIn),
          dstRepeatSize(dstRepeatSizeIn),
          srcRepeatSize(srcRepeatSizeIn)
    {}

    uint16_t dstStride = DEFAULT_DATA_COPY_STRIDE;
    uint16_t srcStride = DEFAULT_DATA_COPY_STRIDE;
    uint16_t dstRepeatSize = DEFAULT_REPEAT_STRIDE;
    uint16_t srcRepeatSize = DEFAULT_REPEAT_STRIDE;
};
}
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2

namespace AscendC {
class AscendCUtils {
public:
    [aicore] static __inline__ __attribute__((always_inline)) int32_t GetBitSize(int32_t byteSize)
    {
        return byteSize * ONE_BYTE_BIT_SIZE;
    }

    [aicore] static __inline__ __attribute__((always_inline)) int32_t GetC0Size()
    {
        return DEFAULT_C0_SIZE;
    }

    [aicore] static __inline__ __attribute__((always_inline)) int32_t GetC0Count(const int32_t dtypeSize)
    {
                                                                                                 ;
        return GetC0Size() / dtypeSize;
    }

    [aicore] static __inline__ __attribute__((always_inline)) int32_t GetDefaultBlockNum()
    {
        return DEFAULT_BLK_NUM;
    }

    [aicore] static __inline__ __attribute__((always_inline)) int64_t GetRsvdCnt()
    {
        return get_rsvd_cnt();
    }

    template <typename T, bool isSetMask = true>
    [aicore] static __inline__ __attribute__((always_inline)) void SetMask(const uint64_t& maskHigh, const uint64_t& maskLow)
    {
        if constexpr (!isSetMask) {
            return;
        }

        if constexpr(g_coreType != AscendC::AIC) {
            set_vector_mask(maskHigh, maskLow);
        }
    }

    template <typename T, bool isSetMask = true> [aicore] static __inline__ __attribute__((always_inline)) void SetMask(int32_t len)
    {
        if constexpr (!isSetMask) {
            return;
        }

        int32_t typeLen = 0;
        if constexpr (IsSameType<T, int4b_t>::value) {
            typeLen = DEFAULT_BLOCK_SIZE * INT4_TWO;
        } else {
            typeLen = DEFAULT_BLOCK_SIZE / sizeof(T);
        }
        constexpr int32_t halfTypeLen = 64;
        constexpr int32_t lenCoeff = 2;
        if (len == halfTypeLen) {
            SetMask<T>(0, FULL_MASK);
            return;
        } else if (len == typeLen || len >= halfTypeLen * lenCoeff) {
            SetMask<T>(FULL_MASK, FULL_MASK);
            return;
        }
        SetMask<T>(static_cast<uint64_t>(
            (len > halfTypeLen) ? (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(len - halfTypeLen)) - 1) : 0),
            static_cast<uint64_t>(
            (len > halfTypeLen) ? FULL_MASK : (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(len)) - 1)));
    }

    template <typename T> [aicore] static __inline__ __attribute__((always_inline)) void SetMaskCount()
    {
        set_mask_count();
    }

    template <typename T> [aicore] static __inline__ __attribute__((always_inline)) void SetMaskNorm()
    {
        set_mask_norm();
    }


    [aicore] static __inline__ __attribute__((always_inline)) void SetOverflow(uint64_t ctrlValue)
    {


        if (ctrlValue == 1) {
            set_ctrl(sbitset1(get_ctrl(), CTRL_48_BIT));
        } else {
            set_ctrl(sbitset0(get_ctrl(), CTRL_48_BIT));
        }
    }
# 137 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
    template <bool isSetMask = true> [aicore] static __inline__ __attribute__((always_inline)) void ResetMask()
    {
        if constexpr (!isSetMask) {
            return;
        }
        if constexpr(g_coreType != AscendC::AIC) {
            set_vector_mask(FULL_MASK, FULL_MASK);
        }
    }

    template <bool isInt4 = false>
    [aicore] __inline__ __attribute__((always_inline)) static IntriInfo CalIntriInfo(
        const uint32_t dtypeSize, const uint32_t calCount, uint32_t repStride = DEFAULT_BLK_NUM)
    {
        IntriInfo retIntriInfo;
        retIntriInfo.c0Count = GetC0Count(dtypeSize);
        if constexpr (isInt4) {
            retIntriInfo.c0Count = GetC0Size() * INT4_TWO;
        }
        uint32_t repeatCount = repStride * retIntriInfo.c0Count;
        retIntriInfo.repeat = calCount / repeatCount;
        retIntriInfo.tail = calCount % repeatCount;
        retIntriInfo.repeatRounding = retIntriInfo.repeat / MAX_REPEAT_TIMES;
        retIntriInfo.repeatRemaining = retIntriInfo.repeat % MAX_REPEAT_TIMES;

        return retIntriInfo;
    }

    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) __attribute__((cce_unif_buff)) T* GetTemporaryBufferAddr(const int32_t bufferOffset, const int32_t bufferSize)
    {
# 183 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
        (void)bufferSize;
        __attribute__((cce_unif_buff)) T* addr = reinterpret_cast<__attribute__((cce_unif_buff)) T*>((uint64_t)(0) + bufferOffset);

        return addr;
    }

    template <typename T> [aicore] static __inline__ __attribute__((always_inline)) void FreeTemporaryBuffer(__attribute__((cce_unif_buff)) T* addr)
    {
        (void)addr;
    }


    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) __attribute__((cce_fixpipe_buff)) T* GetTemporaryFbBufferAddr(const int32_t bufferOffset, const int32_t bufferSize)
    {
# 209 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
        (void)bufferSize;
        __attribute__((cce_fixpipe_buff)) T* addr = reinterpret_cast<__attribute__((cce_fixpipe_buff)) T*>((uint64_t)(0) + bufferOffset);

        return addr;
    }

    template <typename T> [aicore] static __inline__ __attribute__((always_inline)) void FreeTemporaryFbBuffer(__attribute__((cce_fixpipe_buff)) T* addr)
    {
        (void)addr;
    }


    [aicore] static __inline__ __attribute__((always_inline)) uint64_t GetGMLen(const DataCopyParams& intriParams, const bool& isSrc,
        const bool& isMovAlignIntri)
    {
        uint16_t stride = intriParams.dstStride;
        uint16_t burstLenUnit = 32;
        uint16_t strideUnit = 32;
        if (isSrc) {
            stride = intriParams.srcStride;
        }
        if (isMovAlignIntri) {
            burstLenUnit = 1;
            strideUnit = 1;
        }
        if (intriParams.blockLen == 0) {
            return 0;
        }
        uint64_t gmLen = static_cast<uint64_t>(intriParams.blockCount) * intriParams.blockLen * burstLenUnit +
            (intriParams.blockCount - 1) * stride * strideUnit;
        return gmLen;
    }

    [aicore] static __inline__ __attribute__((always_inline)) uint64_t GetGMLen(const DataCopyExtParams& intriParams, const bool& isSrc,
        const bool& isMovAlignIntri)
    {
        uint16_t stride = intriParams.dstStride;
        uint16_t burstLenUnit = 32;
        uint16_t strideUnit = 32;
        if (isSrc) {
            stride = intriParams.srcStride;
        }
        if (isMovAlignIntri) {
            burstLenUnit = 1;
            strideUnit = 1;
        }
        if (intriParams.blockLen == 0) {
            return 0;
        }
        uint64_t gmLen = static_cast<uint64_t>(intriParams.blockCount) * intriParams.blockLen * burstLenUnit +
            (intriParams.blockCount - 1) * stride * strideUnit;
        return gmLen;
    }

    [aicore] static __inline__ __attribute__((always_inline)) uint64_t GetGMLen(const uint64_t& srcEleSize, const Nd2NzParams& intriParams)
    {
        uint64_t gmLen = (static_cast<uint64_t>(intriParams.ndNum) - 1) * srcEleSize * intriParams.srcNdMatrixStride +
            (intriParams.nValue - 1) * intriParams.srcDValue * srcEleSize + intriParams.dValue * srcEleSize;
        return gmLen;
    }

    [aicore] static __inline__ __attribute__((always_inline)) bool OOMCheckAddrIsOverflow(uintptr_t gmAddrConvert, const uint64_t& gmLen)
    {
# 298 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
        return false;
    }

    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) void CheckGmMemOverflow(__attribute__((cce_global)) T* gmAddr, const bool& isSrc,
        const uint64_t& gmLen)
    {
# 335 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
    }

    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) void CheckGmMemOverflowNormal(__attribute__((cce_global)) T *gmAddr, __attribute__((cce_global)) uint8_t *workSpace,
        const bool isSrc, const bool isMovAlignIntri, const DataCopyParams& intriParams)
    {
        (void)(workSpace);
        uint64_t gmLen = GetGMLen(intriParams, isSrc, isMovAlignIntri);
        CheckGmMemOverflow(gmAddr, isSrc, gmLen);
    }

    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) void CheckGmMemOverflowNormal(__attribute__((cce_global)) T* gmAddr, __attribute__((cce_global)) uint8_t* workSpace,
        const bool isSrc, const bool isMovAlignIntri, const DataCopyExtParams& intriParams)
    {
        (void)(workSpace);
        uint64_t gmLen = GetGMLen(intriParams, isSrc, isMovAlignIntri);
        CheckGmMemOverflow(gmAddr, isSrc, gmLen);
    }

    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) void CheckGmMemOverflowNd2Nz(__attribute__((cce_global)) T* gmAddr, __attribute__((cce_global)) uint8_t* workSpace,
        const bool isSrc, const Nd2NzParams& intriParams)
    {
        (void)(workSpace);
        uint64_t srcEleSize = sizeof(T);
        uint64_t gmLen = GetGMLen(srcEleSize, intriParams);
        CheckGmMemOverflow(gmAddr, isSrc, gmLen);
    }
};
# 500 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
constexpr uint32_t BF16_TO_FP32_MAN_LEN = 16;
[aicore] __inline__ __attribute__((always_inline)) bfloat16_t ToBfloat16(const float& fVal)
{
    float fNum = fVal;
    union ToBfloat16Union {
        [aicore] ToBfloat16Union() {}
        uint16_t val;
        bfloat16_t bNum;
    } bf16Union;
    union FloattoInt32Union {
        [aicore] FloattoInt32Union() {}
        float ftmp;
        uint32_t uret;
    } int32Union;
    int32Union.ftmp = fNum;
    bf16Union.val = int32Union.uret >> BF16_TO_FP32_MAN_LEN;
    return bf16Union.bNum;
}

[aicore] __inline__ __attribute__((always_inline)) float ToFloat(const bfloat16_t& bVal)
{
    bfloat16_t bNum = bVal;
    union ToFloatUnion {
        [aicore] ToFloatUnion() {}
        uint32_t val;
        float fNum;
    } floatUnion;
    union ToUint16Union {
        [aicore] ToUint16Union() {}
        bfloat16_t uret;
        uint16_t num;
    } u16Union;
    u16Union.uret = bNum;
    floatUnion.val = u16Union.num << BF16_TO_FP32_MAN_LEN;
    return floatUnion.fNum;
}

}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_reg.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_reg.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_aipp.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_aipp.h"
namespace AscendC {
enum class AippInputFormat : uint8_t {
    YUV420SP_U8 = 0,
    XRGB8888_U8 = 1,
    RGB888_U8 = 4,
    YUV400_U8 = 9,
};

template <typename U>
struct AippPaddingParams {
    uint32_t paddingMode{ 0 };
    U paddingValueCh0{ 0 };
    U paddingValueCh1{ 0 };
    U paddingValueCh2{ 0 };
    U paddingValueCh3{ 0 };
};

struct AippSwapParams {
    bool isSwapRB{ false };
    bool isSwapUV{ false };
    bool isSwapAX{ false };
};

struct AippSingleLineParams {
    bool isSingleLineCopy{ false };
};

struct AippDataTypeConvParams {
    uint8_t dtcMeanCh0{ 0 };
    uint8_t dtcMeanCh1{ 0 };
    uint8_t dtcMeanCh2{ 0 };
    __cce_half dtcMinCh0{ 0 };
    __cce_half dtcMinCh1{ 0 };
    __cce_half dtcMinCh2{ 0 };
    __cce_half dtcVarCh0{ 1.0 };
    __cce_half dtcVarCh1{ 1.0 };
    __cce_half dtcVarCh2{ 1.0 };
    uint32_t dtcRoundMode{ 0 };
};

template <typename U>
struct AippChannelPaddingParams {
    uint32_t cPaddingMode;
    U cPaddingValue;
};

struct AippColorSpaceConvParams {
    bool isEnableCsc{ false };
    int16_t cscMatrixR0C0{ 0 };
    int16_t cscMatrixR0C1{ 0 };
    int16_t cscMatrixR0C2{ 0 };
    int16_t cscMatrixR1C0{ 0 };
    int16_t cscMatrixR1C1{ 0 };
    int16_t cscMatrixR1C2{ 0 };
    int16_t cscMatrixR2C0{ 0 };
    int16_t cscMatrixR2C1{ 0 };
    int16_t cscMatrixR2C2{ 0 };
    uint8_t cscBiasIn0{ 0 };
    uint8_t cscBiasIn1{ 0 };
    uint8_t cscBiasIn2{ 0 };
    uint8_t cscBiasOut0{ 0 };
    uint8_t cscBiasOut1{ 0 };
    uint8_t cscBiasOut2{ 0 };
};


template <typename U>
struct AippParams {
    AippPaddingParams<U> paddingParams;
    AippSwapParams swapParams;
    AippSingleLineParams singleLineParams;
    AippDataTypeConvParams dtcParams;
    AippChannelPaddingParams<U> cPaddingParams;
    AippColorSpaceConvParams cscParams;
};

}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_reg.h" 2

namespace AscendC {
constexpr uint64_t MASK_PLACEHOLDER = 0;
constexpr uint64_t MASK_PLACEHOLDER_LIST[2] = {0, 0};

enum class MaskMode : uint8_t {
    NORMAL = 0,
    COUNTER
};

template <typename T, MaskMode mode>
[aicore] static __inline__ __attribute__((always_inline)) void SetVectorMaskImpl(const uint64_t maskHigh, const uint64_t maskLow)
{
    if constexpr(g_coreType != AscendC::AIC) {
        set_vector_mask(maskHigh, maskLow);
    }
}

template <typename T, MaskMode mode>
[aicore] static __inline__ __attribute__((always_inline)) void SetVectorMaskImpl(int32_t len)
{
    if constexpr (mode == MaskMode::COUNTER) {
        SetVectorMaskImpl<T, mode>(0, len);
        return;
    }
    AscendCUtils::SetMask<T>(len);
}

[aicore] __inline__ __attribute__((always_inline)) void ResetMaskImpl()
{
    if constexpr(g_coreType != AscendC::AIC) {
        set_vector_mask(FULL_MASK, FULL_MASK);
    }
}

template <pipe_t pipe> [aicore] __inline__ __attribute__((always_inline)) void PipeBarrierImpl()
{
# 73 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_reg.h"
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (pipe == PIPE_V) {
            return;
        }
    }

    pipe_barrier(pipe);
}

enum class CacheLine : uint64_t {
    SINGLE_CACHE_LINE = 0,
    ENTIRE_DATA_CACHE
};

enum class DcciDst : uint64_t {
    CACHELINE_ALL = 0,
    CACHELINE_UB,
    CACHELINE_OUT,
    CACHELINE_ATOMIC
};


template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] __inline__ __attribute__((always_inline)) void DcciGMImpl(__attribute__((cce_global)) T* dst)
{
    dcci(static_cast<__attribute__((cce_global)) void *>(dst), static_cast<uint64_t>(entireType), static_cast<uint64_t>(dcciDst));
}

template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] __inline__ __attribute__((always_inline)) void DcciUBImpl(__attribute__((cce_unif_buff)) T* dst)
{
    dcci(static_cast<__attribute__((cce_unif_buff)) void *>(dst), static_cast<uint64_t>(entireType), static_cast<uint64_t>(dcciDst));
}



template <typename T, CacheLine entireType>
[aicore] __inline__ __attribute__((always_inline)) void DcciGMImpl(__attribute__((cce_global)) T* dst)
{
    dcci(static_cast<__attribute__((cce_global)) void *>(dst), static_cast<uint64_t>(entireType));
}


[aicore] __inline__ __attribute__((always_inline)) void SetMaskCountImpl()
{
    set_mask_count();
}

[aicore] __inline__ __attribute__((always_inline)) void SetMaskNormImpl()
{
    set_mask_norm();
}

[aicore] __inline__ __attribute__((always_inline)) void SetLreluMode(bool lreluMode)
{
    if (lreluMode) {
        set_ctrl(sbitset1(get_ctrl(), LEAKY_RELU_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), LEAKY_RELU_MODE_BIT));
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetHF32ModeImpl(bool hf32Mode)
{
    if (hf32Mode) {
        set_ctrl(sbitset1(get_ctrl(), HF32_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), HF32_MODE_BIT));
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetHF32TransModeImpl(bool hf32TransMode)
{
    if (hf32TransMode) {
        set_ctrl(sbitset1(get_ctrl(), HF32_TRANS_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), HF32_TRANS_MODE_BIT));
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetMMLayoutTransformImpl(bool mmLayoutMode)
{
    if (mmLayoutMode) {
        set_ctrl(sbitset1(get_ctrl(), MM_LAYOUT_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), MM_LAYOUT_MODE_BIT));
    }
}

template <bool castMode>
[aicore] __inline__ __attribute__((always_inline)) void SetCastOverflowModeImpl()
{
    if constexpr (castMode) {
        set_ctrl(sbitset1(get_ctrl(), CAST_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), CAST_MODE_BIT));
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl0(__attribute__((cce_global)) T* src0)
{
    uint64_t aippConfig0 = reinterpret_cast<uint64_t>(src0) & 0xffffffffffff;

    set_aipp_spr_0(aippConfig0);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl1(__attribute__((cce_global)) T* src1, AippParams<U>& config)
{
    uint64_t aippConfig1 = reinterpret_cast<uint64_t>(src1) & 0xffffffffffff;

    if (config.cscParams.isEnableCsc) {
        aippConfig1 |= static_cast<uint64_t>(1) << AIPP_OFFSET_CSC_ENABLE;
    }

    set_aipp_spr_1(aippConfig1);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl2(AippParams<U>& config)
{
    uint16_t cscMatrixR0C0 = GetScalarBitcodeValue(config.cscParams.cscMatrixR0C0);
    uint16_t cscMatrixR0C1 = GetScalarBitcodeValue(config.cscParams.cscMatrixR0C1);
    uint16_t cscMatrixR0C2 = GetScalarBitcodeValue(config.cscParams.cscMatrixR0C2);
    uint16_t cscMatrixR1C0 = GetScalarBitcodeValue(config.cscParams.cscMatrixR1C0);

    uint64_t aippConfig2 = static_cast<uint64_t>(cscMatrixR0C0);
    aippConfig2 |= static_cast<uint64_t>(cscMatrixR0C1) << AIPP_OFFSET_CH1;
    aippConfig2 |= static_cast<uint64_t>(cscMatrixR0C2) << AIPP_OFFSET_CH2;
    aippConfig2 |= static_cast<uint64_t>(cscMatrixR1C0) << AIPP_OFFSET_CH3;

    set_aipp_spr_2(aippConfig2);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl3(AippParams<U>& config)
{
    uint16_t cscMatrixR1C1 = GetScalarBitcodeValue(config.cscParams.cscMatrixR1C1);
    uint16_t cscMatrixR1C2 = GetScalarBitcodeValue(config.cscParams.cscMatrixR1C2);
    uint16_t cscMatrixR2C0 = GetScalarBitcodeValue(config.cscParams.cscMatrixR2C0);
    uint16_t cscMatrixR2C1 = GetScalarBitcodeValue(config.cscParams.cscMatrixR2C1);

    uint64_t aippConfig3 = static_cast<uint64_t>(cscMatrixR1C1);
    aippConfig3 |= static_cast<uint64_t>(cscMatrixR1C2) << AIPP_OFFSET_CH1;
    aippConfig3 |= static_cast<uint64_t>(cscMatrixR2C0) << AIPP_OFFSET_CH2;
    aippConfig3 |= static_cast<uint64_t>(cscMatrixR2C1) << AIPP_OFFSET_CH3;

    set_aipp_spr_3(aippConfig3);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl4(AippParams<U>& config)
{
    uint16_t cscMatrixR2C2 = GetScalarBitcodeValue(config.cscParams.cscMatrixR2C2);
    uint8_t cscBiasOut0 = GetScalarBitcodeValue(config.cscParams.cscBiasOut0);
    uint8_t cscBiasOut1 = GetScalarBitcodeValue(config.cscParams.cscBiasOut1);
    uint8_t cscBiasOut2 = GetScalarBitcodeValue(config.cscParams.cscBiasOut2);
    uint8_t cscBiasIn0 = GetScalarBitcodeValue(config.cscParams.cscBiasIn0);
    uint8_t cscBiasIn1 = GetScalarBitcodeValue(config.cscParams.cscBiasIn1);
    uint8_t cscBiasIn2 = GetScalarBitcodeValue(config.cscParams.cscBiasIn2);

    uint64_t aippConfig4 = static_cast<uint64_t>(cscMatrixR2C2);
    aippConfig4 |= static_cast<uint64_t>(cscBiasOut0) << AIPP_OFFSET_CSC_OUT_CH0;
    aippConfig4 |= static_cast<uint64_t>(cscBiasOut1) << AIPP_OFFSET_CSC_OUT_CH1;
    aippConfig4 |= static_cast<uint64_t>(cscBiasOut2) << AIPP_OFFSET_CSC_OUT_CH2;
    aippConfig4 |= static_cast<uint64_t>(cscBiasIn0) << AIPP_OFFSET_CSC_IN_CH0;
    aippConfig4 |= static_cast<uint64_t>(cscBiasIn1) << AIPP_OFFSET_CSC_IN_CH1;
    aippConfig4 |= static_cast<uint64_t>(cscBiasIn2) << AIPP_OFFSET_CSC_IN_CH2;

    set_aipp_spr_4(aippConfig4);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl5(AippParams<U>& config)
{



    uint8_t dtcMeanCh0 = GetScalarBitcodeValue(config.dtcParams.dtcMeanCh0);
    uint8_t dtcMeanCh1 = GetScalarBitcodeValue(config.dtcParams.dtcMeanCh1);
    uint8_t dtcMeanCh2 = GetScalarBitcodeValue(config.dtcParams.dtcMeanCh2);

    uint64_t aippConfig5 = static_cast<uint64_t>(dtcMeanCh0);
    aippConfig5 |= static_cast<uint64_t>(dtcMeanCh1) << AIPP_OFFSET_CH1;
    aippConfig5 |= static_cast<uint64_t>(dtcMeanCh2) << AIPP_OFFSET_CH2;

    set_aipp_spr_5(aippConfig5);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl6(AippParams<U>& config)
{



    uint16_t dtcMinCh0 = GetScalarBitcodeValue(config.dtcParams.dtcMinCh0);
    uint16_t dtcMinCh1 = GetScalarBitcodeValue(config.dtcParams.dtcMinCh1);
    uint16_t dtcMinCh2 = GetScalarBitcodeValue(config.dtcParams.dtcMinCh2);

    uint64_t aippConfig6 = static_cast<uint64_t>(dtcMinCh0);
    aippConfig6 |= static_cast<uint64_t>(dtcMinCh1) << AIPP_OFFSET_CH1;
    aippConfig6 |= static_cast<uint64_t>(dtcMinCh2) << AIPP_OFFSET_CH2;

    set_aipp_spr_6(aippConfig6);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl7(AippParams<U>& config)
{



    uint16_t dtcVarCh0 = GetScalarBitcodeValue(config.dtcParams.dtcVarCh0);
    uint16_t dtcVarCh1 = GetScalarBitcodeValue(config.dtcParams.dtcVarCh1);
    uint16_t dtcVarCh2 = GetScalarBitcodeValue(config.dtcParams.dtcVarCh2);

    uint64_t aippConfig7 = static_cast<uint64_t>(dtcVarCh0);
    aippConfig7 |= static_cast<uint64_t>(dtcVarCh1) << AIPP_OFFSET_CH1;
    aippConfig7 |= static_cast<uint64_t>(dtcVarCh2) << AIPP_OFFSET_CH2;

    set_aipp_spr_7(aippConfig7);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl8(AippParams<U>& config)
{
    uint64_t aippConfig8 = 0;
    if constexpr(IsSameType<U, int8_t>::value || IsSameType<U, uint8_t>::value) {
        uint8_t paddingValueCh0 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh0);
        uint8_t paddingValueCh1 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh1);
        uint8_t paddingValueCh2 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh2);
        uint8_t paddingValueCh3 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh3);

        aippConfig8 |= static_cast<uint64_t>(paddingValueCh0);
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh1) << AIPP_OFFSET_CH1;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh2) << AIPP_OFFSET_CH2;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh3) << AIPP_OFFSET_CH3;
    } else {
        uint16_t paddingValueCh0 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh0);
        uint16_t paddingValueCh1 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh1);
        uint16_t paddingValueCh2 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh2);
        uint16_t paddingValueCh3 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh3);

        aippConfig8 |= static_cast<uint64_t>(paddingValueCh0);
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh1) << AIPP_OFFSET_CH1;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh2) << AIPP_OFFSET_CH2;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh3) << AIPP_OFFSET_CH3;
    }

    set_aipp_spr_8(aippConfig8);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl9(AippInputFormat format, AippParams<U>& config)
{
    uint64_t aippConfig9 = 0;

    if constexpr(IsSameType<U, int8_t>::value || IsSameType<U, uint8_t>::value) {
        uint8_t cPaddingValue = GetScalarBitcodeValue(config.cPaddingParams.cPaddingValue);
        aippConfig9 |= static_cast<uint64_t>(cPaddingValue);
    } else {
        uint16_t cPaddingValue = GetScalarBitcodeValue(config.cPaddingParams.cPaddingValue);
        aippConfig9 |= static_cast<uint64_t>(cPaddingValue);
    }

    if (config.swapParams.isSwapRB) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SWAP_RB;
    }
    if (config.swapParams.isSwapUV) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SWAP_UV;
    }
    if (config.swapParams.isSwapAX) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SWAP_AX;
    }

    aippConfig9 |= (static_cast<uint64_t>(format) & 0x1f) << AIPP_OFFSET_FORMAT;

    if (config.singleLineParams.isSingleLineCopy) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SINGLE_LINE;
    }

    aippConfig9 |= (static_cast<uint64_t>(config.paddingParams.paddingMode) & 0x3) << AIPP_OFFSET_PADDING_MODE;





    aippConfig9 |= (static_cast<uint64_t>(config.cPaddingParams.cPaddingMode) & 0x1) << AIPP_OFFSET_CPADDING_MODE;

    set_aipp_spr_9(aippConfig9);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl18(AippParams<U>& config)
{

    return;

    float dtcVarCh0f = static_cast<float>(config.dtcParams.dtcVarCh0);
    float dtcVarCh1f = static_cast<float>(config.dtcParams.dtcVarCh1);
    uint32_t dtcVarCh0 = GetScalarBitcodeValue(dtcVarCh0f);
    uint32_t dtcVarCh1 = GetScalarBitcodeValue(dtcVarCh1f);

    uint64_t aippConfig18 = static_cast<uint64_t>(dtcVarCh0);
    aippConfig18 |= static_cast<uint64_t>(dtcVarCh1) << AIPP_OFFSET_DTC_CH1;

    set_aipp_spr_18(aippConfig18);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl19(AippParams<U>& config)
{

    return;

    float dtcVarCh2f = static_cast<float>(config.dtcParams.dtcVarCh2);
    uint32_t dtcVarCh2 = GetScalarBitcodeValue(dtcVarCh2f);
    uint64_t aippConfig19 = static_cast<uint64_t>(dtcVarCh2);
    set_aipp_spr_19(aippConfig19);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl20(AippParams<U>& config)
{

    return;

    float dtcMeanCh0f = static_cast<float>(config.dtcParams.dtcMeanCh0 * 1.0f);
    float dtcMeanCh1f = static_cast<float>(config.dtcParams.dtcMeanCh1 * 1.0f);

    uint32_t dtcMeanCh0 = GetScalarBitcodeValue(dtcMeanCh0f);
    uint32_t dtcMeanCh1 = GetScalarBitcodeValue(dtcMeanCh1f);

    uint64_t aippConfig20 = static_cast<uint64_t>(dtcMeanCh0);
    aippConfig20 |= static_cast<uint64_t>(dtcMeanCh1) << AIPP_OFFSET_DTC_CH1;

    set_aipp_spr_20(aippConfig20);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl21(AippParams<U>& config)
{

    return;

    float dtcMeanCh2f = static_cast<float>(config.dtcParams.dtcMeanCh2 * 1.0f);
    uint32_t dtcMeanCh2 = GetScalarBitcodeValue(dtcMeanCh2f);
    uint64_t aippConfig21 = static_cast<uint64_t>(dtcMeanCh2);
    set_aipp_spr_21(aippConfig21);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl(__attribute__((cce_global)) T* src0, __attribute__((cce_global)) T* src1,
    AippInputFormat format, AippParams<U>& config)
{

    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
# 448 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_reg.h"
    SetAippFunctionsImpl0<T>(src0);
    SetAippFunctionsImpl1<T, U>(src1, config);
    SetAippFunctionsImpl2<U>(config);
    SetAippFunctionsImpl3<U>(config);
    SetAippFunctionsImpl4<U>(config);
    SetAippFunctionsImpl5<U>(config);
    SetAippFunctionsImpl6<U>(config);
    SetAippFunctionsImpl7<U>(config);
    SetAippFunctionsImpl8<U>(config);
    SetAippFunctionsImpl9<U>(format, config);

}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl(__attribute__((cce_global)) T* src0, AippInputFormat format, AippParams<U> config)
{

    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }

    SetAippFunctionsImpl(src0, reinterpret_cast<__attribute__((cce_global)) T*>(0), format, config);
}


}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_process_lock.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_tensor_trait.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_tensor_trait.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_coord.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_coord.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_layout.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_layout.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/tuple.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/tuple.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/tuple_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/tuple_impl.h"
# 1 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 1 3
# 33 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3







namespace std __attribute__ ((__visibility__ ("default")))
{


  template<typename... _Elements>
    class tuple;

  template<typename _Tp>
    class reference_wrapper;
# 64 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<typename _Tp, _Tp __v>
    struct integral_constant
    {
      static constexpr _Tp value = __v;
      typedef _Tp value_type;
      typedef integral_constant<_Tp, __v> type;
      constexpr operator value_type() const noexcept { return value; }




      constexpr value_type operator()() const noexcept { return value; }

    };

  template<typename _Tp, _Tp __v>
    constexpr _Tp integral_constant<_Tp, __v>::value;


  using true_type = integral_constant<bool, true>;


  using false_type = integral_constant<bool, false>;



  template<bool __v>
    using __bool_constant = integral_constant<bool, __v>;






  template<bool __v>
    using bool_constant = integral_constant<bool, __v>;




  template<bool, typename, typename>
    struct conditional;


  template <typename _Type>
    struct __type_identity
    { using type = _Type; };

  template<typename _Tp>
    using __type_identity_t = typename __type_identity<_Tp>::type;

  template<typename...>
    struct __or_;

  template<>
    struct __or_<>
    : public false_type
    { };

  template<typename _B1>
    struct __or_<_B1>
    : public _B1
    { };

  template<typename _B1, typename _B2>
    struct __or_<_B1, _B2>
    : public conditional<_B1::value, _B1, _B2>::type
    { };

  template<typename _B1, typename _B2, typename _B3, typename... _Bn>
    struct __or_<_B1, _B2, _B3, _Bn...>
    : public conditional<_B1::value, _B1, __or_<_B2, _B3, _Bn...>>::type
    { };

  template<typename...>
    struct __and_;

  template<>
    struct __and_<>
    : public true_type
    { };

  template<typename _B1>
    struct __and_<_B1>
    : public _B1
    { };

  template<typename _B1, typename _B2>
    struct __and_<_B1, _B2>
    : public conditional<_B1::value, _B2, _B1>::type
    { };

  template<typename _B1, typename _B2, typename _B3, typename... _Bn>
    struct __and_<_B1, _B2, _B3, _Bn...>
    : public conditional<_B1::value, __and_<_B2, _B3, _Bn...>, _B1>::type
    { };

  template<typename _Pp>
    struct __not_
    : public __bool_constant<!bool(_Pp::value)>
    { };





  template<typename... _Bn>
    __inline__ __attribute__((always_inline)) constexpr bool __or_v = __or_<_Bn...>::value;
  template<typename... _Bn>
    __inline__ __attribute__((always_inline)) constexpr bool __and_v = __and_<_Bn...>::value;




  template<typename... _Bn>
    struct conjunction
    : __and_<_Bn...>
    { };

  template<typename... _Bn>
    struct disjunction
    : __or_<_Bn...>
    { };

  template<typename _Pp>
    struct negation
    : __not_<_Pp>
    { };




  template<typename... _Bn>
    __inline__ __attribute__((always_inline)) constexpr bool conjunction_v = conjunction<_Bn...>::value;

  template<typename... _Bn>
    __inline__ __attribute__((always_inline)) constexpr bool disjunction_v = disjunction<_Bn...>::value;

  template<typename _Pp>
    __inline__ __attribute__((always_inline)) constexpr bool negation_v = negation<_Pp>::value;





  template<typename>
    struct is_reference;
  template<typename>
    struct is_function;
  template<typename>
    struct is_void;
  template<typename>
    struct remove_cv;
  template<typename>
    struct is_const;


  template<typename>
    struct __is_array_unknown_bounds;




  template <typename _Tp, size_t = sizeof(_Tp)>
    constexpr true_type __is_complete_or_unbounded(__type_identity<_Tp>)
    { return {}; }

  template <typename _TypeIdentity,
      typename _NestedType = typename _TypeIdentity::type>
    constexpr typename __or_<
      is_reference<_NestedType>,
      is_function<_NestedType>,
      is_void<_NestedType>,
      __is_array_unknown_bounds<_NestedType>
    >::type __is_complete_or_unbounded(_TypeIdentity)
    { return {}; }






  template<typename _Tp>
    struct __success_type
    { typedef _Tp type; };

  struct __failure_type
  { };


  template<typename _Tp>
    using __remove_cv_t = typename remove_cv<_Tp>::type;



  template<typename>
    struct __is_void_helper
    : public false_type { };

  template<>
    struct __is_void_helper<void>
    : public true_type { };



  template<typename _Tp>
    struct is_void
    : public __is_void_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename>
    struct __is_integral_helper
    : public false_type { };

  template<>
    struct __is_integral_helper<bool>
    : public true_type { };

  template<>
    struct __is_integral_helper<char>
    : public true_type { };

  template<>
    struct __is_integral_helper<signed char>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned char>
    : public true_type { };





  template<>
    struct __is_integral_helper<wchar_t>
    : public true_type { };
# 310 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<>
    struct __is_integral_helper<char16_t>
    : public true_type { };

  template<>
    struct __is_integral_helper<char32_t>
    : public true_type { };

  template<>
    struct __is_integral_helper<short>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned short>
    : public true_type { };

  template<>
    struct __is_integral_helper<int>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned int>
    : public true_type { };

  template<>
    struct __is_integral_helper<long>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned long>
    : public true_type { };

  template<>
    struct __is_integral_helper<long long>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned long long>
    : public true_type { };
# 391 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<typename _Tp>
    struct is_integral
    : public __is_integral_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename>
    struct __is_floating_point_helper
    : public false_type { };

  template<>
    struct __is_floating_point_helper<float>
    : public true_type { };

  template<>
    struct __is_floating_point_helper<double>
    : public true_type { };

  template<>
    struct __is_floating_point_helper<long double>
    : public true_type { };
# 421 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<typename _Tp>
    struct is_floating_point
    : public __is_floating_point_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename>
    struct is_array
    : public false_type { };

  template<typename _Tp, std::size_t _Size>
    struct is_array<_Tp[_Size]>
    : public true_type { };

  template<typename _Tp>
    struct is_array<_Tp[]>
    : public true_type { };

  template<typename>
    struct __is_pointer_helper
    : public false_type { };

  template<typename _Tp>
    struct __is_pointer_helper<_Tp*>
    : public true_type { };


  template<typename _Tp>
    struct is_pointer
    : public __is_pointer_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename>
    struct is_lvalue_reference
    : public false_type { };

  template<typename _Tp>
    struct is_lvalue_reference<_Tp&>
    : public true_type { };


  template<typename>
    struct is_rvalue_reference
    : public false_type { };

  template<typename _Tp>
    struct is_rvalue_reference<_Tp&&>
    : public true_type { };

  template<typename>
    struct __is_member_object_pointer_helper
    : public false_type { };

  template<typename _Tp, typename _Cp>
    struct __is_member_object_pointer_helper<_Tp _Cp::*>
    : public __not_<is_function<_Tp>>::type { };


  template<typename _Tp>
    struct is_member_object_pointer
    : public __is_member_object_pointer_helper<__remove_cv_t<_Tp>>::type
    { };

  template<typename>
    struct __is_member_function_pointer_helper
    : public false_type { };

  template<typename _Tp, typename _Cp>
    struct __is_member_function_pointer_helper<_Tp _Cp::*>
    : public is_function<_Tp>::type { };


  template<typename _Tp>
    struct is_member_function_pointer
    : public __is_member_function_pointer_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_enum
    : public integral_constant<bool, __is_enum(_Tp)>
    { };


  template<typename _Tp>
    struct is_union
    : public integral_constant<bool, __is_union(_Tp)>
    { };


  template<typename _Tp>
    struct is_class
    : public integral_constant<bool, __is_class(_Tp)>
    { };


  template<typename _Tp>
    struct is_function
    : public __bool_constant<!is_const<const _Tp>::value> { };

  template<typename _Tp>
    struct is_function<_Tp&>
    : public false_type { };

  template<typename _Tp>
    struct is_function<_Tp&&>
    : public false_type { };



  template<typename>
    struct __is_null_pointer_helper
    : public false_type { };

  template<>
    struct __is_null_pointer_helper<std::nullptr_t>
    : public true_type { };


  template<typename _Tp>
    struct is_null_pointer
    : public __is_null_pointer_helper<__remove_cv_t<_Tp>>::type
    { };



  template<typename _Tp>
    struct __is_nullptr_t
    : public is_null_pointer<_Tp>
    { } __attribute__ ((__deprecated__ ("use '" "std::is_null_pointer" "' instead")));




  template<typename _Tp>
    struct is_reference
    : public __or_<is_lvalue_reference<_Tp>,
                   is_rvalue_reference<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_arithmetic
    : public __or_<is_integral<_Tp>, is_floating_point<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_fundamental
    : public __or_<is_arithmetic<_Tp>, is_void<_Tp>,
     is_null_pointer<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_object
    : public __not_<__or_<is_function<_Tp>, is_reference<_Tp>,
                          is_void<_Tp>>>::type
    { };

  template<typename>
    struct is_member_pointer;


  template<typename _Tp>
    struct is_scalar
    : public __or_<is_arithmetic<_Tp>, is_enum<_Tp>, is_pointer<_Tp>,
                   is_member_pointer<_Tp>, is_null_pointer<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_compound
    : public __not_<is_fundamental<_Tp>>::type { };


  template<typename _Tp>
    struct __is_member_pointer_helper
    : public false_type { };

  template<typename _Tp, typename _Cp>
    struct __is_member_pointer_helper<_Tp _Cp::*>
    : public true_type { };



  template<typename _Tp>
    struct is_member_pointer
    : public __is_member_pointer_helper<__remove_cv_t<_Tp>>::type
    { };

  template<typename, typename>
    struct is_same;


  template<typename _Tp, typename... _Types>
    using __is_one_of = __or_<is_same<_Tp, _Types>...>;


  template<typename _Tp>
    using __is_signed_integer = __is_one_of<__remove_cv_t<_Tp>,
   signed char, signed short, signed int, signed long,
   signed long long
# 637 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
   >;


  template<typename _Tp>
    using __is_unsigned_integer = __is_one_of<__remove_cv_t<_Tp>,
   unsigned char, unsigned short, unsigned int, unsigned long,
   unsigned long long
# 656 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
   >;


  template<typename _Tp>
    using __is_standard_integer
      = __or_<__is_signed_integer<_Tp>, __is_unsigned_integer<_Tp>>;


  template<typename...> using __void_t = void;



  template<typename _Tp, typename = void>
    struct __is_referenceable
    : public false_type
    { };

  template<typename _Tp>
    struct __is_referenceable<_Tp, __void_t<_Tp&>>
    : public true_type
    { };





  template<typename>
    struct is_const
    : public false_type { };

  template<typename _Tp>
    struct is_const<_Tp const>
    : public true_type { };


  template<typename>
    struct is_volatile
    : public false_type { };

  template<typename _Tp>
    struct is_volatile<_Tp volatile>
    : public true_type { };


  template<typename _Tp>
    struct is_trivial
    : public integral_constant<bool, __is_trivial(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_trivially_copyable
    : public integral_constant<bool, __is_trivially_copyable(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_standard_layout
    : public integral_constant<bool, __is_standard_layout(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };





  template<typename _Tp>
    struct

    is_pod
    : public integral_constant<bool, __is_pod(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };




  template<typename _Tp>
    struct
    [[__deprecated__]]
    is_literal_type
    : public integral_constant<bool, __is_literal_type(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_empty
    : public integral_constant<bool, __is_empty(_Tp)>
    { };


  template<typename _Tp>
    struct is_polymorphic
    : public integral_constant<bool, __is_polymorphic(_Tp)>
    { };





  template<typename _Tp>
    struct is_final
    : public integral_constant<bool, __is_final(_Tp)>
    { };



  template<typename _Tp>
    struct is_abstract
    : public integral_constant<bool, __is_abstract(_Tp)>
    { };


  template<typename _Tp,
    bool = is_arithmetic<_Tp>::value>
    struct __is_signed_helper
    : public false_type { };

  template<typename _Tp>
    struct __is_signed_helper<_Tp, true>
    : public integral_constant<bool, _Tp(-1) < _Tp(0)>
    { };



  template<typename _Tp>
    struct is_signed
    : public __is_signed_helper<_Tp>::type
    { };


  template<typename _Tp>
    struct is_unsigned
    : public __and_<is_arithmetic<_Tp>, __not_<is_signed<_Tp>>>
    { };


  template<typename _Tp, typename _Up = _Tp&&>
    _Up
    __declval(int);

  template<typename _Tp>
    _Tp
    __declval(long);


  template<typename _Tp>
    auto declval() noexcept -> decltype(__declval<_Tp>(0));

  template<typename, unsigned = 0>
    struct extent;

  template<typename>
    struct remove_all_extents;


  template<typename _Tp>
    struct __is_array_known_bounds
    : public integral_constant<bool, (extent<_Tp>::value > 0)>
    { };

  template<typename _Tp>
    struct __is_array_unknown_bounds
    : public __and_<is_array<_Tp>, __not_<extent<_Tp>>>
    { };
# 842 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  struct __do_is_destructible_impl
  {
    template<typename _Tp, typename = decltype(declval<_Tp&>().~_Tp())>
      static true_type __test(int);

    template<typename>
      static false_type __test(...);
  };

  template<typename _Tp>
    struct __is_destructible_impl
    : public __do_is_destructible_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp,
           bool = __or_<is_void<_Tp>,
                        __is_array_unknown_bounds<_Tp>,
                        is_function<_Tp>>::value,
           bool = __or_<is_reference<_Tp>, is_scalar<_Tp>>::value>
    struct __is_destructible_safe;

  template<typename _Tp>
    struct __is_destructible_safe<_Tp, false, false>
    : public __is_destructible_impl<typename
               remove_all_extents<_Tp>::type>::type
    { };

  template<typename _Tp>
    struct __is_destructible_safe<_Tp, true, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_destructible_safe<_Tp, false, true>
    : public true_type { };



  template<typename _Tp>
    struct is_destructible
    : public __is_destructible_safe<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };







  struct __do_is_nt_destructible_impl
  {
    template<typename _Tp>
      static __bool_constant<noexcept(declval<_Tp&>().~_Tp())>
      __test(int);

    template<typename>
      static false_type __test(...);
  };

  template<typename _Tp>
    struct __is_nt_destructible_impl
    : public __do_is_nt_destructible_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp,
           bool = __or_<is_void<_Tp>,
                        __is_array_unknown_bounds<_Tp>,
                        is_function<_Tp>>::value,
           bool = __or_<is_reference<_Tp>, is_scalar<_Tp>>::value>
    struct __is_nt_destructible_safe;

  template<typename _Tp>
    struct __is_nt_destructible_safe<_Tp, false, false>
    : public __is_nt_destructible_impl<typename
               remove_all_extents<_Tp>::type>::type
    { };

  template<typename _Tp>
    struct __is_nt_destructible_safe<_Tp, true, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nt_destructible_safe<_Tp, false, true>
    : public true_type { };



  template<typename _Tp>
    struct is_nothrow_destructible
    : public __is_nt_destructible_safe<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename... _Args>
    struct __is_constructible_impl
    : public __bool_constant<__is_constructible(_Tp, _Args...)>
    { };



  template<typename _Tp, typename... _Args>
    struct is_constructible
      : public __is_constructible_impl<_Tp, _Args...>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_default_constructible
    : public __is_constructible_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_copy_constructible_impl;

  template<typename _Tp>
    struct __is_copy_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_copy_constructible_impl<_Tp, true>
    : public __is_constructible_impl<_Tp, const _Tp&>
    { };



  template<typename _Tp>
    struct is_copy_constructible
    : public __is_copy_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_move_constructible_impl;

  template<typename _Tp>
    struct __is_move_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_move_constructible_impl<_Tp, true>
    : public __is_constructible_impl<_Tp, _Tp&&>
    { };



  template<typename _Tp>
    struct is_move_constructible
    : public __is_move_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename... _Args>
    using __is_nothrow_constructible_impl
      = __bool_constant<__is_nothrow_constructible(_Tp, _Args...)>;



  template<typename _Tp, typename... _Args>
    struct is_nothrow_constructible
    : public __is_nothrow_constructible_impl<_Tp, _Args...>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_nothrow_default_constructible
    : public __bool_constant<__is_nothrow_constructible(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nothrow_copy_constructible_impl;

  template<typename _Tp>
    struct __is_nothrow_copy_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nothrow_copy_constructible_impl<_Tp, true>
    : public __is_nothrow_constructible_impl<_Tp, const _Tp&>
    { };



  template<typename _Tp>
    struct is_nothrow_copy_constructible
    : public __is_nothrow_copy_constructible_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nothrow_move_constructible_impl;

  template<typename _Tp>
    struct __is_nothrow_move_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nothrow_move_constructible_impl<_Tp, true>
    : public __is_nothrow_constructible_impl<_Tp, _Tp&&>
    { };



  template<typename _Tp>
    struct is_nothrow_move_constructible
    : public __is_nothrow_move_constructible_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename _Up>
    struct is_assignable
    : public __bool_constant<__is_assignable(_Tp, _Up)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_copy_assignable_impl;

  template<typename _Tp>
    struct __is_copy_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_copy_assignable_impl<_Tp, true>
    : public __bool_constant<__is_assignable(_Tp&, const _Tp&)>
    { };


  template<typename _Tp>
    struct is_copy_assignable
    : public __is_copy_assignable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_move_assignable_impl;

  template<typename _Tp>
    struct __is_move_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_move_assignable_impl<_Tp, true>
    : public __bool_constant<__is_assignable(_Tp&, _Tp&&)>
    { };


  template<typename _Tp>
    struct is_move_assignable
    : public __is_move_assignable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, typename _Up>
    using __is_nothrow_assignable_impl
      = __bool_constant<__is_nothrow_assignable(_Tp, _Up)>;


  template<typename _Tp, typename _Up>
    struct is_nothrow_assignable
    : public __is_nothrow_assignable_impl<_Tp, _Up>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nt_copy_assignable_impl;

  template<typename _Tp>
    struct __is_nt_copy_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nt_copy_assignable_impl<_Tp, true>
    : public __is_nothrow_assignable_impl<_Tp&, const _Tp&>
    { };


  template<typename _Tp>
    struct is_nothrow_copy_assignable
    : public __is_nt_copy_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nt_move_assignable_impl;

  template<typename _Tp>
    struct __is_nt_move_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nt_move_assignable_impl<_Tp, true>
    : public __is_nothrow_assignable_impl<_Tp&, _Tp&&>
    { };


  template<typename _Tp>
    struct is_nothrow_move_assignable
    : public __is_nt_move_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename... _Args>
    struct is_trivially_constructible
    : public __bool_constant<__is_trivially_constructible(_Tp, _Args...)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_trivially_default_constructible
    : public __bool_constant<__is_trivially_constructible(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  struct __do_is_implicitly_default_constructible_impl
  {
    template <typename _Tp>
    static void __helper(const _Tp&);

    template <typename _Tp>
    static true_type __test(const _Tp&,
                            decltype(__helper<const _Tp&>({}))* = 0);

    static false_type __test(...);
  };

  template<typename _Tp>
    struct __is_implicitly_default_constructible_impl
    : public __do_is_implicitly_default_constructible_impl
    {
      typedef decltype(__test(declval<_Tp>())) type;
    };

  template<typename _Tp>
    struct __is_implicitly_default_constructible_safe
    : public __is_implicitly_default_constructible_impl<_Tp>::type
    { };

  template <typename _Tp>
    struct __is_implicitly_default_constructible
    : public __and_<__is_constructible_impl<_Tp>,
      __is_implicitly_default_constructible_safe<_Tp>>
    { };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_copy_constructible_impl;

  template<typename _Tp>
    struct __is_trivially_copy_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_copy_constructible_impl<_Tp, true>
    : public __and_<__is_copy_constructible_impl<_Tp>,
      integral_constant<bool,
   __is_trivially_constructible(_Tp, const _Tp&)>>
    { };


  template<typename _Tp>
    struct is_trivially_copy_constructible
    : public __is_trivially_copy_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_move_constructible_impl;

  template<typename _Tp>
    struct __is_trivially_move_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_move_constructible_impl<_Tp, true>
    : public __and_<__is_move_constructible_impl<_Tp>,
      integral_constant<bool,
   __is_trivially_constructible(_Tp, _Tp&&)>>
    { };


  template<typename _Tp>
    struct is_trivially_move_constructible
    : public __is_trivially_move_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename _Up>
    struct is_trivially_assignable
    : public __bool_constant<__is_trivially_assignable(_Tp, _Up)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_copy_assignable_impl;

  template<typename _Tp>
    struct __is_trivially_copy_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_copy_assignable_impl<_Tp, true>
    : public __bool_constant<__is_trivially_assignable(_Tp&, const _Tp&)>
    { };


  template<typename _Tp>
    struct is_trivially_copy_assignable
    : public __is_trivially_copy_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_move_assignable_impl;

  template<typename _Tp>
    struct __is_trivially_move_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_move_assignable_impl<_Tp, true>
    : public __bool_constant<__is_trivially_assignable(_Tp&, _Tp&&)>
    { };


  template<typename _Tp>
    struct is_trivially_move_assignable
    : public __is_trivially_move_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_trivially_destructible
    : public __and_<__is_destructible_safe<_Tp>,
      __bool_constant<__has_trivial_destructor(_Tp)>>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };



  template<typename _Tp>
    struct has_virtual_destructor
    : public integral_constant<bool, __has_virtual_destructor(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };





  template<typename _Tp>
    struct alignment_of
    : public integral_constant<std::size_t, alignof(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename>
    struct rank
    : public integral_constant<std::size_t, 0> { };

  template<typename _Tp, std::size_t _Size>
    struct rank<_Tp[_Size]>
    : public integral_constant<std::size_t, 1 + rank<_Tp>::value> { };

  template<typename _Tp>
    struct rank<_Tp[]>
    : public integral_constant<std::size_t, 1 + rank<_Tp>::value> { };


  template<typename, unsigned _Uint>
    struct extent
    : public integral_constant<std::size_t, 0> { };

  template<typename _Tp, unsigned _Uint, std::size_t _Size>
    struct extent<_Tp[_Size], _Uint>
    : public integral_constant<std::size_t,
          _Uint == 0 ? _Size : extent<_Tp,
          _Uint - 1>::value>
    { };

  template<typename _Tp, unsigned _Uint>
    struct extent<_Tp[], _Uint>
    : public integral_constant<std::size_t,
          _Uint == 0 ? 0 : extent<_Tp,
             _Uint - 1>::value>
    { };





  template<typename _Tp, typename _Up>
    struct is_same

    : public integral_constant<bool, __is_same(_Tp, _Up)>



    { };
# 1420 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<typename _Base, typename _Derived>
    struct is_base_of
    : public integral_constant<bool, __is_base_of(_Base, _Derived)>
    { };

  template<typename _From, typename _To,
           bool = __or_<is_void<_From>, is_function<_To>,
                        is_array<_To>>::value>
    struct __is_convertible_helper
    {
      typedef typename is_void<_To>::type type;
    };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"
  template<typename _From, typename _To>
    class __is_convertible_helper<_From, _To, false>
    {
      template<typename _To1>
 static void __test_aux(_To1) noexcept;

      template<typename _From1, typename _To1,
        typename = decltype(__test_aux<_To1>(std::declval<_From1>()))>
 static true_type
 __test(int);

      template<typename, typename>
 static false_type
 __test(...);

    public:
      typedef decltype(__test<_From, _To>(0)) type;
    };
#pragma GCC diagnostic pop


  template<typename _From, typename _To>
    struct is_convertible
    : public __is_convertible_helper<_From, _To>::type
    { };


  template<typename _ToElementType, typename _FromElementType>
    using __is_array_convertible
      = is_convertible<_FromElementType(*)[], _ToElementType(*)[]>;

  template<typename _From, typename _To,
           bool = __or_<is_void<_From>, is_function<_To>,
                        is_array<_To>>::value>
    struct __is_nt_convertible_helper
    : is_void<_To>
    { };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"
  template<typename _From, typename _To>
    class __is_nt_convertible_helper<_From, _To, false>
    {
      template<typename _To1>
 static void __test_aux(_To1) noexcept;

      template<typename _From1, typename _To1>
 static
 __bool_constant<noexcept(__test_aux<_To1>(std::declval<_From1>()))>
 __test(int);

      template<typename, typename>
 static false_type
 __test(...);

    public:
      using type = decltype(__test<_From, _To>(0));
    };
#pragma GCC diagnostic pop
# 1512 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<typename _Tp>
    struct remove_const
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_const<_Tp const>
    { typedef _Tp type; };


  template<typename _Tp>
    struct remove_volatile
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_volatile<_Tp volatile>
    { typedef _Tp type; };


  template<typename _Tp>
    struct remove_cv
    { using type = _Tp; };

  template<typename _Tp>
    struct remove_cv<const _Tp>
    { using type = _Tp; };

  template<typename _Tp>
    struct remove_cv<volatile _Tp>
    { using type = _Tp; };

  template<typename _Tp>
    struct remove_cv<const volatile _Tp>
    { using type = _Tp; };


  template<typename _Tp>
    struct add_const
    { typedef _Tp const type; };


  template<typename _Tp>
    struct add_volatile
    { typedef _Tp volatile type; };


  template<typename _Tp>
    struct add_cv
    {
      typedef typename
      add_const<typename add_volatile<_Tp>::type>::type type;
    };






  template<typename _Tp>
    using remove_const_t = typename remove_const<_Tp>::type;


  template<typename _Tp>
    using remove_volatile_t = typename remove_volatile<_Tp>::type;


  template<typename _Tp>
    using remove_cv_t = typename remove_cv<_Tp>::type;


  template<typename _Tp>
    using add_const_t = typename add_const<_Tp>::type;


  template<typename _Tp>
    using add_volatile_t = typename add_volatile<_Tp>::type;


  template<typename _Tp>
    using add_cv_t = typename add_cv<_Tp>::type;





  template<typename _Tp>
    struct remove_reference
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_reference<_Tp&>
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_reference<_Tp&&>
    { typedef _Tp type; };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __add_lvalue_reference_helper
    { typedef _Tp type; };

  template<typename _Tp>
    struct __add_lvalue_reference_helper<_Tp, true>
    { typedef _Tp& type; };


  template<typename _Tp>
    struct add_lvalue_reference
    : public __add_lvalue_reference_helper<_Tp>
    { };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __add_rvalue_reference_helper
    { typedef _Tp type; };

  template<typename _Tp>
    struct __add_rvalue_reference_helper<_Tp, true>
    { typedef _Tp&& type; };


  template<typename _Tp>
    struct add_rvalue_reference
    : public __add_rvalue_reference_helper<_Tp>
    { };



  template<typename _Tp>
    using remove_reference_t = typename remove_reference<_Tp>::type;


  template<typename _Tp>
    using add_lvalue_reference_t = typename add_lvalue_reference<_Tp>::type;


  template<typename _Tp>
    using add_rvalue_reference_t = typename add_rvalue_reference<_Tp>::type;







  template<typename _Unqualified, bool _IsConst, bool _IsVol>
    struct __cv_selector;

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, false, false>
    { typedef _Unqualified __type; };

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, false, true>
    { typedef volatile _Unqualified __type; };

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, true, false>
    { typedef const _Unqualified __type; };

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, true, true>
    { typedef const volatile _Unqualified __type; };

  template<typename _Qualified, typename _Unqualified,
    bool _IsConst = is_const<_Qualified>::value,
    bool _IsVol = is_volatile<_Qualified>::value>
    class __match_cv_qualifiers
    {
      typedef __cv_selector<_Unqualified, _IsConst, _IsVol> __match;

    public:
      typedef typename __match::__type __type;
    };


  template<typename _Tp>
    struct __make_unsigned
    { typedef _Tp __type; };

  template<>
    struct __make_unsigned<char>
    { typedef unsigned char __type; };

  template<>
    struct __make_unsigned<signed char>
    { typedef unsigned char __type; };

  template<>
    struct __make_unsigned<short>
    { typedef unsigned short __type; };

  template<>
    struct __make_unsigned<int>
    { typedef unsigned int __type; };

  template<>
    struct __make_unsigned<long>
    { typedef unsigned long __type; };

  template<>
    struct __make_unsigned<long long>
    { typedef unsigned long long __type; };
# 1736 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<typename _Tp,
    bool _IsInt = is_integral<_Tp>::value,
    bool _IsEnum = is_enum<_Tp>::value>
    class __make_unsigned_selector;

  template<typename _Tp>
    class __make_unsigned_selector<_Tp, true, false>
    {
      using __unsigned_type
 = typename __make_unsigned<__remove_cv_t<_Tp>>::__type;

    public:
      using __type
 = typename __match_cv_qualifiers<_Tp, __unsigned_type>::__type;
    };

  class __make_unsigned_selector_base
  {
  protected:
    template<typename...> struct _List { };

    template<typename _Tp, typename... _Up>
      struct _List<_Tp, _Up...> : _List<_Up...>
      { static constexpr size_t __size = sizeof(_Tp); };

    template<size_t _Sz, typename _Tp, bool = (_Sz <= _Tp::__size)>
      struct __select;

    template<size_t _Sz, typename _Uint, typename... _UInts>
      struct __select<_Sz, _List<_Uint, _UInts...>, true>
      { using __type = _Uint; };

    template<size_t _Sz, typename _Uint, typename... _UInts>
      struct __select<_Sz, _List<_Uint, _UInts...>, false>
      : __select<_Sz, _List<_UInts...>>
      { };
  };


  template<typename _Tp>
    class __make_unsigned_selector<_Tp, false, true>
    : __make_unsigned_selector_base
    {

      using _UInts = _List<unsigned char, unsigned short, unsigned int,
      unsigned long, unsigned long long>;

      using __unsigned_type = typename __select<sizeof(_Tp), _UInts>::__type;

    public:
      using __type
 = typename __match_cv_qualifiers<_Tp, __unsigned_type>::__type;
    };






  template<>
    struct __make_unsigned<wchar_t>
    {
      using __type
 = typename __make_unsigned_selector<wchar_t, false, true>::__type;
    };
# 1812 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<>
    struct __make_unsigned<char16_t>
    {
      using __type
 = typename __make_unsigned_selector<char16_t, false, true>::__type;
    };

  template<>
    struct __make_unsigned<char32_t>
    {
      using __type
 = typename __make_unsigned_selector<char32_t, false, true>::__type;
    };






  template<typename _Tp>
    struct make_unsigned
    { typedef typename __make_unsigned_selector<_Tp>::__type type; };


  template<>
    struct make_unsigned<bool>;




  template<typename _Tp>
    struct __make_signed
    { typedef _Tp __type; };

  template<>
    struct __make_signed<char>
    { typedef signed char __type; };

  template<>
    struct __make_signed<unsigned char>
    { typedef signed char __type; };

  template<>
    struct __make_signed<unsigned short>
    { typedef signed short __type; };

  template<>
    struct __make_signed<unsigned int>
    { typedef signed int __type; };

  template<>
    struct __make_signed<unsigned long>
    { typedef signed long __type; };

  template<>
    struct __make_signed<unsigned long long>
    { typedef signed long long __type; };
# 1892 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<typename _Tp,
    bool _IsInt = is_integral<_Tp>::value,
    bool _IsEnum = is_enum<_Tp>::value>
    class __make_signed_selector;

  template<typename _Tp>
    class __make_signed_selector<_Tp, true, false>
    {
      using __signed_type
 = typename __make_signed<__remove_cv_t<_Tp>>::__type;

    public:
      using __type
 = typename __match_cv_qualifiers<_Tp, __signed_type>::__type;
    };


  template<typename _Tp>
    class __make_signed_selector<_Tp, false, true>
    {
      typedef typename __make_unsigned_selector<_Tp>::__type __unsigned_type;

    public:
      typedef typename __make_signed_selector<__unsigned_type>::__type __type;
    };






  template<>
    struct __make_signed<wchar_t>
    {
      using __type
 = typename __make_signed_selector<wchar_t, false, true>::__type;
    };
# 1940 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<>
    struct __make_signed<char16_t>
    {
      using __type
 = typename __make_signed_selector<char16_t, false, true>::__type;
    };

  template<>
    struct __make_signed<char32_t>
    {
      using __type
 = typename __make_signed_selector<char32_t, false, true>::__type;
    };






  template<typename _Tp>
    struct make_signed
    { typedef typename __make_signed_selector<_Tp>::__type type; };


  template<>
    struct make_signed<bool>;



  template<typename _Tp>
    using make_signed_t = typename make_signed<_Tp>::type;


  template<typename _Tp>
    using make_unsigned_t = typename make_unsigned<_Tp>::type;





  template<typename _Tp>
    struct remove_extent
    { typedef _Tp type; };

  template<typename _Tp, std::size_t _Size>
    struct remove_extent<_Tp[_Size]>
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_extent<_Tp[]>
    { typedef _Tp type; };


  template<typename _Tp>
    struct remove_all_extents
    { typedef _Tp type; };

  template<typename _Tp, std::size_t _Size>
    struct remove_all_extents<_Tp[_Size]>
    { typedef typename remove_all_extents<_Tp>::type type; };

  template<typename _Tp>
    struct remove_all_extents<_Tp[]>
    { typedef typename remove_all_extents<_Tp>::type type; };



  template<typename _Tp>
    using remove_extent_t = typename remove_extent<_Tp>::type;


  template<typename _Tp>
    using remove_all_extents_t = typename remove_all_extents<_Tp>::type;




  template<typename _Tp, typename>
    struct __remove_pointer_helper
    { typedef _Tp type; };

  template<typename _Tp, typename _Up>
    struct __remove_pointer_helper<_Tp, _Up*>
    { typedef _Up type; };


  template<typename _Tp>
    struct remove_pointer
    : public __remove_pointer_helper<_Tp, __remove_cv_t<_Tp>>
    { };

  template<typename _Tp, bool = __or_<__is_referenceable<_Tp>,
          is_void<_Tp>>::value>
    struct __add_pointer_helper
    { typedef _Tp type; };

  template<typename _Tp>
    struct __add_pointer_helper<_Tp, true>
    { typedef typename remove_reference<_Tp>::type* type; };


  template<typename _Tp>
    struct add_pointer
    : public __add_pointer_helper<_Tp>
    { };



  template<typename _Tp>
    using remove_pointer_t = typename remove_pointer<_Tp>::type;


  template<typename _Tp>
    using add_pointer_t = typename add_pointer<_Tp>::type;


  template<std::size_t _Len>
    struct __aligned_storage_msa
    {
      union __type
      {
 unsigned char __data[_Len];
 struct __attribute__((__aligned__)) { } __align;
      };
    };
# 2076 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<std::size_t _Len, std::size_t _Align =
    __alignof__(typename __aligned_storage_msa<_Len>::__type)>
    struct aligned_storage
    {
      union type
      {
 unsigned char __data[_Len];
 struct __attribute__((__aligned__((_Align)))) { } __align;
      };
    };

  template <typename... _Types>
    struct __strictest_alignment
    {
      static const size_t _S_alignment = 0;
      static const size_t _S_size = 0;
    };

  template <typename _Tp, typename... _Types>
    struct __strictest_alignment<_Tp, _Types...>
    {
      static const size_t _S_alignment =
        alignof(_Tp) > __strictest_alignment<_Types...>::_S_alignment
 ? alignof(_Tp) : __strictest_alignment<_Types...>::_S_alignment;
      static const size_t _S_size =
        sizeof(_Tp) > __strictest_alignment<_Types...>::_S_size
 ? sizeof(_Tp) : __strictest_alignment<_Types...>::_S_size;
    };
# 2115 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template <size_t _Len, typename... _Types>
    struct aligned_union
    {
    private:
      static_assert(sizeof...(_Types) != 0, "At least one type is required");

      using __strictest = __strictest_alignment<_Types...>;
      static const size_t _S_len = _Len > __strictest::_S_size
 ? _Len : __strictest::_S_size;
    public:

      static const size_t alignment_value = __strictest::_S_alignment;

      typedef typename aligned_storage<_S_len, alignment_value>::type type;
    };

  template <size_t _Len, typename... _Types>
    const size_t aligned_union<_Len, _Types...>::alignment_value;





  template<typename _Up,
    bool _IsArray = is_array<_Up>::value,
    bool _IsFunction = is_function<_Up>::value>
    struct __decay_selector;


  template<typename _Up>
    struct __decay_selector<_Up, false, false>
    { typedef __remove_cv_t<_Up> __type; };

  template<typename _Up>
    struct __decay_selector<_Up, true, false>
    { typedef typename remove_extent<_Up>::type* __type; };

  template<typename _Up>
    struct __decay_selector<_Up, false, true>
    { typedef typename add_pointer<_Up>::type __type; };



  template<typename _Tp>
    class decay
    {
      typedef typename remove_reference<_Tp>::type __remove_type;

    public:
      typedef typename __decay_selector<__remove_type>::__type type;
    };




  template<typename _Tp>
    struct __strip_reference_wrapper
    {
      typedef _Tp __type;
    };

  template<typename _Tp>
    struct __strip_reference_wrapper<reference_wrapper<_Tp> >
    {
      typedef _Tp& __type;
    };


  template<typename _Tp>
    using __decay_t = typename decay<_Tp>::type;

  template<typename _Tp>
    using __decay_and_strip = __strip_reference_wrapper<__decay_t<_Tp>>;




  template<bool, typename _Tp = void>
    struct enable_if
    { };


  template<typename _Tp>
    struct enable_if<true, _Tp>
    { typedef _Tp type; };




  template<bool _Cond, typename _Tp = void>
    using __enable_if_t = typename enable_if<_Cond, _Tp>::type;


  template<typename... _Cond>
    using _Require = __enable_if_t<__and_<_Cond...>::value>;


  template<typename _Tp>
    using __remove_cvref_t
     = typename remove_cv<typename remove_reference<_Tp>::type>::type;




  template<bool _Cond, typename _Iftrue, typename _Iffalse>
    struct conditional
    { typedef _Iftrue type; };


  template<typename _Iftrue, typename _Iffalse>
    struct conditional<false, _Iftrue, _Iffalse>
    { typedef _Iffalse type; };


  template<typename... _Tp>
    struct common_type;




  struct __do_common_type_impl
  {
    template<typename _Tp, typename _Up>
      using __cond_t
 = decltype(true ? std::declval<_Tp>() : std::declval<_Up>());



    template<typename _Tp, typename _Up>
      static __success_type<__decay_t<__cond_t<_Tp, _Up>>>
      _S_test(int);
# 2255 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
    template<typename, typename>
      static __failure_type
      _S_test_2(...);

    template<typename _Tp, typename _Up>
      static decltype(_S_test_2<_Tp, _Up>(0))
      _S_test(...);
  };


  template<>
    struct common_type<>
    { };


  template<typename _Tp0>
    struct common_type<_Tp0>
    : public common_type<_Tp0, _Tp0>
    { };


  template<typename _Tp1, typename _Tp2,
    typename _Dp1 = __decay_t<_Tp1>, typename _Dp2 = __decay_t<_Tp2>>
    struct __common_type_impl
    {


      using type = common_type<_Dp1, _Dp2>;
    };

  template<typename _Tp1, typename _Tp2>
    struct __common_type_impl<_Tp1, _Tp2, _Tp1, _Tp2>
    : private __do_common_type_impl
    {


      using type = decltype(_S_test<_Tp1, _Tp2>(0));
    };


  template<typename _Tp1, typename _Tp2>
    struct common_type<_Tp1, _Tp2>
    : public __common_type_impl<_Tp1, _Tp2>::type
    { };

  template<typename...>
    struct __common_type_pack
    { };

  template<typename, typename, typename = void>
    struct __common_type_fold;


  template<typename _Tp1, typename _Tp2, typename... _Rp>
    struct common_type<_Tp1, _Tp2, _Rp...>
    : public __common_type_fold<common_type<_Tp1, _Tp2>,
    __common_type_pack<_Rp...>>
    { };




  template<typename _CTp, typename... _Rp>
    struct __common_type_fold<_CTp, __common_type_pack<_Rp...>,
         __void_t<typename _CTp::type>>
    : public common_type<typename _CTp::type, _Rp...>
    { };


  template<typename _CTp, typename _Rp>
    struct __common_type_fold<_CTp, _Rp, void>
    { };

  template<typename _Tp, bool = is_enum<_Tp>::value>
    struct __underlying_type_impl
    {
      using type = __underlying_type(_Tp);
    };

  template<typename _Tp>
    struct __underlying_type_impl<_Tp, false>
    { };



  template<typename _Tp>
    struct underlying_type
    : public __underlying_type_impl<_Tp>
    { };


  template<typename _Tp>
    struct __declval_protector
    {
      static const bool __stop = false;
    };






  template<typename _Tp>
    auto declval() noexcept -> decltype(__declval<_Tp>(0))
    {
      static_assert(__declval_protector<_Tp>::__stop,
      "declval() must not be used!");
      return __declval<_Tp>(0);
    }


  template<typename _Signature>
    struct result_of;






  struct __invoke_memfun_ref { };
  struct __invoke_memfun_deref { };
  struct __invoke_memobj_ref { };
  struct __invoke_memobj_deref { };
  struct __invoke_other { };


  template<typename _Tp, typename _Tag>
    struct __result_of_success : __success_type<_Tp>
    { using __invoke_type = _Tag; };


  struct __result_of_memfun_ref_impl
  {
    template<typename _Fp, typename _Tp1, typename... _Args>
      static __result_of_success<decltype(
      (std::declval<_Tp1>().*std::declval<_Fp>())(std::declval<_Args>()...)
      ), __invoke_memfun_ref> _S_test(int);

    template<typename...>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_memfun_ref
    : private __result_of_memfun_ref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg, _Args...>(0)) type;
    };


  struct __result_of_memfun_deref_impl
  {
    template<typename _Fp, typename _Tp1, typename... _Args>
      static __result_of_success<decltype(
      ((*std::declval<_Tp1>()).*std::declval<_Fp>())(std::declval<_Args>()...)
      ), __invoke_memfun_deref> _S_test(int);

    template<typename...>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_memfun_deref
    : private __result_of_memfun_deref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg, _Args...>(0)) type;
    };


  struct __result_of_memobj_ref_impl
  {
    template<typename _Fp, typename _Tp1>
      static __result_of_success<decltype(
      std::declval<_Tp1>().*std::declval<_Fp>()
      ), __invoke_memobj_ref> _S_test(int);

    template<typename, typename>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_memobj_ref
    : private __result_of_memobj_ref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg>(0)) type;
    };


  struct __result_of_memobj_deref_impl
  {
    template<typename _Fp, typename _Tp1>
      static __result_of_success<decltype(
      (*std::declval<_Tp1>()).*std::declval<_Fp>()
      ), __invoke_memobj_deref> _S_test(int);

    template<typename, typename>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_memobj_deref
    : private __result_of_memobj_deref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg>(0)) type;
    };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_memobj;

  template<typename _Res, typename _Class, typename _Arg>
    struct __result_of_memobj<_Res _Class::*, _Arg>
    {
      typedef __remove_cvref_t<_Arg> _Argval;
      typedef _Res _Class::* _MemPtr;
      typedef typename conditional<__or_<is_same<_Argval, _Class>,
        is_base_of<_Class, _Argval>>::value,
        __result_of_memobj_ref<_MemPtr, _Arg>,
        __result_of_memobj_deref<_MemPtr, _Arg>
      >::type::type type;
    };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_memfun;

  template<typename _Res, typename _Class, typename _Arg, typename... _Args>
    struct __result_of_memfun<_Res _Class::*, _Arg, _Args...>
    {
      typedef typename remove_reference<_Arg>::type _Argval;
      typedef _Res _Class::* _MemPtr;
      typedef typename conditional<is_base_of<_Class, _Argval>::value,
        __result_of_memfun_ref<_MemPtr, _Arg, _Args...>,
        __result_of_memfun_deref<_MemPtr, _Arg, _Args...>
      >::type::type type;
    };






  template<typename _Tp, typename _Up = __remove_cvref_t<_Tp>>
    struct __inv_unwrap
    {
      using type = _Tp;
    };

  template<typename _Tp, typename _Up>
    struct __inv_unwrap<_Tp, reference_wrapper<_Up>>
    {
      using type = _Up&;
    };

  template<bool, bool, typename _Functor, typename... _ArgTypes>
    struct __result_of_impl
    {
      typedef __failure_type type;
    };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_impl<true, false, _MemPtr, _Arg>
    : public __result_of_memobj<__decay_t<_MemPtr>,
    typename __inv_unwrap<_Arg>::type>
    { };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_impl<false, true, _MemPtr, _Arg, _Args...>
    : public __result_of_memfun<__decay_t<_MemPtr>,
    typename __inv_unwrap<_Arg>::type, _Args...>
    { };


  struct __result_of_other_impl
  {
    template<typename _Fn, typename... _Args>
      static __result_of_success<decltype(
      std::declval<_Fn>()(std::declval<_Args>()...)
      ), __invoke_other> _S_test(int);

    template<typename...>
      static __failure_type _S_test(...);
  };

  template<typename _Functor, typename... _ArgTypes>
    struct __result_of_impl<false, false, _Functor, _ArgTypes...>
    : private __result_of_other_impl
    {
      typedef decltype(_S_test<_Functor, _ArgTypes...>(0)) type;
    };


  template<typename _Functor, typename... _ArgTypes>
    struct __invoke_result
    : public __result_of_impl<
        is_member_object_pointer<
          typename remove_reference<_Functor>::type
        >::value,
        is_member_function_pointer<
          typename remove_reference<_Functor>::type
        >::value,
 _Functor, _ArgTypes...
      >::type
    { };


  template<typename _Functor, typename... _ArgTypes>
    struct result_of<_Functor(_ArgTypes...)>
    : public __invoke_result<_Functor, _ArgTypes...>
    { };



  template<size_t _Len, size_t _Align =
     __alignof__(typename __aligned_storage_msa<_Len>::__type)>
    using aligned_storage_t = typename aligned_storage<_Len, _Align>::type;

  template <size_t _Len, typename... _Types>
    using aligned_union_t = typename aligned_union<_Len, _Types...>::type;


  template<typename _Tp>
    using decay_t = typename decay<_Tp>::type;


  template<bool _Cond, typename _Tp = void>
    using enable_if_t = typename enable_if<_Cond, _Tp>::type;


  template<bool _Cond, typename _Iftrue, typename _Iffalse>
    using conditional_t = typename conditional<_Cond, _Iftrue, _Iffalse>::type;


  template<typename... _Tp>
    using common_type_t = typename common_type<_Tp...>::type;


  template<typename _Tp>
    using underlying_type_t = typename underlying_type<_Tp>::type;


  template<typename _Tp>
    using result_of_t = typename result_of<_Tp>::type;





  template<typename...> using void_t = void;





  template<typename _Default, typename _AlwaysVoid,
    template<typename...> class _Op, typename... _Args>
    struct __detector
    {
      using value_t = false_type;
      using type = _Default;
    };


  template<typename _Default, template<typename...> class _Op,
     typename... _Args>
    struct __detector<_Default, __void_t<_Op<_Args...>>, _Op, _Args...>
    {
      using value_t = true_type;
      using type = _Op<_Args...>;
    };


  template<typename _Default, template<typename...> class _Op,
    typename... _Args>
    using __detected_or = __detector<_Default, void, _Op, _Args...>;


  template<typename _Default, template<typename...> class _Op,
    typename... _Args>
    using __detected_or_t
      = typename __detected_or<_Default, _Op, _Args...>::type;
# 2649 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template <typename _Tp>
    struct __is_swappable;

  template <typename _Tp>
    struct __is_nothrow_swappable;

  template<typename>
    struct __is_tuple_like_impl : false_type
    { };

  template<typename... _Tps>
    struct __is_tuple_like_impl<tuple<_Tps...>> : true_type
    { };


  template<typename _Tp>
    struct __is_tuple_like
    : public __is_tuple_like_impl<__remove_cvref_t<_Tp>>::type
    { };


  template<typename _Tp>

    __inline__ __attribute__((always_inline))
    _Require<__not_<__is_tuple_like<_Tp>>,
      is_move_constructible<_Tp>,
      is_move_assignable<_Tp>>
    swap(_Tp&, _Tp&)
    noexcept(__and_<is_nothrow_move_constructible<_Tp>,
             is_nothrow_move_assignable<_Tp>>::value);

  template<typename _Tp, size_t _Nm>

    __inline__ __attribute__((always_inline))
    __enable_if_t<__is_swappable<_Tp>::value>
    swap(_Tp (&__a)[_Nm], _Tp (&__b)[_Nm])
    noexcept(__is_nothrow_swappable<_Tp>::value);


  namespace __swappable_details {
    using std::swap;

    struct __do_is_swappable_impl
    {
      template<typename _Tp, typename
               = decltype(swap(std::declval<_Tp&>(), std::declval<_Tp&>()))>
        static true_type __test(int);

      template<typename>
        static false_type __test(...);
    };

    struct __do_is_nothrow_swappable_impl
    {
      template<typename _Tp>
        static __bool_constant<
          noexcept(swap(std::declval<_Tp&>(), std::declval<_Tp&>()))
        > __test(int);

      template<typename>
        static false_type __test(...);
    };

  }

  template<typename _Tp>
    struct __is_swappable_impl
    : public __swappable_details::__do_is_swappable_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp>
    struct __is_nothrow_swappable_impl
    : public __swappable_details::__do_is_nothrow_swappable_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp>
    struct __is_swappable
    : public __is_swappable_impl<_Tp>::type
    { };

  template<typename _Tp>
    struct __is_nothrow_swappable
    : public __is_nothrow_swappable_impl<_Tp>::type
    { };







  template<typename _Tp>
    struct is_swappable
    : public __is_swappable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_nothrow_swappable
    : public __is_nothrow_swappable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };



  template<typename _Tp>
    __inline__ __attribute__((always_inline)) constexpr bool is_swappable_v =
      is_swappable<_Tp>::value;


  template<typename _Tp>
    __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_swappable_v =
      is_nothrow_swappable<_Tp>::value;



  namespace __swappable_with_details {
    using std::swap;

    struct __do_is_swappable_with_impl
    {
      template<typename _Tp, typename _Up, typename
               = decltype(swap(std::declval<_Tp>(), std::declval<_Up>())),
               typename
               = decltype(swap(std::declval<_Up>(), std::declval<_Tp>()))>
        static true_type __test(int);

      template<typename, typename>
        static false_type __test(...);
    };

    struct __do_is_nothrow_swappable_with_impl
    {
      template<typename _Tp, typename _Up>
        static __bool_constant<
          noexcept(swap(std::declval<_Tp>(), std::declval<_Up>()))
          &&
          noexcept(swap(std::declval<_Up>(), std::declval<_Tp>()))
        > __test(int);

      template<typename, typename>
        static false_type __test(...);
    };

  }

  template<typename _Tp, typename _Up>
    struct __is_swappable_with_impl
    : public __swappable_with_details::__do_is_swappable_with_impl
    {
      typedef decltype(__test<_Tp, _Up>(0)) type;
    };


  template<typename _Tp>
    struct __is_swappable_with_impl<_Tp&, _Tp&>
    : public __swappable_details::__do_is_swappable_impl
    {
      typedef decltype(__test<_Tp&>(0)) type;
    };

  template<typename _Tp, typename _Up>
    struct __is_nothrow_swappable_with_impl
    : public __swappable_with_details::__do_is_nothrow_swappable_with_impl
    {
      typedef decltype(__test<_Tp, _Up>(0)) type;
    };


  template<typename _Tp>
    struct __is_nothrow_swappable_with_impl<_Tp&, _Tp&>
    : public __swappable_details::__do_is_nothrow_swappable_impl
    {
      typedef decltype(__test<_Tp&>(0)) type;
    };



  template<typename _Tp, typename _Up>
    struct is_swappable_with
    : public __is_swappable_with_impl<_Tp, _Up>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "first template argument must be a complete class or an unbounded array");
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Up>{}),
 "second template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename _Up>
    struct is_nothrow_swappable_with
    : public __is_nothrow_swappable_with_impl<_Tp, _Up>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "first template argument must be a complete class or an unbounded array");
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Up>{}),
 "second template argument must be a complete class or an unbounded array");
    };



  template<typename _Tp, typename _Up>
    __inline__ __attribute__((always_inline)) constexpr bool is_swappable_with_v =
      is_swappable_with<_Tp, _Up>::value;


  template<typename _Tp, typename _Up>
    __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_swappable_with_v =
      is_nothrow_swappable_with<_Tp, _Up>::value;
# 2876 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
  template<typename _Result, typename _Ret,
    bool = is_void<_Ret>::value, typename = void>
    struct __is_invocable_impl
    : false_type
    {
      using __nothrow_type = false_type;
    };


  template<typename _Result, typename _Ret>
    struct __is_invocable_impl<_Result, _Ret,
                                true,
          __void_t<typename _Result::type>>
    : true_type
    {
      using __nothrow_type = true_type;
    };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"

  template<typename _Result, typename _Ret>
    struct __is_invocable_impl<_Result, _Ret,
                                false,
          __void_t<typename _Result::type>>
    {
    private:



      static typename _Result::type _S_get() noexcept;

      template<typename _Tp>
 static void _S_conv(_Tp) noexcept;


      template<typename _Tp, bool _Check_Noex = false,
        typename = decltype(_S_conv<_Tp>(_S_get())),
        bool _Noex = noexcept(_S_conv<_Tp>(_S_get()))>
 static __bool_constant<_Check_Noex ? _Noex : true>
 _S_test(int);

      template<typename _Tp, bool = false>
 static false_type
 _S_test(...);

    public:

      using type = decltype(_S_test<_Ret>(1));


      using __nothrow_type = decltype(_S_test<_Ret, true>(1));
    };
#pragma GCC diagnostic pop

  template<typename _Fn, typename... _ArgTypes>
    struct __is_invocable
    : __is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, void>::type
    { };

  template<typename _Fn, typename _Tp, typename... _Args>
    constexpr bool __call_is_nt(__invoke_memfun_ref)
    {
      using _Up = typename __inv_unwrap<_Tp>::type;
      return noexcept((std::declval<_Up>().*std::declval<_Fn>())(
     std::declval<_Args>()...));
    }

  template<typename _Fn, typename _Tp, typename... _Args>
    constexpr bool __call_is_nt(__invoke_memfun_deref)
    {
      return noexcept(((*std::declval<_Tp>()).*std::declval<_Fn>())(
     std::declval<_Args>()...));
    }

  template<typename _Fn, typename _Tp>
    constexpr bool __call_is_nt(__invoke_memobj_ref)
    {
      using _Up = typename __inv_unwrap<_Tp>::type;
      return noexcept(std::declval<_Up>().*std::declval<_Fn>());
    }

  template<typename _Fn, typename _Tp>
    constexpr bool __call_is_nt(__invoke_memobj_deref)
    {
      return noexcept((*std::declval<_Tp>()).*std::declval<_Fn>());
    }

  template<typename _Fn, typename... _Args>
    constexpr bool __call_is_nt(__invoke_other)
    {
      return noexcept(std::declval<_Fn>()(std::declval<_Args>()...));
    }

  template<typename _Result, typename _Fn, typename... _Args>
    struct __call_is_nothrow
    : __bool_constant<
 std::__call_is_nt<_Fn, _Args...>(typename _Result::__invoke_type{})
      >
    { };

  template<typename _Fn, typename... _Args>
    using __call_is_nothrow_
      = __call_is_nothrow<__invoke_result<_Fn, _Args...>, _Fn, _Args...>;


  template<typename _Fn, typename... _Args>
    struct __is_nothrow_invocable
    : __and_<__is_invocable<_Fn, _Args...>,
             __call_is_nothrow_<_Fn, _Args...>>::type
    { };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"
  struct __nonesuchbase {};
  struct __nonesuch : private __nonesuchbase {
    ~__nonesuch() = delete;
    __nonesuch(__nonesuch const&) = delete;
    void operator=(__nonesuch const&) = delete;
  };
#pragma GCC diagnostic pop






  template<typename _Functor, typename... _ArgTypes>
    struct invoke_result
    : public __invoke_result<_Functor, _ArgTypes...>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Functor>{}),
 "_Functor must be a complete class or an unbounded array");
      static_assert((std::__is_complete_or_unbounded(
 __type_identity<_ArgTypes>{}) && ...),
 "each argument type must be a complete class or an unbounded array");
    };


  template<typename _Fn, typename... _Args>
    using invoke_result_t = typename invoke_result<_Fn, _Args...>::type;


  template<typename _Fn, typename... _ArgTypes>
    struct is_invocable
    : __is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, void>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
      static_assert((std::__is_complete_or_unbounded(
 __type_identity<_ArgTypes>{}) && ...),
 "each argument type must be a complete class or an unbounded array");
    };


  template<typename _Ret, typename _Fn, typename... _ArgTypes>
    struct is_invocable_r
    : __is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, _Ret>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
      static_assert((std::__is_complete_or_unbounded(
 __type_identity<_ArgTypes>{}) && ...),
 "each argument type must be a complete class or an unbounded array");
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Ret>{}),
 "_Ret must be a complete class or an unbounded array");
    };


  template<typename _Fn, typename... _ArgTypes>
    struct is_nothrow_invocable
    : __and_<__is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, void>,
      __call_is_nothrow_<_Fn, _ArgTypes...>>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
      static_assert((std::__is_complete_or_unbounded(
 __type_identity<_ArgTypes>{}) && ...),
 "each argument type must be a complete class or an unbounded array");
    };


  template<typename _Result, typename _Ret>
    using __is_nt_invocable_impl
      = typename __is_invocable_impl<_Result, _Ret>::__nothrow_type;



  template<typename _Ret, typename _Fn, typename... _ArgTypes>
    struct is_nothrow_invocable_r
    : __and_<__is_nt_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, _Ret>,
             __call_is_nothrow_<_Fn, _ArgTypes...>>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
      static_assert((std::__is_complete_or_unbounded(
 __type_identity<_ArgTypes>{}) && ...),
 "each argument type must be a complete class or an unbounded array");
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Ret>{}),
 "_Ret must be a complete class or an unbounded array");
    };
# 3094 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_void_v = is_void<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_null_pointer_v = is_null_pointer<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_integral_v = is_integral<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_floating_point_v = is_floating_point<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_array_v = is_array<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_pointer_v = is_pointer<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_lvalue_reference_v =
    is_lvalue_reference<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_rvalue_reference_v =
    is_rvalue_reference<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_member_object_pointer_v =
    is_member_object_pointer<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_member_function_pointer_v =
    is_member_function_pointer<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_enum_v = is_enum<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_union_v = is_union<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_class_v = is_class<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_function_v = is_function<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_reference_v = is_reference<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_arithmetic_v = is_arithmetic<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_fundamental_v = is_fundamental<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_object_v = is_object<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_scalar_v = is_scalar<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_compound_v = is_compound<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_member_pointer_v = is_member_pointer<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_const_v = is_const<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_volatile_v = is_volatile<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivial_v = is_trivial<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_copyable_v =
    is_trivially_copyable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_standard_layout_v = is_standard_layout<_Tp>::value;
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wdeprecated-declarations"
template <typename _Tp>

  __inline__ __attribute__((always_inline)) constexpr bool is_pod_v = is_pod<_Tp>::value;
template <typename _Tp>
  [[__deprecated__]]
  __inline__ __attribute__((always_inline)) constexpr bool is_literal_type_v = is_literal_type<_Tp>::value;
#pragma GCC diagnostic pop
 template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_empty_v = is_empty<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_polymorphic_v = is_polymorphic<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_abstract_v = is_abstract<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_final_v = is_final<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_signed_v = is_signed<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_unsigned_v = is_unsigned<_Tp>::value;
template <typename _Tp, typename... _Args>
  __inline__ __attribute__((always_inline)) constexpr bool is_constructible_v =
    is_constructible<_Tp, _Args...>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_default_constructible_v =
    is_default_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_copy_constructible_v =
    is_copy_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_move_constructible_v =
    is_move_constructible<_Tp>::value;
template <typename _Tp, typename _Up>
  __inline__ __attribute__((always_inline)) constexpr bool is_assignable_v = is_assignable<_Tp, _Up>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_copy_assignable_v = is_copy_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_move_assignable_v = is_move_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_destructible_v = is_destructible<_Tp>::value;
template <typename _Tp, typename... _Args>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_constructible_v =
    is_trivially_constructible<_Tp, _Args...>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_default_constructible_v =
    is_trivially_default_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_copy_constructible_v =
    is_trivially_copy_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_move_constructible_v =
    is_trivially_move_constructible<_Tp>::value;
template <typename _Tp, typename _Up>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_assignable_v =
    is_trivially_assignable<_Tp, _Up>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_copy_assignable_v =
    is_trivially_copy_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_move_assignable_v =
    is_trivially_move_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_destructible_v =
    is_trivially_destructible<_Tp>::value;
template <typename _Tp, typename... _Args>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_constructible_v =
    is_nothrow_constructible<_Tp, _Args...>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_default_constructible_v =
    is_nothrow_default_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_copy_constructible_v =
    is_nothrow_copy_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_move_constructible_v =
    is_nothrow_move_constructible<_Tp>::value;
template <typename _Tp, typename _Up>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_assignable_v =
    is_nothrow_assignable<_Tp, _Up>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_copy_assignable_v =
    is_nothrow_copy_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_move_assignable_v =
    is_nothrow_move_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_destructible_v =
    is_nothrow_destructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool has_virtual_destructor_v =
    has_virtual_destructor<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr size_t alignment_of_v = alignment_of<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr size_t rank_v = rank<_Tp>::value;
template <typename _Tp, unsigned _Idx = 0>
  __inline__ __attribute__((always_inline)) constexpr size_t extent_v = extent<_Tp, _Idx>::value;

template <typename _Tp, typename _Up>
  __inline__ __attribute__((always_inline)) constexpr bool is_same_v = __is_same(_Tp, _Up);




template <typename _Base, typename _Derived>
  __inline__ __attribute__((always_inline)) constexpr bool is_base_of_v = is_base_of<_Base, _Derived>::value;
template <typename _From, typename _To>
  __inline__ __attribute__((always_inline)) constexpr bool is_convertible_v = is_convertible<_From, _To>::value;
template<typename _Fn, typename... _Args>
  __inline__ __attribute__((always_inline)) constexpr bool is_invocable_v = is_invocable<_Fn, _Args...>::value;
template<typename _Fn, typename... _Args>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_invocable_v
    = is_nothrow_invocable<_Fn, _Args...>::value;
template<typename _Ret, typename _Fn, typename... _Args>
  __inline__ __attribute__((always_inline)) constexpr bool is_invocable_r_v
    = is_invocable_r<_Ret, _Fn, _Args...>::value;
template<typename _Ret, typename _Fn, typename... _Args>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_invocable_r_v
    = is_nothrow_invocable_r<_Ret, _Fn, _Args...>::value;





  template<typename _Tp>
    struct has_unique_object_representations
    : bool_constant<__has_unique_object_representations(
      remove_cv_t<remove_all_extents_t<_Tp>>
      )>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    __inline__ __attribute__((always_inline)) constexpr bool has_unique_object_representations_v
      = has_unique_object_representations<_Tp>::value;





  template<typename _Tp>
    struct is_aggregate
    : bool_constant<__is_aggregate(remove_cv_t<_Tp>)>
    { };


  template<typename _Tp>
    __inline__ __attribute__((always_inline)) constexpr bool is_aggregate_v = is_aggregate<_Tp>::value;
# 3599 "/usr/lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/type_traits" 3
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/tuple_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/ignore.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/ignore.h"
namespace AscendC {
namespace Std {

struct ignore_t
{
    [host,aicore] __inline__ __attribute__((always_inline)) ignore_t() = default;

    template <typename Tp, typename...Ts>
    [host,aicore] __inline__ __attribute__((always_inline)) constexpr ignore_t(const Tp&, const Ts&...) noexcept {}

    template <typename Tp>
    [host,aicore] __inline__ __attribute__((always_inline)) constexpr const ignore_t& operator=(const Tp&) const noexcept
    {
        return *this;
    }
};

constexpr ignore_t ignore{};

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/tuple_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/move.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/move.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_reference.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_reference.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_reference {
    using type = Tp;
};

template <typename Tp>
struct remove_reference<Tp&> {
    using type = Tp;
};

template <typename Tp>
struct remove_reference<Tp&&> {
    using type = Tp;
};

template <typename Tp>
using remove_reference_t = typename remove_reference<Tp>::type;

}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/move.h" 2

namespace AscendC {
namespace Std {

template <typename Tp>
[host,aicore] __inline__ __attribute__((always_inline)) constexpr remove_reference_t<Tp>&& move(Tp&& t) noexcept
{
    using Up = remove_reference_t<Tp> ;
    return static_cast<Up&&>(t);
}

}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/tuple_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/forward.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/forward.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_reference.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_reference.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/integral_constant.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/integral_constant.h"
namespace AscendC {
namespace Std {

template <typename Tp, Tp v>
struct integral_constant
{
    static constexpr const Tp value = v;

    using value_type = Tp;
    using type = integral_constant;

    [host,aicore] __inline__ __attribute__((always_inline)) constexpr operator value_type() const noexcept {
        return value;
    }

    [host,aicore] __inline__ __attribute__((always_inline)) constexpr value_type operator()() const noexcept {
        return value;
    }
};

template <typename Tp, Tp v>
constexpr const Tp integral_constant<Tp, v>::value;

using true_type = integral_constant<bool, true>;
using false_type = integral_constant<bool, false>;

template <bool b>
using bool_constant = integral_constant<bool, b>;

}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_reference.h" 2

namespace AscendC {
namespace Std {

template <typename Tp>
struct is_lvalue_reference : public false_type {};

template <typename Tp>
struct is_lvalue_reference<Tp&> : public true_type {};

template <typename Tp>
struct is_rvalue_reference : public false_type {};

template <typename Tp>
struct is_rvalue_reference<Tp&&> : public true_type {};

template <typename Tp>
struct is_reference : public false_type {};

template <typename Tp>
struct is_reference<Tp&> : public true_type {};

template <typename Tp>
struct is_reference<Tp&&> : public true_type {};

template <typename Tp>
constexpr bool is_lvalue_reference_v = is_lvalue_reference<Tp>::value;

template <typename Tp>
constexpr bool is_rvalue_reference_v = is_rvalue_reference<Tp>::value;

template <typename Tp>
constexpr bool is_reference_v = is_reference<Tp>::value;

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/forward.h" 2

namespace AscendC {
namespace Std {

template <typename Tp>
[host,aicore] __inline__ __attribute__((always_inline)) constexpr Tp&& forward(remove_reference_t<Tp>& t) noexcept
{
    return static_cast<Tp&&>(t);
}

template <typename Tp>
[host,aicore] __inline__ __attribute__((always_inline)) constexpr Tp&& forward(remove_reference_t<Tp>&& t) noexcept
{
    static_assert(!is_lvalue_reference<Tp>::value, "cannot forward an rvalue as an lvalue");
    return static_cast<Tp&&>(t);
}

}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/tuple_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../type_traits/decay.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../type_traits/decay.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/add_pointer.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/add_pointer.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_referenceable.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_referenceable.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_same.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_same.h"
namespace AscendC {
namespace Std {

template <typename TP, typename Up>
struct is_same : public false_type {};

template <typename TP>
struct is_same<TP, TP> : public true_type {};

template <typename TP, typename Up>
constexpr bool is_same_v = false;

template <typename TP>
constexpr bool is_same_v<TP, TP> = true;

template <typename Tp, typename Up>
using IsSame = bool_constant<is_same_v<Tp, Up>>;

template <typename Tp, typename Up>
using IsNotSame = bool_constant<!is_same_v<Tp, Up>>;

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_referenceable.h" 2

namespace AscendC {
namespace Std {

struct IsReferenceableImpl
{
    template <typename Tp>
    [host,aicore] __inline__ __attribute__((always_inline)) static Tp& Test(int32_t);

    template <typename Tp>
    [host,aicore] __inline__ __attribute__((always_inline)) static false_type Test(uint32_t);
};

template <typename Tp>
struct is_referenceable
    : integral_constant<bool, IsNotSame<decltype(IsReferenceableImpl::Test<Tp>(0)), false_type>::value> {};

}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/add_pointer.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_void.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_void.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_cv.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_cv.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_const.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_const.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_const {
    using type = Tp;
};

template <typename Tp>
struct remove_const<const Tp> {
    using type = Tp ;
};

template <typename Tp>
using remove_const_t = typename remove_const<Tp>::type;

}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_cv.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_volatile.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_volatile.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_volatile {
    using type = Tp;
};

template <typename Tp>
struct remove_volatile<volatile Tp> {
    using type = Tp;
};

template <typename Tp>
using remove_volatile_t = typename remove_volatile<Tp>::type;

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_cv.h" 2

namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_cv {
    using type = remove_volatile_t<remove_const_t<Tp>>;
};

template <typename Tp>
using remove_cv_t = remove_volatile_t<remove_const_t<Tp>>;

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_void.h" 2

namespace AscendC {
namespace Std {

template <typename Tp>
struct is_void : public is_same<remove_cv_t<Tp>, void> {};

template <typename Tp>
constexpr bool is_void_v = is_void<Tp>::value;

}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/add_pointer.h" 2



namespace AscendC {
namespace Std {

template <typename Tp, bool = is_referenceable<Tp>::value || is_void<Tp>::value>
struct AddPointerImpl {
    using type = remove_reference_t<Tp>*;
};

template <typename Tp>
struct AddPointerImpl<Tp, false> {
    using type = Tp;
};

template <typename Tp>
using add_pointer_t = typename AddPointerImpl<Tp>::type;

template <typename Tp>
struct add_pointer {
    using type = add_pointer_t<Tp>;
};

}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../type_traits/decay.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/conditional.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/conditional.h"
namespace AscendC {
namespace Std {

namespace conditional_impl {

template <bool>
struct IfImpl;

template <>
struct IfImpl<true>
{
    template <typename IfRes, typename ElseRes>
    using Select = IfRes;
};

template <>
struct IfImpl<false>
{
    template <typename IfRes, typename ElseRes>
    using Select = ElseRes;
};

template <bool Cond, typename IfRes, typename ElseRes>
using If = typename IfImpl<Cond>::template Select<IfRes, ElseRes>;

}

template <bool Bp, typename If, typename Then>
struct conditional {
    using type = If;
};

template <typename If, typename Then>
struct conditional<false, If, Then> {
    using type = Then;
};

template <bool Bp, typename If, typename Then>
using conditional_t = typename conditional<Bp, If, Then>::type;

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../type_traits/decay.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_array.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_array.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct is_array : public false_type {};

template <typename Tp>
struct is_array<Tp[]> : public true_type {};

template <typename Tp, size_t Np>
struct is_array<Tp[Np]> : public true_type {};

template <typename Tp>
constexpr bool is_array_v = is_array<Tp>::value;

}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../type_traits/decay.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_function.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_function.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_const.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_const.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct is_const : public false_type {};

template <typename Tp>
struct is_const<Tp const> : public true_type {};

template <typename Tp>
constexpr bool is_const_v = is_const<Tp>::value;

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_function.h" 2


namespace AscendC {
namespace Std {

template <typename T>
struct is_function : public bool_constant<!(is_reference_v<T> || is_const_v<const T>)> {};

template <typename Tp>
constexpr bool is_function_v = is_function<Tp>::value;

}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../type_traits/decay.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_extent.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/remove_extent.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_extent {
  using type = Tp;
};

template <typename Tp>
struct remove_extent<Tp[]> {
  using type = Tp;
};

template <typename Tp, size_t Np>
struct remove_extent<Tp[Np]> {
  using type = Tp;
};

template <typename Tp>
using remove_extent_t = typename remove_extent<Tp>::type;

}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../type_traits/decay.h" 2


namespace AscendC {
namespace Std {

template <typename Up, bool>
struct DecayImpl {
    using type = remove_cv_t<Up>;
};

template <typename Up>
struct DecayImpl<Up, true>
{
public:
    using type = conditional_t<is_array<Up>::value, remove_extent_t<Up>*,
    conditional_t<is_function<Up>::value, add_pointer_t<Up>, remove_cv_t<Up>>>;
};

template <typename Tp>
struct decay
{
private:
    using Up = remove_reference_t<Tp>;

public:
    using type = typename DecayImpl<Up, is_referenceable<Up>::value>::type;
};

template <typename Tp>
using decay_t = typename decay<Tp>::type;

}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/tuple_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../type_traits/enable_if.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../type_traits/enable_if.h"
namespace AscendC {
namespace Std {

template <bool, typename Tp = void>
struct enable_if {};

template <typename Tp>
struct enable_if<true, Tp> {
    using type = Tp;
};

template <bool Bp, typename Tp = void>
using enable_if_t = typename enable_if<Bp, Tp>::type;

}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/tuple_impl.h" 2







namespace AscendC {
namespace Std {

constexpr uint32_t ASCENDC_STD_TUPLE_STACK_DEEP = 64;

template <size_t N = 0, typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) void tuple_static_assert()
{
    static_assert(N < ASCENDC_STD_TUPLE_STACK_DEEP, "Index overflow. The index must be smaller than 64!");
    static_assert(sizeof...(Tps) <= ASCENDC_STD_TUPLE_STACK_DEEP, "The number of template elements must be <= 64!");
}

template <typename ...Tps>
class tuple;

template <>
class tuple <> {};

template <typename Tp, typename ...Tps>
struct tuple_constraints
{
    using removeType = typename remove_reference<Tp>::type;
    static constexpr bool variadic_copy_constructible = !is_same_v<Tp, removeType&&>;
};

template <typename Tp, typename ...Tps>
class tuple<Tp, Tps...> : public tuple<Tps...>
{
public:
    [host,aicore] __inline__ __attribute__((always_inline)) tuple() : tuple<Tps...>(), value() {
        tuple_static_assert<0, Tp, Tps...>();
    }

    template <typename Constraints = tuple_constraints<Tp, Tps...>,
        enable_if_t<Constraints::variadic_copy_constructible, int> = 0>
    [host,aicore] __inline__ __attribute__((always_inline)) tuple(const Tp& val, const Tps& ...params) : tuple<Tps...>(params...), value(val) {
        tuple_static_assert<0, Tp, Tps...>();
    }

    template <typename Constraints = tuple_constraints<Tp, Tps...>,
        enable_if_t<!Constraints::variadic_copy_constructible, int> = 0>
    [host,aicore] __inline__ __attribute__((always_inline)) tuple(Tp&& val, Tps&& ...params) : tuple<Tps...>(forward<Tps>(params)...), value(forward<Tp>(val)) {
        tuple_static_assert<0, Tp, Tps...>();
    }

    [host,aicore] __inline__ __attribute__((always_inline)) Tp& GetValue() noexcept {
        return value;
    }

    [host,aicore] __inline__ __attribute__((always_inline)) const Tp& GetValue() const noexcept {
        return value;
    }

    template <typename Head, typename ...Args>
    [host,aicore] __inline__ __attribute__((always_inline)) tuple<Tp, Tps...>& operator=(const tuple<Head, Args...>& t)
    {
        static_assert(sizeof...(Tps) == sizeof...(Args), "Both tuples must have the same number of elements");
        this->value = t.value;
        tuple<Tps...>(*this) = tuple<Args...>(t);
        return *this;
    }

private:
    template <typename...> friend class tuple;
    Tp value;
};


template <typename ...Tps>
struct tuple_size;

template <typename ...Tps>
struct tuple_size<tuple<Tps...>> : integral_constant<size_t, sizeof...(Tps)> {};

template <typename T>
struct tuple_size<const T> : public integral_constant<size_t, tuple_size<T>::value> {};

template <typename T>
struct tuple_size<volatile T> : public integral_constant<size_t, tuple_size<T>::value> {};

template <typename T>
struct tuple_size<const volatile T> : public integral_constant<size_t, tuple_size<T>::value> {};

template <typename T>
constexpr size_t tuple_size_v = tuple_size<T>::value;


template <size_t N, typename ...Tps>
struct tuple_element;

template <size_t N>
struct tuple_element<N, tuple<>> {
    static_assert(N < 0, "The index(N) is greater than the number of elements!");
};

template <size_t N, typename Tp, typename ...Tps>
struct tuple_element<N, tuple<Tp, Tps...>> : public tuple_element <N - 1, tuple<Tps...>>{};

template <typename Tp, typename ...Tps>
struct tuple_element<0, tuple<Tp, Tps...>> {
    using type = Tp;
    using tuple_t = tuple<Tp, Tps...>;
};

template <size_t N, typename T>
struct tuple_element<N, const T> {
    using type = const typename remove_const<typename tuple_element<N, T>::type>::type;
};

template <size_t N, typename T>
struct tuple_element<N, volatile T> {
    using type = volatile typename remove_volatile<typename tuple_element<N, T>::type>::type;
};

template <size_t N, typename T>
struct tuple_element<N, const volatile T> {
    using type = const volatile typename remove_cv<typename tuple_element<N, T>::type>::type;
};


template <typename T>
struct unwrap_refwrapper {
    using type = T;
};

template <typename T>
struct unwrap_refwrapper<std::reference_wrapper<T>> {
    using type = T&;
};

template <typename T>
using unwrap_decay_t = typename unwrap_refwrapper<decay_t<T>>::type;

template <typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) constexpr tuple<unwrap_decay_t<Tps>...> make_tuple(Tps&& ...args)
{
    tuple_static_assert<0, Tps...>();
    return tuple<unwrap_decay_t<Tps>...>(forward<Tps>(args)...);
}


template <typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) constexpr tuple<Tps& ...> tie(Tps& ...args) noexcept
{
    tuple_static_assert<0, Tps...>();
    return tuple<Tps&...>(args...);
}


template <typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) constexpr tuple<Tps&&...> forward_as_tuple(Tps&& ...args) noexcept
{
    tuple_static_assert<0, Tps...>();
    return tuple<Tps&&...>(forward<Tps>(args)...);
}


template <size_t N, typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) typename tuple_element<N, tuple<Tps...> >::type& get(tuple<Tps...>& t) noexcept
{
    tuple_static_assert<N, Tps...>();
    using type = typename tuple_element<N, tuple<Tps...> >::type;
    using tuple_t = typename tuple_element<N, tuple<Tps...> >::tuple_t;
    return static_cast<type&>(static_cast<tuple_t &>(t).GetValue());
}

template <size_t N, typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) const typename tuple_element<N, tuple<Tps...> >::type& get(const tuple<Tps...>& t) noexcept
{
    using type = const typename tuple_element<N, tuple<Tps...> >::type;
    return static_cast<type&>(get<N, Tps...>(const_cast<tuple<Tps...>&>(t)));
}

template <size_t N, typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) typename tuple_element<N, tuple<Tps...> >::type&& get(tuple<Tps...>&& t) noexcept
{
    using type = typename tuple_element<N, tuple<Tps...> >::type;
    return static_cast<type&&>(get<N, Tps...>(static_cast<tuple<Tps...>&>(t)));
}

template <size_t N, typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) const typename tuple_element<N, tuple<Tps...> >::type&& get(const tuple<Tps...>&& t) noexcept
{
    using type = const typename tuple_element<N, tuple<Tps...> >::type;
    return static_cast<type&&>(get<N, Tps...>(const_cast<tuple<Tps...>&&>(t)));
}

}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/tuple.h" 2

namespace AscendC {
namespace Std {


template <typename ...Tps>
class tuple;


template <typename ...Tps>
struct tuple_size;


template <size_t N, typename ...Tps>
struct tuple_element;


template <typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) constexpr tuple<unwrap_decay_t<Tps>...> make_tuple(Tps&& ...args);


template <typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) constexpr tuple<Tps& ...> tie(Tps& ...args) noexcept;


template <size_t N, typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) typename tuple_element<N, tuple<Tps...> >::type& get(tuple<Tps...>& t) noexcept;

template <size_t N, typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) const typename tuple_element<N, tuple<Tps...> >::type& get(const tuple<Tps...>& t) noexcept;

template <size_t N, typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) typename tuple_element<N, tuple<Tps...> >::type&& get(tuple<Tps...>&& t) noexcept;

template <size_t N, typename ...Tps>
[host,aicore] __inline__ __attribute__((always_inline)) const typename tuple_element<N, tuple<Tps...> >::type&& get(const tuple<Tps...>&& t) noexcept;

}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_layout.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/add_const.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/add_const.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct add_const {
    using type = const Tp;
};

template <typename Tp>
using add_const_t = typename add_const<Tp>::type;

}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/add_cv.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/add_cv.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct add_cv {
    using type = const volatile Tp;
};

template <typename Tp>
using add_cv_t = typename add_cv<Tp>::type;

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/add_lvalue_reference.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/add_lvalue_reference.h"
namespace AscendC {
namespace Std {

template <typename Tp, bool = is_referenceable<Tp>::value>
struct AddLvalueReferenceImpl {
    using type = Tp;
};

template <typename Tp>
struct AddLvalueReferenceImpl<Tp, true> {
    using type = Tp&;
};

template <typename Tp>
using add_lvalue_reference_t = typename AddLvalueReferenceImpl<Tp>::type;

template <typename Tp>
struct add_lvalue_reference {
    using type = add_lvalue_reference_t<Tp>;
};

}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/add_rvalue_reference.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/add_rvalue_reference.h"
namespace AscendC {
namespace Std {

template <typename Tp, bool = is_referenceable<Tp>::value>
struct AddRvalueReferenceImpl {
    using type = Tp;
};

template <typename Tp>
struct AddRvalueReferenceImpl<Tp, true> {
    using type = Tp&&;
};

template <typename Tp>
using add_rvalue_reference_t = typename AddRvalueReferenceImpl<Tp>::type;

template <typename Tp>
struct add_rvalue_reference {
    using type = add_rvalue_reference_t<Tp>;
};

}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/add_volatile.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/add_volatile.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct add_volatile {
    using type = volatile Tp;
};

template <typename Tp>
using add_volatile_t = typename add_volatile<Tp>::type;

}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2





# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_base_of.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_base_of.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_class.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_class.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_union.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_union.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct is_union : bool_constant<std::is_union<Tp>::value> {};

template <typename Tp>
constexpr bool is_union_v = is_union<Tp>::value;

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/is_class.h" 2

namespace AscendC {
namespace Std {

namespace IsClassImpl {

template <typename Tp>
[host,aicore] __inline__ __attribute__((always_inline)) bool_constant<!is_union_v<Tp>> Test(int32_t Tp::*);

template <typename Tp>
[host,aicore] __inline__ __attribute__((always_inline)) false_type Test(uint32_t);

}

template <typename Tp>
struct is_class : decltype(IsClassImpl::Test<Tp>(nullptr)) {};

template <typename Tp>
constexpr bool is_class_v = is_class<Tp>::value;

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_base_of.h" 2

namespace AscendC {
namespace Std {

template <typename Base, typename Derived>
struct IsBaseOfImpl {
private:
    template <typename B>
    [host,aicore] __inline__ __attribute__((always_inline)) static true_type TestPtrConv(const volatile B *);

    template <typename B>
    [host,aicore] __inline__ __attribute__((always_inline)) static false_type TestPtrConv(const volatile void *);

    template <typename B, typename D>
    [host,aicore] __inline__ __attribute__((always_inline)) static auto IsBaseOf(int32_t) -> decltype(TestPtrConv<B>(static_cast<D *>(nullptr)));

    template <typename B, typename D>
    [host,aicore] __inline__ __attribute__((always_inline)) static auto IsBaseOf(uint32_t) -> true_type;

public:
    static constexpr bool value = decltype(IsBaseOf<Base, Derived>(0))::value;
};

template <typename Base, typename Derived>
struct is_base_of : bool_constant<is_class_v<Base> && is_class_v<Derived> && IsBaseOfImpl<Base, Derived>::value> {};

template <typename Base, typename Derived>
constexpr bool is_base_of_v = is_base_of<Base, Derived>::value;

}
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_constant.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_constant.h"
 namespace AscendC {
 namespace Std {

 template <auto n, typename T>
 struct is_constant : false_type {};

 template <auto n, typename T>
 struct is_constant<n, T const> : is_constant<n,T> {};

 template <auto n, typename T>
 struct is_constant<n, T const&> : is_constant<n,T> {};

 template <auto n, typename T>
 struct is_constant<n, T &> : is_constant<n,T> {};

 template <auto n, typename T>
 struct is_constant<n, T &&> : is_constant<n,T> {};

 template <auto n, typename T, T v>
 struct is_constant<n, integral_constant<T,v>> : bool_constant<v == n> {};

 }
 }
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_convertible.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_convertible.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/../utility/declval.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/tuple/../utility/../type_traits/../utility/declval.h"
namespace AscendC {
namespace Std {

template <typename T>
[host,aicore] typename add_rvalue_reference<T>::type declval() noexcept
{
    static_assert(!std::is_abstract<T>::value || std::is_polymorphic<T>::value,
        "Std::declval() cannot be used with polymorphic and abstract types !");
    return static_cast<typename add_rvalue_reference<T>::type>(*static_cast<T*>(nullptr));
}

}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_convertible.h" 2

namespace AscendC {
namespace Std {

template <typename From, typename To>
struct IsConvertibleImpl {
private:
    template <typename T>
    [host,aicore] __inline__ __attribute__((always_inline)) static auto TestReturnable(int32_t) -> decltype(void(static_cast<T (*)()>(nullptr)), true_type{});

    template <typename T>
    [host,aicore] __inline__ __attribute__((always_inline)) static auto TestReturnable(uint32_t) -> false_type;

    template <typename F, typename T>
    [host,aicore] __inline__ __attribute__((always_inline)) static auto TestImplicitlyConvertible(int32_t) ->
        decltype(void(declval<void (&)(T)>()(declval<F>())), true_type{});

    template <typename F, typename T>
    [host,aicore] __inline__ __attribute__((always_inline)) static auto TestImplicitlyConvertible(uint32_t) -> false_type;

public:
    static constexpr bool value =
        decltype(TestReturnable<To>(0))::value && decltype(TestImplicitlyConvertible<From, To>(0))::value;
};

template <typename From, typename To>
struct is_convertible : bool_constant<(is_void_v<From> && is_void_v<To>) || IsConvertibleImpl<From, To>::value> {};

template <typename From, typename To>
constexpr bool is_convertible_v = is_convertible<From, To>::value;

template <typename Ty>
struct is_convertible<Ty&, volatile Ty&> : true_type {};

template <typename Ty>
struct is_convertible<volatile Ty&, volatile Ty&> : true_type {};

template <typename Ty>
struct is_convertible<Ty&, const volatile Ty&> : true_type {};

template <typename Ty>
struct is_convertible<volatile Ty&, const volatile Ty&> : true_type {};

template <typename Ty>
constexpr bool is_convertible_v<Ty&, volatile Ty&> = true;

template <typename Ty>
constexpr bool is_convertible_v<volatile Ty&, volatile Ty&> = true;

template <typename Ty>
constexpr bool is_convertible_v<Ty&, const volatile Ty&> = true;

template <typename Ty>
constexpr bool is_convertible_v<volatile Ty&, const volatile Ty&> = true;

}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_floating_point.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_floating_point.h"
namespace AscendC {
namespace Std {

template <typename T>
struct is_floating_point {
private:
    template <typename Head, typename... Args>
    [host,aicore] __inline__ __attribute__((always_inline)) static constexpr bool IsUnqualifiedAnyOf() {
        return (... || is_same_v<remove_cv_t<Head>, Args>);
    }

public:
    static constexpr bool value = IsUnqualifiedAnyOf<T, float, double, long double, __cce_half>();
};

template <typename Tp>
constexpr bool is_floating_point_v = is_floating_point<Tp>::value;

}
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_integral.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_integral.h"
namespace AscendC {
namespace Std {

template <typename T>
struct is_integral {
private:
    template <typename Tp, typename... Tps>
    [host,aicore] __inline__ __attribute__((always_inline)) static constexpr bool IsUnqualifiedAnyOf() {
        return (... || is_same_v<remove_cv_t<Tp>, Tps>);
    }

public:
    static constexpr bool value = IsUnqualifiedAnyOf<T,
        bool, unsigned long long, long long, unsigned long, long,
            unsigned int, int, unsigned short, short, unsigned char, signed char, char>();
};

template <typename T, T v>
struct is_integral<integral_constant<T,v>> : true_type {};

template <typename T>
constexpr bool is_integral_v = is_integral<T>::value;

}
}
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_pointer.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_pointer.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct IsPointerImpl : public false_type {};

template <typename Tp>
struct IsPointerImpl<Tp*> : public true_type {};

template <typename Tp>
struct is_pointer : public IsPointerImpl<remove_cv_t<Tp>> {};

template <typename Tp>
constexpr bool is_pointer_v = is_pointer<Tp>::value;

}
}
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2



# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_tuple.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/is_tuple.h"
namespace AscendC {
namespace Std {

template <typename T>
struct IsTupleImpl {
    private:
        template <typename Ts>
        [host,aicore] __inline__ __attribute__((always_inline)) static auto HasTupleSize(int32_t) -> bool_constant<(tuple_size<Ts>::value >= 0)>;

        template <typename Ts>
        [host,aicore] __inline__ __attribute__((always_inline)) static auto HasTupleSize(uint32_t) -> false_type;

    public:
        static constexpr bool value = decltype(HasTupleSize<T>((int32_t)0))::value;
};

template <typename T>
struct is_tuple : bool_constant<IsTupleImpl<T>::value> {};

template <typename T>
constexpr bool is_tuple_v = is_tuple<T>::value;

}
}
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2




# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/remove_cvref.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/remove_cvref.h"
namespace AscendC {
namespace Std {

template <typename Tp>
using remove_cvref_t = remove_cv_t<remove_reference_t<Tp>>;

template <typename Tp>
struct remove_cvref {
    using type = remove_cvref_t<Tp>;
};

template <typename Tp, typename Up>
struct is_same_uncvref : IsSame<remove_cvref_t<Tp>, remove_cvref_t<Up>> {};

}
}
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/remove_pointer.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/type_traits/remove_pointer.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_pointer {
    using type = Tp;
};

template <typename Tp>
struct remove_pointer<Tp*> {
    using type = Tp;
};

template <typename Tp>
struct remove_pointer<Tp* const> {
    using type = Tp;
};

template <typename Tp>
struct remove_pointer<Tp* volatile> {
    using type = Tp;
};

template <typename Tp>
struct remove_pointer<Tp* const volatile> {
    using type = Tp;
};

template <typename Tp>
using remove_pointer_t = typename remove_pointer<Tp>::type;

}
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/type_traits.h" 2



namespace AscendC {
namespace Std {


template <bool, typename Tp>
struct enable_if;


template <bool Bp, typename If, typename Then>
struct conditional;


template <typename From, typename To>
struct is_convertible;


template <typename Base, typename Derived>
struct is_base_of;


template <typename Tp, typename Up>
struct is_same;

}
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_layout.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/utility.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/utility.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/utility/integer_sequence.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/utility/integer_sequence.h"
namespace AscendC {
namespace Std {
namespace Impl {
constexpr size_t MaxIntegerSequenceSize = 64;
constexpr size_t SpiltSize = 2;
};
template <class T, T... Ns> struct IntegerSequence {
  using type = IntegerSequence;
  using valueType = T;
  static_assert((0 <= sizeof...(Ns) && sizeof...(Ns) <= Impl::MaxIntegerSequenceSize), "Std::index_sequence size must be within [0,64].");
  [host,aicore] __inline__ __attribute__((always_inline)) static constexpr size_t size() { return sizeof...(Ns); }
};
namespace Impl {
template <class T, class Seq0, class Seq1> struct MergeSeq {};
template <class T, T... Ns0, T... Ns1>
struct MergeSeq<T, IntegerSequence<T, Ns0...>, IntegerSequence<T, Ns1...>>
    : IntegerSequence<T, Ns0..., (sizeof...(Ns0) + Ns1)...> {};

template <class T, size_t N>
struct MakeIntegerSequence
    : Impl::MergeSeq<T, typename MakeIntegerSequence<T, N / SpiltSize>::type,
                     typename MakeIntegerSequence<T, N - N / SpiltSize>::type> {
};

template <class T> struct MakeIntegerSequence<T, 0> : IntegerSequence<T> {};
template <class T> struct MakeIntegerSequence<T, 1> : IntegerSequence<T, 0> {};
};
template <class T, T N>
using MakeIntegerSequenceNoChecked =
    typename Impl::MakeIntegerSequence<T, N>::type;
template <class T, T N> struct MakeIntegerSequenceChecked {
  static_assert(0 <= N && N <= Impl::MaxIntegerSequenceSize,
                "Std::make_index_sequence must be within [0,64].");
  typedef MakeIntegerSequenceNoChecked<T, 0 <= N ? N : 0> type;
};

template <class T, T N>
using MakeIntegerSequence = typename MakeIntegerSequenceChecked<T, N>::type;
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/utility.h" 2


namespace AscendC {
namespace Std {
template <size_t... Idx>
using index_sequence = IntegerSequence<size_t, Idx...>;
template <size_t N>
using make_index_sequence = MakeIntegerSequence<size_t, N>;
}
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_layout.h" 2

namespace AscendC {

template <typename CoordType, typename ShapeType, typename StrideType>
[aicore] __inline__ __attribute__((always_inline)) constexpr auto Crd2Idx(const CoordType& coord, const ShapeType& shape, const StrideType& stride);

template <typename... Shapes>
using Shape = Std::tuple<Shapes...>;

template <typename... Strides>
using Stride = Std::tuple<Strides...>;

template <typename... Ts>
[aicore] __inline__ __attribute__((always_inline)) constexpr Shape<Ts...> MakeShape(const Ts&... t) {
    return {t...};
}

template <typename... Ts>
[aicore] __inline__ __attribute__((always_inline)) constexpr Stride<Ts...> MakeStride(const Ts&... t) {
    return {t...};
}

template <typename ShapeType, typename StrideType>
struct Layout : private Std::tuple<ShapeType, StrideType> {
    [aicore] __inline__ __attribute__((always_inline)) constexpr Layout(const ShapeType& shape = {}, const StrideType& stride = {})
        : Std::tuple<ShapeType, StrideType>(shape, stride) {
            static_assert(Std::is_tuple_v<ShapeType> && Std::is_tuple_v<StrideType>, "Shape or Stride is not tuple!");
        }

    [aicore] __inline__ __attribute__((always_inline)) constexpr decltype(auto) layout() {
        return *this;
    }

    [aicore] __inline__ __attribute__((always_inline)) constexpr decltype(auto) layout() const {
        return *this;
    }

    template <size_t... I>
    [aicore] __inline__ __attribute__((always_inline)) constexpr decltype(auto) GetShape() {
        return GetValue<0, I...>(static_cast<Std::tuple<ShapeType, StrideType>&>(*this));
    }

    template <size_t... I>
    [aicore] __inline__ __attribute__((always_inline)) constexpr decltype(auto) GetShape() const {
        return GetValue<0, I...>(static_cast<const Std::tuple<ShapeType, StrideType>&>(*this));
    }

    template <size_t... I>
    [aicore] __inline__ __attribute__((always_inline)) constexpr decltype(auto) GetStride() {
        return GetValue<1, I...>(static_cast<Std::tuple<ShapeType, StrideType>&>(*this));
    }

    template <size_t... I>
    [aicore] __inline__ __attribute__((always_inline)) constexpr decltype(auto) GetStride() const {
        return GetValue<1, I...>(static_cast<const Std::tuple<ShapeType, StrideType>&>(*this));
    }

    template <typename CoordType>
    [aicore] __inline__ __attribute__((always_inline)) constexpr auto operator()(const CoordType& coord) const
    {
        return Crd2Idx(coord, GetShape(), GetStride());
    }

private:
    template<size_t index, size_t I, size_t... Is, typename Tuple>
    [aicore] __inline__ __attribute__((always_inline)) constexpr decltype(auto) GetValue(const Tuple& t)
    {
        auto tupleEle = Std::get<index>(t);
        return Std::make_tuple(Std::get<I>(tupleEle), Std::get<Is>(tupleEle)...);
    }

    template<size_t index, size_t I, size_t... Is, typename Tuple>
    [aicore] __inline__ __attribute__((always_inline)) constexpr decltype(auto) GetValue(const Tuple& t) const
    {
        auto tupleEle = Std::get<index>(t);
        return Std::make_tuple(Std::get<I>(tupleEle), Std::get<Is>(tupleEle)...);
    }

    template<size_t index, typename Tuple>
    [aicore] __inline__ __attribute__((always_inline)) constexpr decltype(auto) GetValue(const Tuple& t)
    {
        return Std::get<index>(t);
    }

    template<size_t index, typename Tuple>
    [aicore] __inline__ __attribute__((always_inline)) constexpr decltype(auto) GetValue(const Tuple& t) const
    {
        return Std::get<index>(t);
    }
};

template <typename ShapeType, typename StrideType>
[aicore] __inline__ __attribute__((always_inline)) constexpr auto MakeLayout(const ShapeType& shape, const StrideType& stride)
{
    static_assert(Std::is_tuple_v<ShapeType> && Std::is_tuple_v<StrideType>, "Shape or Stride is not tuple!");
    return Layout<ShapeType, StrideType>(shape, stride);
}

template <typename T>
struct is_layout : Std::false_type {};

template <typename ShapeType, typename StrideType>
struct is_layout<Layout<ShapeType, StrideType>> : Std::true_type {};

template <typename T>
constexpr bool is_layout_v = is_layout<T>::value;

}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_coord.h" 2

namespace AscendC {

namespace CoordImpl {
template <size_t v>
using Int = Std::integral_constant<size_t, v>;
using IntZero = Int<0>;
}







template <auto t, auto u> [aicore] __inline__ __attribute__((always_inline)) constexpr CoordImpl::Int<(t + u)> operator + (CoordImpl::Int<t>, CoordImpl::Int<u>) { return {}; };
template <auto t, auto u> [aicore] __inline__ __attribute__((always_inline)) constexpr CoordImpl::Int<(t - u)> operator - (CoordImpl::Int<t>, CoordImpl::Int<u>) { return {}; };
template <auto t, auto u> [aicore] __inline__ __attribute__((always_inline)) constexpr CoordImpl::Int<(t * u)> operator * (CoordImpl::Int<t>, CoordImpl::Int<u>) { return {}; };
template <auto t, auto u> [aicore] __inline__ __attribute__((always_inline)) constexpr CoordImpl::Int<(t / u)> operator / (CoordImpl::Int<t>, CoordImpl::Int<u>) { return {}; };
template <auto t, auto u> [aicore] __inline__ __attribute__((always_inline)) constexpr CoordImpl::Int<(t % u)> operator % (CoordImpl::Int<t>, CoordImpl::Int<u>) { return {}; };



template <typename... Coords>
using Coord = Std::tuple<Coords...>;

template <typename... Ts>
[aicore] __inline__ __attribute__((always_inline)) constexpr Coord<Ts...> MakeCoord(Ts const&... t) {
  return {t...};
}

template <typename TupleType>
using tuple_sequence = Std::make_index_sequence<Std::tuple_size_v<Std::remove_cvref_t<TupleType>>>;

template <typename T, typename F, typename G, size_t... I>
[aicore] __inline__ __attribute__((always_inline)) constexpr auto TupleApply(T&& t, F&& f, G&& g, Std::index_sequence<I...>)
{
    return g(f(Std::get<I>(static_cast<T&&>(t)))...);
}

template <typename T, typename F, typename G>
[aicore] __inline__ __attribute__((always_inline)) constexpr auto TransformApply(T&& t, F&& f, G&& g)
{
    if constexpr (Std::is_tuple_v<Std::remove_cvref_t<T>>) {
        return TupleApply(static_cast<T&&>(t), f, g, tuple_sequence<T>{});
    } else {
        return g(f(static_cast<T&&>(t)));
    }
}

struct MultipliesUnaryLeftFold {
    template <typename... T>
    [aicore] __inline__ __attribute__((always_inline)) constexpr auto operator()(T&&... t) const {
        return (... * t);
    }
};

struct Product
{
    template <typename IntTuple>
    [aicore] __inline__ __attribute__((always_inline)) constexpr auto operator()(const IntTuple& intT) const
    {
        if constexpr (Std::is_tuple_v<IntTuple>) {
            if constexpr (Std::tuple_size_v<IntTuple> == 0) {
                return CoordImpl::Int<1>{};
            } else {
                return TransformApply(intT, Product{}, MultipliesUnaryLeftFold{});
            }
        } else if constexpr (Std::is_integral<IntTuple>::value) {
            return intT;
        } else {
            static_assert(sizeof(IntTuple) == 0, "Invalid Product parameters");
        }
    }
};

template <typename CoordType, typename ShapeType, typename StrideType>
[aicore] __inline__ __attribute__((always_inline)) constexpr auto Crd2Idx(const CoordType& coord, const ShapeType& shape, const StrideType& stride);

template <typename CoordType, typename ShapeType, typename StrideType, size_t... Is>
[aicore] __inline__ __attribute__((always_inline)) constexpr auto Crd2IdxTTT(const CoordType& coord, const ShapeType& shape, const StrideType& stride,
    Std::index_sequence<Is...>)
{
  return (... + Crd2Idx(Std::get<Is>(coord), Std::get<Is>(shape), Std::get<Is>(stride)));
}

template <typename CInt, typename STuple, typename DTuple, size_t I0, size_t... Is>
[aicore] __inline__ __attribute__((always_inline)) constexpr auto Crd2IdxITT(const CInt& coord, const STuple& shape, const DTuple& stride,
    Std::index_sequence<I0,Is...>)
{
    if constexpr (sizeof...(Is) == 0) {
        return Crd2Idx(coord, Std::get<I0>(shape), Std::get<I0>(stride));
    } else if constexpr (Std::is_constant<0, CInt>::value) {
        return Crd2Idx(CoordImpl::IntZero{}, Std::get<I0>(shape), Std::get<I0>(stride)) +
            (CoordImpl::IntZero{} + ... + Crd2Idx(CoordImpl::IntZero{}, Std::get<Is>(shape), Std::get<Is>(stride)));
    } else {
        auto prod = Product{}(Std::get<I0>(shape));
        auto div = coord / prod;
        auto mod = coord % prod;
        return Crd2Idx(mod, Std::get<I0>(shape), Std::get<I0>(stride)) +
            Crd2IdxITT(div, shape, stride, Std::index_sequence<Is...>{});
    }
}

template <typename CoordType, typename ShapeType, typename StrideType>
[aicore] __inline__ __attribute__((always_inline)) constexpr auto Crd2Idx(const CoordType& coord, const ShapeType& shape, const StrideType& stride)
{
    if constexpr (Std::is_tuple_v<CoordType>) {
        if constexpr (Std::is_tuple_v<ShapeType>) {
            static_assert(Std::tuple_size_v<CoordType> == Std::tuple_size_v<ShapeType>, "Shape and Coord Mismatched Ranks");
            static_assert(Std::tuple_size_v<CoordType> == Std::tuple_size_v<StrideType>, "Stride and Coord Mismatched Ranks");
            return Crd2IdxTTT(coord, shape, stride, tuple_sequence<CoordType>{});
        } else {
            static_assert(sizeof(CoordType) == 0, "Invalid parameters, ShapeType is not tuple!");
        }
    } else {
        if constexpr (Std::is_tuple_v<ShapeType>) {
            static_assert(Std::tuple_size_v<ShapeType> == Std::tuple_size_v<StrideType>, "Shape and Stride Mismatched Ranks");
            return Crd2IdxITT(coord, shape, stride, tuple_sequence<ShapeType>{});
        } else {
            return coord * stride;
        }
    }
}

template <typename CoordType, typename ShapeType, typename StrideType>
[aicore] __inline__ __attribute__((always_inline)) constexpr auto Crd2Idx(const CoordType& coord, const Layout<ShapeType, StrideType>& layout)
{
  return Crd2Idx(coord, layout.GetShape(), layout.GetStride());
}

}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_tensor_trait.h" 2


namespace AscendC {







template <typename T, TPosition pos = TPosition::GM, typename LayoutType = Layout<Shape<>, Stride<>>>
struct TensorTrait {
    using LiteType = T;
    using LiteLayoutType = LayoutType;
    static constexpr const TPosition tPos = pos;
public:
    [aicore] __inline__ __attribute__((always_inline)) TensorTrait(const LayoutType& t = {}) {
        static_assert(is_layout_v<LayoutType>, "TensorTrait without layout instantiation!");
        this->layout_ = t;
    }

    [aicore] __inline__ __attribute__((always_inline)) LayoutType& GetLayout() {
        static_assert(is_layout_v<LayoutType>, "TensorTrait without layout instantiation!");
        return layout_;
    }

    [aicore] __inline__ __attribute__((always_inline)) const LayoutType& GetLayout() const {
        static_assert(is_layout_v<LayoutType>, "TensorTrait without layout instantiation!");
        return layout_;
    }

    [aicore] __inline__ __attribute__((always_inline)) void SetLayout(const LayoutType& t) {
        static_assert(is_layout_v<LayoutType>, "TensorTrait without layout instantiation!");
        this->layout_ = t;
    }
private:
    LayoutType layout_ = {};
};

template <typename T, TPosition pos, typename LayoutType>
[aicore] __inline__ __attribute__((always_inline)) constexpr auto MakeTensorTrait(const LayoutType& t)
{
    static_assert(is_layout_v<LayoutType>, "Input parameters does not contain the layout type!");
    return TensorTrait<T, pos, LayoutType>(t);
}

template <typename T>
struct is_tensorTrait : Std::false_type {};

template <typename T, TPosition pos, typename ShapeType, typename StrideType>
struct is_tensorTrait<TensorTrait<T, pos, Layout<ShapeType, StrideType>>> : Std::true_type {};

template <typename T>
constexpr bool is_tensorTrait_v = is_tensorTrait<T>::value;

}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h" 2

namespace AscendC {
class TPipe;
class KfcCommClient;

[aicore] __inline__ __attribute__((always_inline)) constexpr int16_t GetDataBlockSizeInBytes()
{
    return ONE_BLK_SIZE;
}
}





[[block_local]] __inline__ AscendC::TPipe* g_vecTPipePtr;
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h"
[[block_local]] __inline__ __cce_half g_deqValue;




[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_sysWorkspaceReserved;
[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_dumpWorkspaceReserved;
[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_hcclContextReserved[2];




[aicore] __inline__ __attribute__((always_inline)) AscendC::TPipe* GetTPipePtr()
{




    return g_vecTPipePtr;






}


namespace AscendC {
template <typename T, MaskMode mode = MaskMode::NORMAL>
[aicore] static __inline__ __attribute__((always_inline)) void SetVectorMask(const uint64_t maskHigh, const uint64_t maskLow)
{





    SetVectorMaskImpl<T, mode>(maskHigh, maskLow);
}

template <typename T, MaskMode mode = MaskMode::NORMAL>
[aicore] static __inline__ __attribute__((always_inline)) void SetVectorMask(int32_t len)
{



    SetVectorMaskImpl<T, mode>(len);
}

[aicore] __inline__ __attribute__((always_inline)) void ResetMask()
{



    ResetMaskImpl();
}


template <MemDsbT arg0>
[aicore] __inline__ __attribute__((always_inline)) void DataSyncBarrier()
{
    DataSyncBarrierImpl<arg0>();
}


template <HardEvent event> [aicore] __inline__ __attribute__((always_inline)) void SetFlag(int32_t eventID)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (event == HardEvent::MTE2_V || event == HardEvent::V_MTE2 || event == HardEvent::MTE3_V ||
            event == HardEvent::V_MTE3 || event == HardEvent::V_V || event == HardEvent::S_V ||
            event == HardEvent::V_S) {
            return;
        }
    }
    SetFlagImpl<event>(eventID);
}

template <HardEvent event> [aicore] __inline__ __attribute__((always_inline)) void WaitFlag(int32_t eventID)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (event == HardEvent::MTE2_V || event == HardEvent::V_MTE2 || event == HardEvent::MTE3_V ||
            event == HardEvent::V_MTE3 || event == HardEvent::V_V || event == HardEvent::S_V ||
            event == HardEvent::V_S) {
            return;
        }
    }
    WaitFlagImpl(event, eventID);
}

template <pipe_t pipe> [aicore] __inline__ __attribute__((always_inline)) void PipeBarrier()
{
    PipeBarrierImpl<pipe>();
}


template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] __inline__ __attribute__((always_inline)) void DataCacheCleanAndInvalid(const GlobalTensor<T>& dstTensor)
{
    DcciGMImpl<T, entireType, dcciDst>(const_cast<__attribute__((cce_global)) T*>(dstTensor.GetPhyAddr()));
}

template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] __inline__ __attribute__((always_inline)) void DataCacheCleanAndInvalid(const LocalTensor<T>& dstTensor)
{
    DcciUBImpl<T, entireType, dcciDst>(const_cast<__attribute__((cce_unif_buff)) T*>(dstTensor.GetPhyAddr()));
}



template <typename T, CacheLine entireType>
[aicore] __inline__ __attribute__((always_inline)) void DataCacheCleanAndInvalid(const GlobalTensor<T>& dstTensor)
{
    DcciGMImpl<T, entireType>(const_cast<__attribute__((cce_global)) T*>(dstTensor.GetPhyAddr()));
}


[aicore] __inline__ __attribute__((always_inline)) void SetMaskCount()
{
    SetMaskCountImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void SetMaskNorm()
{
    SetMaskNormImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void SetHF32Mode(bool hf32Mode)
{
    SetHF32ModeImpl(hf32Mode);
}

[aicore] __inline__ __attribute__((always_inline)) void SetHF32TransMode(bool hf32TransMode)
{
    SetHF32TransModeImpl(hf32TransMode);
}

[aicore] __inline__ __attribute__((always_inline)) void SetMMLayoutTransform(bool mmLayoutMode)
{
    SetMMLayoutTransformImpl(mmLayoutMode);
}

template <uint32_t index>
[aicore] __inline__ __attribute__((always_inline)) void SetHcclContext(__attribute__((cce_global)) uint8_t* context)
{
    if constexpr (index > 1) {
        return;
    }
    g_hcclContextReserved[index] = context;
}

template <uint32_t index>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) GetHcclContext(void)
{
    if constexpr (index > 1) {
        return nullptr;
    }
    return g_hcclContextReserved[index];
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctions(const GlobalTensor<T>& src0, AippInputFormat format, AippParams<U> config)
{
    SetAippFunctionsImpl<PrimT<T>, U>(const_cast<__attribute__((cce_global)) PrimT<T>*>(src0.GetPhyAddr()), format, config);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctions(const GlobalTensor<T>& src0, const GlobalTensor<T>& src1,
    AippInputFormat format, AippParams<U> config)
{
    SetAippFunctionsImpl<PrimT<T>, U>(const_cast<__attribute__((cce_global)) PrimT<T>*>(src0.GetPhyAddr()),
        const_cast<__attribute__((cce_global)) PrimT<T>*>(src1.GetPhyAddr()), format, config);
}

}

[[deprecated("NOTICE: SetDumpWorkSpacePtr has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) SetDumpWorkSpacePtr(__attribute__((cce_global)) uint8_t* workspace)
{
    return g_dumpWorkspaceReserved = workspace;
}
[[deprecated("NOTICE: GetDumpWorkSpacePtr has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) GetDumpWorkSpacePtr()
{
    return g_dumpWorkspaceReserved;
}




[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) GetSysWorkSpacePtr()
{
    return g_sysWorkspaceReserved;
}
[[deprecated(
    "NOTICE: SetSysWorkSpacePtr has been deprecated and will be removed in the next version.")]]
[aicore] __inline__ __attribute__((always_inline)) void SetSysWorkSpacePtr(__attribute__((cce_global)) uint8_t* workspace)
{
    g_sysWorkspaceReserved = workspace;
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_symbol_override_impl.h" 1
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_symbol_override_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_cmp_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_cmp_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_binary.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_binary.h"
namespace AscendC {
struct BinaryRepeatParams {
    [aicore] BinaryRepeatParams() {}

    [aicore] BinaryRepeatParams(const uint8_t dstBlkStrideIn, const uint8_t src0BlkStrideIn,
        const uint8_t src1BlkStrideIn, const uint8_t dstRepStrideIn, const uint8_t src0RepStrideIn,
        const uint8_t src1RepStrideIn)
        : dstBlkStride(dstBlkStrideIn),
          src0BlkStride(src0BlkStrideIn),
          src1BlkStride(src1BlkStrideIn),
          dstRepStride(dstRepStrideIn),
          src0RepStride(src0RepStrideIn),
          src1RepStride(src1RepStrideIn)
    {}

    uint32_t blockNumber = DEFAULT_BLK_NUM;
    uint8_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
};
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_cmp_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_unary.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_unary.h"
namespace AscendC {
struct UnaryRepeatParams {
    [aicore] UnaryRepeatParams() {}

    [aicore] UnaryRepeatParams(const uint16_t dstBlkStrideIn, const uint16_t srcBlkStrideIn,
        const uint8_t dstRepStrideIn, const uint8_t srcRepStrideIn)
        : dstBlkStride(dstBlkStrideIn),
          srcBlkStride(srcBlkStrideIn),
          dstRepStride(dstRepStrideIn),
          srcRepStride(srcRepStrideIn)
    {}

    [aicore] UnaryRepeatParams(const uint16_t dstBlkStrideIn, const uint16_t srcBlkStrideIn,
        const uint8_t dstRepStrideIn, const uint8_t srcRepStrideIn, const bool halfBlockIn)
        : dstBlkStride(dstBlkStrideIn),
          srcBlkStride(srcBlkStrideIn),
          dstRepStride(dstRepStrideIn),
          srcRepStride(srcRepStrideIn),
          halfBlock(halfBlockIn)
    {}

    uint32_t blockNumber = DEFAULT_BLK_NUM;
    uint16_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint16_t srcBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t srcRepStride = DEFAULT_REPEAT_STRIDE;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
    bool halfBlock = false;
};
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_cmp_impl.h" 2

namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    uint8_t repeat, const BinaryRepeatParams& repeatParams)
{
    switch (cmpMode) {
        case CMPMODE::LT: {
            vcmpv_lt(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GT: {
            vcmpv_gt(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::EQ: {
            vcmpv_eq(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::LE: {
            vcmpv_le(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GE: {
            vcmpv_ge(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::NE: {
            vcmpv_ne(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        default:

                                                                                                                   ;
            break;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VcmpIntrinsicsImpl(__attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const BinaryRepeatParams& repeatParams)
{
    switch (cmpMode) {
        case CMPMODE::LT: {
            vcmp_lt(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GT: {
            vcmp_gt(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::EQ: {
            vcmp_eq(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::LE: {
            vcmp_le(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GE: {
            vcmp_ge(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::NE: {
            vcmp_ne(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        default:

                                                                                                                   ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void VcmpvIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) int32_t* src0, __attribute__((cce_unif_buff)) int32_t* src1,
    CMPMODE cmpMode, uint8_t repeat, const BinaryRepeatParams& repeatParams)
{


                                                                                               ;
    vcmpv_eq(dst, src0, src1, repeat, repeatParams.dstBlkStride,
        repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
        repeatParams.src0RepStride, repeatParams.src1RepStride);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void CompareCompute(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        struct BinaryRepeatParams repeatParams;
        uint32_t sumRepeat = calCount * sizeof(T) / ONE_REPEAT_BYTE_SIZE;
        constexpr uint32_t repeatNormal = 252;
        uint32_t repeatRound = sumRepeat / repeatNormal;
        uint32_t repeatTail = sumRepeat % repeatNormal;
        uint32_t srcOffset = repeatNormal * ONE_REPEAT_BYTE_SIZE / sizeof(T);
        uint32_t dstOffset = srcOffset / ONE_BYTE_BIT_SIZE;

        for (uint32_t i = 0; i < repeatRound; ++i) {
            VcmpvImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + i * dstOffset,
                src0 + i * srcOffset,
                src1 + i * srcOffset, cmpMode, MASK_PLACEHOLDER, repeatNormal,
                repeatParams);
        }
        VcmpvImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + repeatRound * dstOffset,
            src0 + repeatRound * srcOffset,
            src1 + repeatRound * srcOffset, cmpMode, MASK_PLACEHOLDER, repeatTail,
            repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask[], uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {
        VcmpvIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpImpl(__attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask[], const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        VcmpIntrinsicsImpl(src0, src1, cmpMode, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask, uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {
        VcmpvIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpImpl(__attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        VcmpIntrinsicsImpl(src0, src1, cmpMode, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (IsSameType<T, int32_t>::value) {


                                                                  ;
        }
        CompareCompute(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode, calCount);
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvsIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (cmpMode) {
        case CMPMODE::LT: {
            vcmpvs_lt(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::GT: {
            vcmpvs_gt(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::EQ: {
            vcmpvs_eq(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::LE: {
            vcmpvs_le(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::GE: {
            vcmpvs_ge(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::NE: {
            vcmpvs_ne(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        default:

                                                                                                                   ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void VcmpvsIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) int32_t* src0, int32_t src1,
    CMPMODE cmpMode, uint8_t repeat, const UnaryRepeatParams& repeatParams)
{


                                                                                                     ;
    vcmpvs_eq(dst, src0, src1, repeat,
        static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalarCompute(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        struct UnaryRepeatParams repeatParams;
        uint32_t sumRepeat = calCount * sizeof(T) / ONE_REPEAT_BYTE_SIZE;
        constexpr uint32_t repeatNormal = 252;
        uint32_t repeatRound = sumRepeat / repeatNormal;
        uint32_t repeatTail = sumRepeat % repeatNormal;
        uint32_t srcOffset = repeatNormal * ONE_REPEAT_BYTE_SIZE / sizeof(T);
        uint32_t dstOffset = srcOffset / ONE_BYTE_BIT_SIZE;
        for (uint32_t i = 0; i < repeatRound; ++i) {
            VcmpvsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + i * dstOffset,
                src0 + i * srcOffset,
                src1, cmpMode, MASK_PLACEHOLDER, repeatNormal,
                repeatParams);
        }
        VcmpvsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + repeatRound * dstOffset,
            src0 + repeatRound * srcOffset,
            src1, cmpMode, MASK_PLACEHOLDER, repeatTail,
            repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvsImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {
        VcmpvsIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTimes, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvsImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint64_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {
        VcmpvsIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvsImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (IsSameType<T, int32_t>::value) {


                                                                  ;
        }
        CompareScalarCompute(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode, calCount);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetCmpMaskImpl(__attribute__((cce_unif_buff)) T* dst)
{
    get_cmpmask(dst);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetCmpMaskImpl(__attribute__((cce_unif_buff)) T* src)
{
    set_cmpmask(src);
}
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_symbol_override_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_binary_impl.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_binary_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float, int16_t, int32_t>(), "Failed to check dtype in Add, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vadd(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AddIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AddIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                    ;
        set_mask_count();
        set_vector_mask(0, calCount);
        vadd(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float, int16_t, int32_t>(), "Failed to check dtype in Sub, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vsub(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        SubIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        SubIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                    ;
        set_mask_count();
        set_vector_mask(0, calCount);
        vsub(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MulIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float, int16_t, int32_t>(), "Failed to check dtype in Mul, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmul(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MulIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MulIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MulImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                    ;
        set_mask_count();
        set_vector_mask(0, calCount);
        vmul(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DivIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float>(), "Failed to check dtype in Div, current api support dtype combination "
        "is src and dst both: half / float.");
    vdiv(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void DivImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        DivIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void DivImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        DivIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DivImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                          ;
        set_mask_count();
        set_vector_mask(0, calCount);
        vdiv(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MaxIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float, int16_t, int32_t>(), "Failed to check dtype in Max, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmax(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MaxImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MaxIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MaxImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MaxIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MaxImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                    ;
        set_mask_count();
        set_vector_mask(0, calCount);
        vmax(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MinIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float, int16_t, int32_t>(), "Failed to check dtype in Min, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmin(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MinImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MinIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MinImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MinIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MinImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                    ;
        set_mask_count();
        set_vector_mask(0, calCount);
        vmin(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AndIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, uint16_t>(), "Failed to check dtype in And, current api support dtype "
        "combination is src and dst both: int16_t / uint16_t.");
    vand(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AndImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AndIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AndImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AndIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AndImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                                ;
        set_mask_count();
        set_vector_mask(0, calCount);

        vand((__attribute__((cce_unif_buff)) int16_t*)dst, (__attribute__((cce_unif_buff)) int16_t*)src0, (__attribute__((cce_unif_buff)) int16_t*)src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void OrIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, uint16_t>(), "Failed to check dtype in Or, current api support dtype "
        "combination is src and dst both: int16_t / uint16_t.");
    vor(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
        repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void OrImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        OrIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void OrImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        OrIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void OrImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                        ;
        set_mask_count();
        set_vector_mask(0, calCount);

        vor((__attribute__((cce_unif_buff)) int16_t*)dst, (__attribute__((cce_unif_buff)) int16_t*)src0, (__attribute__((cce_unif_buff)) int16_t*)src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, __cce_half, float>(), "Failed to check dtype in AddRelu, current api support dtype "
        "combination is src and dst both: int16_t / half / float.");
    vaddrelu(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AddReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AddReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                             ;
        set_mask_count();
        set_vector_mask(0, calCount);
        vaddrelu((__attribute__((cce_unif_buff)) T*)dst, (__attribute__((cce_unif_buff)) T*)src0, (__attribute__((cce_unif_buff)) T*)src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




struct AddDeqReluParams {
    [aicore] AddDeqReluParams(){};

    uint32_t needTmpSize = 0;
    uint32_t calcSize = 0;
    uint32_t src0Offset = 0;
    uint32_t src1Offset = 0;
    uint32_t dstOffset = 0;
    uint32_t tailSrc0Offset = 0;
    uint32_t tailSrc1Offset = 0;
    uint32_t tailDstOffset = 0;
    uint64_t mask1;
    uint64_t mask2[2];
    uint8_t maskMode = 0;
    uint16_t mainBlock = 0;
    uint16_t tailSize = 0;

    uint8_t repeat = 0;
    uint8_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
};

[aicore] __inline__ __attribute__((always_inline)) void SetAddDeqReluMaskCal(AddDeqReluParams &params)
{
    if (params.maskMode == ADDDEQRELU_MASK_MODE_ONE) {
        AscendCUtils::SetMask<__cce_half>(params.mask1);
    } else if (params.maskMode == ADDDEQRELU_MASK_MODE_TWO) {
        AscendCUtils::SetMask<__cce_half>(params.mask2[1], params.mask2[0]);
    }
}

template <bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqReluComput(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    __attribute__((cce_unif_buff)) int32_t *tmpBuffer, AddDeqReluParams &params)
{

    vadd(tmpBuffer, src0, src1, params.repeat, DEFAULT_BLK_STRIDE, params.src0BlkStride,
        params.src1BlkStride, DEFAULT_REPEAT_STRIDE, params.src0RepStride, params.src1RepStride);
    pipe_barrier(PIPE_V);

    __attribute__((cce_unif_buff)) float *src0FloatTmp = reinterpret_cast<__attribute__((cce_unif_buff)) float *>(src0);
    vconv_s322f32(src0FloatTmp, tmpBuffer, params.repeat, static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    vmuls(src0FloatTmp, src0FloatTmp, static_cast<float>(DEQ_SHIFT_RIGHT_17_BIT), params.repeat,
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    vmuls(src0FloatTmp, src0FloatTmp, static_cast<float>(g_deqValue), params.repeat,
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    vmuls(src0FloatTmp, src0FloatTmp, static_cast<float>(DEQ_SHIFT_LEFT_17_BIT), params.repeat,
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    if constexpr (isSetMask) {
        SetAddDeqReluMaskCal(params);
    }
    __attribute__((cce_unif_buff)) __cce_half *src1HalfTmp = reinterpret_cast<__attribute__((cce_unif_buff)) __cce_half *>(src1);
    vconv_f322f16(src1HalfTmp, src0FloatTmp, params.repeat, 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    pipe_barrier(PIPE_V);

    __attribute__((cce_unif_buff)) __cce_half *tmpBufferHalf = reinterpret_cast<__attribute__((cce_unif_buff)) __cce_half *>(tmpBuffer);
    if (params.maskMode != 0) {
        set_mask_count();
        set_vector_mask(0, static_cast<uint64_t>(params.calcSize));
    }
    vector_dup(tmpBufferHalf, static_cast<__cce_half>(0), 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), 1,
        DEFAULT_REPEAT_STRIDE, 0);
    if (params.maskMode != 0) {
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
    pipe_barrier(PIPE_V);

    if constexpr (isSetMask) {
        SetAddDeqReluMaskCal(params);
    }
    if (params.maskMode == 0) {
        vmax(dst, tmpBufferHalf, src1HalfTmp, params.repeat, params.dstBlkStride, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, params.dstRepStride, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    } else {
        vmax(dst, tmpBufferHalf, src1HalfTmp, params.repeat, params.dstBlkStride, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, params.dstRepStride, HALF_DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    }
    pipe_barrier(PIPE_V);
}

[aicore] __inline__ __attribute__((always_inline)) void GetAddDeqReluParamCal(AddDeqReluParams &params, uint8_t repeatTimes)
{
    if (params.needTmpSize <= TMP_UB_SIZE / sizeof(int32_t)) {
        params.calcSize = params.needTmpSize;
        if (params.maskMode != 0) {
            params.repeat = repeatTimes;
        }
    } else {
        params.calcSize = TMP_UB_SIZE / sizeof(int32_t);
        if (params.maskMode != 0) {
            params.repeat = params.calcSize / B32_DATA_NUM_PER_REPEAT;
        }
    }
    if (params.maskMode == 0) {
        params.repeat = repeatTimes;
    }
    params.mainBlock = params.needTmpSize / params.calcSize;
    params.tailSize = params.needTmpSize % params.calcSize;
    if (params.maskMode == 0) {
        params.src0Offset = params.calcSize;
        params.src1Offset = params.calcSize;
        params.dstOffset = params.calcSize;
    } else {
        params.src0Offset = params.repeat * params.src0RepStride * B32_DATA_NUM_PER_BLOCK;
        params.src1Offset = params.repeat * params.src1RepStride * B32_DATA_NUM_PER_BLOCK;
        params.dstOffset = params.repeat * params.dstRepStride * B16_DATA_NUM_PER_BLOCK;
    }
    params.tailSrc0Offset = params.mainBlock * params.src0Offset;
    params.tailSrc1Offset = params.mainBlock * params.src1Offset;
    params.tailDstOffset = params.mainBlock * params.dstOffset;
}

[aicore] __inline__ __attribute__((always_inline)) void AddDeqReluImpl(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    const int32_t &calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AddDeqReluParams params;
        params.needTmpSize = calCount;
        GetAddDeqReluParamCal(params, 1);
        __attribute__((cce_unif_buff)) int32_t *tmpBuffer = AscendCUtils::GetTemporaryBufferAddr<int32_t>(TMP_UB_OFFSET, params.calcSize);
        set_mask_count();
        set_vector_mask(0, static_cast<uint64_t>(params.calcSize));
        for (int i = 0; i < params.mainBlock; i++) {
            AddDeqReluComput<false>(dst + i * params.dstOffset, src0 + i * params.src0Offset,
                src1 + i * params.src1Offset, tmpBuffer, params);
        }
        if (params.tailSize != 0) {
            params.calcSize = params.tailSize;
            set_vector_mask(0, static_cast<uint64_t>(params.calcSize));
            AddDeqReluComput<false>(dst + params.tailDstOffset, src0 + params.tailSrc0Offset,
                src1 + params.tailSrc1Offset, tmpBuffer, params);
        }
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}


template <bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqReluImpl(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    const uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AddDeqReluParams params;
        params.maskMode = ADDDEQRELU_MASK_MODE_ONE;
        params.mask1 = mask;
        params.needTmpSize = repeatTimes * DEFAULT_BLOCK_SIZE / sizeof(int32_t);

        params.dstBlkStride = repeatParams.dstBlkStride;
        params.src0BlkStride = repeatParams.src0BlkStride;
        params.src1BlkStride = repeatParams.src1BlkStride;
        params.dstRepStride = repeatParams.dstRepStride;
        params.src0RepStride = repeatParams.src0RepStride;
        params.src1RepStride = repeatParams.src1RepStride;
        GetAddDeqReluParamCal(params, repeatTimes);
        __attribute__((cce_unif_buff)) int32_t *tmpBuffer = AscendCUtils::GetTemporaryBufferAddr<int32_t>(TMP_UB_OFFSET, params.calcSize);
        if constexpr (isSetMask) {
            AscendCUtils::SetMask<int32_t>(mask);
        }
        for (int i = 0; i < params.mainBlock; i++) {
            AddDeqReluComput<isSetMask>(dst + i * params.dstOffset, src0 + i * params.src0Offset,
                src1 + i * params.src1Offset, tmpBuffer, params);
        }
        if (params.tailSize != 0) {
            if constexpr (isSetMask) {
                AscendCUtils::SetMask<int32_t>(mask);
            }
            params.repeat = repeatTimes - params.repeat * params.mainBlock;
            AddDeqReluComput<isSetMask>(dst + params.tailDstOffset, src0 + params.tailSrc0Offset,
                src1 + params.tailSrc1Offset, tmpBuffer, params);
        }
    }
}

template <bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqReluImpl(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    const uint64_t mask[], const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AddDeqReluParams params;
        params.maskMode = ADDDEQRELU_MASK_MODE_TWO;
        params.mask2[0] = mask[0];
        params.mask2[1] = mask[1];
        params.needTmpSize = repeatTimes * DEFAULT_BLOCK_SIZE / sizeof(int32_t);

        params.dstBlkStride = repeatParams.dstBlkStride;
        params.src0BlkStride = repeatParams.src0BlkStride;
        params.src1BlkStride = repeatParams.src1BlkStride;
        params.dstRepStride = repeatParams.dstRepStride;
        params.src0RepStride = repeatParams.src0RepStride;
        params.src1RepStride = repeatParams.src1RepStride;

        GetAddDeqReluParamCal(params, repeatTimes);
        __attribute__((cce_unif_buff)) int32_t *tmpBuffer = AscendCUtils::GetTemporaryBufferAddr<int32_t>(TMP_UB_OFFSET, params.calcSize);
        if constexpr (isSetMask) {
            AscendCUtils::SetMask<int32_t>(mask[1], mask[0]);
        }
        for (int i = 0; i < params.mainBlock; i++) {
            AddDeqReluComput<isSetMask>(dst + i * params.dstOffset, src0 + i * params.src0Offset,
                src1 + i * params.src1Offset, tmpBuffer, params);
        }
        if (params.tailSize != 0) {
            if constexpr (isSetMask) {
                AscendCUtils::SetMask<int32_t>(mask[1], mask[0]);
            }
            params.repeat = repeatTimes - params.repeat * params.mainBlock;
            AddDeqReluComput<isSetMask>(dst + params.tailDstOffset, src0 + params.tailSrc0Offset,
                src1 + params.tailSrc1Offset, tmpBuffer, params);
        }
    }
}



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float>(), "Failed to check dtype in FusedMulAdd, current api support dtype "
        "combination is src and dst both: half / float.");
    vmadd(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        FusedMulAddIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        FusedMulAddIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                       ;
        set_mask_count();
        set_vector_mask(0, calCount);
        vmadd(dst, src0, src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDstIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<Tuple<T, U>, Tuple<__cce_half, __cce_half>, Tuple<float, float>, Tuple<float, __cce_half>>(), "Failed to "
        "check dtype in MulAddDst, current api support dtype combination is src: half, dst: half / float; src: float, "
        "dst: float.");
    vmla(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDstImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MulAddDstIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDstImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MulAddDstIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDstImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                                                       ;
        set_mask_count();
        set_vector_mask(0, calCount);
        if constexpr (sizeof(T) == sizeof(U)) {
            vmla(dst, src0, src1, 1,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        } else {
            vmla(dst, src0, src1, 1,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
        }
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float>(), "Failed to check dtype in FusedMulAddRelu, current api support dtype "
        "combination is src and dst both: half / float.");
    vmaddrelu(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    const uint64_t mask[], const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        FusedMulAddReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    const uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        FusedMulAddReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                           ;
        set_mask_count();
        set_vector_mask(0, calCount);
        vmaddrelu(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, __cce_half, float>(), "Failed to check dtype in SubRelu, current api support dtype "
        "combination is src and dst both: int16_t / half / float.");
    vsubrelu(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                             ;
        set_mask_count();
        set_vector_mask(0, calCount);
        vsubrelu(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        SubReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        SubReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}
}
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_symbol_override_impl.h" 2





#pragma begin_pipe(V)
namespace AscendC {
template <typename T> class LocalTensor;


template <typename T> class SymbolOverrideAdd {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAdd(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        AddImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};

template <typename T> class SymbolOverrideSub {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideSub(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        SubImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};

template <typename T> class SymbolOverrideMul {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideMul(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        MulImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};

template <typename T> class SymbolOverrideDiv {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideDiv(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        DivImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};


template <typename T> class SymbolOverrideAnd {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAnd(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        if constexpr(SupportType<T, int32_t, uint32_t>()) {
            AndImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize() * 2);
        } else {
            AndImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
        }
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};

template <typename T> class SymbolOverrideOr {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideOr(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        if constexpr(SupportType<T, int32_t, uint32_t>()) {
            OrImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize() * 2);
        } else {
            OrImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
        }
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};


template <typename T> class SymbolOverrideCompare {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor,
        CMPMODE cmpMode)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor), cmpMode_(cmpMode)
    {}

    template <typename U> [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<U> &dstTensor) const
    {




        VcmpvImpl((__attribute__((cce_unif_buff)) U *)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) T *)this->src1Tensor_.GetPhyAddr(), cmpMode_, this->src0Tensor_.GetSize());
    }

private:
    const LocalTensor<T> src0Tensor_;
    const LocalTensor<T> src1Tensor_;
    CMPMODE cmpMode_;
};
}
#pragma end_pipe
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_base.h" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_base.h"
namespace AscendC {
using TBufHandle = uint8_t*;
using TEventID = int8_t;
using TTagType = int32_t;

enum class TBufState : uint8_t {
    FREE = 0,
    OCCUPIED,
    ENQUE,
    DEQUE,
};

struct TBufType {
    TBufState state;
    HardEvent freeBufEvt;
    TEventID enQueEvtID;
    TEventID freeBufEvtID;
    uint32_t address;
    uint32_t dataLen;
    TTagType usertag;
                                      ;
};

struct TBuffAddr {
    uint32_t dataLen;
    uint32_t bufferAddr;
    TBufHandle bufferHandle;
    uint8_t logicPos;



};

template <typename T> class BaseLocalTensor {
public:
    using PrimType = PrimT<T>;
# 120 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_base.h"
    [aicore] __inline__ __attribute__((always_inline)) void SetAddr(const TBuffAddr& bufferAddr)
    {
        this->address_ = bufferAddr;
    }
    [[deprecated("NOTICE: InitBuffer has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
    [aicore] __inline__ __attribute__((always_inline)) void InitBuffer(const uint32_t bufferOffset, const uint32_t bufferSize)
    {
        this->address_.bufferAddr = (uint64_t)(0) + bufferOffset;
        if constexpr (IsSameType<PrimType, int4b_t>::value) {
            this->address_.dataLen = bufferSize * INT4_BIT_NUM / ONE_BYTE_BIT_SIZE;
        } else {
            this->address_.dataLen = bufferSize * sizeof(PrimType);
        }
    }

    [aicore] __inline__ __attribute__((always_inline)) TBufHandle GetBufferHandle() const
    {
        return address_.bufferHandle;
    }
public:
    TBuffAddr address_;
};

template <typename T> class BaseGlobalTensor {
public:
    using PrimType = PrimT<T>;
    [aicore] __inline__ __attribute__((always_inline)) void SetAddr(const uint64_t offset)
    {
        if constexpr (IsSameType<PrimType, int4b_t>::value) {
            address_ = address_ + offset / INT4_TWO;
            oriAddress_ = oriAddress_ + offset / INT4_TWO;
        } else {
            address_ = address_ + offset;
            oriAddress_ = oriAddress_ + offset;
        }
    }
public:
    __attribute__((cce_global)) PrimType* address_;
    __attribute__((cce_global)) PrimType* oriAddress_;
};

template <typename T> class BaseTensor {};

template <typename T> class BaseTensorTraitTensor {};

template <typename T, TPosition pos, typename LayoutType>
class BaseTensorTraitTensor<TensorTrait<T, pos, LayoutType>> {
public:
    [aicore] __inline__ __attribute__((always_inline)) TensorTrait<T, pos, LayoutType>& GetTensorTrait();
    [aicore] __inline__ __attribute__((always_inline)) const TensorTrait<T, pos, LayoutType>& GetTensorTrait() const;
    [aicore] __inline__ __attribute__((always_inline)) void SetTensorTrait(const TensorTrait<T, pos, LayoutType>& trait);
private:
    TensorTrait<T, pos, LayoutType> trait = {};
};

template <typename T, TPosition pos, typename LayoutType>
[aicore] __inline__ __attribute__((always_inline)) TensorTrait<T, pos, LayoutType>& BaseTensorTraitTensor<TensorTrait<T, pos, LayoutType>>::GetTensorTrait() {
    return this->trait;
}

template <typename T, TPosition pos, typename LayoutType>
[aicore] __inline__ __attribute__((always_inline)) const TensorTrait<T, pos, LayoutType>& BaseTensorTraitTensor<TensorTrait<T, pos, LayoutType>>::GetTensorTrait() const {
    return this->trait;
}

template <typename T, TPosition pos, typename LayoutType>
[aicore] __inline__ __attribute__((always_inline)) void BaseTensorTraitTensor<TensorTrait<T, pos, LayoutType>>::SetTensorTrait(const TensorTrait<T, pos, LayoutType>& trait) {
    this->trait = trait;
}

}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h" 2

namespace AscendC {
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h"
struct ShapeInfo {
public:
    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo() {}
    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo(const uint8_t inputShapeDim, const uint32_t inputShape[],
        const uint8_t inputOriginalShapeDim, const uint32_t inputOriginalShape[], const DataFormat inputFormat)
        : shapeDim(inputShapeDim), originalShapeDim(inputOriginalShapeDim), dataFormat(inputFormat)
    {




          ;
        for (int index = 0; index < shapeDim; ++index) {
            shape[index] = inputShape[index];
        }
        for (int index = 0; index < originalShapeDim; ++index) {
            originalShape[index] = inputOriginalShape[index];
        }
    }
    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo(const uint8_t inputShapeDim, const uint32_t inputShape[], const DataFormat inputFormat)
        : shapeDim(inputShapeDim), originalShapeDim(inputShapeDim), dataFormat(inputFormat)
    {



          ;
        for (int index = 0; index < shapeDim; ++index) {
            shape[index] = inputShape[index];
            originalShape[index] = inputShape[index];
        }
    }

    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo(const uint8_t inputShapeDim, const uint32_t inputShape[])
        : shapeDim(inputShapeDim), originalShapeDim(inputShapeDim), dataFormat(DataFormat::ND)
    {



          ;
        for (int index = 0; index < shapeDim; ++index) {
            shape[index] = inputShape[index];
            originalShape[index] = inputShape[index];
        }
    }
    uint8_t shapeDim;
    uint8_t originalShapeDim;
    uint32_t shape[8];
    uint32_t originalShape[8];
    DataFormat dataFormat;
};






template <typename U, typename T>
struct ShapeInfoParams {
    [aicore] ShapeInfoParams() {};
    using Params = ShapeInfo;
};
template <typename T>
struct ShapeInfoParams<TensorTrait<T>, T> {
    [aicore] ShapeInfoParams() {};
    using Params = int8_t;
};

[aicore] __inline__ __attribute__((always_inline)) uint64_t GetShapeSize(const ShapeInfo& shapeInfo)
{
    int shapeSize = 1;
    for (int index = 0; index < shapeInfo.shapeDim; ++index) {
        shapeSize *= shapeInfo.shape[index];
    }
    return shapeSize;
}

template <typename T> class LocalTensor : public BaseLocalTensor<T>, public BaseTensorTraitTensor<T> {
public:
    using PrimType = PrimT<T>;
    [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T>() {};
# 139 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h"
    [aicore] __inline__ __attribute__((always_inline)) uint64_t GetPhyAddr() const;
    [aicore] __inline__ __attribute__((always_inline)) uint64_t GetPhyAddr(const uint32_t offset) const;
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) PrimType GetValue(const uint32_t index) const;
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) __attribute__((cce_unif_buff)) PrimType& operator()(const uint32_t offset) const;
    template <typename CAST_T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<CAST_T> ReinterpretCast() const;
    template <typename T1> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
        void SetValue(const uint32_t index, const T1 value) const;
    [aicore] __inline__ __attribute__((always_inline)) LocalTensor operator[](const uint32_t offset) const;

    template <typename T1>
    [[deprecated("NOTICE: SetAddrWithOffset has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
    [aicore] __inline__ __attribute__((always_inline)) void SetAddrWithOffset(LocalTensor<T1> &src, uint32_t offset);

    [aicore] __inline__ __attribute__((always_inline)) int32_t GetPosition() const;
    [aicore] __inline__ __attribute__((always_inline)) void SetSize(const uint32_t size);
    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetSize() const;

    [[deprecated("NOTICE: GetLength has been deprecated and will be removed in the next version. Please do not use "
                 "it!")]]
    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetLength() const;

    [[deprecated("NOTICE: SetBufferLen has been deprecated and will be removed in the next version. Please do not use "
                 "it!")]]
    [aicore] __inline__ __attribute__((always_inline)) void SetBufferLen(uint32_t dataLen);
    [aicore] __inline__ __attribute__((always_inline)) void SetUserTag(const TTagType tag);
    [aicore] __inline__ __attribute__((always_inline)) TTagType GetUserTag() const;

    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideAdd<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideSub<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideMul<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideDiv<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideOr<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideAnd<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideCompare<float>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideCompare<__cce_half>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAdd<T> operator + (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideSub<T> operator - (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideMul<T> operator *(const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideDiv<T> operator / (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideOr<T> operator | (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAnd<T> operator & (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator < (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator > (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator != (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator == (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator <= (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator >= (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) void SetShapeInfo(const ShapeInfo& shapeInfo);
    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo GetShapeInfo() const;

public:

    typename ShapeInfoParams<T, PrimType>::Params shapeInfo_;





private:



};

template <typename T> class GlobalTensor : public BaseGlobalTensor<T>, public BaseTensorTraitTensor<T> {
public:
    using PrimType = PrimT<T>;
    [aicore] __inline__ __attribute__((always_inline)) GlobalTensor<T>() {}
    [aicore] __inline__ __attribute__((always_inline)) void SetGlobalBuffer(__attribute__((cce_global)) PrimType* buffer, uint64_t bufferSize);
    [aicore] __inline__ __attribute__((always_inline)) void SetGlobalBuffer(__attribute__((cce_global)) PrimType* buffer);
    [aicore] __inline__ __attribute__((always_inline)) const __attribute__((cce_global)) PrimType* GetPhyAddr() const;
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) PrimType* GetPhyAddr(const uint64_t offset) const;
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) PrimType GetValue(const uint64_t offset) const;
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) __attribute__((cce_global)) PrimType& operator()(const uint64_t offset) const;
    [aicore] __inline__ __attribute__((always_inline)) void SetValue(const uint64_t offset, PrimType value);

    [aicore] __inline__ __attribute__((always_inline)) uint64_t GetSize() const;
    [aicore] __inline__ __attribute__((always_inline)) GlobalTensor operator[](const uint64_t offset) const;
    [aicore] __inline__ __attribute__((always_inline)) void SetShapeInfo(const ShapeInfo& shapeInfo);
    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo GetShapeInfo() const;
    template<CacheRwMode rwMode = CacheRwMode::RW>
    [aicore] __inline__ __attribute__((always_inline)) void SetL2CacheHint(CacheMode mode);

public:

    uint64_t bufferSize_;

    typename ShapeInfoParams<T, PrimType>::Params shapeInfo_;

    CacheMode cacheMode_ = CacheMode::CACHE_MODE_NORMAL;
};
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_impl.h" 2

namespace AscendC {
# 324 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_impl.h"
template <typename T> [aicore] __inline__ __attribute__((always_inline)) uint64_t LocalTensor<T>::GetPhyAddr() const
{
    return GetPhyAddr(0);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) uint64_t LocalTensor<T>::GetPhyAddr(const uint32_t offset) const
{
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        return this->address_.bufferAddr + offset / INT4_TWO;
    } else {
        return this->address_.bufferAddr + offset * sizeof(PrimType);
    }
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
    typename LocalTensor<T>::PrimType LocalTensor<T>::GetValue(const uint32_t index) const
{
    if constexpr(g_coreType == AscendC::AIC) {
        if (GetPhyType(AscendC::TPosition(this->GetPosition())) == Hardware::UB) {
            return PrimType(0);
        }
    }
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        LocalTensor<uint8_t> tmpLocalTensor = this->ReinterpretCast<uint8_t>();
        uint8_t val = tmpLocalTensor.GetValue(index / INT4_TWO);
        return static_cast<int4b_t>(val >> (INT4_BIT_NUM * (index % INT4_TWO)));
    } else {
        return *(reinterpret_cast<__attribute__((cce_unif_buff)) PrimType*>(GetPhyAddr(index)));
    }
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
    __attribute__((cce_unif_buff)) typename LocalTensor<T>::PrimType& LocalTensor<T>::operator()(const uint32_t offset) const
{
    return *(reinterpret_cast<__attribute__((cce_unif_buff)) PrimType*>(GetPhyAddr(offset)));
}

template <typename T>
template <typename CAST_T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<CAST_T> LocalTensor<T>::ReinterpretCast() const
{
    LocalTensor<CAST_T> tensorOut;
    tensorOut.address_.logicPos = static_cast<uint8_t>(this->GetPosition());
    tensorOut.address_.bufferHandle = this->GetBufferHandle();
    if constexpr (IsHalfByteDataType<PrimType>()) {
        tensorOut.address_.dataLen = this->GetSize() / INT4_TWO;
    } else {
        tensorOut.address_.dataLen = this->GetSize() * sizeof(PrimType);
    }
    tensorOut.address_.bufferAddr = this->address_.bufferAddr;
    if constexpr (is_tensorTrait_v<T> && is_tensorTrait_v<CAST_T>) {
        tensorOut.SetTensorTrait(this->GetTensorTrait());
    }
    return tensorOut;
}

template <typename T>
template <typename T1> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
    void LocalTensor<T>::SetValue(const uint32_t index, const T1 value) const
{
    if constexpr(g_coreType == AscendC::AIC) {
        if (GetPhyType(AscendC::TPosition(this->GetPosition())) == Hardware::UB) {
            return;
        }
    }
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        LocalTensor<uint8_t> tmpLocalTensor = this->ReinterpretCast<uint8_t>();
        uint8_t mask = (index % INT4_TWO == 0)? 0xf0 : 0xf;
        uint32_t idx = index / INT4_TWO;
        uint8_t val = tmpLocalTensor.GetValue(idx) & mask;
        uint8_t shift = (index % INT4_TWO == 0)? 0 : INT4_BIT_NUM;
        tmpLocalTensor.SetValue(idx, val + (value.storage << shift));
    } else {
        *(reinterpret_cast<__attribute__((cce_unif_buff)) PrimType*>(static_cast<uint64_t>(this->address_.bufferAddr))
            + index) = static_cast<PrimType>(value);
    }
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> LocalTensor<T>::operator[](const uint32_t offset) const
{
    LocalTensor retLocalTensor = *this;
    if constexpr (IsHalfByteDataType<PrimType>()) {
        retLocalTensor.address_.dataLen -= (offset / INT4_TWO);
        retLocalTensor.address_.bufferAddr = retLocalTensor.address_.bufferAddr + offset / INT4_TWO;
    } else {
        retLocalTensor.address_.dataLen -= (offset * sizeof(PrimType));
        retLocalTensor.address_.bufferAddr = retLocalTensor.address_.bufferAddr + offset * sizeof(PrimType);
    }
    return retLocalTensor;
}

template <typename T>
template <typename T1>
[[deprecated("NOTICE: SetAddrWithOffset has been deprecated and will be removed in the next version. "
    "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::SetAddrWithOffset(LocalTensor<T1> &src, uint32_t offset)
{
    this->address_ = src.address_;
    this->address_.bufferAddr += offset * sizeof(PrimT<T1>);
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) int32_t LocalTensor<T>::GetPosition() const
{
    return this->address_.logicPos;
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::SetSize(const uint32_t size)
{
# 436 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_impl.h"
    if constexpr (IsHalfByteDataType<PrimType>()) {
        this->address_.dataLen = size / INT4_TWO;
    } else {
        this->address_.dataLen = size * sizeof(PrimType);
    }
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t LocalTensor<T>::GetSize() const
{
    if constexpr (IsHalfByteDataType<PrimType>()) {
        return this->address_.dataLen * INT4_TWO;
    } else {
        return this->address_.dataLen / sizeof(PrimType);
    }
}

template <typename T>
[[deprecated("NOTICE: GetLength has been deprecated and will be removed in the next version. Please do not use "
                "it!")]]
[aicore] __inline__ __attribute__((always_inline)) uint32_t LocalTensor<T>::GetLength() const
{
    return this->address_.dataLen;
}

template <typename T>
[[deprecated("NOTICE: SetBufferLen has been deprecated and will be removed in the next version. Please do not use "
                "it!")]]
[aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::SetBufferLen(uint32_t dataLen)
{
    this->address_.dataLen = dataLen;
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::SetUserTag(const TTagType tag)
{
    auto ptr = reinterpret_cast<TBufType*>(this->address_.bufferHandle);

                                                                            ;
    ptr->usertag = tag;
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) TTagType LocalTensor<T>::GetUserTag() const
{
    auto ptr = reinterpret_cast<TBufType*>(this->address_.bufferHandle);

                                                                            ;
    return ptr->usertag;
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideAdd<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideSub<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideMul<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideDiv<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideOr<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideAnd<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void
    LocalTensor<T>::operator = (const SymbolOverrideCompare<float>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void
    LocalTensor<T>::operator = (const SymbolOverrideCompare<__cce_half>& symbolOverride)
{
    symbolOverride.Process(*this);
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAdd<T>
    LocalTensor<T>::operator + (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideAdd<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideSub<T>
    LocalTensor<T>::operator - (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideSub<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideMul<T>
    LocalTensor<T>::operator *(const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideMul<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideDiv<T>
    LocalTensor<T>::operator / (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideDiv<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideOr<T>
    LocalTensor<T>::operator | (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideOr<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAnd<T>
    LocalTensor<T>::operator & (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideAnd<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator < (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::LT);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator > (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::GT);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator != (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::NE);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator == (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::EQ);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator <= (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::LE);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator >= (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::GE);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void
    LocalTensor<T>::SetShapeInfo(const ShapeInfo& shapeInfo)
{
    static_assert(IsSameType<T, PrimType>::value, "Only primitive type Tensor has shape info!");

        shapeInfo_ = shapeInfo;

}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) ShapeInfo LocalTensor<T>::GetShapeInfo() const
{
    static_assert(IsSameType<T, PrimType>::value, "Only primitive type Tensor has shape info!");

        return shapeInfo_;




}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void
    GlobalTensor<T>::SetGlobalBuffer(__attribute__((cce_global)) typename GlobalTensor<T>::PrimType* buffer, uint64_t bufferSize)
{
    if (this->cacheMode_ == CacheMode::CACHE_MODE_NORMAL) {
        this->address_ = buffer;
    } else {
        this->address_ = L2CacheAlter<PrimType, CacheRwMode::RW>(buffer, cacheMode_);
    }
    this->oriAddress_ = buffer;
    bufferSize_ = bufferSize;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GlobalTensor<T>::SetGlobalBuffer(__attribute__((cce_global)) typename GlobalTensor<T>::PrimType* buffer)
{
    if (this->cacheMode_ == CacheMode::CACHE_MODE_NORMAL) {
        this->address_ = buffer;
    } else {
        this->address_ = L2CacheAlter<PrimType, CacheRwMode::RW>(buffer, cacheMode_);
    }
    this->oriAddress_ = buffer;



}

template <typename T> [aicore] __inline__ __attribute__((always_inline))
    const __attribute__((cce_global)) typename GlobalTensor<T>::PrimType* GlobalTensor<T>::GetPhyAddr() const
{
    return this->address_;
}

template <typename T> [aicore] __inline__ __attribute__((always_inline))
    __attribute__((cce_global)) typename GlobalTensor<T>::PrimType* GlobalTensor<T>::GetPhyAddr(const uint64_t offset) const
{
    if constexpr (IsHalfByteDataType<PrimType>()) {

                                                                                                   ;
        return this->address_ + offset / INT4_TWO;
    } else {
        return this->address_ + offset;
    }
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
    typename GlobalTensor<T>::PrimType GlobalTensor<T>::GetValue(const uint64_t offset) const
{
    if constexpr (IsHalfByteDataType<PrimType>()) {
        __attribute__((cce_global)) uint8_t *addr = reinterpret_cast<__attribute__((cce_global)) uint8_t *>(this->oriAddress_) + offset / INT4_TWO;
        return static_cast<PrimType>((*addr) >> (INT4_BIT_NUM * (offset % INT4_TWO)));
    } else {
        return this->oriAddress_[offset];
    }
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
    __attribute__((cce_global)) typename GlobalTensor<T>::PrimType& GlobalTensor<T>::operator()(const uint64_t offset) const
{
    return this->oriAddress_[offset];
}

template <typename T> [aicore] __inline__ __attribute__((always_inline))
    void GlobalTensor<T>::SetValue(const uint64_t offset, typename GlobalTensor<T>::PrimType value)
{
    if constexpr (IsHalfByteDataType<PrimType>()) {
        __attribute__((cce_global)) uint8_t *addr = reinterpret_cast<__attribute__((cce_global)) uint8_t *>(this->oriAddress_) + offset / INT4_TWO;
        uint8_t mask = (offset % INT4_TWO == 0)? 0xf0 : 0xf;

        uint8_t val = (*addr) & mask;
        uint8_t shift = (offset % INT4_TWO == 0)? 0 : INT4_BIT_NUM;
        *addr = val + (value.storage << shift);
    } else {
        this->oriAddress_[offset] = value;
    }
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) uint64_t GlobalTensor<T>::GetSize() const
{
    return bufferSize_;
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) GlobalTensor<T> GlobalTensor<T>::operator[](const uint64_t offset) const
{
    GlobalTensor retLocalTensor = *this;
    if constexpr (IsHalfByteDataType<PrimType>()) {
        retLocalTensor.address_ = retLocalTensor.address_ + offset / INT4_TWO;
        retLocalTensor.oriAddress_ = retLocalTensor.oriAddress_ + offset / INT4_TWO;
    } else {
        retLocalTensor.address_ = retLocalTensor.address_ + offset;
        retLocalTensor.oriAddress_ = retLocalTensor.oriAddress_ + offset;
    }
    return retLocalTensor;
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void GlobalTensor<T>::SetShapeInfo(const ShapeInfo& shapeInfo)
{
    static_assert(IsSameType<T, PrimType>::value, "Only primitive type Tensor has shape info!");

    shapeInfo_ = shapeInfo;

}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) ShapeInfo GlobalTensor<T>::GetShapeInfo() const
{
    static_assert(IsSameType<T, PrimType>::value, "Only primitive type Tensor has shape info!");

    return shapeInfo_;




}

template <typename T>
template<CacheRwMode rwMode>
[aicore] __inline__ __attribute__((always_inline)) void GlobalTensor<T>::SetL2CacheHint(CacheMode mode) {
    this->cacheMode_ = mode;
    if (mode == CacheMode::CACHE_MODE_NORMAL) {
        this->address_ = this->oriAddress_;
    } else {
        this->address_ = L2CacheAlter<PrimType, rwMode>(this->oriAddress_, mode);
    }



}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h" 2





# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_common_impl.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_mm.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_mm.h"
namespace AscendC {

using LoadData2dParams = struct LoadData2DParams;
struct LoadData2DParams {
    [aicore] LoadData2DParams() {}

    [aicore] LoadData2DParams(const uint16_t startIndexIn, const uint8_t repeatTimesIn, const uint16_t srcStrideIn,
        const uint8_t sidIn, const uint16_t dstGapIn, const bool ifTransposeIn, const uint8_t addrModeIn)
        : startIndex(startIndexIn),
          repeatTimes(repeatTimesIn),
          srcStride(srcStrideIn),
          sid(sidIn),
          dstGap(dstGapIn),
          ifTranspose(ifTransposeIn),
          addrMode(addrModeIn)
    {}

    uint16_t startIndex = 0;
    uint16_t dstGap = 0;
    uint16_t srcStride = 0;
    bool ifTranspose = 0;
    uint8_t repeatTimes = 0;

    uint8_t sid = 0;
    uint8_t addrMode = 0;
};

struct LoadData2DParamsV2 {
    [aicore] LoadData2DParamsV2() {}

    [aicore] LoadData2DParamsV2(const uint32_t mStartPositionIn, const uint32_t kStartPositionIn,
        const uint16_t mStepIn, const uint16_t kStepIn, const int32_t srcStrideIn, const uint16_t dstStrideIn,
        const bool ifTransposeIn, const uint8_t sidIn)
        : mStartPosition(mStartPositionIn),
          kStartPosition(kStartPositionIn),
          mStep(mStepIn),
          kStep(kStepIn),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn),
          ifTranspose(ifTransposeIn),
          sid(sidIn)
    {}

    uint32_t mStartPosition = 0;
    uint32_t kStartPosition = 0;
    uint16_t mStep = 0;
    uint16_t kStep = 0;
    int32_t srcStride = 0;
    uint16_t dstStride = 0;
    bool ifTranspose = false;
    uint8_t sid = 0;
};

template <typename T>
struct LoadData3DParamsV1 {
    [aicore] LoadData3DParamsV1()
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = 0;
        }
    }

    [aicore] LoadData3DParamsV1(const uint8_t padListIn[PAD_SIZE], const uint16_t l1HIn, const uint16_t l1WIn,
        const uint16_t c1IndexIn, const uint8_t fetchFilterWIn, const uint8_t fetchFilterHIn, const int16_t leftTopWIn,
        const int16_t leftTopHIn, const uint8_t strideWIn, const uint8_t strideHIn, const uint8_t filterWIn,
        const uint8_t filterHIn, const uint8_t dilationFilterWIn, const uint8_t dilationFilterHIn,
        const uint8_t jumpStrideIn, const uint8_t repeatModeIn, const uint8_t repeatTimeIn, const uint8_t cSizeIn,
        const T padValueIn)
        : l1H(l1HIn),
          l1W(l1WIn),
          c1Index(c1IndexIn),
          fetchFilterW(fetchFilterWIn),
          fetchFilterH(fetchFilterHIn),
          leftTopW(leftTopWIn),
          leftTopH(leftTopHIn),
          strideW(strideWIn),
          strideH(strideHIn),
          filterW(filterWIn),
          filterH(filterHIn),
          dilationFilterW(dilationFilterWIn),
          dilationFilterH(dilationFilterHIn),
          jumpStride(jumpStrideIn),
          repeatMode(repeatModeIn),
          repeatTime(repeatTimeIn),
          cSize(cSizeIn),
          padValue(padValueIn)
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = padListIn[i];
        }
    }

    uint8_t padList[PAD_SIZE] = {0};
    uint8_t strideW = 0;
    uint8_t strideH = 0;
    uint8_t filterW = 0;
    uint8_t filterH = 0;
    uint8_t dilationFilterW = 0;
    uint8_t dilationFilterH = 0;
    uint8_t jumpStride = 0;
    uint8_t repeatMode = 0;
    uint8_t repeatTime = 0;
    uint8_t cSize = 0;
    T padValue = 0;
    uint8_t fetchFilterW = 0;
    uint8_t fetchFilterH = 0;
    uint16_t l1H = 0;
    uint16_t l1W = 0;
    uint16_t c1Index = 0;
    int16_t leftTopW = 0;
    int16_t leftTopH = 0;
};

template <typename T>
struct LoadData3DParamsV2 {
    [aicore] LoadData3DParamsV2()
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = 0;
        }
    }

    [aicore] LoadData3DParamsV2(const uint8_t padListIn[PAD_SIZE], const uint16_t l1HIn, const uint16_t l1WIn,
        const uint16_t channelSizeIn, const uint16_t kExtensionIn, const uint16_t mExtensionIn,
        const uint16_t kStartPtIn, const uint16_t mStartPtIn, const uint8_t strideWIn, const uint8_t strideHIn,
        const uint8_t filterWIn, const uint8_t filterHIn, const uint8_t dilationFilterWIn,
        const uint8_t dilationFilterHIn, const bool enTransposeIn, const bool enSmallKIn, const T padValueIn)
        : l1H(l1HIn),
          l1W(l1WIn),
          channelSize(channelSizeIn),
          kExtension(kExtensionIn),
          mExtension(mExtensionIn),
          kStartPt(kStartPtIn),
          mStartPt(mStartPtIn),
          strideW(strideWIn),
          strideH(strideHIn),
          filterW(filterWIn),
          filterH(filterHIn),
          dilationFilterW(dilationFilterWIn),
          dilationFilterH(dilationFilterHIn),
          enTranspose(enTransposeIn),
          enSmallK(enSmallKIn),
          padValue(padValueIn)
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = padListIn[i];
        }
    }

    [aicore] LoadData3DParamsV2(const uint8_t padListIn[PAD_SIZE], const uint16_t l1HIn, const uint16_t l1WIn,
        const uint16_t channelSizeIn, const uint16_t kExtensionIn, const uint16_t mExtensionIn,
        const uint16_t kStartPtIn, const uint16_t mStartPtIn, const uint8_t strideWIn, const uint8_t strideHIn,
        const uint8_t filterWIn, const uint8_t filterHIn, const uint8_t dilationFilterWIn,
        const uint8_t dilationFilterHIn, const bool enTransposeIn, const bool enSmallKIn, const T padValueIn,
        const bool filterSizeWIn, const bool filterSizeHIn, const bool fMatrixCtrlIn)
        : l1H(l1HIn),
          l1W(l1WIn),
          channelSize(channelSizeIn),
          kExtension(kExtensionIn),
          mExtension(mExtensionIn),
          kStartPt(kStartPtIn),
          mStartPt(mStartPtIn),
          strideW(strideWIn),
          strideH(strideHIn),
          filterW(filterWIn),
          filterH(filterHIn),
          dilationFilterW(dilationFilterWIn),
          dilationFilterH(dilationFilterHIn),
          enTranspose(enTransposeIn),
          enSmallK(enSmallKIn),
          padValue(padValueIn),
          filterSizeW(filterSizeWIn),
          filterSizeH(filterSizeHIn),
          fMatrixCtrl(fMatrixCtrlIn)
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = padListIn[i];
        }
    }

    uint8_t padList[PAD_SIZE] = {0};
    uint16_t l1H = 0;
    uint16_t l1W = 0;
    uint16_t channelSize = 0;
    uint16_t kExtension = 0;
    uint16_t mExtension = 0;
    uint16_t kStartPt = 0;
    uint16_t mStartPt = 0;

    uint8_t strideW = 1;
    uint8_t strideH = 1;
    uint8_t filterW = 1;
    uint8_t filterH = 1;
    uint8_t dilationFilterW = 1;
    uint8_t dilationFilterH = 1;
    bool enTranspose = false;
    bool enSmallK = false;
    T padValue = 0;
    bool filterSizeW = false;
    bool filterSizeH = false;
    bool fMatrixCtrl = false;
};

struct LoadData3DParamsV2Pro {
    [aicore] LoadData3DParamsV2Pro() {}

    [aicore] LoadData3DParamsV2Pro(const uint16_t channelSizeIn, const bool enTransposeIn, const bool enSmallKIn,
        const bool filterSizeWIn, const bool filterSizeHIn, const bool fMatrixCtrlIn, const uint64_t extConfigIn,
        const uint64_t filterConfigIn)
        : channelSize(channelSizeIn),
          enTranspose(enTransposeIn),
          enSmallK(enSmallKIn),
          filterSizeW(filterSizeWIn),
          filterSizeH(filterSizeHIn),
          fMatrixCtrl(fMatrixCtrlIn),
          extConfig(extConfigIn),
          filterConfig(filterConfigIn)
    {}

    uint16_t channelSize = 0;
    bool enTranspose = false;
    bool enSmallK = false;
    bool filterSizeW = false;
    bool filterSizeH = false;
    bool fMatrixCtrl = false;
    uint64_t extConfig = 0;
    uint64_t filterConfig = 0X10101010101;
};

struct LoadData2dTransposeParams {
    [aicore] LoadData2dTransposeParams() {}

    [aicore] LoadData2dTransposeParams(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstfracGapIn, const uint8_t addrModeIn)
        : startIndex(startIndexIn),
          repeatTimes(repeatTimesIn),
          srcStride(srcStrideIn),
          dstGap(dstGapIn),
          dstFracGap(dstfracGapIn),
          addrMode(addrModeIn)
    {}

    [aicore] LoadData2dTransposeParams(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstfracGapIn)
        : startIndex(startIndexIn),
          repeatTimes(repeatTimesIn),
          srcStride(srcStrideIn),
          dstGap(dstGapIn),
          dstFracGap(dstfracGapIn)
    {}

    uint16_t startIndex = 0;
    uint8_t repeatTimes = 0;
    uint16_t srcStride = 0;
    uint16_t dstGap = 0;
    uint16_t dstFracGap = 0;
    uint8_t addrMode = 0;
};

struct LoadData2dTransposeParamsV2 {
    [aicore] LoadData2dTransposeParamsV2() {}

    [aicore] LoadData2dTransposeParamsV2(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstFracGapIn,
        const uint16_t srcFracGapIn)
        : startIndex(startIndexIn),
          repeatTimes(repeatTimesIn),
          srcStride(srcStrideIn),
          dstGap(dstGapIn),
          dstFracGap(dstFracGapIn),
          srcFracGap(srcFracGapIn)
    {}

    [aicore] LoadData2dTransposeParamsV2(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstFracGapIn,
        const uint16_t srcFracGapIn, const uint8_t addrModeIn)
        : startIndex(startIndexIn),
          repeatTimes(repeatTimesIn),
          srcStride(srcStrideIn),
          dstGap(dstGapIn),
          dstFracGap(dstFracGapIn),
          srcFracGap(srcFracGapIn),
          addrMode(addrModeIn)
    {}

    uint16_t startIndex = 0;
    uint8_t repeatTimes = 0;
    uint16_t srcStride = 0;
    uint16_t dstGap = 0;
    uint16_t dstFracGap = 0;
    uint16_t srcFracGap = 0;
    uint8_t addrMode = 0;
};

struct MmadParams {
    [aicore] MmadParams() {}

    [aicore] MmadParams(const uint16_t mIn, const uint16_t nIn, const uint16_t kIn, const bool isBiasIn,
        const int32_t fmOffsetIn, const bool enSsparseIn, const bool enWinogradAIn, const bool enWinogradBIn)
        : m(mIn),
          n(nIn),
          k(kIn),
          isBias(isBiasIn),
          fmOffset(fmOffsetIn),
          enSsparse(enSsparseIn),
          enWinogradA(enWinogradAIn),
          enWinogradB(enWinogradBIn)
    {}

    [aicore] MmadParams(const uint16_t mIn, const uint16_t nIn, const uint16_t kIn, const uint8_t unitFlagIn,
        const bool cmatrixSourceIn, const bool cmatrixInitValIn)
        : m(mIn),
          n(nIn),
          k(kIn),
          unitFlag(unitFlagIn),
          cmatrixSource(cmatrixSourceIn),
          cmatrixInitVal(cmatrixInitValIn)
    {}

    uint16_t m = 0;
    uint16_t n = 0;
    uint16_t k = 0;


    bool isBias = false;

    int32_t fmOffset = 0;

    bool enSsparse = false;

    bool enWinogradA = false;

    bool enWinogradB = false;
    uint8_t unitFlag = 0;
    bool kDirectionAlign = false;

    bool cmatrixSource = false;

    bool cmatrixInitVal = true;
};

template <typename U>
struct InitConstValueParams {
    [aicore] InitConstValueParams() {}

    [aicore] InitConstValueParams(const uint16_t repeatTimesIn,
        const uint16_t blockNumIn, const uint16_t dstGapIn, const U initValueIn)
        : repeatTimes(repeatTimesIn),
          blockNum(blockNumIn),
          dstGap(dstGapIn),
          initValue(initValueIn)
    {}

    [aicore] InitConstValueParams(const uint16_t repeatTimesIn, const U initValueIn)
        : repeatTimes(repeatTimesIn),
          initValue(initValueIn)
    {}

    uint16_t repeatTimes = 0;
    uint16_t blockNum = 0;
    uint16_t dstGap = 0;
    U initValue = 0;
};

enum class FmatrixMode : uint8_t {
    FMATRIX_LEFT = 0,
    FMATRIX_RIGHT = 1,
};
# 412 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_mm.h"
struct LoadDataRepeatParam {
    [aicore] LoadDataRepeatParam() {}

    [aicore] LoadDataRepeatParam(const uint16_t repeatStrideIn, const uint8_t repeatTimeIn,
        const uint8_t repeatModeIn)
        : repeatStride(repeatStrideIn),
          repeatTime(repeatTimeIn),
          repeatMode(repeatModeIn)
    {}

    uint16_t repeatStride = 0;
    uint8_t repeatTime = 1;
    uint8_t repeatMode = 0;
    uint8_t reserved = 0;
};


struct LoadImageToLocalParams {
    [aicore] LoadImageToLocalParams() {}

    [aicore] LoadImageToLocalParams(const uint16_t horizSizeIn, const uint16_t vertSizeIn,
        const uint16_t horizStartPosIn, const uint16_t vertStartPosIn, const uint16_t srcHorizSizeIn,
        const uint8_t topPadSizeIn, const uint8_t botPadSizeIn, const uint16_t leftPadSizeIn,
        const uint16_t rightPadSizeIn)
        : horizSize(horizSizeIn),
          vertSize(vertSizeIn),
          horizStartPos(horizStartPosIn),
          vertStartPos(vertStartPosIn),
          srcHorizSize(srcHorizSizeIn),
          topPadSize(topPadSizeIn),
          botPadSize(botPadSizeIn),
          leftPadSize(leftPadSizeIn),
          rightPadSize(rightPadSizeIn)
    {}

    uint16_t horizSize = 0;
    uint16_t vertSize = 0;
    uint16_t horizStartPos = 0;
    uint16_t vertStartPos = 0;
    uint16_t srcHorizSize = 0;
    uint8_t topPadSize = 0;
    uint8_t botPadSize = 0;
    uint16_t leftPadSize = 0;
    uint16_t rightPadSize = 0;
    uint8_t sid = 0;
};

struct CheckLocalMemoryIAParam {
    [aicore] CheckLocalMemoryIAParam() {}

    [aicore] CheckLocalMemoryIAParam(const uint8_t enableBitIn, const uint32_t startAddrIn, const uint32_t endAddrIn,
        const bool isScalarReadIn, const bool isScalarWriteIn, const bool isVectorReadIn, const bool isVectorWriteIn,
        const bool isMteReadIn, const bool isMteWriteIn, const bool isEnableIn)
        : enableBit(enableBitIn),
          startAddr(startAddrIn),
          endAddr(endAddrIn),
          isScalarRead(isScalarReadIn),
          isScalarWrite(isScalarWriteIn),
          isVectorRead(isVectorReadIn),
          isVectorWrite(isVectorWriteIn),
          isMteRead(isMteReadIn),
          isMteWrite(isMteWriteIn),
          isEnable(isEnableIn)
    {}

    uint8_t enableBit = 0;
    uint32_t startAddr = 0;
    uint32_t endAddr = 0;
    bool isScalarRead = false;
    bool isScalarWrite = false;
    bool isVectorRead = false;
    bool isVectorWrite = false;
    bool isMteRead = false;
    bool isMteWrite = false;
    bool isEnable = false;
    uint32_t reserved = 0;
};
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_common_impl.h" 2
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockIdxImpl()
{






    return get_subblockid();

}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetTaskRationImpl()
{



    return get_subblockdim();

}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockIdxImpl()
{
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_common_impl.h"
    if constexpr(g_coreType == AscendC::AIV) {
        return get_block_idx() * GetTaskRationImpl() + get_subblockid();
    } else {
        return get_block_idx();
    }

}

[[deprecated(
    "NOTICE: SetSysWorkSpace has been deprecated and will be removed in the next version.")]]
[aicore] __inline__ __attribute__((always_inline)) void SetSysWorkspace(__attribute__((cce_global)) uint8_t* workspace)
{




    if (g_sysWorkspaceReserved == nullptr) {
        g_sysWorkspaceReserved = workspace;
    }

}

[aicore] __inline__ __attribute__((always_inline)) void SetSysWorkspaceForce(__attribute__((cce_global)) uint8_t* workspace)
{




    g_sysWorkspaceReserved = workspace;

}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetUserWorkspace(__attribute__((cce_global)) uint8_t* workspace)
{





    (void)(workspace);
    return g_sysWorkspaceReserved + RESERVED_WORKSPACE;

}

template <atomic_type_t type, atomic_op_t op>
[aicore] __inline__ __attribute__((always_inline)) void SetStoreAtomicConfigImpl()
{
    set_st_atomic_cfg(type, op);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetStoreAtomicConfigImpl()
{
    return get_st_atomic_cfg();
}

[aicore] __inline__ __attribute__((always_inline)) void GetStoreAtomicConfigImpl(uint16_t &atomicType, uint16_t &atomicOp)
{
    int64_t stAtomic = get_st_atomic_cfg();
    constexpr uint64_t typeMask = 0x7;
    constexpr uint64_t opBit = 4;
    constexpr uint64_t opMask = 0x3;
    atomicType = (static_cast<uint64_t>(stAtomic) & typeMask);
    atomicOp = ((static_cast<uint64_t>(stAtomic) >> opBit) & opMask);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCachePreloadImpl(const GlobalTensor<uint64_t> &srcTensor, const T cacheOffset)
{
    if constexpr ((IsSameType<T, int16_t>::value) || (IsSameType<T, int64_t>::value)) {
        dc_preload((__attribute__((cce_global)) uint64_t *)srcTensor.GetPhyAddr(), cacheOffset);
    } else {
        static_assert((220 == 220), "current data type is not supported on current device");
    }
}

[aicore] __inline__ __attribute__((always_inline)) void PreLoadImpl(void *pc, const int64_t preFetchLen)
{
    preload(pc, preFetchLen);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetICachePreloadStatusImpl()
{
    return get_icache_prl_st();
}

[aicore] __inline__ __attribute__((always_inline)) void CheckLocalMemoryIAImpl(const CheckLocalMemoryIAParam& checkParams)
{
    uint64_t config = 0;
    config = config | (static_cast<uint64_t>(checkParams.startAddr) << 48);
    config = config | (static_cast<uint64_t>(checkParams.endAddr) << 32);
    config = config | (static_cast<uint64_t>(checkParams.isScalarRead) << 31);
    config = config | (static_cast<uint64_t>(checkParams.isScalarWrite) << 30);
    config = config | (static_cast<uint64_t>(checkParams.isVectorRead) << 29);
    config = config | (static_cast<uint64_t>(checkParams.isVectorWrite) << 28);
    config = config | (static_cast<uint64_t>(checkParams.isMteRead) << 27);
    config = config | (static_cast<uint64_t>(checkParams.isMteWrite) << 26);
    config = config | (checkParams.reserved << 1);
    config = config | (static_cast<uint8_t>(checkParams.isEnable));
    if (checkParams.enableBit == SET_DATA_EXP_ZERO) {
        set_data_exp_0(config);
    } else if (checkParams.enableBit == SET_DATA_EXP_ONE) {
        set_data_exp_1(config);
    } else if (checkParams.enableBit == SET_DATA_EXP_TWO) {
        set_data_exp_2(config);
    } else if (checkParams.enableBit == SET_DATA_EXP_THREE) {
        set_data_exp_3(config);
    } else {
        static_assert((220 == 220), "unsupport this enableBit on current device");
    }
}

[aicore] __inline__ __attribute__((always_inline)) void PreLoad(const int64_t preFetchLen)
{
    int64_t pc = get_pc() & 0xFFFFFFFFFFFF;
    PreLoadImpl(reinterpret_cast<void *>(pc), preFetchLen);
}
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h" 1
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_set_atomic_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_set_atomic_impl.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicTypeImpl()
{


                                 ;
    if constexpr(IsSameType<T, float>::value) {
        set_atomic_f32();
    } else if constexpr(IsSameType<T, __cce_half>::value) {
        set_atomic_f16();
    } else if constexpr(IsSameType<T, int16_t>::value) {
        set_atomic_s16();
    } else if constexpr(IsSameType<T, int32_t>::value) {
        set_atomic_s32();
    } else if constexpr(IsSameType<T, int8_t>::value) {
        set_atomic_s8();
    } else if constexpr(IsSameType<T, bfloat16_t>::value) {
        set_atomic_bf16();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void SetAtomicNoneImpl()
{
    set_atomic_none();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicAddImpl()
{


                                 ;
    set_atomic_add();
    SetAtomicTypeImpl<T>();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMaxImpl()
{


                                 ;
    set_atomic_max();
    SetAtomicTypeImpl<T>();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMinImpl()
{


                                 ;
    set_atomic_min();
    SetAtomicTypeImpl<T>();
}

}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_log.h" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h" 2

namespace AscendC {


[aicore] __inline__ __attribute__((always_inline)) void Barrier()
{



    __asm__ __volatile__("");

}

enum class KFC_Enum : uint16_t {
    SERVICE_ID_MASK = 0xFF00,
    SERVICE_ID_SCM = 0x0100,
    SCMFUN_GM2L1,
    SCMFUN_GM2L1ND2NZ,
    SERVICE_ID_MATMUL = 0x0300,
    MMFUN_MASK = 0x0380,
    MMFUN_ITERATE = 0x0380,
    MMFUN_ITERATE_ALL = 0x0381,
    MMFUN_INIT = 0x0301,
    MMFUN_GET_TENSOR_C,
    MMFUN_ITERATE_ALL_RESP,
    MMFUN_GET_TENSOR_C_RESP,
    MMFUN_GET_OFFSET_C,
    MMFUN_GET_OFFSET_C_RESP,
    MMFUN_SET_ORG_SHAPE,
    MMFUN_SET_HF32,
    MMFUN_SET_USER_DEF_INFO,
    MMFUN_ITERATE_BATCH_ALL,
    MMFUN_ITERATE_BATCH_ALL_RESP,
    MMFUN_ITERATE_N_BATCH_ALL,
    MMFUN_ITERATE_N_BATCH_ALL_RESP,
    MMFUN_END,
    CONVFUNC_ITERATE,
    CONVFUNC_ITERATE_ALL,
    CONVFUNC_GET_TENSOR_C,
    CONVFUNC_END,
    SERVICE_QUIT = 0xfd00,
    SERVICE_BALANCE = 0xfe00,
    SERVICE_ID_NONE = 0xff00
};

enum class MSG_STATE : uint8_t {
    STATE_INVALID,
    STATE_SET,
};
# 95 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h"
constexpr int32_t MIX_NUM = 2;

constexpr int32_t MAX_BLOCK_AIV_NUM = 2;
constexpr int32_t MIX_COEFFICIENT = 1;
constexpr int32_t MAX_MATMUL_OBJ = 8;
constexpr int MAX_AIV_NUM = 50;
constexpr int MAX_AIC_NUM = 25;
constexpr int ALIGN_SIZE = 32;
constexpr int BIDIRECTION_NUM = 2;
constexpr bool KFC_APPLY_MSG = true;
constexpr uint64_t INC_PROCESS_CHECK = 14;
constexpr uint64_t WORKSPACE_UB_SIZE = TOTAL_UB_SIZE;
constexpr int32_t MAX_GROUP_ID = 32;
constexpr int32_t MM_CNT_MAX = 1024;
constexpr int32_t QUIT_CNT = 4;
constexpr int32_t MAX_SYNC_COUNT = 100000000;
constexpr int32_t MMCNT_L1_RESERVERD_SIZE = 64;
constexpr uint16_t KFC_SYNC_ID = 15;

struct TilingInfo {
    __attribute__((cce_global)) uint8_t* tilingAddr;
};

struct MatmulOrgShape {
    uint32_t orgM;
    uint32_t orgN;
    uint32_t orgKa;
    uint32_t orgKb;
    uint32_t orgKc;
};
# 136 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h"
struct MatmulConfigParams {

    uint32_t enAtomic : 8;
    uint32_t enSequentialWrite : 1;
    uint32_t isTransA : 1;
    uint32_t isTransB : 1;
    uint32_t enPartialSum : 1;
    uint32_t setTail : 1;
    uint32_t setTensorA : 1;
    uint32_t setTensorB : 1;
    uint32_t setTensorBias : 1;
    uint32_t setClearBias : 1;
    uint32_t cIsTscm : 1;
    uint32_t isFirstIter : 1;
    uint32_t sync : 1;
    uint32_t enHF32 : 1;
    uint32_t hf32TransMode : 1;
    uint32_t setQuant : 1;
    uint32_t setBatch : 1;
    uint32_t waitIterateAll : 1;
    uint32_t waitIterateBatch : 1;
    uint32_t iterateFakeMsg : 1;

    uint32_t singleM;
    uint32_t singleN;
    uint32_t singleK;
    uint32_t sizeAmatrix;
    uint32_t sizeBmatrix;

    uint64_t aAddr;
    uint64_t bAddr;
    uint64_t cAddr;
    uint64_t biasAddr;
    uint64_t quantAddr;
    uint32_t quantSize;
    uint32_t quantMode;
    uint64_t quantScalar;
    uint32_t batchA;
    uint32_t batchB;
    uint32_t matrixStrideA;
    uint32_t matrixStrideB;
    uint32_t matrixStrideC;
    uint32_t batchLoop;
    uint32_t counterId;
    uint32_t reserved0;
    uint64_t dataPtr;
};

struct Conv3DBpInputConfigParams {
    uint32_t enAtomic : 8;
    uint32_t enSequentialWrite : 1;
    uint32_t setTensorWeight : 1;
    uint32_t sync : 1;
    uint32_t isFirstIter : 1;
    uint32_t setTensorFmap : 1;
    uint32_t setTensorOutBackprop : 1;
    uint32_t setSingleShape : 1;
    uint32_t setStartIdx : 1;
    uint32_t enPartialSum : 1;

    uint32_t singleShapeN;
    uint32_t singleShapeD;
    uint32_t curDinStartIdx;

    uint32_t singleShapeM;
    uint32_t singleShapeK;

    uint64_t weightAddr;
    uint64_t outBackpropAddr;
    uint64_t outputAddr;

    int32_t curHoStartIdx;
    uint32_t res;
    uint32_t res1[16];
};

struct Conv3DBpFilterConfigParams {
    uint32_t enAtomic : 8;
    uint32_t enSequentialWrite : 1;
    uint32_t setTensorFmap : 1;
    uint32_t setTensorOutBackprop : 1;
    uint32_t setSingleShape : 1;
    uint32_t sync : 1;
    uint32_t isFirstIter : 1;
    uint32_t waitIterateAll : 1;
    uint32_t enPartialSum : 1;

    uint32_t singleShapeM;
    uint32_t singleShapeN;
    uint32_t singleShapeK;

    uint64_t fmapAddr;
    uint64_t outBackpropAddr;
    uint64_t outputAddr;

    uint32_t curHoStartIdx;
    uint32_t fmapSize;
    uint32_t outBackpropSize;
    uint32_t res;

    uint32_t res1[16];
};

struct Conv3DForwardConfigParams {
    uint32_t enAtomic: 8;
    uint32_t enSequentialWrite: 1;
    uint32_t enSetTensorFmap: 1;
    uint32_t enSetTensorWeight: 1;
    uint32_t enSetTensorBias: 1;
    uint32_t sync: 1;
    uint32_t enSetSingleOutputShape: 1;
    uint32_t enSetFmapStartPosition: 1;
    uint32_t waitIterateAll: 1;
    uint32_t enPartialSum: 1;
    uint32_t fmapSize;

    uint64_t fmapAddr;
    uint64_t weightAddr;
    uint64_t biasAddr;
    uint64_t outputAddr;

    uint32_t weightSize;
    uint32_t biasSize;
    uint32_t singleCoreBatch;
    uint32_t singleCo;
    uint32_t singleDo;
    uint32_t singleCoreM;
    uint32_t singleGroupOpt;
    uint32_t diStartPos;
    uint32_t mStartPos;
    uint32_t ciStartPos;
    uint32_t res[10];
};

struct MatmulUserDefInfo {
    uint64_t tilingPtr;
};

constexpr uint16_t KFC_MSG_BYTE_OFFSET = 16;

[aicore] __inline__ __attribute__((always_inline)) uint16_t KfcMsgGetEvtCnt(uint32_t flag)
{
    return flag & 0x00007fff;
}

[aicore] __inline__ __attribute__((always_inline)) uint16_t KfcMsgGetInstID(uint32_t flag)
{
    return flag & 0x000000ff;
}
[aicore] __inline__ __attribute__((always_inline)) KFC_Enum KfcMsgGetFunID(uint32_t flag)
{
    return static_cast<KFC_Enum>((flag & 0xffff0000) >> KFC_MSG_BYTE_OFFSET);
}
[aicore] __inline__ __attribute__((always_inline)) uint32_t KfcMsgGetState(uint32_t flag)
{
    return (flag & 0x00008000);
}
[aicore] __inline__ __attribute__((always_inline)) uint32_t KfcMsgMakeFlag(KFC_Enum funID, uint16_t instID)
{
    return (((static_cast<uint16_t>(funID) << KFC_MSG_BYTE_OFFSET) + 0x8000) + (instID));
}


struct KfcMsg {
    uint32_t head = 0;
    int32_t ubAddr = -1;
    union {
        uint8_t buffer[120];
        TilingInfo tilingInfo;
        MatmulConfigParams body;
        MatmulOrgShape orgShape;
        MatmulUserDefInfo userDefInfo;
        Conv3DBpInputConfigParams convBpInputBody;
        Conv3DBpFilterConfigParams convBpFilterBody;
        Conv3DForwardConfigParams convForwardBody;
    };
};
struct MsgUBAvalied {
    uint32_t head;
    uint32_t res;
    uint8_t buffer[56];
};
struct MsgMatmulCnt {
    uint32_t head;
    uint32_t res;
    uint8_t buffer[56];
};

struct QuitCnt {
    int32_t head;
    uint32_t res;
    uint8_t buffer[56];
};

struct MmTaskCnt {
    uint32_t head;
    uint32_t res;
    uint8_t buffer[56];
};

struct MsgGroupSync {
    int32_t syncCount;
    uint8_t res[28];
    uint32_t allNumber;
    uint8_t buffer[28];
};

struct MsgGroupSyncAux {

    int32_t curNumber;
    uint8_t res[28];
    uint32_t idField;
    uint8_t buffer[28];
};

[aicore] __inline__ __attribute__((always_inline)) constexpr int AlignTo32(int size)
{
    return (size + ALIGN_SIZE - 1) / ALIGN_SIZE * ALIGN_SIZE;
}

struct SysWorkspaceDesc {
    KfcMsg kfcMsg[MAX_AIV_NUM * BIDIRECTION_NUM * 64 * MIX_COEFFICIENT];
    MsgMatmulCnt cntMsg[MAX_AIV_NUM * MIX_COEFFICIENT][MAX_MATMUL_OBJ];
    MsgUBAvalied ubMsg[MAX_AIV_NUM];
    uint8_t ubMap[MAX_AIV_NUM][WORKSPACE_UB_SIZE];
    QuitCnt quitCnt[QUIT_CNT];
    MmTaskCnt mmTaskCnt[MM_CNT_MAX];
    MsgGroupSync groupSyncMsg[MAX_GROUP_ID];
    MsgGroupSyncAux groupSyncAuxMsg[MAX_GROUP_ID];
};

[aicore] __inline__ __attribute__((always_inline)) void ClearWorkspaceImpl(__attribute__((cce_global)) uint8_t* workspace)
{
    constexpr uint32_t size = BIDIRECTION_NUM * 64 * AlignTo32(sizeof(KfcMsg)) * MIX_NUM;
    constexpr uint32_t sizeUbmsg = MIX_NUM * AlignTo32(sizeof(MsgUBAvalied));
    constexpr uint32_t offsetUbMsg = MAX_AIV_NUM * BIDIRECTION_NUM * 64 *
        MIX_COEFFICIENT * AlignTo32(sizeof(KfcMsg)) + MAX_AIV_NUM * MIX_COEFFICIENT *
        MAX_MATMUL_OBJ * AlignTo32(sizeof(MsgMatmulCnt));
    constexpr uint32_t block = size / 2048;
    uint32_t ubOffset11 = 0;
    uint32_t msgOffset11 = 0;
    if constexpr (MIX_NUM == 1) {
        msgOffset11 = BIDIRECTION_NUM * 64 * AlignTo32(sizeof(KfcMsg));
        ubOffset11 = AlignTo32(sizeof(MsgUBAvalied));
    }
    __attribute__((cce_global)) uint8_t* msgStartAddr = (__attribute__((cce_global)) uint8_t*)(workspace + (size + msgOffset11) * GetBlockIdxImpl());
    __attribute__((cce_global)) uint8_t* ubMsgStartAddr =
        (__attribute__((cce_global)) uint8_t*)(workspace + offsetUbMsg + (sizeUbmsg + ubOffset11) * GetBlockIdxImpl());
    create_cbuf_matrix((__attribute__((cce_cube_buff)) uint32_t*)(0), 0x10040, 0);
    SetFlag<HardEvent::MTE2_MTE3>(EVENT_ID0);
    WaitFlag<HardEvent::MTE2_MTE3>(EVENT_ID0);
    for (size_t i = 0; i < block; i++) {
        copy_cbuf_to_gm((__attribute__((cce_global)) void*)(msgStartAddr), (__attribute__((cce_cube_buff)) void*)(0), 0, 1, 64, 1, 1);
        msgStartAddr += 2048;
    }
    copy_cbuf_to_gm((__attribute__((cce_global)) void*)(ubMsgStartAddr), (__attribute__((cce_cube_buff)) void*)(0), 0, 1, sizeUbmsg / 32, 1, 1);
    PipeBarrier<PIPE_ALL>();
}
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetMsgHead(__attribute__((cce_global)) uint8_t* workspace, int i)
{
# 404 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h"
                                                                                                                ;

                                                                                 ;

    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);

    auto flatBlockID = get_block_idx() * MAX_BLOCK_AIV_NUM + i;
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&ptr->kfcMsg[flatBlockID * BIDIRECTION_NUM * 64]);
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetUBMapAddr(__attribute__((cce_global)) uint8_t* workspace, int i = 0)
{







                                                                                 ;

    auto flatBlockID = get_block_idx() * MAX_BLOCK_AIV_NUM + i;
    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(ptr->ubMap[flatBlockID]);
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetMatmulIncAddr(__attribute__((cce_global)) uint8_t* workspace, uint32_t flatBlockID, uint32_t instID)
{


                                                                                                                 ;

                                                                                 ;
    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&(ptr->cntMsg[flatBlockID][instID]));
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetUBAvaliedAddr(__attribute__((cce_global)) uint8_t* workspace, uint32_t i = 0)
{
                                ;
    auto flatBlockID = get_block_idx() * MAX_BLOCK_AIV_NUM + i;
    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&(ptr->ubMsg[flatBlockID]));
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg *AllocMessageImpl(
    __attribute__((cce_global)) KfcMsg *&msgSendHead, uint8_t &msgSendPos, __attribute__((cce_global)) KfcMsg *&msgSendStart)
{
    auto msg = msgSendHead;

                                                                                   ;

                                                                                    ;
    if constexpr (KFC_APPLY_MSG) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while (static_cast<bool>(KfcMsgGetState(msg->head))) {
            Barrier();
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
            Barrier();
        }
    }

                                                                                                                      ;
    msgSendPos++;
    if (msgSendPos >= 64) {
        msgSendPos = 0;
        msgSendHead = msgSendStart;
    } else {
        msgSendHead++;
    }
    return msg;
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg *RcvMessageImpl(
    __attribute__((cce_global)) KfcMsg *&msgRcvHead, uint8_t &msgRcvPos, __attribute__((cce_global)) KfcMsg *&msgRcvStart)
{



                                                                                  ;

                                                                                   ;


      ;
    __attribute__((cce_global)) KfcMsg* msg = msgRcvHead;
    Barrier();
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
    Barrier();

    dc_preload((__attribute__((cce_global)) uint64_t*)msg, int64_t(0));
# 506 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h"
    if (!(static_cast<bool>(KfcMsgGetState(msg->head)))) {
        return nullptr;
    }
    msgRcvPos++;
    if (msgRcvPos >= 64) {
        msgRcvPos = 0;
        msgRcvHead = msgRcvStart;
    } else {
        msgRcvHead++;
    }
    return msg;
}

[aicore] __inline__ __attribute__((always_inline)) void FreeMessageImpl(__attribute__((cce_global)) KfcMsg *msg)
{

                                                                           ;
    __asm__ __volatile__("" ::: "memory");
    *(reinterpret_cast<__attribute__((cce_global)) uint64_t *>(msg)) = 0;
    Barrier();
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
    Barrier();
}

[aicore] __inline__ __attribute__((always_inline)) void RollBackMsgImpl(__attribute__((cce_global)) KfcMsg *&msgRcvHead, uint8_t &msgRcvPos)
{
    if (msgRcvPos == 0) {
        msgRcvPos = 64;
        msgRcvHead = msgRcvHead + 64 -1;
    } else {
        msgRcvPos--;
        msgRcvHead--;
    }
}

}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h" 2






namespace AscendC {

template <int depth>
struct TBufHandleAux {
    using T = TBufHandle[depth];
};

template <>
struct TBufHandleAux<1> {
    using T = TBufHandle;
};
constexpr TEventID INVALID_TEVENTID = (static_cast<TEventID>(-1));


struct TEventPool {
    uint64_t eventOccupy;
};

struct TPipeBufPool {
    uint32_t maxAddr;
};
# 66 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h"
struct TShareBuf {
    enum class ShareHard : uint8_t {
        L1 = 0,
        L0C = 1,
        UB = 2,
        MAX,
    };
    int32_t start[static_cast<uint8_t>(ShareHard::MAX)];
    int32_t maxAddr[static_cast<uint8_t>(ShareHard::MAX)];
                                                                     ;
};

struct SpmInfo {
    uint64_t spmAddr;
    int32_t spmBuffSize;
    uint8_t spmBufType;
};

struct TPipeImpl {
    struct TEventPool eventPool_[EVENT_NUM];
    struct TPipeBufPool bufPool_[static_cast<uint8_t>(Hardware::MAX)];



    struct TBufType buf_[64];
    TShareBuf shareBufPool_;
    SpmInfo spmInfo_;

    uint32_t tscmBufferPtr_;
    uint8_t curBufSize_;
    bool isDestroy;
};

constexpr uint32_t defaultBufIDSize = 4;

template <uint32_t bufIDSize = defaultBufIDSize>
struct TBufPoolImpl {
    struct TBufType buf_[bufIDSize];
    uint32_t startAddr_;
    uint32_t maxAddr_;
    uint32_t maxLen_;
    uint8_t curBufSize_;
    uint8_t isReset_;
};

class TPipeBase {
public:
    [aicore] __inline__ __attribute__((always_inline)) void InitShareBufStart(uint32_t mode, uint32_t* shareLens, uint32_t lens, uint8_t subBlockIdx);
    [aicore] __inline__ __attribute__((always_inline)) void InitShareBufEnd();

protected:
    TPipeImpl g_tpipeImpl;
    [aicore] __inline__ __attribute__((always_inline)) void AuxShareBufStart(uint32_t mode, uint32_t* shareLens, uint8_t pos, Hardware hard,
                                            uint8_t subBlockIdx);
};

[aicore] __inline__ __attribute__((always_inline)) void TPipeBase::InitShareBufStart(uint32_t mode, uint32_t* shareLens, uint32_t lens,
                                                    uint8_t subBlockIdx)
{






    (void)(lens);



                                                                                                             ;
    AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L1), Hardware::L1, subBlockIdx);
    AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L0C), Hardware::L0C, subBlockIdx);



    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0A)].maxAddr = 0;
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0B)].maxAddr = 0;

    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::BIAS)].maxAddr = 0;

    return;
}

[aicore] __inline__ __attribute__((always_inline)) void TPipeBase::InitShareBufEnd()
{

    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L1)].maxAddr =
        g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L1)];
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0C)].maxAddr =
        g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L0C)];





    return;
}

[aicore] __inline__ __attribute__((always_inline)) void TPipeBase::AuxShareBufStart(uint32_t mode, uint32_t* shareLens, uint8_t pos, Hardware hard,
                                                   uint8_t subBlockIdx)
{
    uint8_t hardU8 = static_cast<uint8_t>(hard);
    if (__builtin_expect(!!(g_tpipeImpl.shareBufPool_.start[pos] == -1), 0)) {

        g_tpipeImpl.shareBufPool_.start[pos] = this->g_tpipeImpl.bufPool_[hardU8].maxAddr;
        g_tpipeImpl.shareBufPool_.maxAddr[pos] = g_tpipeImpl.shareBufPool_.start[pos] + shareLens[pos];
                                                                          ;
    } else {


                                                                              ;

        g_tpipeImpl.shareBufPool_.maxAddr[pos] = this->g_tpipeImpl.bufPool_[hardU8].maxAddr;
        g_tpipeImpl.bufPool_[hardU8].maxAddr = g_tpipeImpl.shareBufPool_.start[pos];
    }

    if (mode == 1 && subBlockIdx == 1) {
        this->g_tpipeImpl.bufPool_[hardU8].maxAddr += shareLens[pos] / HALF_FACTOR;
    }




      ;
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tpipe.h" 2
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tpipe.h"
namespace AscendC {
class TPipe;
template <TPosition src, TPosition dst, int32_t depth, auto mask = 0> class TQueBind {
public:
    [aicore] __inline__ __attribute__((always_inline)) TQueBind();
    [aicore] __inline__ __attribute__((always_inline)) void FreeBuffer(TBufHandle buf);
    [aicore] __inline__ __attribute__((always_inline)) TBuffAddr GetBufferAddr(TBufHandle buf);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<T> AllocTensor();
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) void FreeTensor(LocalTensor<T>& tensor);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) bool EnQue(const LocalTensor<T>& tensor);
    [aicore] __inline__ __attribute__((always_inline)) bool EnQue(TBufHandle buf);
    template <TPosition srcUserPos, TPosition dstUserPos, typename T>
    [aicore] __inline__ __attribute__((always_inline)) bool EnQue(const LocalTensor<T>& tensor);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> DeQue();
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle DeQue();
    template <TPosition srcUserPos, TPosition dstUserPos, typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> DeQue();
    [aicore] __inline__ __attribute__((always_inline)) bool VacantInQue();
    [aicore] __inline__ __attribute__((always_inline)) bool HasTensorInQue();
    [aicore] __inline__ __attribute__((always_inline)) int32_t GetTensorCountInQue();
    [aicore] __inline__ __attribute__((always_inline)) bool HasIdleBuffer();
    [aicore] __inline__ __attribute__((always_inline)) void FreeAllEvent();
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) TBufState GetState(const LocalTensor<T>& tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) void InitStartBufHandle(TBufHandle startBufhandle, uint8_t num, uint32_t len);
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void InitBufHandle(T* bufPool, uint32_t index, TBufHandle bufhandle,
        uint32_t curPoolAddr, uint32_t len);
protected:
    static constexpr TQueConfig config = GetTQueConfig(mask);
    static constexpr bool nd2nz = config.nd2nz;
    static constexpr bool nz2nd = config.nz2nd;
    static constexpr bool scmBlockGroup = config.scmBlockGroup;
    static constexpr bool enableLoopQueue = config.enableLoopQueue;
    static constexpr TPosition srcPosition = src;
    static constexpr TPosition dstPosition = dst;
    static constexpr Hardware srcHardType = GetPhyType(src);
    static constexpr Hardware dstHardType = GetPhyType(dst);
    static constexpr HardEvent enQueEvt = GetQueEvt(srcHardType, dstHardType, true, nd2nz, nz2nd);
    static constexpr HardEvent freeBufEvt = GetQueEvt(srcHardType, dstHardType, false, nd2nz, nz2nd);
    static constexpr int32_t queDepth = depth;
    union {
        uint64_t value;
        struct {
            uint8_t bufNum = 0;
            uint8_t usedCount;
            uint16_t head;
            uint16_t tail;
            uint8_t bufUsedCount;
            uint8_t bufCursor;
        };

    };
    typename TBufHandleAux<depth>::T que_;
    struct TBufType* bufStart;
                               ;
    friend class TPipe;
    template <TPosition pos, int32_t d, auto m> friend class TQue;
    template<TPosition pos, uint32_t bufIDSize> friend class TBufPool;



private:
    [aicore] __inline__ __attribute__((always_inline)) void SetTBufPoolHandle(uint64_t bufPoolHandle);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> Buf2Tensor(TBufHandle buf);
    [aicore] __inline__ __attribute__((always_inline)) TBufState GetState(const TBufHandle& handle) const;
    static constexpr bool isTQue = true;
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle AllocBuffer();
    template <TPosition srcUserPos, TPosition dstUserPos> [aicore] __inline__ __attribute__((always_inline)) bool EnQue(TBufHandle buf);
    template <TPosition srcUserPos, TPosition dstUserPos> [aicore] __inline__ __attribute__((always_inline)) TBufHandle DeQue();
};





template <TPosition pos, int32_t depth, auto mask = 0>
class TQue : public TQueBind<GetBufferLogicPos(pos, true), GetBufferLogicPos(pos, false), depth, mask> {
public:
    [aicore] __inline__ __attribute__((always_inline)) TQue() = default;
private:
    friend class TPipe;
    template<TPosition bufPos, uint32_t bufIDSize> friend class TBufPool;
    static constexpr bool isTQue = true;
};

template <TPosition pos = TPosition::LCM> class TBuf : public TQueBind<pos, pos, 0, 0> {
public:
    [aicore] __inline__ __attribute__((always_inline)) TBuf() = default;
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> Get();
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> Get(uint32_t len);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> GetWithOffset(uint32_t size, uint32_t bufOffset);

    template <typename T> [aicore] __inline__ __attribute__((always_inline)) void EnQue(const LocalTensor<T>& tensor);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> DeQue();
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> AllocTensor();
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) void FreeTensor(LocalTensor<T>& tensor);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) TBufState GetState(const LocalTensor<T>& tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) bool EnQue(TBufHandle buf);
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle DeQue();
    [aicore] __inline__ __attribute__((always_inline)) void FreeBuffer(TBufHandle buf);
    [aicore] __inline__ __attribute__((always_inline)) TBuffAddr GetBufferAddr(TBufHandle buf);
    [aicore] __inline__ __attribute__((always_inline)) void InitStartBufHandle(TBufHandle startBufhandle, uint8_t num, uint32_t len);

private:
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle Get();
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle Get(uint32_t len);
    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetBufLen() const;
    [aicore] __inline__ __attribute__((always_inline)) void SetTpipeBuf(TBufType* bufStartIn, uint32_t bufLenIn);
    template <TPosition posPopBuffer>
    friend [aicore] __inline__ __attribute__((always_inline)) bool PopStackBuffer(TBuf<posPopBuffer> &popBuffer, TBufType &bufStart);
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle AllocBuffer();

private:
    struct TBufType* bufStart;
    uint32_t bufLen;
    uint32_t offset;
    friend class TPipe;
    template<TPosition bufPos, uint32_t bufIDSize> friend class TBufPool;
    static constexpr bool isTQue = false;
};

template <TPosition pos, uint32_t bufIDSize = defaultBufIDSize>
class TBufPool {
public:
    static constexpr TPosition poolPos = pos;
public:
    [aicore] __inline__ __attribute__((always_inline)) TBufPool();
    [aicore] __inline__ __attribute__((always_inline)) ~TBufPool();
    template <class T> [aicore] __inline__ __attribute__((always_inline)) bool InitBuffer(T& que, uint8_t num, uint32_t len);
    template <TPosition bufPos> [aicore] __inline__ __attribute__((always_inline)) bool InitBuffer(TBuf<bufPos>& buf, uint32_t len);
    template <class T, class U> [aicore] __inline__ __attribute__((always_inline)) bool InitBufPool(T& bufPool, uint32_t len, U& shareBuf);
    template <class T> [aicore] __inline__ __attribute__((always_inline)) bool InitBufPool(T& bufPool, uint32_t len);
    [aicore] __inline__ __attribute__((always_inline)) void Reset();
protected:
    TBufPoolImpl<bufIDSize> tBufPoolImpl;
private:
    [aicore] __inline__ __attribute__((always_inline)) void Init();
    [aicore] __inline__ __attribute__((always_inline)) void ResetPool();
private:
    friend class TPipe;
    template <TPosition src, TPosition dst, int32_t depth, auto mask> friend class TQueBind;
    template <TPosition bufPos, int32_t depth, auto mask> friend class TQue;
    template <TPosition bufPos> friend class TBuf;
    static constexpr bool isTbufPool = true;
};
# 238 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tpipe.h"
class TPipe : public TPipeBase {
public:
    [aicore] __inline__ __attribute__((always_inline)) TPipe();
    [aicore] __inline__ __attribute__((always_inline)) ~TPipe();
    [aicore] __inline__ __attribute__((always_inline)) void Init();
    template <class T> [aicore] __inline__ __attribute__((always_inline)) bool InitBuffer(T& que, uint8_t num, uint32_t len);
    template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) bool InitBuffer(TBuf<pos>& buf, uint32_t len);
    template <class T> [aicore] __inline__ __attribute__((always_inline)) bool InitBufPool(T& bufPool, uint32_t len);
    template <class T, class U> [aicore] __inline__ __attribute__((always_inline)) bool InitBufPool(T& bufPool, uint32_t len, U& shareBuf);
    template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) TEventID AllocEventID();
    template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) void ReleaseEventID(TEventID id);
    template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) TEventID FetchEventID();
    [aicore] __inline__ __attribute__((always_inline)) TEventID FetchEventID(HardEvent evt);
    template <TPosition pos, typename T>
    [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> GetAbsAddr(int32_t offset, int32_t size) const;
    template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBuffAddr GetAbsAddr(int32_t offset, int32_t len) const;
# 266 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tpipe.h"
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void InitSpmBuffer(const GlobalTensor<T>& workspace, const int32_t bufferSize);
    [aicore] __inline__ __attribute__((always_inline)) void InitSpmBuffer(const int32_t bufferSize);
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void WriteSpmBuffer(const LocalTensor<T>& writeLocal, const DataCopyParams& copyParams,
        int32_t writeOffset = 0);
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void ReadSpmBuffer(const LocalTensor<T>& readLocal, const DataCopyParams& copyParams,
        int32_t readOffset = 0);
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void WriteSpmBuffer(const LocalTensor<T>& writeLocal, const int32_t writeSize,
        int32_t writeOffset = 0);
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void ReadSpmBuffer(const LocalTensor<T>& readLocal, const int32_t readSize,
        int32_t readOffset = 0);
    [aicore] __inline__ __attribute__((always_inline)) void Destroy();
    [aicore] __inline__ __attribute__((always_inline)) void Reset();





protected:
    template <TPosition src, TPosition dst, int32_t depth, auto mask> friend class TQueBind;
    template <TPosition pos, int32_t depth, auto mask> friend class TQue;
    template <TPosition pos> friend class TBuf;
    template<TPosition pos, uint32_t bufIDSize> friend class TBufPool;
    template <TPosition pos> friend [aicore] __inline__ __attribute__((always_inline)) bool PopStackBuffer(TBuf<pos>& popBuffer, TBufType& bufStart);
    template <typename T, TPosition pos> friend [aicore] __inline__ __attribute__((always_inline)) bool PopStackBuffer(LocalTensor<T>& popLocal);




private:



    friend [aicore] __inline__ __attribute__((always_inline)) void InitShareBufStart(TPipe* tpipe, uint32_t mode, uint32_t* shareLens,
        uint32_t lens, uint8_t subBlockIdx);
    friend [aicore] __inline__ __attribute__((always_inline)) void InitShareBufEnd(TPipe* tpipe);
    [aicore] __inline__ __attribute__((always_inline)) void InitSocState() const;
    [aicore] __inline__ __attribute__((always_inline)) void ResetPool();
    template <class T> [aicore] __inline__ __attribute__((always_inline)) bool TscmInitBuffer(T& que, uint8_t num, uint32_t len);



    template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) uint64_t GetQueueEndAddress();
};

template<pipe_t src, pipe_t dst>
class TQueSync {
public:
    [aicore] __inline__ __attribute__((always_inline)) void SetFlag(TEventID id);
    [aicore] __inline__ __attribute__((always_inline)) void WaitFlag(TEventID id);
};

template <TPosition pos, int32_t depth = 1, auto mask = 0>
using TSCM = TQueBind<pos, TPosition::TSCM, depth, mask>;
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_dump_tensor_intf.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_dump_tensor_intf.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const LocalTensor<T> &tensor, uint32_t desc, uint32_t dumpSize);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const LocalTensor<T>& tensor, uint32_t desc,
    uint32_t dumpSize, const ShapeInfo& shapeInfo);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const GlobalTensor<T>& tensor, uint32_t desc,
    uint32_t dumpSize, const ShapeInfo& shapeInfo);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpAccChkPoint(const LocalTensor<T> &tensor,
    uint32_t index, uint32_t countOff, uint32_t dumpSize);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpAccChkPoint(const GlobalTensor<T> &tensor,
    uint32_t index, uint32_t countOff, uint32_t dumpSize);

template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void PRINTF(__attribute__((cce_global)) const char* fmt, Args&&... args);
template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void printf(__attribute__((cce_global)) const char* fmt, Args&&... args);
# 70 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_dump_tensor_intf.h"
enum class TimeStampId : uint32_t {
    TIME_STAMP_WRAP_FIRST = 0x000,
    TIME_STAMP_WRAP_MC2_CTX,
    TIME_STAMP_WRAP_WK_SPACE,
    TIME_STAMP_WRAP_INIT_DUMP,
    TIME_STAMP_WRAP_FFTS_ADDR,
    TIME_STAMP_WRAP_CLEAR_WK_SPAC,

    TIME_STAMP_TPIPE = 0x030,
    TIME_STAMP_BUFFER,

    TIME_STAMP_MATMUL_SERVER = 0x060,
    TIME_STAMP_MATMUL_SERVER_INIT,
    TIME_STAMP_MATMUL_SERVER_OBJ,
    TIME_STAMP_MATMUL_MATRIX_KFC,
    TIME_STAMP_MATMUL_CLIENT_KFC,
    TIME_STAMP_MATMUL_WAIT_EVE,
    TIME_STAMP_MATMUL_OBJ,

    TIME_STAMP_TILING_DATA = 0x090,
    TIME_STAMP_TILING_DATA_STRUCT,
    TIME_STAMP_TILING_DATA_MEMBER,


    TIME_STAMP_MC2_START = 0x1000,
    TIME_STAMP_MC2_END = 0x1fff,

    TIME_STAMP_MAX = 0xffff,
};

[aicore] __inline__ __attribute__((always_inline)) void PrintTimeStamp(uint32_t descId);
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tquebind_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tquebind_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) constexpr bool IsAivTscm(TPosition src, TPosition dst)
{

    if (GetPosition(src, dst) == TPosition::TSCM) {
        return true;
    }




    return false;
}


template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) TQueBind<src, dst, depth, mask>::TQueBind()
{



}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) void TQueBind<src, dst, depth, mask>::InitStartBufHandle(
    TBufHandle startBufhandle, uint8_t num, uint32_t len)
{
    static_assert(isTQue, "InitTQueAddr only support TQue class");
    auto ptr = reinterpret_cast<TBufType*>(startBufhandle);
    this->value = num;
    this->bufStart = ptr;
                                        ;
    return;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TQueBind<src, dst, depth, mask>::InitBufHandle(T* bufPool,
    uint32_t index, TBufHandle bufhandle, uint32_t curPoolAddr, uint32_t len)
{
    (void)(bufPool);
    (void)(index);
                                                                                                                       ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    auto ptr = reinterpret_cast<TBufType*>(bufhandle);
    ptr->state = TBufState::FREE;
    ptr->freeBufEvt = freeBufEvt;
    ptr->enQueEvtID = INVALID_TEVENTID;
    ptr->freeBufEvtID = INVALID_TEVENTID;
    ptr->address = curPoolAddr;
    ptr->dataLen = len;
    ptr->usertag = -1;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<T> TQueBind<src, dst, depth, mask>::AllocTensor()
{
    auto buf = AllocBuffer();
    return Buf2Tensor<T>(buf);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TQueBind<src, dst, depth, mask>::FreeTensor(LocalTensor<T>& tensor)
{
    FreeBuffer(tensor.GetBufferHandle());
    return;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) bool TQueBind<src, dst, depth, mask>::EnQue(const LocalTensor<T>& tensor)
{
    auto buf = tensor.GetBufferHandle();
    return EnQue(reinterpret_cast<TBufHandle>(buf));
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos, typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) bool TQueBind<src, dst, depth, mask>::EnQue(const LocalTensor<T>& tensor)
{
    auto buf = tensor.GetBufferHandle();
    return EnQue<srcUserPos, dstUserPos>(reinterpret_cast<TBufHandle>(buf));
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) bool TQueBind<src, dst, depth, mask>::EnQue(TBufHandle buf)
{
    static_assert(((srcUserPos == TPosition::GM) || (srcUserPos == TPosition::VECIN) ||
                (srcUserPos == TPosition::VECOUT) || (srcUserPos == TPosition::VECCALC)) &&
                "enque only support src position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(((dstUserPos == TPosition::GM) || (dstUserPos == TPosition::VECIN) ||
                (dstUserPos == TPosition::VECOUT) || (dstUserPos == TPosition::VECCALC)) &&
                "enque only support dst position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(!((srcUserPos == TPosition::GM) && (dstUserPos == TPosition::GM)) &&
                "enque src and dst position cannot be GM at the same time.");
    constexpr Hardware srcUserHardType = GetPhyType(srcUserPos);
    constexpr Hardware dstUserHardType = GetPhyType(dstUserPos);
    constexpr HardEvent enQueUserEvt = GetQueEvt(srcUserHardType, dstUserHardType, true, false, false);




      ;
    auto ptr = reinterpret_cast<TBufType*>(buf);
    if constexpr (depth == 1) {
        this->que_ = buf;
    } else {
        this->que_[this->tail] = buf;
    }
    this->usedCount++;




      ;



      ;
                                                ;
                                             ;


    if constexpr (enQueUserEvt == HardEvent::V_V) {
        SetFlag<enQueUserEvt>(0);
        ptr->enQueEvtID = 0;
    } else {
        auto enQueUserEvtID = GetTPipePtr()->AllocEventID<enQueUserEvt>();
        SetFlag<enQueUserEvt>(enQueUserEvtID);
        ptr->enQueEvtID = enQueUserEvtID;
    }
    if constexpr (depth != 1) {
        if (++this->tail >= depth) {
            this->tail = 0;
        }
    }






    return true;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) bool TQueBind<src, dst, depth, mask>::EnQue(TBufHandle buf)
{



      ;
    auto ptr = reinterpret_cast<TBufType*>(buf);
    if constexpr (depth == 1) {
        this->que_ = buf;
    } else {
        this->que_[this->tail] = buf;
    }
    this->usedCount++;



      ;


      ;
                                             ;





    if (g_coreType != AIV || (GetPosition(src, dst) != TPosition::TSCM)) {
        auto enQueEvtID = GetTPipePtr()->AllocEventID<enQueEvt>();
        SetFlag<enQueEvt>(enQueEvtID);
        ptr->enQueEvtID = enQueEvtID;
    }





    if constexpr (depth != 1) {
        if (++this->tail >= depth) {
            this->tail = 0;
        }
    }






    return true;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<T> TQueBind<src, dst, depth, mask>::DeQue()
{
    auto buf = DeQue();
    auto ret = Buf2Tensor<T>(buf);
    return ret;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos, typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<T> TQueBind<src, dst, depth, mask>::DeQue()
{
    auto buf = DeQue<srcUserPos, dstUserPos>();
    auto ret = Buf2Tensor<T>(buf);
    return ret;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) TBufHandle TQueBind<src, dst, depth, mask>::DeQue()
{
    TBufHandle buf;
    if constexpr (depth == 1) {
        buf = this->que_;
    } else {
        buf = this->que_[this->head];
    }
                                                                                             ;
    auto ptr = reinterpret_cast<TBufType*>(buf);



      ;



      ;
    this->usedCount--;



                                             ;

    if (g_coreType != AIV || (GetPosition(src, dst) != TPosition::TSCM)) {
        if (ptr->enQueEvtID != INVALID_TEVENTID) {
            WaitFlag<enQueEvt>(ptr->enQueEvtID);
            GetTPipePtr()->ReleaseEventID<enQueEvt>(ptr->enQueEvtID);
            ptr->enQueEvtID = INVALID_TEVENTID;
        }
    }







    if constexpr (depth != 1) {
        if (++this->head >= depth) {
            this->head = 0;
        }
    }






    return reinterpret_cast<TBufHandle>(buf);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) TBufHandle TQueBind<src, dst, depth, mask>::DeQue()
{
    static_assert(((srcUserPos == TPosition::GM) || (srcUserPos == TPosition::VECIN) ||
                (srcUserPos == TPosition::VECOUT) || (srcUserPos == TPosition::VECCALC)) &&
                "DeQue only support src position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(((dstUserPos == TPosition::GM) || (dstUserPos == TPosition::VECIN) ||
                (dstUserPos == TPosition::VECOUT) || (dstUserPos == TPosition::VECCALC)) &&
                "DeQue only support dst position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(!((srcUserPos == TPosition::GM) && (dstUserPos == TPosition::GM)) &&
                "DeQue src and dst position cannot be GM at the same time.");
    constexpr Hardware srcUserHardType = GetPhyType(srcUserPos);
    constexpr Hardware dstUserHardType = GetPhyType(dstUserPos);
    constexpr HardEvent deQueUserEvt = GetQueEvt(srcUserHardType, dstUserHardType, true, false, false);

    TBufHandle buf;
    if constexpr (depth == 1) {
        buf = this->que_;
    } else {
        buf = this->que_[this->head];
    }


      ;
    auto ptr = reinterpret_cast<TBufType*>(buf);




      ;



      ;
    this->usedCount--;


      ;
                                             ;

    if constexpr (deQueUserEvt == HardEvent::V_V) {
        WaitFlag<deQueUserEvt>(0);
        ptr->enQueEvtID = INVALID_TEVENTID;
    } else {
        if (ptr->enQueEvtID != INVALID_TEVENTID) {
            WaitFlag<deQueUserEvt>(ptr->enQueEvtID);
            GetTPipePtr()->ReleaseEventID<deQueUserEvt>(ptr->enQueEvtID);
            ptr->enQueEvtID = INVALID_TEVENTID;
        }
    }

    if constexpr (depth != 1) {
        if (++this->head >= depth) {
            this->head = 0;
        }
    }






    return reinterpret_cast<TBufHandle>(buf);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) void TQueBind<src, dst, depth, mask>::FreeBuffer(TBufHandle buf)
{
    auto ptr = reinterpret_cast<TBufType*>(buf);



      ;


      ;
    if constexpr (!IsAivTscm(src, dst)) {
# 383 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tquebind_impl.h"
        ptr->freeBufEvtID = GetTPipePtr()->AllocEventID<freeBufEvt>();
        SetFlag<freeBufEvt>(ptr->freeBufEvtID);
        if constexpr (enableLoopQueue) {
            ptr->freeBufEvt = freeBufEvt;
        }

    } else if constexpr (srcHardType == Hardware::GM) {
        if constexpr(g_coreType == AscendC::AIC) {
            ptr->freeBufEvtID = GetTPipePtr()->AllocEventID<freeBufEvt>();
            SetFlag<freeBufEvt>(ptr->freeBufEvtID);
            if constexpr (enableLoopQueue) {
                ptr->freeBufEvt = freeBufEvt;
            }
        }
    }
    ptr->state = TBufState::FREE;
    this->bufUsedCount--;






    return;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) TBufHandle TQueBind<src, dst, depth, mask>::AllocBuffer()
{
                                ;


      ;
    TBufType* ret;
    do {
        ret = this->bufStart + this->bufCursor;
        if constexpr (config.bufferNumber != 1) {
            this->bufCursor += 1;
            if (this->bufCursor == this->bufNum) {
                this->bufCursor = 0;
            }
        }
        if (ret->state == TBufState::FREE) {
            ret->state = TBufState::OCCUPIED;
            if constexpr (IsAivTscm(src, dst)) {
                if constexpr (srcHardType == Hardware::UB) {
                    break;
                } else if constexpr (srcHardType == Hardware::GM) {
                    if constexpr(g_coreType == AscendC::AIV) {
                        break;
                    }
                }
            }
            if (ret->freeBufEvtID != INVALID_TEVENTID) {
                if constexpr (enableLoopQueue) {
                    if (freeBufEvt == ret->freeBufEvt) {
                        WaitFlag<freeBufEvt>(ret->freeBufEvtID);
                        GetTPipePtr()->ReleaseEventID<freeBufEvt>(ret->freeBufEvtID);
                        ret->freeBufEvtID = INVALID_TEVENTID;
                    } else if (freeBufEvt == HardEvent::V_MTE2 && ret->freeBufEvt == HardEvent::MTE3_V) {
                        WaitFlag<HardEvent::MTE3_V>(ret->freeBufEvtID);
                        GetTPipePtr()->ReleaseEventID<HardEvent::MTE3_V>(ret->freeBufEvtID);
                        ret->freeBufEvtID = INVALID_TEVENTID;
                        TEventID evtId = GetTPipePtr()->AllocEventID<HardEvent::MTE3_MTE2>();
                        SetFlag<HardEvent::MTE3_MTE2>(evtId);
                        WaitFlag<HardEvent::MTE3_MTE2>(evtId);
                        GetTPipePtr()->ReleaseEventID<HardEvent::MTE3_MTE2>(evtId);
                    } else if (freeBufEvt == HardEvent::MTE3_V && ret->freeBufEvt == HardEvent::V_MTE2) {
                        WaitFlag<HardEvent::V_MTE2>(ret->freeBufEvtID);
                        GetTPipePtr()->ReleaseEventID<HardEvent::V_MTE2>(ret->freeBufEvtID);
                        ret->freeBufEvtID = INVALID_TEVENTID;
                    } else {


                          ;
                    }
                } else {
                    WaitFlag<freeBufEvt>(ret->freeBufEvtID);
                    GetTPipePtr()->ReleaseEventID<freeBufEvt>(ret->freeBufEvtID);
                    ret->freeBufEvtID = INVALID_TEVENTID;
                }
            }
            break;
        }


          ;
    } while (true);
    this->bufUsedCount++;
# 485 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tquebind_impl.h"
    return reinterpret_cast<TBufHandle>(ret);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) void TQueBind<src, dst, depth, mask>::FreeAllEvent()
{
    auto ptr = this->bufStart;
    for (int i = 0; i < this->bufNum; i++, ptr++) {


                                                                                     ;
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlag<freeBufEvt>(ptr->freeBufEvtID);
            GetTPipePtr()->ReleaseEventID<freeBufEvt>(ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) void TQueBind<src, dst, depth, mask>::SetTBufPoolHandle(uint64_t bufPoolHandle)
{



    (void)(bufPoolHandle);

}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) int32_t TQueBind<src, dst, depth, mask>::GetTensorCountInQue()
{
    return usedCount;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) TBuffAddr TQueBind<src, dst, depth, mask>::GetBufferAddr(TBufHandle buf)
{
                                                                                                                       ;
    auto ptr = reinterpret_cast<TBufType*>(buf);



      ;

    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(GetPosition(src, dst));
    addr.bufferHandle = buf;
    addr.bufferAddr = ptr->address;
    addr.dataLen = ptr->dataLen;





    return addr;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) TBufState TQueBind<src, dst, depth, mask>::GetState(const LocalTensor<T>& tensor) const
{
    return GetState(tensor.GetBufferHandle());
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) TBufState TQueBind<src, dst, depth, mask>::GetState(const TBufHandle& handle) const
{
    if (handle == nullptr) {
        return TBufState::FREE;
    }
    auto ptr = reinterpret_cast<TBufType*>(handle);



      ;
    return ptr->state;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) bool TQueBind<src, dst, depth, mask>::VacantInQue()
{
    return usedCount < depth;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) bool TQueBind<src, dst, depth, mask>::HasTensorInQue()
{
    return usedCount > 0;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) bool TQueBind<src, dst, depth, mask>::HasIdleBuffer()
{
    return bufUsedCount < bufNum;
}
template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<T> TQueBind<src, dst, depth, mask>::Buf2Tensor(TBufHandle buf)
{
    TBuffAddr addr = GetBufferAddr(buf);
    LocalTensor<T> tensor;
    tensor.SetAddr(addr);
    return tensor;
}
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tquesync_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tquesync_impl.h"
namespace AscendC {
template <pipe_t src, pipe_t dst>
[aicore] __inline__ __attribute__((always_inline)) void TQueSync<src, dst>::SetFlag(TEventID id)
{
    static_assert((src != dst), "src/dst pipe cannot be same.");
    static_assert(IsSupportedPipe(src), "src pipe not supported");
    static_assert(IsSupportedPipe(dst), "dst pipe not supported");


      ;
    set_flag(src, dst, id);
}

template <pipe_t src, pipe_t dst>
[aicore] __inline__ __attribute__((always_inline)) void TQueSync<src, dst>::WaitFlag(TEventID id)
{
    static_assert((src != dst), "src/dst pipe cannot be same.");
    static_assert(IsSupportedPipe(src), "src pipe not supported");
    static_assert(IsSupportedPipe(dst), "dst pipe not supported");


      ;
    wait_flag(src, dst, id);
}
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tbufpool_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tbufpool_impl.h"
namespace AscendC {

template <TPosition pos, uint32_t bufIDSize>
[aicore] __inline__ __attribute__((always_inline)) TBufPool<pos, bufIDSize>::TBufPool()
{
    Init();
}

template <TPosition pos, uint32_t bufIDSize>
[aicore] __inline__ __attribute__((always_inline)) TBufPool<pos, bufIDSize>::~TBufPool()
{
    auto ptr = this->tBufPoolImpl.buf_;
    for (uint8_t i = 0; i < this->tBufPoolImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
    ResetPool();
};

template <TPosition pos, uint32_t bufIDSize>
[aicore] __inline__ __attribute__((always_inline)) void TBufPool<pos, bufIDSize>::ResetPool()
{
    tBufPoolImpl.curBufSize_ = 0;
    tBufPoolImpl.startAddr_ = 0;
    tBufPoolImpl.maxAddr_ = 0;
    tBufPoolImpl.maxLen_ = 0;
}

template <TPosition pos, uint32_t bufIDSize>
[aicore] __inline__ __attribute__((always_inline)) void TBufPool<pos, bufIDSize>::Init()
{
    constexpr auto pool = GetPhyType(pos);
    static_assert((pool == Hardware::L1 || pool == Hardware::UB),
        "TbufPool Position should be one of A1/B1/C1/VECIN/VECOUT/VECCALC");
    ResetPool();
    tBufPoolImpl.isReset_ = true;
}

template <TPosition pos, uint32_t bufIDSize>
[aicore] __inline__ __attribute__((always_inline)) void TBufPool<pos, bufIDSize>::Reset()
{
    auto ptr = this->tBufPoolImpl.buf_;
    for (uint8_t i = 0; i < this->tBufPoolImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
    ResetPool();
    tBufPoolImpl.isReset_ = true;



}

template <TPosition pos, uint32_t bufIDSize>
template <class T>
[aicore] __inline__ __attribute__((always_inline)) bool TBufPool<pos, bufIDSize>::InitBuffer(T &que, uint8_t num, uint32_t len)
{
    static_assert((T::isTQue), "TBufPool::InitBuffer(T& que, uint8_t num, uint32_t len) not supports T as TBuf");
                                                                                                                       ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    que.value = num;
    que.bufStart = this->tBufPoolImpl.buf_ + this->tBufPoolImpl.curBufSize_;
                                      ;





          ;
    auto curPoolAddr = this->tBufPoolImpl.maxAddr_;
    auto ptr = que.bufStart;
# 112 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tbufpool_impl.h"
    for (int32_t i = 0; i < num; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->freeBufEvt = T::freeBufEvt;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }
    this->tBufPoolImpl.maxAddr_ = curPoolAddr;
    this->tBufPoolImpl.curBufSize_ += num;


      ;
    return true;
}

template <TPosition pos, uint32_t bufIDSize>
template <TPosition bufPos>
[aicore] __inline__ __attribute__((always_inline)) bool TBufPool<pos, bufIDSize>::InitBuffer(TBuf<bufPos> &buf, uint32_t len)
{
                                                                                                                       ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr int32_t bufHandleSize = 1;
    buf.bufStart = this->tBufPoolImpl.buf_ + this->tBufPoolImpl.curBufSize_;
    buf.bufLen = len;
    buf.offset = 0;





          ;
    constexpr auto pool = GetPhyType(bufPos);

                                                                                        ;
    auto curPoolAddr = this->tBufPoolImpl.maxAddr_;
    auto ptr = buf.bufStart;
# 159 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tbufpool_impl.h"
    for (uint8_t i = 0; i < bufHandleSize; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }

                                                                                                                  ;
    this->tBufPoolImpl.maxAddr_ = curPoolAddr;
    this->tBufPoolImpl.curBufSize_ += bufHandleSize;





      ;
    return true;
}

template <TPosition pos, uint32_t bufIDSize>
template <class T>
[aicore] __inline__ __attribute__((always_inline)) bool TBufPool<pos, bufIDSize>::InitBufPool(T &bufPool, uint32_t len)
{
    static_assert(
        (T::isTbufPool), "TBufPool::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T as TbufPool");
                                                                                                                       ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr auto pool = GetPhyType(T::poolPos);
    bufPool.tBufPoolImpl.startAddr_ = this->tBufPoolImpl.maxAddr_;
    bufPool.tBufPoolImpl.maxAddr_ = bufPool.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxLen_ = len;





          ;
    auto curPoolAddr = this->tBufPoolImpl.maxAddr_;
# 213 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tbufpool_impl.h"
    curPoolAddr += len;

                                                                                                              ;
    this->tBufPoolImpl.maxAddr_ = curPoolAddr;
    return true;
}

template <TPosition pos, uint32_t bufIDSize>
template <class T, class U>
[aicore] __inline__ __attribute__((always_inline)) bool TBufPool<pos, bufIDSize>::InitBufPool(T &bufPool, uint32_t len, U &shareBuf)
{
    static_assert((T::isTbufPool && U::isTbufPool),
        "TBufPool::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T and U as TBufPool");
                                                                                                                       ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr auto pool = GetPhyType(T::poolPos);
    constexpr auto sharedPool = GetPhyType(U::poolPos);

                                                                                                            ;
    bufPool.tBufPoolImpl.startAddr_ = shareBuf.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxAddr_ = bufPool.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxLen_ = shareBuf.tBufPoolImpl.maxLen_;




      ;
# 253 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tbufpool_impl.h"
    return true;
}
}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tbuf_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tbuf_impl.h"
namespace AscendC {

template <TPosition pos>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<T> TBuf<pos>::Get(uint32_t len)
{
    uint32_t dataLen;
    if constexpr (IsSameType<T, int4b_t>::value) {
        dataLen = len / INT4_TWO;
    } else {
        dataLen = len * sizeof(T);
    }







    auto ptr = this->bufStart;
    ptr->dataLen = dataLen;
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = reinterpret_cast<TBufHandle>(ptr);
    addr.bufferAddr = ptr->address;
    addr.dataLen = ptr->dataLen;
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tbuf_impl.h"
    LocalTensor<T> tensor;
    tensor.SetAddr(addr);
    return tensor;
}

template <TPosition pos> template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<T> TBuf<pos>::Get()
{
    if constexpr (IsSameType<T, int4b_t>::value) {
        return Get<T>(bufLen * INT4_TWO);
    } else {
        return Get<T>(bufLen / sizeof(T));
    }
}

template <TPosition pos>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<T> TBuf<pos>::GetWithOffset(uint32_t size, uint32_t bufOffset)
{
    auto ptr = this->bufStart;
    ptr->dataLen = size * sizeof(T);
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = reinterpret_cast<TBufHandle>(ptr);
    addr.bufferAddr = ptr->address + bufOffset;
    addr.dataLen = ptr->dataLen;




    LocalTensor<T> tensor;
    tensor.SetAddr(addr);
    return tensor;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) void TBuf<pos>::SetTpipeBuf(TBufType* bufStartIn, uint32_t bufLenIn)
{
    this->bufStart = bufStartIn;
    this->bufLen = bufLenIn;
    this->offset = 0;
}

template <TPosition pos> template <typename T> [aicore] __inline__ __attribute__((always_inline)) void TBuf<pos>::EnQue(const LocalTensor<T>& tensor)
{
    (void)(0);
}

template <TPosition pos> template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> TBuf<pos>::DeQue()
{
    return Get<T>();
}

template <TPosition pos>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<T> TBuf<pos>::AllocTensor()
{
    return Get<T>();
}

template <TPosition pos> template <typename T> [aicore] __inline__ __attribute__((always_inline)) void TBuf<pos>::FreeTensor(LocalTensor<T>& tensor)
{
    (void)(0);
}

template <TPosition pos>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) TBufState TBuf<pos>::GetState(const LocalTensor<T>& tensor) const
{
    TBufHandle handle = tensor.GetBufferHandle();
    if (handle == nullptr) {
        return TBufState::FREE;
    }
    auto ptr = reinterpret_cast<TBufType*>(handle);
    return ptr->state;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) bool TBuf<pos>::EnQue(TBufHandle buf)
{
    return true;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBufHandle TBuf<pos>::DeQue()
{
    return Get();
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBufHandle TBuf<pos>::AllocBuffer()
{
    return Get();
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) void TBuf<pos>::FreeBuffer(TBufHandle buf)
{
    (void)(0);
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBuffAddr TBuf<pos>::GetBufferAddr(TBufHandle buf)
{
    auto ptr = reinterpret_cast<TBufType*>(buf);
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = buf;
    addr.bufferAddr = ptr->address;
    addr.dataLen = ptr->dataLen;




    return addr;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) void TBuf<pos>::InitStartBufHandle(
    TBufHandle startBufhandle, uint8_t num, uint32_t len)
{
    static_assert(!isTQue, "InitTBufAddr only support TBuf class");
                                                                                                                       ;
    auto ptr = reinterpret_cast<TBufType*>(startBufhandle);
    this->bufStart = ptr;
    this->bufLen = len;
    this->offset = 0;
    return;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBufHandle TBuf<pos>::Get(uint32_t len)
{



    this->bufStart->dataLen = len;
    return reinterpret_cast<TBufHandle>(this->bufStart);
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBufHandle TBuf<pos>::Get()
{
    return Get(bufLen);
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) uint32_t TBuf<pos>::GetBufLen() const
{
    return bufLen;
}

}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h" 2

namespace AscendC {
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
[aicore] __inline__ __attribute__((always_inline)) TPipe::TPipe()
{
    InitSocState();
    Init();



}

[aicore] __inline__ __attribute__((always_inline)) TPipe::~TPipe()
{
    if (g_tpipeImpl.isDestroy) {
        return;
    }
    Destroy();
};

[aicore] __inline__ __attribute__((always_inline)) void TPipe::Init()
{
    ResetPool();



    if constexpr(g_coreType == AscendC::AIC) {
        auto enQueEvtID = this->AllocEventID<HardEvent::M_MTE1>();
                                                                                                  ;
        SetFlag<HardEvent::M_MTE1>(static_cast<event_t>(enQueEvtID));
        enQueEvtID = this->AllocEventID<HardEvent::M_MTE1>();
                                                                                                  ;
        SetFlag<HardEvent::M_MTE1>(static_cast<event_t>(enQueEvtID));

        enQueEvtID = this->AllocEventID<HardEvent::M_MTE1>();
                                                                                                  ;

        SetFlag<HardEvent::M_MTE1>(static_cast<event_t>(enQueEvtID));
    }
# 121 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    g_vecTPipePtr = this;






    g_tpipeImpl.isDestroy = false;
}

template <class T> [aicore] __inline__ __attribute__((always_inline)) bool TPipe::InitBuffer(T& que, uint8_t num, uint32_t len)
{
    static_assert((T::isTQue), "TPipe::InitBuffer(T& que, uint8_t num, uint32_t len) not supports T as TBuf");


                                         ;

                                                                                                                    ;
                                                                        ;
    if constexpr (T::dstPosition == TPosition::TSCM) {
        return TscmInitBuffer(que, num, len);
    }
    Hardware pool = GetBufferPos(T::srcPosition, T::dstPosition);
# 154 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    que.value = num;
    que.bufStart = this->g_tpipeImpl.buf_ + this->g_tpipeImpl.curBufSize_;
                                      ;

                                                                                                               ;
                                                                                                                 ;
    auto curPoolAddr = this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
    auto ptr = que.bufStart;







    for (int32_t i = 0; i < num; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->freeBufEvt = T::freeBufEvt;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }

                                                                                                            ;
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr = curPoolAddr;
    this->g_tpipeImpl.curBufSize_ += num;



                     ;




                                                     ;



    return true;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) bool TPipe::InitBuffer(TBuf<pos>& buf, uint32_t len)
{
    constexpr auto pool = GetPhyType(pos);
# 213 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr int32_t bufHandleSize = 1;
    buf.bufStart = this->g_tpipeImpl.buf_ + this->g_tpipeImpl.curBufSize_;
    buf.bufLen = len;
    buf.offset = 0;

                                                                                                               ;

    auto curPoolAddr = g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
    auto ptr = buf.bufStart;






    for (uint8_t i = 0; i < bufHandleSize; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }


      ;
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr = curPoolAddr;
    this->g_tpipeImpl.curBufSize_ += bufHandleSize;



                     ;



    return true;
}

template <class T>
[aicore] __inline__ __attribute__((always_inline)) bool TPipe::InitBufPool(T &bufPool, uint32_t len)
{
    static_assert(
        (T::isTbufPool), "TPipe::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T as TbufPool");
    constexpr auto pool = GetPhyType(T::poolPos);
# 269 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    bufPool.tBufPoolImpl.startAddr_ = this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
    bufPool.tBufPoolImpl.maxAddr_ = bufPool.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxLen_ = len;
    auto curPoolAddr = this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
# 285 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    curPoolAddr += len;

                                                                                                              ;
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr = curPoolAddr;






          ;
    return true;
}

template <class T, class U>
[aicore] __inline__ __attribute__((always_inline)) bool TPipe::InitBufPool(T &bufPool, uint32_t len, U &shareBuf)
{
    static_assert((T::isTbufPool && U::isTbufPool),
        "TPipe::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T and U as TBufPool");
    constexpr auto pool = GetPhyType(T::poolPos);

                                                                                                     ;
# 317 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;

    bufPool.tBufPoolImpl.startAddr_ = shareBuf.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxAddr_ = bufPool.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxLen_ = shareBuf.tBufPoolImpl.maxLen_;




      ;
# 337 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    return true;
}

template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) TEventID TPipe::AllocEventID()
{

                                                                                                ;
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    auto lastId = sff0(ptr->eventOccupy);



      ;
    ptr->eventOccupy = sbitset1(ptr->eventOccupy, lastId);
    return lastId;
}

template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) void TPipe::ReleaseEventID(TEventID id)
{



      ;
                                                                                                          ;
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    ptr->eventOccupy = sbitset0(ptr->eventOccupy, id);
    return;
}

[aicore] __inline__ __attribute__((always_inline)) TEventID TPipe::FetchEventID(HardEvent evt)
{
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    auto lastId = sff0(ptr->eventOccupy);



      ;
    return lastId;
}

template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) TEventID TPipe::FetchEventID()
{
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    auto lastId = sff0(ptr->eventOccupy);



      ;
    return lastId;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) uint64_t TPipe::GetQueueEndAddress()
{
    Hardware hardType = GetPhyType(pos);
                                                                                                      ;
    return this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(hardType)].maxAddr;
}

[aicore] __inline__ __attribute__((always_inline)) void TPipe::Destroy()
{
    g_tpipeImpl.isDestroy = true;
    auto ptr = this->g_tpipeImpl.buf_;
    for (uint8_t i = 0; i < this->g_tpipeImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }


    if constexpr(g_coreType == AscendC::AIC) {
        WaitFlag<HardEvent::M_MTE1>(0);
        ReleaseEventID<HardEvent::M_MTE1>(0);
        WaitFlag<HardEvent::M_MTE1>(1);
        ReleaseEventID<HardEvent::M_MTE1>(1);

        WaitFlag<HardEvent::M_MTE1>(2);
        ReleaseEventID<HardEvent::M_MTE1>(2);
    }
# 425 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    pipe_barrier(PIPE_ALL);




}

[aicore] __inline__ __attribute__((always_inline)) void TPipe::Reset()
{
    auto ptr = this->g_tpipeImpl.buf_;
    for (uint8_t i = 0; i < this->g_tpipeImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
    InitSocState();
    ResetPool();





}

[aicore] __inline__ __attribute__((always_inline)) void InitShareBufStart(TPipe* tpipe, uint32_t mode, uint32_t* shareLens,
    uint32_t lens, uint8_t subBlockIdx)
{






    (void)(lens);



                                                                                                             ;
    tpipe->AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L1),
        Hardware::L1, subBlockIdx);
    tpipe->AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L0C),
        Hardware::L0C, subBlockIdx);




    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0A)].maxAddr = 0;
    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0B)].maxAddr = 0;

    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::BIAS)].maxAddr = 0;

    return;
}

[aicore] __inline__ __attribute__((always_inline)) void InitShareBufEnd(TPipe* tpipe)
{

    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L1)].maxAddr =
        tpipe->g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L1)];
    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0C)].maxAddr =
        tpipe->g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L0C)];





    return;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TPipe::InitSpmBuffer(const GlobalTensor<T>& workspace, const int32_t bufferSize)
{
    g_tpipeImpl.spmInfo_.spmBuffSize = bufferSize;
    g_tpipeImpl.spmInfo_.spmAddr = reinterpret_cast<uint64_t>(workspace.GetPhyAddr());
    g_tpipeImpl.spmInfo_.spmBufType = static_cast<uint8_t>(Hardware::GM);
}

[aicore] __inline__ __attribute__((always_inline)) void TPipe::InitSpmBuffer(const int32_t bufferSize)
{

    (void)(bufferSize);

                                                                                    ;
# 521 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TPipe::WriteSpmBuffer(const LocalTensor<T>& writeLocal, const DataCopyParams& copyParams,
    int32_t writeOffset)
{




    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
    SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        DataCopyUB2GMImpl(reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(writeLocal.GetPhyAddr()), copyParams);
        event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                        ;
        DataCopyUB2L1Impl(reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(writeLocal.GetPhyAddr()), copyParams);
        event_t eventIDMTE3ToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE1));
        SetFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
        WaitFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
    }
    event_t eventIDMTE3ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_V));
    SetFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
    WaitFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TPipe::ReadSpmBuffer(const LocalTensor<T>& readLocal, const DataCopyParams& copyParams,
    int32_t readOffset)
{




    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        event_t eventIDVToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE2));
        event_t eventIDMTE2ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_V));
        event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
        SetFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        WaitFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        DataCopyGM2UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(readLocal.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, copyParams);
        SetFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);
        WaitFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);

        SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
        WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                       ;
        event_t eventIDVToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE1));
        event_t eventIDMTE1ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_V));
        event_t eventIDMTE1ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_MTE3));
        SetFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        WaitFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        DataCopyL12UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(readLocal.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, copyParams);

        SetFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);
        WaitFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);

        SetFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
        WaitFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TPipe::WriteSpmBuffer(const LocalTensor<T>& writeLocal, const int32_t writeSize,
    int32_t writeOffset)
{




    int computeSize = writeSize != 0 ? writeSize : GetShapeSize(writeLocal.GetShapeInfo());
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = computeSize / AscendCUtils::GetC0Count(sizeof(T));
    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
    event_t eventIDMTE3ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_V));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);

    SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        DataCopyUB2GMImpl(reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(writeLocal.GetPhyAddr()), repeatParams);
        event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                        ;

                      ;
        DataCopyUB2L1Impl(reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(writeLocal.GetPhyAddr()), repeatParams);
        event_t eventIDMTE3ToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE1));
        SetFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
        WaitFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
    }

    SetFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
    WaitFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TPipe::ReadSpmBuffer(const LocalTensor<T>& readLocal, const int32_t readSize, int32_t readOffset)
{




    int computeSize = readSize != 0 ? readSize : GetShapeSize(readLocal.GetShapeInfo());
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = computeSize / AscendCUtils::GetC0Count(sizeof(T));
    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        event_t eventIDVToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE2));
        event_t eventIDMTE2ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_V));
        event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
        SetFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        WaitFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        DataCopyGM2UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(readLocal.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, repeatParams);

        SetFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);
        WaitFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);

        SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
        WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                       ;
                                                                                                                      ;
        event_t eventIDVToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE1));
        event_t eventIDMTE1ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_V));
        event_t eventIDMTE1ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_MTE3));
        SetFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        WaitFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        DataCopyL12UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(readLocal.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, repeatParams);

        SetFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);
        WaitFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);

        SetFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
        WaitFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
    }
}

template <TPosition pos>
[aicore] __inline__ __attribute__((always_inline)) TBuffAddr TPipe::GetAbsAddr(int32_t offset, int32_t len) const
{
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = nullptr;
    addr.bufferAddr = offset;
    addr.dataLen = len;
# 700 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    return addr;
}

template <TPosition pos, typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_alias)) LocalTensor<T> TPipe::GetAbsAddr(int32_t offset, int32_t size) const
{
    TBuffAddr addr = GetAbsAddr<pos>(offset, static_cast<int32_t>((size * sizeof(T))));
    LocalTensor<T> tensor;
    tensor.SetAddr(addr);
    return tensor;
}
# 816 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
[aicore] __inline__ __attribute__((always_inline)) void TPipe::InitSocState() const
{
    set_atomic_none();

    if constexpr(g_coreType == AscendC::AIC) {
        set_mask_norm();
        set_l1_3d_size(static_cast<uint64_t>(0));
        set_padding(static_cast<uint64_t>(0));
    } else {
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
        set_mask_norm();
    }



}

[aicore] __inline__ __attribute__((always_inline)) void TPipe::ResetPool()
{
    g_tpipeImpl.tscmBufferPtr_ = TOTAL_L1_SIZE;
    g_tpipeImpl.curBufSize_ = 0;
    auto buf = g_tpipeImpl.bufPool_;
    for (int32_t i = 0; i < static_cast<int32_t>(Hardware::MAX); i++, buf++) {
        buf->maxAddr = 0;
    }
    auto evt = g_tpipeImpl.eventPool_;
    for (int32_t i = 0; i < EVENT_NUM; i++, evt++) {
        evt->eventOccupy = 0;
    }
    g_tpipeImpl.shareBufPool_.start[static_cast<uint8_t>(TShareBuf::ShareHard::L1)] = -1;
    g_tpipeImpl.shareBufPool_.start[static_cast<uint8_t>(TShareBuf::ShareHard::UB)] = -1;
    g_tpipeImpl.shareBufPool_.start[static_cast<uint8_t>(TShareBuf::ShareHard::L0C)] = -1;
}

template <class T> [aicore] __inline__ __attribute__((always_inline)) bool TPipe::TscmInitBuffer(T& que, uint8_t num, uint32_t len)
{
                                                                                                                       ;



      ;

    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    que.value = num;
    que.bufStart = this->g_tpipeImpl.buf_ + this->g_tpipeImpl.curBufSize_;
                                      ;

    constexpr Hardware pool = Hardware::L1;






    uint32_t curPoolAddr;
    if constexpr (T::scmBlockGroup) {
        curPoolAddr = g_tpipeImpl.tscmBufferPtr_ - num * len;
        g_tpipeImpl.tscmBufferPtr_ -= num * len;
    } else {
        curPoolAddr = g_tpipeImpl.tscmBufferPtr_ - (GetTaskRationImpl() - GetSubBlockIdxImpl()) * len * num;
        g_tpipeImpl.tscmBufferPtr_ -= GetTaskRationImpl() * num * len;
    }

    auto ptr = que.bufStart;
    for (int32_t i = 0; i < num; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->freeBufEvt = T::freeBufEvt;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }





          ;
    this->g_tpipeImpl.curBufSize_ += num;



                      ;
    return true;
}

template <TPosition pos>
[aicore] __inline__ __attribute__((always_inline)) uint64_t TransUBAddr(uint64_t addr)
{




    return addr;
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_type.h" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_prof_trace_intf.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_prof_trace_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_prof_trace.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_prof_trace.h"
namespace AscendC {
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_prof_trace.h"
[aicore] __inline__ __attribute__((always_inline)) void ProfStartImpl()
{


    bisheng::cce::metrics_prof_start();




}

[aicore] __inline__ __attribute__((always_inline)) void ProfStopImpl()
{


    bisheng::cce::metrics_prof_stop();




}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_prof_trace_intf.h" 2

namespace AscendC {
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_prof_trace_intf.h"
[aicore] __inline__ __attribute__((always_inline)) void MetricsProfStart();

[aicore] __inline__ __attribute__((always_inline)) void MetricsProfStop();
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_data_copy_base_impl.h" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_data_copy_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_scm_data_copy_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_scm_data_copy_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_client.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_client.h"
namespace AscendC {
class KfcCommClient {
public:

    __attribute__((cce_global)) KfcMsg *msgSendHead;
    __attribute__((cce_global)) KfcMsg *msgSendStart;


    __attribute__((cce_global)) KfcMsg *msgRcvHead;
    __attribute__((cce_global)) KfcMsg *msgRcvStart;

    __attribute__((cce_global)) uint8_t* ubStart;
    __attribute__((cce_global)) uint8_t* ubAvalidTail;

    __attribute__((cce_unif_buff)) KfcMsg *ubMsg;
    uint32_t head;
    uint32_t tail;
    uint8_t msgRcvPos;
    uint8_t msgSendPos;
    uint8_t eventID_;
    uint8_t enableHardWare;

public:
    [aicore] __inline__ __attribute__((always_inline)) KfcCommClient(__attribute__((cce_global)) uint8_t* workspace, int subBlockID, uint8_t enableHardWare = 0)
    {
        if constexpr(g_coreType == AscendC::AIV) {
            this->enableHardWare = enableHardWare;
            if (enableHardWare) {
                return;
            }
                                                                                                                 ;
                                                                                                                     ;

                                                                                               ;

            this->msgSendStart = (__attribute__((cce_global)) KfcMsg *)GetMsgHead(workspace, subBlockID);
            this->msgRcvStart = this->msgSendStart + 64;

            this->msgSendHead = this->msgSendStart;
            this->msgSendPos = 0;
            this->msgRcvHead = this->msgRcvStart;
            this->msgRcvPos = 0;






            ubMsg = reinterpret_cast<__attribute__((cce_unif_buff)) KfcMsg *>(TOTAL_UB_SIZE - sizeof(KfcMsg));

            eventID_ = GetTPipePtr()->AllocEventID<HardEvent::MTE3_S>();
            SetFlag<HardEvent::MTE3_S>((event_t)eventID_);

            ubStart = GetUBMapAddr(workspace, subBlockID);
            ubAvalidTail = GetUBAvaliedAddr(workspace, subBlockID);
            head = 0;
            tail = 0;
        }
    }

    [aicore] __inline__ __attribute__((always_inline)) ~KfcCommClient()
    {
        if constexpr(g_coreType == AscendC::AIV) {
            if (this->enableHardWare) {
                return;
            }
            if constexpr (MIX_NUM == 1) {

                if (GetSubBlockIdxImpl() == 1) {
                    return;
                }
            }
            __attribute__((cce_global)) KfcMsg *msg = AllocMessage();

                                                                                                                     ;
            uint32_t quitSignal = KfcMsgMakeFlag(KFC_Enum::SERVICE_QUIT, 0);
            *((__attribute__((cce_global)) uint32_t *)msg) = quitSignal;
# 113 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_client.h"
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        }
    }

    template <bool isAck>
    [aicore] __inline__ __attribute__((always_inline)) void PostMessage(__attribute__((cce_global)) KfcMsg *msg)
    {
        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventID);
        WaitFlag<HardEvent::S_MTE3>(eventID);
        PipeBarrier<PIPE_MTE3>();
        copy_ubuf_to_gm((__attribute__((cce_global)) void *)msg, (__attribute__((cce_unif_buff)) void *)ubMsg, 0, 1, sizeof(KfcMsg) / ONE_BLK_SIZE, 0, 0);
# 135 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_client.h"
        SetFlag<HardEvent::MTE3_S>((event_t)this->eventID_);
    }

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg *AllocMessage()
    {
        auto ret = AllocMessageImpl(this->msgSendHead, this->msgSendPos, this->msgSendStart);
        WaitFlag<HardEvent::MTE3_S>((event_t)this->eventID_);

                                                                                     ;
        return ret;
    }

    [aicore] __inline__ __attribute__((always_inline)) void FreeMessage(__attribute__((cce_global)) KfcMsg *msg)
    {
        FreeMessageImpl(msg);
    }

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* AllocUB(uint32_t size, int32_t &tailInfo)
    {
# 164 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_client.h"
        __attribute__((cce_global)) uint8_t* ret;
        if (head + size >= WORKSPACE_UB_SIZE) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(ubAvalidTail), cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
            tail = *(reinterpret_cast<__attribute__((cce_global)) uint32_t *>(ubAvalidTail));
            while (head < tail || tail == 0) {
                Barrier();
                dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(ubAvalidTail), cache_line_t::SINGLE_CACHE_LINE,
                    dcci_dst_t::CACHELINE_OUT);
                Barrier();
                tail = *(reinterpret_cast<__attribute__((cce_global)) uint32_t *>(ubAvalidTail));
            }
            if (tail == head && size == tail) {
                tail = 0;
            }
            head = 0;
        }

        while (head < tail && (head + size >= tail)) {
            Barrier();
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(ubAvalidTail), cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
            Barrier();
            tail = *(reinterpret_cast<__attribute__((cce_global)) uint32_t *>(ubAvalidTail));
        }





        ret = ubStart + head;
        head += size;
        tailInfo = head;
        return ret;
    }

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg *RcvMessage()
    {
        auto ret = RcvMessageImpl(this->msgRcvHead, this->msgRcvPos, this->msgRcvStart);
        return ret;
    }
};





[[block_local]] __inline__ AscendC::KfcCommClient* g_kfcClient;





[aicore] __inline__ __attribute__((always_inline)) AscendC::KfcCommClient* GetKfcClient()
{


    return reinterpret_cast<AscendC::KfcCommClient*>(g_kfcClient);







}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_scm_data_copy_impl.h" 2


namespace AscendC {
struct Gm2L1Params {
    __attribute__((cce_cube_buff)) void* dst = nullptr;
    __attribute__((cce_global)) void* src = nullptr;
    DataCopyParams intri;
};
struct Gm2L1Nd2NzParams {
    __attribute__((cce_cube_buff)) void* dst = nullptr;
    __attribute__((cce_global)) void* src = nullptr;
    uint8_t dataTypeLen = 2;
    Nd2NzParams intri;
};




[aicore] __inline__ __attribute__((always_inline)) void ScmDataCopyMsg(__attribute__((cce_cube_buff)) void* dst, __attribute__((cce_global)) void* src, const DataCopyParams& intriParams,
    int32_t ubAddr)
{
                             ;
                                     ;
    auto msg = GetKfcClient()->AllocMessage();
                                                             ;

    __attribute__((cce_unif_buff)) struct Gm2L1Params* p = (__attribute__((cce_unif_buff)) struct Gm2L1Params*)&(GetKfcClient()->ubMsg->buffer);
    p->dst = dst;
    p->src = src;
    p->intri.blockCount = intriParams.blockCount;
    p->intri.blockLen = intriParams.blockLen;
    p->intri.srcStride = intriParams.srcStride;
    p->intri.dstStride = intriParams.dstStride;
    GetKfcClient()->ubMsg->ubAddr = ubAddr;
    GetKfcClient()->ubMsg->head = KfcMsgMakeFlag(KFC_Enum::SCMFUN_GM2L1, 0);
    GetKfcClient()->PostMessage<false>(msg);
}

[aicore] __inline__ __attribute__((always_inline)) void ScmDataCopyND2NZMsg(__attribute__((cce_cube_buff)) void* dst, __attribute__((cce_global)) void* src, const uint8_t dataTypeSize,
    const Nd2NzParams& intriParams, int32_t ubAddr)
{
                             ;
                          ;
                          ;
                                     ;
    auto msg = GetKfcClient()->AllocMessage();
                                                                  ;

    auto p = (__attribute__((cce_unif_buff)) struct Gm2L1Nd2NzParams*)&(GetKfcClient()->ubMsg->buffer);
    p->dst = dst;
    p->src = src;
    p->dataTypeLen = dataTypeSize;
    p->intri.ndNum = intriParams.ndNum;
    p->intri.nValue = intriParams.nValue;
    p->intri.dValue = intriParams.dValue;
    p->intri.srcNdMatrixStride = intriParams.srcNdMatrixStride;
    p->intri.dstNzC0Stride = intriParams.dstNzC0Stride;
    p->intri.dstNzNStride = intriParams.dstNzNStride;
    p->intri.dstNzMatrixStride = intriParams.dstNzMatrixStride;
    p->intri.srcDValue = intriParams.srcDValue;
    GetKfcClient()->ubMsg->ubAddr = ubAddr;
    GetKfcClient()->ubMsg->head = KfcMsgMakeFlag(KFC_Enum::SCMFUN_GM2L1ND2NZ, 0);
    GetKfcClient()->PostMessage<false>(msg);
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h" 2


namespace AscendC {


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CheckDataCopyPadParams(uint16_t blockCount, uint32_t blockLen, bool isGMtoUB)
{
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
}


[aicore] __inline__ __attribute__((always_inline)) void CheckDataCopyParams(uint16_t blockCount, uint16_t blockLen)
{
                                                                                  ;
                                                                              ;
}

[aicore] __inline__ __attribute__((always_inline)) void ValidateUbL1Address(uint64_t absUbAddr, uint64_t absL1Addr, uint32_t tensorSize)
{


      ;



      ;


      ;



      ;
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);

                                                   ;
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, false, intriParams);
        }
        copy_gm_to_ubuf((__attribute__((cce_unif_buff)) void*)dst, (__attribute__((cce_global)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
                                                                                                                   ;
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, false, intriParams);
        }
        copy_gm_to_cbuf((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)src, (int8_t)0, (uint16_t)intriParams.blockCount,
            (uint16_t)intriParams.blockLen, (uint16_t)intriParams.srcStride, (uint16_t)intriParams.dstStride, (pad_t)0);
    } else if constexpr(g_coreType == AscendC::AIV) {
# 110 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
        ScmDataCopyMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)src, intriParams, -1);


    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                     ;
                                                                                                     ;
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);

                                                   ;

        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, false, intriParams);
        }
        copy_ubuf_to_gm((__attribute__((cce_global)) void*)dst, (__attribute__((cce_unif_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);

                                                   ;

                                                   ;
        copy_ubuf_to_ubuf((__attribute__((cce_unif_buff)) void*)dst, (__attribute__((cce_unif_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                                       ;

                                                   ;

        uint32_t tensorSize = intriParams.blockCount * intriParams.blockLen * 32;
        int32_t ubAddr = -1;







        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);

        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
        copy_ubuf_to_gm((__attribute__((cce_global)) void*)gmAddr, (__attribute__((cce_unif_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.srcStride);
# 190 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
        ScmDataCopyMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, intriParams, ubAddr);

    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2L1ND2NZImpl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const Nd2NzParams& intriParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    if constexpr(g_coreType == AscendC::AIV) {
                                         ;

                                                   ;
        uint32_t tensorSize = intriParams.nValue * intriParams.dValue;
        int32_t ubAddr = -1;
# 219 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);

        copy_ubuf_to_gm((__attribute__((cce_global)) void*)gmAddr, (__attribute__((cce_unif_buff)) void*)src, 0, 1, tensorSize * sizeof(T) / 32, 0, 0);





        ScmDataCopyND2NZMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, sizeof(T), intriParams, ubAddr);

    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams& intriParams)
{
                                                                                ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12BTImpl(const uint64_t dst, __attribute__((cce_cube_buff)) T* src, const uint16_t isenableConv,
    const DataCopyParams &intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
        copy_cbuf_to_bt(dst, (__attribute__((cce_cube_buff)) void*)src, isenableConv, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12FBImpl(__attribute__((cce_fixpipe_buff)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams &intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
        copy_cbuf_to_fbuf((__attribute__((cce_fixpipe_buff)) void*)dst, (__attribute__((cce_cube_buff)) void*)src, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2L1ND2NZImplBase(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNd2Nz(src, workSpace, true, intriParams);
        }
        if constexpr (sizeof(T) == B8_BYTE_SIZE) {
            copy_gm_to_cbuf_multi_nd2nz_b8((__attribute__((cce_cube_buff)) T*)dst, (__attribute__((cce_global)) T*)src, 0, intriParams.ndNum, intriParams.nValue,
                intriParams.dValue, intriParams.srcNdMatrixStride, intriParams.srcDValue, intriParams.dstNzC0Stride,
                intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
        } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
            copy_gm_to_cbuf_multi_nd2nz_b16((__attribute__((cce_cube_buff)) T*)dst, (__attribute__((cce_global)) T*)src, 0, intriParams.ndNum, intriParams.nValue,
                intriParams.dValue, intriParams.srcNdMatrixStride, intriParams.srcDValue, intriParams.dstNzC0Stride,
                intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
        } else if constexpr (sizeof(T) == B32_BYTE_SIZE) {
            copy_gm_to_cbuf_multi_nd2nz_b32s((__attribute__((cce_cube_buff)) T*)dst, (__attribute__((cce_global)) T*)src, 0, intriParams.ndNum, intriParams.nValue,
                intriParams.dValue, intriParams.srcNdMatrixStride, intriParams.srcDValue, intriParams.dstNzC0Stride,
                intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
        }
    } else if constexpr(g_coreType == AscendC::AIV) {





        ScmDataCopyND2NZMsg(dst, src, sizeof(T), intriParams, -1);

    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2L1ND2NZImpl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams)
{

                                                       ;
    if constexpr (SupportType<T, int4b_t>()) {
        DataCopyGM2L1ND2NZImplBase((__attribute__((cce_cube_buff)) int8_t *)dst, (__attribute__((cce_global)) int8_t *)src, intriParams);
    } else if (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE || sizeof(T) == B32_BYTE_SIZE){
        DataCopyGM2L1ND2NZImplBase(dst, src, intriParams);
    } else {


                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
                                                                                                                   ;
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, false, intriParams);
        }
        copy_cbuf_to_gm((__attribute__((cce_global)) void*)dst, (__attribute__((cce_cube_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CopyIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const CopyRepeatParams& repeatParams)
{
                                                                                                 ;
                                                                                                 ;
    if constexpr(sizeof(T) == B16_BYTE_SIZE) {
        vcopy((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src, repeatTimes, repeatParams.dstStride,
            repeatParams.srcStride, repeatParams.dstRepeatSize, repeatParams.srcRepeatSize);
    } else if constexpr(sizeof(T) == B32_BYTE_SIZE) {
        vcopy((__attribute__((cce_unif_buff)) uint32_t*)dst, (__attribute__((cce_unif_buff)) uint32_t*)src, repeatTimes, repeatParams.dstStride,
            repeatParams.srcStride, repeatParams.dstRepeatSize, repeatParams.srcRepeatSize);
    } else {

                                                                                                                      ;
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CopyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const uint64_t mask[], uint8_t repeatTimes,
    const CopyRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        CopyIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CopyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const CopyRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        CopyIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}



template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12L0CImpl(__attribute__((cce_cube_c)) DstT* dst, __attribute__((cce_cube_buff)) SrcT* src, const DataCopyParams& intriParams,
    const DataCopyEnhancedParams& enhancedParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        static_assert((SupportType<Tuple<SrcT, DstT>, Tuple<__cce_half, __cce_half>, Tuple<float, __cce_half>, Tuple<float, bfloat16_t>,
            Tuple<float, float>, Tuple<bfloat16_t, bfloat16_t>, Tuple<int32_t, int32_t>, Tuple<uint32_t, uint32_t>,
            Tuple<uint32_t, uint32_t>>()), "Failed to check dtype in DataCopy from A1 / B1 to CO1, current api support "
            "dtype combination is src: half, dst: half; src: float, dst: half / bfloat16_t / float; src: bfloat16_t, "
            "dst: bfloat16_t; src: int32_t, dst: int32_t; src: uint32_t, dst: uint32_t.");
                                                                                                                    ;
                                                                                                             ;
        copy_matrix_cbuf_to_cc((__attribute__((cce_cube_c)) DstT*)dst, (__attribute__((cce_cube_buff)) SrcT*)src, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}




template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL0C2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_c)) U* src, const DataCopyParams& intriParams,
    const DataCopyEnhancedParams& enhancedParams)
{
                                                                 ;
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2L0CImpl(__attribute__((cce_cube_c)) T* dst, __attribute__((cce_unif_buff)) U* src, const DataCopyParams& intriParams,
    const DataCopyEnhancedParams& enhancedParams)
{
                                                                 ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopySliceGm2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParamsIn)
{

                                               ;
    DataCopyPadExtParams<T> padParams{ false, 0, 0, 0 };
    uint16_t burstLen = intriParamsIn.blockLen * ONE_BLK_SIZE;
    DataCopyExtParams intriParams{ intriParamsIn.blockCount, burstLen, intriParamsIn.srcStride, intriParamsIn.dstStride,
        0 };
    DataCopyPadGm2UBImpl(dst, src, intriParams, padParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadGm2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams,
    const DataCopyPadParams& padParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                      ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, static_cast<uint32_t>(intriParams.blockLen), true);
    if (padParams.isPad) {
        set_mov_pad_val(padParams.paddingValue);
    }
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B64_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen,
            (padParams.leftPadding << 1), (padParams.rightPadding << 1), intriParams.srcStride, intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadGm2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyExtParams& intriParams,
    const DataCopyPadExtParams<T>& padParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                      ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, intriParams.blockLen, true);
    if (padParams.isPad) {
        set_mov_pad_val(GetScalarBitcodeValue(static_cast<T>(padParams.paddingValue)));
    }
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B64_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen,
            (padParams.leftPadding << 1), (padParams.rightPadding << 1), intriParams.srcStride, intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopySliceUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParamsIn)
{

                                               ;
    uint32_t burstLen = intriParamsIn.blockLen * ONE_BLK_SIZE;
    DataCopyExtParams intriParams{ intriParamsIn.blockCount, burstLen, intriParamsIn.srcStride, intriParamsIn.dstStride,
        0 };
    DataCopyPadUB2GMImpl(dst, src, intriParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                      ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, static_cast<uint32_t>(intriParams.blockLen), false);
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            (uint32_t)intriParams.srcStride, (uint32_t)intriParams.dstStride);
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            (uint32_t)intriParams.srcStride, (uint32_t)intriParams.dstStride);
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            (uint32_t)intriParams.srcStride, (uint32_t)intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyExtParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                      ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, intriParams.blockLen, false);
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            (uint32_t)intriParams.srcStride, (uint32_t)intriParams.dstStride);
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            (uint32_t)intriParams.srcStride, (uint32_t)intriParams.dstStride);
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            intriParams.srcStride, intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadUB2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams,
    const Nd2NzParams& nd2nzParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, static_cast<uint32_t>(intriParams.blockLen), false);
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                                       ;


                         ;
        uint32_t tensorSize = nd2nzParams.nValue * nd2nzParams.dValue;
        int32_t ubAddr = -1;
# 591 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);


                                                                                            ;
                                                                                                                       ;
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)src, intriParams);




        ScmDataCopyND2NZMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, sizeof(T), nd2nzParams, ubAddr);

    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadUB2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyExtParams& intriParams,
    const Nd2NzParams& nd2nzParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, intriParams.blockLen, false);
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                                       ;


                         ;
        uint32_t tensorSize = nd2nzParams.nValue * nd2nzParams.dValue;
        int32_t ubAddr = -1;
# 638 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);


                                                                                            ;
                                                                                                                       ;
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)src, intriParams);




        ScmDataCopyND2NZMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, sizeof(T), nd2nzParams, ubAddr);

    }
}



[aicore] __inline__ __attribute__((always_inline)) void ScmDataCopy(__attribute__((cce_global)) void* kfcMsgPtr)
{
                                                                                               ;
    auto scmCopyParams = reinterpret_cast<__attribute__((cce_global)) struct Gm2L1Params*>(kfcMsgPtr);



      ;
    auto dst = reinterpret_cast<__attribute__((cce_cube_buff)) void*>(scmCopyParams->dst);
    auto& intriParams = scmCopyParams->intri;







    copy_gm_to_cbuf((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)scmCopyParams->src, (int8_t)0, (uint16_t)intriParams.blockCount,
        (uint16_t)intriParams.blockLen, (uint16_t)intriParams.srcStride, (uint16_t)intriParams.dstStride, (pad_t)0);

    event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE1));
    SetFlag<HardEvent::MTE2_MTE1>(eventID);
    WaitFlag<HardEvent::MTE2_MTE1>(eventID);
}

[aicore] __inline__ __attribute__((always_inline)) void ScmDataCopyND2NZ(__attribute__((cce_global)) void* kfcMsgPtr)
{
                                                                                               ;
    auto scmCopyParams = reinterpret_cast<__attribute__((cce_global)) struct Gm2L1Nd2NzParams*>(kfcMsgPtr);
    auto& intriParams = scmCopyParams->intri;
    auto l1AddrDst = reinterpret_cast<__attribute__((cce_cube_buff)) void*>(scmCopyParams->dst);
# 700 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
    if (scmCopyParams->dataTypeLen == 2) {
        copy_gm_to_cbuf_multi_nd2nz_b16((__attribute__((cce_cube_buff)) __cce_half*)l1AddrDst, (__attribute__((cce_global)) __cce_half*)scmCopyParams->src, 0,
            intriParams.ndNum, intriParams.nValue, intriParams.dValue, intriParams.srcNdMatrixStride,
            intriParams.srcDValue, intriParams.dstNzC0Stride, intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
    } else if (scmCopyParams->dataTypeLen == 4) {
        copy_gm_to_cbuf_multi_nd2nz_b32s((__attribute__((cce_cube_buff)) float*)l1AddrDst, (__attribute__((cce_global)) float*)scmCopyParams->src, 0,
            intriParams.ndNum, intriParams.nValue, intriParams.dValue, intriParams.srcNdMatrixStride,
            intriParams.srcDValue, intriParams.dstNzC0Stride, intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
    } else {



          ;
        copy_gm_to_cbuf_multi_nd2nz_b8((__attribute__((cce_cube_buff)) int8_t*)l1AddrDst, (__attribute__((cce_global)) int8_t*)scmCopyParams->src, 0,
            intriParams.ndNum, intriParams.nValue, intriParams.dValue, intriParams.srcNdMatrixStride,
            intriParams.srcDValue, intriParams.dstNzC0Stride, intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
    }
    event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE1));
    SetFlag<HardEvent::MTE2_MTE1>(eventID);
    WaitFlag<HardEvent::MTE2_MTE1>(eventID);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2UBSingleImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams,
    const int copyTime, const int computeNum)
{

                                               ;
    const uint16_t &nValue = intriParams.nValue;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& computeLen = computeNum * sizeof(T);
    const uint16_t& c0Count = DEFAULT_C0_SIZE / sizeof(T);
    const uint16_t& maxC0Count = MAX_REPEAT_TIMES * c0Count;
    const uint16_t& maxdValue = MAX_REPEAT_TIMES * dValue;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& repeatCount = nValue / MAX_REPEAT_TIMES;
    const uint16_t& repeatTail = nValue % MAX_REPEAT_TIMES;
    const uint16_t& srcCopyStartOffset = copyTime * c0Count;
    const uint16_t& dstCopyStartOffset = copyTime * dstNzC0Stride * (DEFAULT_C0_SIZE / sizeof(T));
    DataCopyExtParams copyParams = { MAX_REPEAT_TIMES, (uint32_t)computeLen,
        (uint32_t)(intriParams.srcDValue * sizeof(T) - computeLen),
        (uint32_t)((dstNzNStride - (uint16_t)DEFAULT_C0_SIZE) / (uint16_t)DEFAULT_C0_SIZE), 0 };
    DataCopyPadExtParams<T> padParams;
    for (int repeatTime = 0; repeatTime < repeatCount; ++repeatTime) {
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) T*)(dst + dstCopyStartOffset + repeatTime * maxC0Count),
            (__attribute__((cce_global)) T*)(src + srcCopyStartOffset + repeatTime * maxdValue), copyParams, padParams);
    }
    copyParams.blockCount = repeatTail;
    if (repeatTail != 0) {
        int dstOffset = (dstCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * c0Count);
        int srcOffset = (srcCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * dValue);
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) T*)(dst + dstOffset), (__attribute__((cce_global)) T*)(src + srcOffset), copyParams, padParams);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2UBND2NZImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams)
{
    if constexpr(g_coreType != AscendC::AIV) {
        return;
    }

                                               ;
    const uint16_t &ndNum = intriParams.ndNum;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& srcNdMatrixStride = intriParams.srcNdMatrixStride;
    const uint16_t& srcDValue = intriParams.srcDValue;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzMatrixStride = intriParams.dstNzMatrixStride;
    const uint16_t& c0Count = DEFAULT_C0_SIZE / sizeof(T);
    for (int index = 0; index < ndNum; ++index) {
        int16_t copyNum = (dValue + c0Count - 1) / c0Count;
        for (int copyTime = 0; copyTime < copyNum; ++copyTime) {
            int computeCount = (dValue >= (copyTime + 1) * c0Count) ? c0Count : (dValue % c0Count);
            DataCopyGM2UBSingleImpl(dst + dstNzMatrixStride, src + srcNdMatrixStride, intriParams, copyTime,
                computeCount);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2GMNZ2NDImplBase(__attribute__((cce_global)) T* dstAddr, __attribute__((cce_unif_buff)) T* srcAddr, uint16_t height,
    uint16_t width, uint16_t srcNStride, uint16_t dstDStride)
{

                                               ;
    const uint16_t BLK_CNT_LIMIT = UINT12_MAX;
    const uint16_t repeatTimes = height / BLK_CNT_LIMIT;
    const uint16_t tailBlock = height % BLK_CNT_LIMIT;
    const uint16_t widthBlkNum = (width + BLOCK_CUBE - 1) / BLOCK_CUBE;

    for (uint16_t i = 0; i < widthBlkNum; ++i) {
        uint16_t num = (i != widthBlkNum -1) ? BLOCK_CUBE : (width - i * BLOCK_CUBE);
        uint32_t blockLen = static_cast<uint32_t>(num * sizeof(T));
        uint32_t dstStride = static_cast<uint32_t>((dstDStride - num) * sizeof(T));
        for (uint16_t j = 0; j < repeatTimes; ++j) {
            DataCopyPadUB2GMImpl(dstAddr + i * BLOCK_CUBE + j * BLK_CNT_LIMIT * dstDStride,
                srcAddr + i * srcNStride * BLOCK_CUBE + j * BLK_CNT_LIMIT * BLOCK_CUBE,
                {BLK_CNT_LIMIT, blockLen, 0, dstStride, 0});
        }
        if (tailBlock) {
            DataCopyPadUB2GMImpl(dstAddr + i * BLOCK_CUBE + repeatTimes * BLK_CNT_LIMIT * dstDStride,
                srcAddr + i * srcNStride * BLOCK_CUBE + repeatTimes * BLK_CNT_LIMIT * BLOCK_CUBE,
                {tailBlock, blockLen, 0, dstStride, 0});
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2GMNZ2NDImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const Nz2NdParamsFull& intriParams)
{

                                               ;

                                                                                       ;
    const uint16_t ndNum = intriParams.ndNum;
    const uint16_t nValue = intriParams.nValue;
    const uint16_t dValue = intriParams.dValue;
    const uint16_t srcNdMatrixStride = intriParams.srcNdMatrixStride;
    const uint16_t srcNStride = intriParams.srcNStride;
    const uint16_t dstDStride = intriParams.dstDStride;
    const uint16_t dstNdMatrixStride = intriParams.dstNdMatrixStride;

    if (ndNum != 1 && nValue != 0) {

                                                                                                                      ;
    }
                                                                                                                ;
    for (uint16_t i = 0; i < ndNum; ++i) {
        DataCopyUB2GMNZ2NDImplBase(dst + i * dstNdMatrixStride, src + i * srcNdMatrixStride * BLOCK_CUBE * BLOCK_CUBE,
            nValue, dValue, srcNStride, dstDStride);
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2UBSingleImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_global)) float* src, const Nd2NzParams& intriParams,
    const int copyTime, const int computeNum)
{

                                               ;
    const uint16_t &nValue = intriParams.nValue;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& computeLen = computeNum * sizeof(float);
    const uint16_t& c0Count = BLOCK_CUBE;
    const uint16_t& maxC0Count = MAX_REPEAT_TIMES * c0Count;
    const uint16_t& maxdValue = MAX_REPEAT_TIMES * dValue;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& repeatCount = nValue / MAX_REPEAT_TIMES;
    const uint16_t& repeatTail = nValue % MAX_REPEAT_TIMES;
    const uint16_t& srcCopyStartOffset = copyTime * c0Count;
    const uint16_t& dstCopyStartOffset = copyTime * dstNzC0Stride * (DEFAULT_C0_SIZE / sizeof(float));
    DataCopyExtParams copyParams = { MAX_REPEAT_TIMES, (uint32_t)computeLen,
        (uint32_t)(intriParams.srcDValue * sizeof(float) - computeLen),
        (uint32_t)((dstNzNStride * DEFAULT_C0_SIZE - (uint16_t)c0Count * sizeof(float)) / (uint16_t)DEFAULT_C0_SIZE),
        0 };
    DataCopyPadExtParams<float> padParams;
    if (computeNum < c0Count) {
        copyParams.dstStride = (c0Count - computeNum) * sizeof(float) / DEFAULT_C0_SIZE;
        padParams.paddingValue = 0;
    }

    for (int repeatTime = 0; repeatTime < repeatCount; ++repeatTime) {
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) float*)(dst + dstCopyStartOffset + repeatTime * maxC0Count),
            (__attribute__((cce_global)) float*)(src + srcCopyStartOffset + repeatTime * maxdValue), copyParams, padParams);
    }
    copyParams.blockCount = repeatTail;
    if (repeatTail != 0) {
        int dstOffset = (dstCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * c0Count);
        int srcOffset = (srcCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * dValue);
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) float*)(dst + dstOffset), (__attribute__((cce_global)) float*)(src + srcOffset), copyParams,
            padParams);
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2UBND2NZImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_global)) float* src, const Nd2NzParams& intriParams)
{
    if constexpr(g_coreType != AscendC::AIV) {
        return;
    }

                                               ;
    const uint16_t &ndNum = intriParams.ndNum;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& srcNdMatrixStride = intriParams.srcNdMatrixStride;
    const uint16_t& srcDValue = intriParams.srcDValue;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzMatrixStride = intriParams.dstNzMatrixStride;
    const uint16_t& c0Count = BLOCK_CUBE;
    for (int index = 0; index < ndNum; ++index) {
        int16_t copyNum = (dValue + c0Count - 1) / c0Count;
        for (int copyTime = 0; copyTime < copyNum; ++copyTime) {
            int computeCount = (dValue >= (copyTime + 1) * c0Count) ? c0Count : (dValue % c0Count);
            DataCopyGM2UBSingleImpl(dst + dstNzMatrixStride, src + srcNdMatrixStride, intriParams, copyTime,
                computeCount);
        }
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL0C2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_cube_c)) U* src, const DataCopyCO12DstParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        switch (intriParams.quantPre) {
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322F16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322BF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::DEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VDEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::QF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VQF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::REQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VREQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            default:


                                                      ;
        }
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL0C2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_c)) U* src, const DataCopyCO12DstParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {





                          ;
        switch (intriParams.quantPre) {
            case QuantMode_t::NoQuant:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::NoQuant,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322F16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322BF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::DEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VDEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::QF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VQF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::REQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VREQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            default:


                                                      ;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2L1Intf(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams)
{
    DataCopyUB2L1Impl((__attribute__((cce_cube_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2L0CIntf(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    DataCopyUB2L0CImpl((__attribute__((cce_cube_c)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams, enhancedParams);
}

#pragma begin_pipe(V)
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2UBIntf(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams)
{
    DataCopyUB2UBImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams);
}
#pragma end_pipe

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12UBIntf(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams)
{
    DataCopyL12UBImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void __attribute__((inout_pipe("MTE1"))) DataCopyL12L0CIntf(const LocalTensor<T> &dstLocal,
    const LocalTensor<T> &srcLocal, const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    DataCopyL12L0CImpl((__attribute__((cce_cube_c)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams, enhancedParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL0C2UBIntf(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams, enhancedParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE1"))) void DataCopyL12BTIntf(const LocalTensor<T> &dstLocal,
    const LocalTensor<T> &srcLocal, const DataCopyParams &repeatParams)
{
    DataCopyL12BTImpl((uint64_t)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), (uint16_t)0,
        repeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("FIX"))) void DataCopyL12FBIntf(const LocalTensor<T> &dstLocal,
    const LocalTensor<T> &srcLocal, const DataCopyParams &repeatParams)
{
    DataCopyL12FBImpl((__attribute__((cce_fixpipe_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        repeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadL12GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams& intriParams)
{
                                                                        ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadL12GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyExtParams& intriParams)
{
                                                                        ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadGM2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams,
    const DataCopyPadParams& padParams)
{
                                                                        ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadGM2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyExtParams& intriParams,
    const DataCopyPadExtParams<T>& padParams)
{
                                                                        ;
}
}
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_data_copy_base_impl.h" 2







namespace AscendC {

enum class ReduceType : uint8_t {
    NO_REDUCE,
    REDUCE_ADD,
    REDUCE_MIN,
    REDUCE_MAX,
};

template <typename T, enum ReduceType reduceType = ReduceType::NO_REDUCE>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyWithReduce(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const uint32_t calCount)
{
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = calCount / AscendCUtils::GetC0Count(sizeof(T));
    DataCopyWithReduce<T, reduceType>(dstGlobal, srcLocal, repeatParams);
}

template <typename T, enum ReduceType reduceType = ReduceType::NO_REDUCE>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyWithReduce(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const DataCopyParams& repeatParams)
{
    AscendC::SetAtomicNoneImpl();
    if constexpr (reduceType == ReduceType::REDUCE_ADD) {
        AscendC::SetAtomicAddImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MIN) {
        AscendC::SetAtomicMinImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MAX) {
        AscendC::SetAtomicMaxImpl<T>();
    }
    DataCopy(dstGlobal, srcLocal, repeatParams);
    AscendC::SetAtomicNoneImpl();
}

template <typename T, enum ReduceType reduceType = ReduceType::NO_REDUCE>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadWithReduce(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const DataCopyExtParams& dataCopyExtParams)
{
    AscendC::SetAtomicNoneImpl();
    if constexpr (reduceType == ReduceType::REDUCE_ADD) {
        AscendC::SetAtomicAddImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MIN) {
        AscendC::SetAtomicMinImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MAX) {
        AscendC::SetAtomicMaxImpl<T>();
    }
    DataCopyPad(dstGlobal, srcLocal, dataCopyExtParams);
    AscendC::SetAtomicNoneImpl();
}


[aicore] __inline__ __attribute__((always_inline)) void DataCopyGetOffsetList(
    const SliceInfo sliceInfo[], uint32_t shapeInfo[], const uint32_t dimValue, uint32_t *count, uint32_t *offsetList)
{
    uint32_t sliceSize = 1;
    uint32_t copyCount = 1;
    uint32_t currentCount = 1;
    uint32_t preCopyCount = 0;
    uint32_t iter = 0;
    uint32_t totalSliceCount = 0;

    for (uint32_t i = 0; i < dimValue; i++) {
        if (i == 0) {
            *(offsetList + totalSliceCount) = 0;
            totalSliceCount++;
            continue;
        }
        iter = 0;
        sliceSize = sliceSize * shapeInfo[i - 1];
        currentCount =
            (sliceInfo[i].endIndex - sliceInfo[i].startIndex + 1 + sliceInfo[i].stride) / (1 + sliceInfo[i].stride);
        preCopyCount = copyCount;
        copyCount = copyCount * currentCount;
        for (uint32_t j = preCopyCount; j < copyCount; j += preCopyCount) {
            iter++;
            for (uint32_t k = 0; k < preCopyCount; k++) {
                *(offsetList + totalSliceCount) =
                    (*(offsetList + k)) + (iter * (1 + sliceInfo[i].stride)) * sliceSize;
                totalSliceCount++;
            }
        }
    }
    *count = totalSliceCount;
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t DataCopyGetPhyStartIndex(
    const SliceInfo sliceInfo[], uint32_t shapeInfo[], const uint32_t dimValue)
{
    uint32_t phyStartIndex = 0;
    uint32_t sliceSize = 1;
    for (uint32_t i = 0; i < dimValue; i++) {
        if (i == 0) {
            phyStartIndex = phyStartIndex + sliceInfo[i].startIndex;
        } else {
            sliceSize = sliceSize * shapeInfo[i - 1];
            phyStartIndex = phyStartIndex + sliceSize * sliceInfo[i].startIndex;
        }
    }
    return phyStartIndex;
}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h" 2

namespace AscendC {
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void __attribute__((inout_pipe("MTE2")))
    DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal, const DataCopyParams& repeatParams);
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
                                                     const Nd2NzParams& intriParams);
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcGlobal,
                                     const Nd2NzParams& intriParams);
# 97 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
                                                     const DataCopyParams& repeatParams);
# 111 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                const DataCopyParams& repeatParams);
# 125 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<dst_T>& dstLocal, const LocalTensor<src_T>& srcLocal,
                                const DataCopyParams& repeatParams);
# 142 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                              const uint64_t mask[], const uint8_t repeatTimes,
                                              const CopyRepeatParams& repeatParams);


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                              const uint64_t mask, const uint8_t repeatTimes,
                                              const CopyRepeatParams& repeatParams);
# 162 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
                                                     const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[],
                                                     const uint32_t dimValue = 1);
# 176 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
                                                     const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[],
                                                     const uint32_t dimValue = 1);
# 188 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
                                                     const uint32_t calCount);
# 199 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
                                                     const uint32_t calCount);
# 210 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                const uint32_t calCount);







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
                                                     const Nz2NdParamsFull& intriParams);
# 242 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
                                                     const DataCopyParams& intriParams,
                                                     const DataCopyEnhancedParams& enhancedParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
                                                     const DataCopyParams& intriParams,
                                                     const DataCopyEnhancedParams& enhancedParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams);

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                const DataCopyCO12DstParams& intriParams);

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<U>& srcLocal,
                                const DataCopyCO12DstParams& intriParams);



template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, bfloat16_t>::value && IsSameType<PrimT<U>, float>::value,
                                  bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams);



template <
    typename T, typename U,
    typename std::enable_if<IsSameType<PrimT<T>, __cce_half>::value && IsSameType<PrimT<U>, float>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, __cce_half>::value && IsSameType<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, int16_t>::value && IsSameType<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value && IsSameType<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, uint8_t>::value && IsSameType<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <
    typename T, typename U,
    typename std::enable_if<IsSameType<PrimT<T>, float>::value && IsSameType<PrimT<U>, __cce_half>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T>& dstLocal,
                                                        const GlobalTensor<T>& srcGlobal,
                                                        const DataCopyParams& dataCopyParams,
                                                        const DataCopyPadParams& padParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T>& dstGlobal,
                                                        const LocalTensor<T>& srcLocal,
                                                        const DataCopyParams& dataCopyParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPad(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                        const DataCopyParams& dataCopyParams, const Nd2NzParams& nd2nzParams);


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T>& dstLocal,
                                                        const GlobalTensor<T>& srcGlobal,
                                                        const DataCopyExtParams& dataCopyParams,
                                                        const DataCopyPadExtParams<T>& padParams);



template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, U>::value && (!IsSameType<T, U>::value), bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T>& dstLocal,
                                                        const GlobalTensor<T>& srcGlobal,
                                                        const DataCopyExtParams& dataCopyParams,
                                                        const DataCopyPadExtParams<U>& padParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T>& dstGlobal,
                                                        const LocalTensor<T>& srcLocal,
                                                        const DataCopyExtParams& dataCopyParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPad(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                        const DataCopyExtParams& dataCopyParams, const Nd2NzParams& nd2nzParams);

template <typename T, TPosition pos = TPosition::MAX>
[aicore] __inline__ __attribute__((always_inline)) void SetPadValue(T paddingValue);
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_mm_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_mm_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0ACal(__attribute__((cce_cube_a)) T* dst, __attribute__((cce_cube_buff)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            load_cbuf_to_ca_s4((__attribute__((cce_cube_a)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParam.startIndex,
                loadDataParam.repeatTimes, loadDataParam.srcStride, loadDataParam.dstGap, loadDataParam.sid, 0, inc);
        } else {
            if (loadDataParam.ifTranspose) {
                load_cbuf_to_ca(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 1, inc);
            } else {
                load_cbuf_to_ca(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 0, inc);
            }
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0BCal(__attribute__((cce_cube_b)) T* dst, __attribute__((cce_cube_buff)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            load_cbuf_to_cb_s4((__attribute__((cce_cube_b)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParam.startIndex,
                loadDataParam.repeatTimes, loadDataParam.srcStride, loadDataParam.dstGap, loadDataParam.sid, 0, inc);
        } else {
            if (loadDataParam.ifTranspose) {
                load_cbuf_to_cb(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 1, inc);
            } else {
                load_cbuf_to_cb(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 0, inc);
            }
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L0ACal(__attribute__((cce_cube_a)) T* dst, __attribute__((cce_global)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        load_gm_to_ca(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
            loadDataParam.dstGap, loadDataParam.sid, (addr_cal_mode_t)0);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L0BCal(__attribute__((cce_cube_b)) T* dst, __attribute__((cce_global)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        load_gm_to_cb(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
            loadDataParam.dstGap, loadDataParam.sid, (addr_cal_mode_t)0);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L1Cal(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if (loadDataParam.addrMode == 0) {
            load_gm_to_cbuf(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                loadDataParam.dstGap, loadDataParam.sid, inc);
        } else {
            load_gm_to_cbuf(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                loadDataParam.dstGap, loadDataParam.sid, dec);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_global)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_global)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L1Cal(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_global)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                            ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0ATransposeCal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData2dTransposeParams &loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {


                                                                                                                   ;
        if constexpr (!IsSameType<T, int4b_t>::value) {
            load_cbuf_to_ca_transpose(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes,
                loadDataParam.srcStride, loadDataParam.dstGap, inc, loadDataParam.dstFracGap);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0BTransposeCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData2dTransposeParams &loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {



                         ;
        if constexpr (IsSameType<T, int4b_t>::value) {
            load_cbuf_to_cb_transpose_s4((__attribute__((cce_cube_b)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParam.startIndex,
                loadDataParam.repeatTimes, loadDataParam.srcStride, loadDataParam.dstGap, inc,
                loadDataParam.dstFracGap);
        } else {
            load_cbuf_to_cb_transpose(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes,
                loadDataParam.srcStride, loadDataParam.dstGap, inc, loadDataParam.dstFracGap);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0BTransposeCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData2dTransposeParamsV2 &loadDataParam)
{
                                                                                                             ;
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2<T> &loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_ca_s4((__attribute__((cce_cube_a)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParams.kExtension,
                loadDataParams.mExtension, loadDataParams.kStartPt, loadDataParams.mStartPt, loadDataParams.strideW,
                loadDataParams.strideH, loadDataParams.filterW, loadDataParams.filterH, loadDataParams.dilationFilterW,
                loadDataParams.dilationFilterH, loadDataParams.filterSizeW, loadDataParams.filterSizeH,
                loadDataParams.enTranspose, loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
        } else {
            img2colv2_cbuf_to_ca(dst, src, loadDataParams.kExtension, loadDataParams.mExtension,
                loadDataParams.kStartPt, loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH,
                loadDataParams.filterW, loadDataParams.filterH, loadDataParams.dilationFilterW,
                loadDataParams.dilationFilterH, loadDataParams.filterSizeW, loadDataParams.filterSizeH,
                loadDataParams.enTranspose, loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2<T> &loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (!IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_cb(dst, src, loadDataParams.kExtension, loadDataParams.mExtension,
                loadDataParams.kStartPt, loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH,
                loadDataParams.filterW, loadDataParams.filterH, loadDataParams.dilationFilterW,
                loadDataParams.dilationFilterH, loadDataParams.filterSizeW, loadDataParams.filterSizeH,
                loadDataParams.enTranspose, loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
        }
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) bfloat16_t* dst, __attribute__((cce_cube_buff)) bfloat16_t* src,
    const LoadData3DParamsV2<bfloat16_t>& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        img2colv2_cbuf_to_ca(reinterpret_cast<__attribute__((cce_cube_a)) __cce_half*>(dst),
            reinterpret_cast<__attribute__((cce_cube_buff)) __cce_half*>(src),
            loadDataParams.kExtension, loadDataParams.mExtension, loadDataParams.kStartPt,
            loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH, loadDataParams.filterW,
            loadDataParams.filterH, loadDataParams.dilationFilterW, loadDataParams.dilationFilterH,
            loadDataParams.filterSizeW, loadDataParams.filterSizeH, loadDataParams.enTranspose,
            loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) bfloat16_t* dst, __attribute__((cce_cube_buff)) bfloat16_t* src,
    const LoadData3DParamsV2<bfloat16_t>& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        img2colv2_cbuf_to_cb(reinterpret_cast<__attribute__((cce_cube_b)) __cce_half*>(dst),
            reinterpret_cast<__attribute__((cce_cube_buff)) __cce_half*>(src),
            loadDataParams.kExtension, loadDataParams.mExtension, loadDataParams.kStartPt,
            loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH, loadDataParams.filterW,
            loadDataParams.filterH, loadDataParams.dilationFilterW, loadDataParams.dilationFilterH,
            loadDataParams.filterSizeW, loadDataParams.filterSizeH, loadDataParams.enTranspose,
            loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_ca_s4((__attribute__((cce_cube_a)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParams.extConfig,
                loadDataParams.extConfig >> LOAD_M_EXTENSION, loadDataParams.extConfig >> LOAD_K_START_POSITION,
                loadDataParams.extConfig >> LOAD_M_START_POSITION, loadDataParams.filterConfig,
                loadDataParams.filterConfig >> LOAD_STRIDE_H, loadDataParams.filterConfig >> LOAD_FILTER_W,
                loadDataParams.filterConfig >> LOAD_FILTER_H, loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
                loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
                loadDataParams.channelSize);
        } else {
            img2colv2_cbuf_to_ca(dst, src, loadDataParams.extConfig, loadDataParams.extConfig >> LOAD_M_EXTENSION,
                loadDataParams.extConfig >> LOAD_K_START_POSITION, loadDataParams.extConfig >> LOAD_M_START_POSITION,
                loadDataParams.filterConfig, loadDataParams.filterConfig >> LOAD_STRIDE_H,
                loadDataParams.filterConfig >> LOAD_FILTER_W, loadDataParams.filterConfig >> LOAD_FILTER_H,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
                loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
                loadDataParams.channelSize);
        }
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) bfloat16_t* dst, __attribute__((cce_cube_buff)) bfloat16_t* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        img2colv2_cbuf_to_ca((__attribute__((cce_cube_a)) __cce_half*)dst, (__attribute__((cce_cube_buff)) __cce_half*)src,
            loadDataParams.extConfig, loadDataParams.extConfig >> LOAD_M_EXTENSION,
            loadDataParams.extConfig >> LOAD_K_START_POSITION, loadDataParams.extConfig >> LOAD_M_START_POSITION,
            loadDataParams.filterConfig, loadDataParams.filterConfig >> LOAD_STRIDE_H,
            loadDataParams.filterConfig >> LOAD_FILTER_W, loadDataParams.filterConfig >> LOAD_FILTER_H,
            loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
            loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
            loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
            loadDataParams.channelSize);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (!IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_cb(dst, src, loadDataParams.extConfig, loadDataParams.extConfig >> LOAD_M_EXTENSION,
                loadDataParams.extConfig >> LOAD_K_START_POSITION, loadDataParams.extConfig >> LOAD_M_START_POSITION,
                loadDataParams.filterConfig, loadDataParams.filterConfig >> LOAD_STRIDE_H,
                loadDataParams.filterConfig >> LOAD_FILTER_W, loadDataParams.filterConfig >> LOAD_FILTER_H,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
                loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
                loadDataParams.channelSize);
        }
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) bfloat16_t* dst, __attribute__((cce_cube_buff)) bfloat16_t* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        img2colv2_cbuf_to_cb((__attribute__((cce_cube_b)) __cce_half*)dst, (__attribute__((cce_cube_buff)) __cce_half*)src,
            loadDataParams.extConfig, loadDataParams.extConfig >> LOAD_M_EXTENSION,
            loadDataParams.extConfig >> LOAD_K_START_POSITION, loadDataParams.extConfig >> LOAD_M_START_POSITION,
            loadDataParams.filterConfig, loadDataParams.filterConfig >> LOAD_STRIDE_H,
            loadDataParams.filterConfig >> LOAD_FILTER_W, loadDataParams.filterConfig >> LOAD_FILTER_H,
            loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
            loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
            loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
            loadDataParams.channelSize);
    }
}



template <typename DstT, typename Src0T, typename Src1T>
[aicore] __inline__ __attribute__((always_inline)) void MmadCal(__attribute__((cce_cube_c)) DstT* c, __attribute__((cce_cube_a)) Src0T* a, __attribute__((cce_cube_b)) Src1T* b, const MmadParams& mmadParams)
{
    if constexpr(g_coreType == AscendC::AIC) {





                                                            ;
        bool cmatrixInitVal = mmadParams.cmatrixInitVal && (!mmadParams.isBias);
        if constexpr ((IsSameType<Src0T, int4b_t>::value) && (IsSameType<Src1T, int4b_t>::value)) {
            mad_s4(c, (__attribute__((cce_cube_a)) void *)a, (__attribute__((cce_cube_b)) void *)b, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag,
                mmadParams.kDirectionAlign, mmadParams.cmatrixSource, cmatrixInitVal);
        } else {
            mad(c, a, b, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag, mmadParams.kDirectionAlign,
                mmadParams.cmatrixSource, cmatrixInitVal);
        }
    }
}

template <typename DstT, typename Src0T, typename Src1T>
[aicore] __inline__ __attribute__((always_inline)) void MmadCal(__attribute__((cce_cube_c)) DstT* c, __attribute__((cce_cube_a)) Src0T* a, __attribute__((cce_cube_b)) Src1T* b, uint64_t bias,
    const MmadParams& mmadParams, bool cmatrixSource)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr ((IsSameType<Src0T, int4b_t>::value) && (IsSameType<Src1T, int4b_t>::value)) {
            mad_s4(c, (__attribute__((cce_cube_a)) void *)a, (__attribute__((cce_cube_b)) void *)b, mmadParams.m, mmadParams.k, mmadParams.n,
                mmadParams.unitFlag, mmadParams.kDirectionAlign, cmatrixSource,
                mmadParams.cmatrixInitVal);
        } else {
            mad(c, a, b, bias, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag,
                mmadParams.kDirectionAlign, cmatrixSource, mmadParams.cmatrixInitVal);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void MmadSpCal(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, const MmadParams &mmadParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        mad_sp(c, a, b, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag, mmadParams.cmatrixSource,
            mmadParams.cmatrixInitVal);
    }
}

template <typename T = int8_t, typename U = uint8_t,
    typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value, bool>::type = true,
    typename std::enable_if<IsSameType<PrimT<U>, uint8_t>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithSparseCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<U> &idxLocal, const LoadData2dParams &loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {




        uint64_t src0tmp = reinterpret_cast<uint64_t>(srcLocal.GetPhyAddr());
        uint64_t src1tmp = reinterpret_cast<uint64_t>(idxLocal.GetPhyAddr());


        uint64_t srctmp = (src0tmp & 0xffffffff) | ((src1tmp & 0xffffffff) << 32);
        __attribute__((cce_cube_buff)) int8_t *src = reinterpret_cast<__attribute__((cce_cube_buff)) int8_t *>(srctmp);

        load_cbuf_to_cb_sp((__attribute__((cce_cube_b)) int8_t *)dstLocal.GetPhyAddr(), src, loadDataParam.startIndex,
            loadDataParam.repeatTimes);
    }
}

template <typename T = int8_t, typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LoadUnzipIndexCal(const GlobalTensor<T>& srcTensor, uint32_t numOfIndexTabEntry)
{
                                                       ;
}




[aicore] __inline__ __attribute__((always_inline)) void Load3DSetFMatrixCal(uint16_t l1H, uint16_t l1W, const uint8_t padList[4])
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t regFMatrix = 0;
        regFMatrix |= uint64_t(l1W & 0xFFFF);

        uint32_t l1HShiftBit = 16;
        regFMatrix |= uint64_t(l1H & 0xFFFF) << l1HShiftBit;

        uint32_t padNumber = 4;
        uint32_t padListShiftBit = 8;
        uint32_t padListShiftBase = 32;
        for (uint32_t i = 0; i < padNumber; i++) {
            regFMatrix |= uint64_t(padList[i] & 0xFF) << (padListShiftBase + i * padListShiftBit);
        }
        set_fmatrix(regFMatrix);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void Load3DSetFMatrixBCal(uint16_t l1H, uint16_t l1W, const uint8_t padList[4])
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t regFMatrix = 0;
        regFMatrix |= (uint64_t)l1W;

        uint32_t l1HShiftBit = 16;
        regFMatrix |= (uint64_t)l1H << l1HShiftBit;

        uint32_t padNumber = 4;
        uint32_t padListShiftBit = 8;
        uint32_t padListShiftBase = 32;
        for (uint32_t i = 0; i < padNumber; i++) {
            regFMatrix |= uint64_t(padList[i] & 0xFF) << (padListShiftBase + i * padListShiftBit);
        }
        set_fmatrix_b(regFMatrix);
    }
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void Load3DSetPaddingCal(const T padValue)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t paddingValue = 0;
        uint64_t padValueShiftBit = 8;
        if constexpr (sizeof(T) == B16_BYTE_SIZE || sizeof(T) == B32_BYTE_SIZE) {
            paddingValue = (uint64_t)GetScalarBitcodeValue((T)padValue);
        } else {
            paddingValue = (((uint64_t)padValue) << padValueShiftBit) | ((uint64_t)padValue & 0xFF);
        }
        set_padding(paddingValue);
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV1L12L0ACal(__attribute__((cce_cube_a)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV1<T>& loadDataParams)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV1L12L0BCal(__attribute__((cce_cube_b)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV1<T>& loadDataParams)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV1L12UBCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV1<T>& loadDataParams)
{
                                                                                       ;
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12UBCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV2<T>& loadDataParams)
{
                                                                                       ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) int8_t* dst, __attribute__((cce_cube_buff)) int8_t* src,
    const LoadData3DParamsV2<int8_t>& loadDataParams)
{
                                                                                                        ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) uint8_t* dst, __attribute__((cce_cube_buff)) uint8_t* src,
    const LoadData3DParamsV2<uint8_t>& loadDataParams)
{
                                                                                                         ;
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12UBCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
                                                                                          ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) int8_t* dst, __attribute__((cce_cube_buff)) int8_t* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
                                                                                                           ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) uint8_t* dst, __attribute__((cce_cube_buff)) uint8_t* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
                                                                                                            ;
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BroadCastVecToMMCal(__attribute__((cce_cube_c)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t blockCount,
    const uint8_t blockLen, const uint8_t srcGap, const uint8_t dstGap)
{
                                                         ;
}




[aicore] __inline__ __attribute__((always_inline)) void CheckInitConstValueParams(uint16_t repeatTimes, uint16_t blockNum, uint16_t dstGap)
{
                                                                                          ;
                                                                                    ;
                                                                                ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitL1BufferCal(__attribute__((cce_cube_buff)) T *dst, const InitConstValueParams<T> &initConstValueParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckInitConstValueParams(initConstValueParams.repeatTimes, initConstValueParams.blockNum,
            initConstValueParams.dstGap);
        int64_t repeatBit = ((uint64_t)initConstValueParams.blockNum << 16) |
            ((uint64_t)initConstValueParams.dstGap << 32) | initConstValueParams.repeatTimes;
        if constexpr (IsSameType<T, bfloat16_t>::value) {
            create_cbuf_matrix_bf16(dst, repeatBit, initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, uint32_t>::value || IsSameType<T, __cce_half>::value) {
            create_cbuf_matrix(dst, repeatBit, (T)initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, int16_t>::value || IsSameType<T, uint16_t>::value) {
            create_cbuf_matrix(dst, repeatBit, GetScalarBitcodeToHalf(initConstValueParams.initValue));
        } else if constexpr (IsSameType<T, float>::value || IsSameType<T, int32_t>::value) {
            create_cbuf_matrix(dst, repeatBit, (uint32_t)GetScalarBitcodeValue(initConstValueParams.initValue));
        } else {

                                                                                                                     ;
        }
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitL0ANzMatrixCal(__attribute__((cce_cube_a)) T *dst, const InitConstValueParams<T> &initConstValueParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckInitConstValueParams(initConstValueParams.repeatTimes, initConstValueParams.blockNum,
            initConstValueParams.dstGap);
        int64_t repeatBit = ((uint64_t)initConstValueParams.blockNum << 16) |
            ((uint64_t)initConstValueParams.dstGap << 32) | initConstValueParams.repeatTimes;
        if constexpr (IsSameType<T, bfloat16_t>::value) {
            create_ca_matrix_bf16(dst, repeatBit, initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, uint32_t>::value || IsSameType<T, __cce_half>::value) {
            create_ca_matrix(dst, repeatBit, (T)initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, int16_t>::value || IsSameType<T, uint16_t>::value) {
            create_ca_matrix(dst, repeatBit, GetScalarBitcodeToHalf(initConstValueParams.initValue));
        } else if constexpr (IsSameType<T, float>::value || IsSameType<T, int32_t>::value) {
            create_ca_matrix(dst, repeatBit, (uint32_t)GetScalarBitcodeValue(initConstValueParams.initValue));
        } else {

                                                                                                                     ;
        }
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitL0BNzMatrixCal(__attribute__((cce_cube_b)) T *dst, const InitConstValueParams<T> &initConstValueParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckInitConstValueParams(initConstValueParams.repeatTimes, initConstValueParams.blockNum,
            initConstValueParams.dstGap);
        int64_t repeatBit = ((uint64_t)initConstValueParams.blockNum << 16) |
            ((uint64_t)initConstValueParams.dstGap << 32) | initConstValueParams.repeatTimes;
        if constexpr (IsSameType<T, bfloat16_t>::value) {
            create_cb_matrix_bf16(dst, repeatBit, initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, uint32_t>::value || IsSameType<T, __cce_half>::value) {
            create_cb_matrix(dst, repeatBit, (T)initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, int16_t>::value || IsSameType<T, uint16_t>::value) {
            create_cb_matrix(dst, repeatBit, GetScalarBitcodeToHalf(initConstValueParams.initValue));
        } else if constexpr (IsSameType<T, float>::value || IsSameType<T, int32_t>::value) {
            create_cb_matrix(dst, repeatBit, (uint32_t)GetScalarBitcodeValue(initConstValueParams.initValue));
        } else {

                                                                                                                     ;
        }
    }
}



[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataRepeatCal(const LoadDataRepeatParam& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t rptConfig = (uint64_t)repeatParams.repeatStride | ((uint64_t)repeatParams.repeatTime << 16) |
            ((uint64_t)repeatParams.repeatMode << 24);
        set_l3d_rpt(rptConfig);
    }
}




[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataBoundaryCal(uint32_t boundaryValue)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_l1_3d_size(static_cast<uint64_t>(boundaryValue));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadImageToLocalCal(__attribute__((cce_cube_buff)) T *dst, const LoadImageToLocalParams &loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {

                                                                                                ;
        load_image_to_cbuf(dst, static_cast<uint16_t>(loadDataParams.horizSize - 1),
            static_cast<uint16_t>(loadDataParams.vertSize - 1), loadDataParams.horizStartPos,
            loadDataParams.vertStartPos, static_cast<uint16_t>(loadDataParams.srcHorizSize - 1),
            loadDataParams.topPadSize, loadDataParams.botPadSize, loadDataParams.leftPadSize,
            loadDataParams.rightPadSize, loadDataParams.sid);
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzipToL1Cal(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_global)) T *src)
{
                                                      ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzipToL0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_global)) T *src)
{
                                                      ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzipToL0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_global)) T *src)
{
                                                      ;
}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h" 2





# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_check.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_check.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_check.h" 1
# 310 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_check.h"
namespace AscendC {







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CheckTensorAlign(const LocalTensor<T>& tensor, uint32_t alignByte, __attribute__((cce_global)) const char* tensorName,
    __attribute__((cce_global)) const char* apiMsg)
{






}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CheckTensorPos(const LocalTensor<T>& tensor, const Hardware expectPos,
    __attribute__((cce_global)) const char* tensorName, __attribute__((cce_global)) const char* tposName, __attribute__((cce_global)) const char* apiMsg)
{





}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_check.h" 2

namespace AscendC {

template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) bool ChannelSizeRemainder(const uint16_t channelSize, uint16_t remainder[], uint16_t size)
{
    uint16_t oneBlkNum = ONE_BLK_SIZE / sizeof(T);
    if constexpr (IsSameType<T, int4b_t>::value) {
        oneBlkNum = 64;
    }
    for (uint16_t i = 0; i < size; i++) {
        if (channelSize % oneBlkNum == remainder[i]) {
            return true;
        }
    }
    return false;
}

template <typename DstT, typename Src0T, typename Src1T>
[aicore] static __inline__ __attribute__((always_inline)) void CheckMmadAlign(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal) {
    constexpr uint64_t ALIGN_1024B = 1024;
    if constexpr ((IsSameType<PrimT<Src0T>, __cce_half>::value) && (IsSameType<PrimT<Src1T>, __cce_half>::value) &&
        (IsSameType<PrimT<DstT>, __cce_half>::value)) {
        CheckTensorAlign<DstT>(dstLocal, VALUE_512, "dstLocal", "Mmad");
    } else {
        CheckTensorAlign<DstT>(dstLocal, ALIGN_1024B, "dstLocal", "Mmad");
    }
    CheckTensorAlign<Src0T>(fmLocal, VALUE_512, "fmLocal", "Mmad");
    CheckTensorAlign<Src1T>(filterLocal, VALUE_512, "filterLocal", "Mmad");
}


template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void CheckLoadData2dDatatype()
{
# 70 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_check.h"
                                                   ;






}


[aicore] static __inline__ __attribute__((always_inline)) void CheckLoadData3dParams(const uint16_t srcHeight, const uint16_t srcWeight,
    const uint8_t srcWStride, const uint8_t srcHStride)
{
                                                                                                               ;
                                                                                                               ;

                                         ;

                                         ;
}


template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void CheckLoadData3dv2ChannelSize(const uint16_t channelSize)
{
# 124 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_check.h"
    if constexpr (SupportType<PrimT<T>, __cce_half, bfloat16_t>()) {
        uint16_t remainderList[] = {0, 4, 8};



                                                                                    ;
    }

    if constexpr (SupportType<PrimT<T>, float, int32_t, uint32_t>()) {
        uint16_t remainderList[] = {0, 4};



                                                                                         ;
    } else if constexpr (SupportType<PrimT<T>, int8_t, uint8_t>()) {
        uint16_t remainderList[] = {0, 4, 8, 16};



                                                                                         ;
    } else if constexpr (IsSameType<PrimT<T>, int4b_t>::value) {
        uint16_t remainderList[] = {0, 8, 16, 32};



                                                                                      ;
    }

}


template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void CheckLoadData3dv2MatrixParams(const uint16_t kExtension, const uint16_t mExtension,
    const uint16_t kStartPt, const uint16_t mStartPt) {
    constexpr uint16_t base16 = 16;
    if constexpr (SupportType<PrimT<T>, __cce_half, int8_t, int4b_t>()) {


                                                 ;
    }
    uint16_t kExtBase = (SupportType<PrimT<T>, int4b_t>()) ? 64 : ONE_BLK_SIZE / sizeof(PrimT<T>);
    if constexpr (SupportType<PrimT<T>, __cce_half, int8_t, int4b_t, int32_t, uint32_t, float>()) {


                                                                                      ;


                                                                                    ;
    }







                                                                                                      ;

}
}
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_load2d_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_load2d_impl.h"
namespace AscendC {
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_load2d_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParams& loadDataParams)
{





    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1", "LoadData with LoadData2DParams");
    CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "LoadData with LoadData2DParams");
    CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData2DParams");
    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData2DL12L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dstLocal.GetPhyAddr(),
                            (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DL12L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(),
                            (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {


                                                                                                   ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadDataImpl(const LocalTensor<T>& dstLocal,
    const GlobalTensor<T>& srcLocal, const LoadData2DParams& loadDataParams)
{





    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData2DParams");
        LoadData2DGM2L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dstLocal.GetPhyAddr(),
                            (__attribute__((cce_global)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData2DParams");
        LoadData2DGM2L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(),
                            (__attribute__((cce_global)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L1) {
        CheckTensorAlign<T>(dstLocal, ONE_BLK_SIZE, "dstLocal",
            "LoadData with LoadData2DParams");
        LoadData2DGM2L1Cal((__attribute__((cce_cube_buff)) PrimT<T>*)dstLocal.GetPhyAddr(),
                           (__attribute__((cce_global)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {


                                                                                                   ;
    }
}
# 111 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_load2d_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTransposeImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParams& loadDataParams)
{





    CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "LoadDataWithTranspose");

    CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadDataWithTranspose");

    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1", "LoadDataWithTranspose");
    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData2DL12L0ATransposeCal((__attribute__((cce_cube_a)) PrimT<T>*)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
            loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DL12L0BTransposeCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
            loadDataParams);
    } else {

                                                                                                   ;
    }
}
# 153 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_load2d_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTransposeImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParamsV2& loadDataParams)
{





    CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "LoadDataWithTranspose");

    CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadDataWithTranspose");

    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1", "LoadDataWithTranspose");
    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0B) {
        LoadData2DL12L0BTransposeCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
            loadDataParams);
    } else {

                                                                                                   ;
    }
}
# 195 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_load2d_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParamsV2& loadDataParams)
{





    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1",
        "LoadData with LoadData2DParamsV2");
    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData2DL12L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dstLocal.GetPhyAddr(),
                            (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DL12L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(),
                            (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {


                                                                                                   ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadDataImpl(const LocalTensor<T>& dstLocal,
    const GlobalTensor<T>& srcLocal, const LoadData2DParamsV2& loadDataParams)
{





    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData2DGM2L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dstLocal.GetPhyAddr(),
                            (__attribute__((cce_global)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DGM2L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(),
                            (__attribute__((cce_global)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L1) {
        LoadData2DGM2L1Cal((__attribute__((cce_cube_buff)) PrimT<T>*)dstLocal.GetPhyAddr(),
                           (__attribute__((cce_global)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {


                                                                                                   ;
    }
}
}
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h" 2

namespace AscendC {
struct IsResetLoad3dConfig {
    [aicore] constexpr IsResetLoad3dConfig(const bool isSetFMatrixIn, const bool isSetPaddingIn)
    {
        isSetFMatrix = isSetFMatrixIn;
        isSetPadding = isSetPaddingIn;
    }
    bool isSetFMatrix = true;
    bool isSetPadding = true;
};

constexpr IsResetLoad3dConfig IS_RESER_LOAD3D_DEFAULT_CONFIG = {true, true};
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG,
    typename U = PrimT<T>, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV1<U>& loadDataParams)
{
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
                     ;

    if constexpr (defaultConfig.isSetFMatrix) {
        Load3DSetFMatrixCal(loadDataParams.l1H, loadDataParams.l1W, loadDataParams.padList);
    }
    if constexpr (defaultConfig.isSetPadding) {
        Load3DSetPaddingCal(loadDataParams.padValue);
    }

    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1", "LoadData with LoadData3DParamsV1");
    CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "LoadData with LoadData3DParamsV1");
    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData3DParamsV1");
        LoadData3DV1L12L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dstLocal.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData3DParamsV1");
        LoadData3DV1L12L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::UB) {
        CheckTensorAlign<T>(dstLocal, ONE_BLK_SIZE, "dstLocal", "LoadData with LoadData3DParamsV1");
        LoadData3DV1L12UBCal((__attribute__((cce_unif_buff)) PrimT<T>*)dstLocal.GetPhyAddr(),
                             (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {

                                                                                                   ;
    }
}
# 151 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG,
    typename U = PrimT<T>, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2<U>& loadDataParams)
{


      ;
    if constexpr (defaultConfig.isSetFMatrix) {
        Load3DSetFMatrixCal(loadDataParams.l1H, loadDataParams.l1W, loadDataParams.padList);
    }
    if constexpr (defaultConfig.isSetPadding) {
        Load3DSetPaddingCal(loadDataParams.padValue);
    }

    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());






    if (dstScope == Hardware::L0A) {



                                                       ;
    } else if (dstScope == Hardware::L0B) {


                                                                                                        ;
    }
# 196 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1", "LoadData with LoadData3DParamsV2");
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData3DParamsV2");
        LoadData3DV2L12L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dstLocal.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData3DParamsV2");
        LoadData3DV2L12L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::UB) {
        CheckTensorAlign<T>(dstLocal, ONE_BLK_SIZE, "dstLocal", "LoadData with LoadData3DParamsV2");
        LoadData3DV2L12UBCal((__attribute__((cce_unif_buff)) PrimT<T>*)dstLocal.GetPhyAddr(),
                             (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {

                                                                                                   ;
    }
}



template <const IsResetLoad3dConfig& defaultConfig>
[[deprecated("NOTICE: LoadData<IsResetLoad3dConfig> has been deprecated and will be removed in the next version."
             " Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<bfloat16_t>& dstLocal, const LocalTensor<bfloat16_t>& srcLocal,
    const LoadData3DParamsV2<bfloat16_t>& loadDataParams)
{
    LoadDataImpl<bfloat16_t, defaultConfig>(dstLocal, srcLocal, loadDataParams);
}
# 250 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2Pro& loadDataParams)
{





    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData3DV2L12L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dstLocal.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData3DV2L12L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::UB) {
        LoadData3DV2L12UBCal((__attribute__((cce_unif_buff)) PrimT<T>*)dstLocal.GetPhyAddr(),
                             (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {

                                                                                                   ;
    }
}
# 295 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename DstT, typename Src0T, typename Src1T>
[aicore] __inline__ __attribute__((always_inline)) void MmadImpl(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const MmadParams& mmadParams)
{






    MmadCal((__attribute__((cce_cube_c)) PrimT<DstT>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_a)) PrimT<Src0T>*)fmLocal.GetPhyAddr(),
        (__attribute__((cce_cube_b)) PrimT<Src1T>*)filterLocal.GetPhyAddr(), mmadParams);
}

template <typename DstT, typename Src0T, typename Src1T, typename BiasT>
[aicore] __inline__ __attribute__((always_inline)) void MmadImpl(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const LocalTensor<BiasT>& biasLocal, const MmadParams& mmadParams)
{
# 330 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
    const Hardware biasScope = GetPhyType((TPosition)biasLocal.GetPosition());
    bool cmatrixSource = false;
    if (biasScope == Hardware::BIAS) {
        cmatrixSource = true;
    } else if (biasScope == Hardware::L0C) {
        cmatrixSource = false;
    } else {

                                                                                                       ;
    }
    MmadCal((__attribute__((cce_cube_c)) PrimT<DstT>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_a)) PrimT<Src0T>*)fmLocal.GetPhyAddr(),
        (__attribute__((cce_cube_b)) PrimT<Src1T>*)filterLocal.GetPhyAddr(), (uint64_t)biasLocal.GetPhyAddr(), mmadParams, cmatrixSource);
}


template <typename T = int32_t, typename U = int8_t,
    typename std::enable_if<IsSameType<PrimT<T>, int32_t>::value, bool>::type = true,
    typename std::enable_if<IsSameType<PrimT<U>, int8_t>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void MmadSpImpl(const LocalTensor<T>& dstLocal, const LocalTensor<U>& fmLocal,
    const LocalTensor<U>& filterLocal, const MmadParams& mmadParams)
{
    CheckTensorPos<T>(dstLocal, Hardware::L0C, "dstLocal", "CO1", "MmadWithSparse");
    CheckTensorPos<U>(fmLocal, Hardware::L0A, "fmLocal", "A2", "MmadWithSparse");
    CheckTensorPos<U>(filterLocal, Hardware::L0B, "filterLocal", "B2", "MmadWithSparse");
    CheckTensorAlign<T>(dstLocal, 1024, "dstLocal", "MmadWithSparse");
    CheckTensorAlign<U>(fmLocal, VALUE_512, "fmLocal", "MmadWithSparse");
    CheckTensorAlign<U>(filterLocal, VALUE_512, "filterLocal", "MmadWithSparse");
                                                                                 ;
                                                                                 ;
                                                                                 ;
    MmadSpCal((__attribute__((cce_cube_c)) int32_t*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_a)) int8_t*)fmLocal.GetPhyAddr(),
        (__attribute__((cce_cube_b)) int8_t*)filterLocal.GetPhyAddr(), mmadParams);
}

template <typename T = int8_t, typename U = uint8_t,
    typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value, bool>::type = true,
    typename std::enable_if<IsSameType<PrimT<U>, uint8_t>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithSparseImpl(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<U> &idxLocal, const LoadData2dParams &loadDataParam)
{
    CheckTensorPos<T>(dstLocal, Hardware::L0B, "dstLocal", "B2", "LoadDataWithSparse");
    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "B1", "LoadDataWithSparse");
    CheckTensorPos<U>(idxLocal, Hardware::L1, "idxLocal", "B1", "LoadDataWithSparse");
    CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadDataWithSparse");
    CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "LoadDataWithSparse");
    CheckTensorAlign<U>(idxLocal, ONE_BLK_SIZE, "idxLocal", "LoadDataWithSparse");
    LoadDataWithSparseCal(dstLocal, srcLocal, idxLocal, loadDataParam);
}
# 391 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void BroadCastVecToMMImpl(const LocalTensor<T> &dstLocal,
    const LocalTensor<U> &srcLocal, const int32_t blockCount, const uint8_t blockLen, const uint8_t srcGap,
    const uint8_t dstGap)
{





    BroadCastVecToMMCal((__attribute__((cce_cube_c)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<U>*)srcLocal.GetPhyAddr(),
        blockCount, blockLen, srcGap, dstGap);
}
# 413 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Load3DSetPaddingImpl(const T padValue)
{
    Load3DSetPaddingCal(padValue);
}
# 431 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T, typename U = PrimT<T>,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void InitConstValueImpl(const LocalTensor<T> &dstLocal,
    const InitConstValueParams<U> &initConstValueParams)
{
    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "InitConstValue when TPosition is A2");
        InitL0ANzMatrixCal((__attribute__((cce_cube_a)) PrimT<T>*)dstLocal.GetPhyAddr(), initConstValueParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "InitConstValue when TPosition is B2");
        InitL0BNzMatrixCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(), initConstValueParams);
    } else if (dstScope == Hardware::L1) {
        CheckTensorAlign<T>(dstLocal, ONE_BLK_SIZE, "dstLocal", "InitConstValue when TPosition is A1 / B1");
        InitL1BufferCal((__attribute__((cce_cube_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), initConstValueParams);
    } else {

                                                                                                   ;
    }
}
# 463 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
[aicore] __inline__ __attribute__((always_inline)) void SetFmatrixImpl(uint16_t l1H, uint16_t l1W, const uint8_t padList[4],
    const FmatrixMode &fmatrixMode)
{
    if (fmatrixMode == FmatrixMode::FMATRIX_LEFT) {
        Load3DSetFMatrixCal(l1H, l1W, padList);
    } else if (fmatrixMode == FmatrixMode::FMATRIX_RIGHT) {
        Load3DSetFMatrixBCal(l1H, l1W, padList);
    }
}
# 481 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataBoundaryImpl(uint32_t boundaryValue)
{
    SetLoadDataBoundaryCal(boundaryValue);
}




[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataRepeatImpl(const LoadDataRepeatParam& repeatParams)
{
    SetLoadDataRepeatCal(repeatParams);
}
# 503 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzipImpl(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal)
{
    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
# 518 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
    if (dstScope == Hardware::L1) {
        LoadDataUnzipToL1Cal((__attribute__((cce_cube_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimT<T>*)srcGlobal.GetPhyAddr());
    } else if (dstScope == Hardware::L0A) {
        LoadDataUnzipToL0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimT<T>*)srcGlobal.GetPhyAddr());
    } else if (dstScope == Hardware::L0B) {
        LoadDataUnzipToL0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimT<T>*)srcGlobal.GetPhyAddr());
    } else {

                                                       ;
    }
}
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h" 2


namespace AscendC {
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParams& loadDataParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcLocal,
    const LoadData2DParams& loadDataParams);
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParamsV2& loadDataParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcLocal,
    const LoadData2DParamsV2& loadDataParams);
# 105 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG,
    typename U = PrimT<T>, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV1<U>& loadDataParams);
# 137 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG,
    typename U = PrimT<T>, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2<U>& loadDataParams);
# 169 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2Pro& loadDataParams);
# 188 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTranspose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParams& loadDataParams);
# 205 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTranspose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParamsV2& loadDataParams);
# 229 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename DstT, typename Src0T, typename Src1T>
[aicore] __inline__ __attribute__((always_inline)) void Mmad(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const MmadParams& mmadParams);

template <typename DstT, typename Src0T, typename Src1T, typename BiasT>
[aicore] __inline__ __attribute__((always_inline)) void Mmad(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const LocalTensor<BiasT>& biasLocal, const MmadParams& mmadParams);


template <typename T = int32_t, typename U = int8_t,
    typename std::enable_if<IsSameType<PrimT<T>, int32_t>::value, bool>::type = true,
    typename std::enable_if<IsSameType<PrimT<U>, int8_t>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void MmadWithSparse(const LocalTensor<T>& dstLocal, const LocalTensor<U>& fmLocal,
    const LocalTensor<U>& filterLocal, const MmadParams& mmadParams);

template <typename T = int8_t, typename U = uint8_t,
    typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value, bool>::type = true,
    typename std::enable_if<IsSameType<PrimT<U>, uint8_t>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithSparse(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<U> &idxLocal, const LoadData2dParams &loadDataParam);
# 260 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void BroadCastVecToMM(const LocalTensor<T> &dstLocal,
    const LocalTensor<U> &srcLocal, const int32_t blockCount, const uint8_t blockLen, const uint8_t srcGap,
    const uint8_t dstGap);
# 277 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T, typename U = PrimT<T>,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void InitConstValue(const LocalTensor<T> &dstLocal,
    const InitConstValueParams<U> &initConstValueParams);
# 289 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataPaddingValue(const T padValue);
# 302 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
[aicore] __inline__ __attribute__((always_inline)) void SetFmatrix(uint16_t l1H, uint16_t l1W,
    const uint8_t padList[4], const FmatrixMode &fmatrixMode);
# 313 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataBoundary(uint32_t boundaryValue);

[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataRepeat(const LoadDataRepeatParam& repeatParams);
# 334 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadImageToLocal(const LocalTensor<T>& dstLocal, const LoadImageToLocalParams& loadDataParams);
# 346 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzip(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcLocal);
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_gemm_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_gemm_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_gemm_base_impl.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_gemm_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_conv2d.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_conv2d.h"
namespace AscendC {
enum class LoopMode : uint8_t {
    MODE_NM = 0,
    MODE_MN = 1,
    MODE_KM = 2,
    MODE_KN = 3
};

struct Conv2dTilling {
    const uint32_t blockSize = 16;
    LoopMode loopMode = LoopMode::MODE_NM;

    uint32_t c0Size = 32;
    uint32_t dTypeSize = 1;

    uint32_t strideH = 0;
    uint32_t strideW = 0;
    uint32_t dilationH = 0;
    uint32_t dilationW = 0;
    uint32_t hi = 0;
    uint32_t wi = 0;
    uint32_t ho = 0;
    uint32_t wo = 0;

    uint32_t height = 0;
    uint32_t width = 0;

    uint32_t howo = 0;

    uint32_t mNum = 0;
    uint32_t nNum = 0;
    uint32_t kNum = 0;

    uint32_t mBlockNum = 0;
    uint32_t kBlockNum = 0;
    uint32_t nBlockNum = 0;

    uint32_t roundM = 0;
    uint32_t roundN = 0;
    uint32_t roundK = 0;

    uint32_t mTileBlock = 0;
    uint32_t nTileBlock = 0;
    uint32_t kTileBlock = 0;

    uint32_t mIterNum = 0;
    uint32_t nIterNum = 0;
    uint32_t kIterNum = 0;

    uint32_t mTileNums = 0;

    bool mHasTail = false;
    bool nHasTail = false;
    bool kHasTail = false;

    uint32_t kTailBlock = 0;
    uint32_t mTailBlock = 0;
    uint32_t nTailBlock = 0;

    uint32_t mTailNums = 0;
};

struct Conv2dParams {
    [aicore] Conv2dParams() {}

    [aicore] Conv2dParams(const uint32_t imgShapeIn[CONV2D_IMG_SIZE],
        const uint32_t kernelShapeIn[CONV2D_KERNEL_SIZE], const uint32_t strideIn[CONV2D_STRIDE], const uint32_t cinIn,
        const uint32_t coutIn, const uint32_t padListIn[CONV2D_PAD], const uint32_t dilationIn[CONV2D_DILATION],
        const uint32_t initYIn, const bool partialSumIn)
    {
        for (int32_t i = 0; i < CONV2D_IMG_SIZE; ++i) {
            imgShape[i] = imgShapeIn[i];
        }
        for (int32_t i = 0; i < CONV2D_KERNEL_SIZE; ++i) {
            kernelShape[i] = kernelShapeIn[i];
        }
        for (int32_t i = 0; i < CONV2D_STRIDE; ++i) {
            stride[i] = strideIn[i];
        }
        cin = cinIn;
        cout = coutIn;
        for (int32_t i = 0; i < CONV2D_PAD; ++i) {
            padList[i] = padListIn[i];
        }
        for (int32_t i = 0; i < CONV2D_DILATION; ++i) {
            dilation[i] = dilationIn[i];
        }
        initY = initYIn;
        partialSum = partialSumIn;
    }

    uint32_t imgShape[CONV2D_IMG_SIZE] = { 0 };
    uint32_t kernelShape[CONV2D_KERNEL_SIZE] = { 0 };
    uint32_t stride[CONV2D_STRIDE] = { 0 };
    uint32_t cin = 0;
    uint32_t cout = 0;
    uint32_t padList[CONV2D_PAD] = { 0 };
    uint32_t dilation[CONV2D_DILATION] = { 0 };
    uint32_t initY = 0;
    bool partialSum = false;
};

struct GemmTiling {
    [aicore] GemmTiling()
    {
        mIterNum = 1;
        nIterNum = 1;
        kIterNum = 1;
        loopMode = LoopMode::MODE_NM;
    }

    const uint32_t blockSize = 16;
    LoopMode loopMode = LoopMode::MODE_NM;
    uint32_t mNum = 0;
    uint32_t nNum = 0;
    uint32_t kNum = 0;
    uint32_t roundM = 0;
    uint32_t roundN = 0;
    uint32_t roundK = 0;
    uint32_t c0Size = 32;
    uint32_t dtypeSize = 1;
    uint32_t mBlockNum = 0;
    uint32_t nBlockNum = 0;
    uint32_t kBlockNum = 0;
    uint32_t mIterNum = 0;
    uint32_t nIterNum = 0;
    uint32_t kIterNum = 0;
    uint32_t mTileBlock = 0;
    uint32_t nTileBlock = 0;
    uint32_t kTileBlock = 0;
    uint32_t kTailBlock = 0;
    uint32_t mTailBlock = 0;
    uint32_t nTailBlock = 0;
    bool kHasTail = false;
    bool mHasTail = false;
    bool nHasTail = false;
    bool kHasTailEle = false;
    uint32_t kTailEle = 0;
};
}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_gemm_base_impl.h" 2


namespace AscendC {
# 136 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_gemm_base_impl.h"
[aicore] __inline__ __attribute__((always_inline)) void CalculateGemmTiling(GemmTiling& tilling)
{
    tilling.mIterNum = 1;
    tilling.nIterNum = 1;
    tilling.kIterNum = DivCeil(tilling.kBlockNum, tilling.kTileBlock);

    tilling.mTileBlock = DivCeil(tilling.mBlockNum, tilling.mIterNum);
    tilling.nTileBlock = DivCeil(tilling.nBlockNum, tilling.nIterNum);

    tilling.kTailBlock = tilling.kBlockNum - (tilling.kIterNum - 1) * tilling.kTileBlock;
    tilling.mTailBlock = tilling.mBlockNum - (tilling.mIterNum - 1) * tilling.mTileBlock;
    tilling.nTailBlock = tilling.nBlockNum - (tilling.nIterNum - 1) * tilling.nTileBlock;

    tilling.kHasTail = tilling.kTailBlock != tilling.kTileBlock;
    tilling.kHasTailEle = tilling.roundK != tilling.kNum;
    tilling.kTailEle = tilling.kNum % (tilling.kTileBlock * tilling.c0Size);

    if (tilling.mNum != tilling.mTileBlock * tilling.blockSize) {
        tilling.mHasTail = true;
    } else {
        tilling.mHasTail = false;
    }
    tilling.nHasTail = tilling.nTileBlock != tilling.nTailBlock;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0B(uint32_t kBlocks, uint32_t nBlocks, GemmTiling tilling, uint32_t i, uint32_t j,
    const LocalTensor<T>& src1Local, const LocalTensor<T>& L0b)
{
    if (tilling.nIterNum == 1) {
        uint32_t wSize = tilling.blockSize * tilling.c0Size;
        uint32_t wIdx = (i * tilling.kTileBlock * tilling.nBlockNum + j * tilling.nTileBlock) * wSize;
        LoadData2DParams params;
        params.startIndex = 0;
        params.repeatTimes = kBlocks * nBlocks;
        params.srcStride = 1;
        LoadDataImpl(L0b, src1Local[wIdx], params);
    } else {

        for (size_t index = 0; index < kBlocks; ++index) {
            uint32_t wSize = j * tilling.nTileBlock * tilling.blockSize * tilling.c0Size;
            uint32_t wIdx =
                (i * tilling.kTileBlock + index) * tilling.nBlockNum * tilling.blockSize * tilling.c0Size + wSize;
            uint32_t l0bIdx = index * nBlocks * tilling.blockSize * tilling.c0Size;
            LoadData2DParams params;
            params.startIndex = 0;
            params.repeatTimes = nBlocks;
            params.srcStride = 1;
            LoadDataImpl(L0b[l0bIdx], src1Local[wIdx], params);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0A(uint32_t kBlocks, uint32_t mBlocks, GemmTiling tilling, uint32_t i, uint32_t t,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& L0a)
{
    if (kBlocks == 1) {
        uint32_t l1aSize = i * tilling.kTileBlock * tilling.mBlockNum * tilling.blockSize * tilling.c0Size;
        uint32_t l1aOffset = t * tilling.mTileBlock * tilling.blockSize * tilling.c0Size + l1aSize;
        LoadData2DParams params;
        params.startIndex = 0;
        params.repeatTimes = mBlocks;
        params.srcStride = 1;
        LoadDataImpl(L0a, src0Local[l1aOffset], params);
    } else {

        for (size_t index = 0; index < mBlocks; index++) {
            uint32_t l0aOffset = index * kBlocks * tilling.blockSize * tilling.c0Size;
            uint32_t l1aOffset = (t * tilling.mTileBlock + index) * tilling.blockSize * tilling.c0Size +
                i * tilling.kTileBlock * tilling.mBlockNum * tilling.blockSize * tilling.c0Size;
            LoadData2DParams params;
            params.startIndex = 0;
            params.repeatTimes = kBlocks;
            params.srcStride = tilling.mBlockNum;
            LoadDataImpl(L0a[l0aOffset], src0Local[l1aOffset], params);
        }
    }
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void MmadFunc(const LocalTensor<src0_T>& L0a, const LocalTensor<src1_T>& L0b,
    const LocalTensor<dst_T>& L0c, int32_t initValue, GemmTiling tilling, size_t i)
{
    MmadParams mmadParams;
    mmadParams.m = tilling.mTileBlock * tilling.blockSize;
    mmadParams.n = tilling.nTileBlock * tilling.blockSize;
    mmadParams.isBias = 1;

    if (tilling.kIterNum == 1) {
        mmadParams.k = tilling.kNum;
        mmadParams.isBias = initValue;
    } else if (initValue == 1 && tilling.kHasTailEle) {
        if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailEle;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    } else if (initValue != 1 && tilling.kHasTailEle) {
        if (i == 0) {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
            mmadParams.isBias = 0;
        } else if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailEle;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    } else if (initValue == 1 && !tilling.kHasTailEle) {
        if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailBlock * tilling.c0Size;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    } else {
        if (i == 0) {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
            mmadParams.isBias = 0;
        } else if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailBlock * tilling.c0Size;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    }
    MmadImpl(L0c, L0a, L0b, mmadParams);
}

template <typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GetPingPongBuffer(LocalTensor<src0_T>& L0aPing, LocalTensor<src0_T>& L0aPong,
    LocalTensor<src1_T>& L0bPing, LocalTensor<src1_T>& L0bPong)
{

    TBuffAddr tbufaPing;
    tbufaPing.logicPos = (uint8_t)TPosition::A2;
    L0aPing.SetAddr(tbufaPing);
    L0aPing.InitBuffer(0, TOTAL_L0A_SIZE / 2 / sizeof(PrimT<src0_T>));

    TBuffAddr tbufaPong;
    tbufaPong.logicPos = (uint8_t)TPosition::A2;
    L0aPong.SetAddr(tbufaPong);
    L0aPong.InitBuffer(TOTAL_L0A_SIZE / 2, TOTAL_L0A_SIZE / 2 / sizeof(PrimT<src0_T>));


    TBuffAddr tbufbPing;
    tbufbPing.logicPos = (uint8_t)TPosition::B2;
    L0bPing.SetAddr(tbufbPing);
    L0bPing.InitBuffer(0, TOTAL_L0B_SIZE / 2 / sizeof(PrimT<src1_T>));

    TBuffAddr tbufbPong;
    tbufbPong.logicPos = (uint8_t)TPosition::B2;
    L0bPong.SetAddr(tbufbPong);
    L0bPong.InitBuffer(TOTAL_L0B_SIZE / 2, TOTAL_L0B_SIZE / 2 / sizeof(PrimT<src1_T>));
    return;
}

template <typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GetSingleThreadBuffer(LocalTensor<src0_T>& L0a, LocalTensor<src1_T>& L0b)
{

    TBuffAddr tbufa;
    tbufa.logicPos = (uint8_t)TPosition::A2;
    L0a.SetAddr(tbufa);
    L0a.InitBuffer(0, TOTAL_L0A_SIZE / sizeof(PrimT<src0_T>));


    TBuffAddr tbufb;
    tbufb.logicPos = (uint8_t)TPosition::B2;
    L0b.SetAddr(tbufb);
    L0b.InitBuffer(0, TOTAL_L0B_SIZE / sizeof(PrimT<src1_T>));
    return;
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecNmNopingpong(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    LocalTensor<src0_T> L0a;
    LocalTensor<src1_T> L0b;
    GetSingleThreadBuffer(L0a, L0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

            LoadL0B(kBlocks, tilling.nTileBlock, tilling, indexK, indexN, src1Local, L0b);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0A(kBlocks, tilling.mTileBlock, tilling, indexK, indexM, src0Local, L0a);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFunc(L0a, L0b, L0c, initValue, tilling, indexK);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecNmPingPong(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    uint32_t ping = 1;
    LocalTensor<src0_T> L0aPing;
    LocalTensor<src0_T> L0aPong;
    LocalTensor<src1_T> L0bPing;
    LocalTensor<src1_T> L0bPong;
    GetPingPongBuffer(L0aPing, L0aPong, L0bPing, L0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t i = 0; i < tilling.kIterNum; i++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (i == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1Local, L0bPing);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0Local, L0aPing);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(L0aPing, L0bPing, L0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1Local, L0bPong);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0Local, L0aPong);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(L0aPong, L0bPong, L0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }


    WaitFlag<HardEvent::M_MTE1>(eventId0);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId0);
    WaitFlag<HardEvent::M_MTE1>(eventId1);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId1);




}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecNm(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    uint32_t needL0Asize = tilling.roundM * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Bsize = tilling.roundN * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        GemmExecNmNopingpong(L0c, src0Local, src1Local, tilling, initValue);
        return;
    }
    GemmExecNmPingPong(L0c, src0Local, src1Local, tilling, initValue);
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecMnNopingpong(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    LocalTensor<src1_T> L0b;
    LocalTensor<src0_T> L0a;
    GetSingleThreadBuffer(L0a, L0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

            LoadL0A(kBlocks, tilling.mTileBlock, tilling, indexK, indexM, src0Local, L0a);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0B(kBlocks, tilling.nTileBlock, tilling, indexK, indexN, src1Local, L0b);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFunc(L0a, L0b, L0c, initValue, tilling, indexK);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecMnPingPong(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    uint32_t ping = 1;
    LocalTensor<src0_T> L0aPing;
    LocalTensor<src0_T> L0aPong;
    LocalTensor<src1_T> L0bPing;
    LocalTensor<src1_T> L0bPong;
    GetPingPongBuffer(L0aPing, L0aPong, L0bPing, L0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t i = 0; i < tilling.kIterNum; i++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (i == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0Local, L0aPing);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1Local, L0bPing);

                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(L0aPing, L0bPing, L0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0Local, L0aPong);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1Local, L0bPong);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(L0aPong, L0bPong, L0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }

    WaitFlag<HardEvent::M_MTE1>(eventId0);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId0);
    WaitFlag<HardEvent::M_MTE1>(eventId1);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId1);
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecMn(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    uint32_t needL0Bsize = tilling.roundN * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Asize = tilling.roundM * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        GemmExecMnNopingpong(L0c, src0Local, src1Local, tilling, initValue);
        return;
    }
    GemmExecMnPingPong(L0c, src0Local, src1Local, tilling, initValue);
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_gemm_intf.h" 2

namespace AscendC {

template <typename T> [aicore] __inline__ __attribute__((always_inline)) GemmTiling GetGemmTiling(uint32_t m, uint32_t k, uint32_t n);
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_gemm_intf.h"
template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Gemm(const LocalTensor<dst_T>& dstLocal, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, const uint32_t m, const uint32_t k, const uint32_t n, GemmTiling tilling,
    bool partialsum = true, int32_t initValue = 0);
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_fixpipe.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_fixpipe.h"
namespace AscendC {
enum class CO2Layout : uint8_t {
    NZ = 0,
    ROW_MAJOR,
    COLUMN_MAJOR
};

struct FixpipeConfig {
    CO2Layout format;
};

constexpr FixpipeConfig CFG_NZ = {CO2Layout::NZ};
constexpr FixpipeConfig CFG_ROW_MAJOR = {CO2Layout::ROW_MAJOR};
constexpr FixpipeConfig CFG_COLUMN_MAJOR = {CO2Layout::COLUMN_MAJOR};

struct FixpipeParamsV220 {
    [aicore] FixpipeParamsV220() {}

    [aicore] FixpipeParamsV220(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn, const bool reluEnIn)
        : nSize(nSizeIn),
          mSize(mSizeIn),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn),
          reluEn(reluEnIn)
    {}

    [aicore] FixpipeParamsV220(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn, const bool reluEnIn, const QuantMode_t quantPreIn, const int64_t deqScalarIn,
        const uint16_t ndNumIn, const uint16_t srcNdStrideIn, const uint16_t dstNdStrideIn, const uint8_t unitFlagIn)
        : nSize(nSizeIn),
          mSize(mSizeIn),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn),
          reluEn(reluEnIn),
          quantPre(quantPreIn),
          deqScalar(deqScalarIn),
          ndNum(ndNumIn),
          srcNdStride(srcNdStrideIn),
          dstNdStride(dstNdStrideIn),
          unitFlag(unitFlagIn)
    {}

    [aicore] FixpipeParamsV220(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn, const bool reluEnIn, const QuantMode_t quantPreIn, const int64_t deqScalarIn,
        const uint16_t ndNumIn, const uint16_t srcNdStrideIn, const uint16_t dstNdStrideIn, const uint8_t unitFlagIn,
        const bool isChannelSplitIn)
        : nSize(nSizeIn),
          mSize(mSizeIn),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn),
          reluEn(reluEnIn),
          quantPre(quantPreIn),
          deqScalar(deqScalarIn),
          ndNum(ndNumIn),
          srcNdStride(srcNdStrideIn),
          dstNdStride(dstNdStrideIn),
          unitFlag(unitFlagIn),
          isChannelSplit(isChannelSplitIn)
    {}

    uint16_t nSize = 0;
    uint16_t mSize = 0;
    uint16_t srcStride = 0;
    uint32_t dstStride = 0;

    QuantMode_t quantPre = QuantMode_t::NoQuant;
    uint64_t deqScalar;

    uint16_t ndNum = 1;
    uint16_t srcNdStride = 0;
    uint16_t dstNdStride = 0;
    bool reluEn = false;
    uint8_t unitFlag = 0;
    bool isChannelSplit = false;
};

using FixpipeParamsM300 = FixpipeParamsV220;
using FixpipeParamsM310 = FixpipeParamsV220;
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h" 2






# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_set_spr_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_set_spr_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SetQuantPreImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_quant_pre(config);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetNdParaImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_nd_para(config);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetFpcImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_fpc(config);
    }
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_impl.h" 2

namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfigImpl(const LocalTensor<T> &reluPre, const LocalTensor<T> &quantPre,
    bool isUnitFlag = false)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckTensorPos<T>(reluPre, Hardware::FIXBUF, "reluPre", "C2PIPE2GM", "SetFixPipeConfig");
        CheckTensorPos<T>(quantPre, Hardware::FIXBUF, "quantPre", "C2PIPE2GM", "SetFixPipeConfig");
        uint64_t config = 0;
        config = config | ((uint64_t)reluPre.GetPhyAddr() >> 6);
        config = config | (((uint64_t)quantPre.GetPhyAddr() >> 7) << 8);
        config = config | ((uint64_t)isUnitFlag << 63);
        set_fpc(config);
    }
}

template <typename T, bool setRelu = false>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfigImpl(const LocalTensor<T> &preTensor, bool isUnitFlag = false)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckTensorPos<T>(preTensor, Hardware::FIXBUF, "preTensor", "C2PIPE2GM", "SetFixPipeConfig");
        uint64_t config = 0;
        if constexpr (setRelu) {
            config = config | ((uint64_t)preTensor.GetPhyAddr() >> 6);
        } else {
            config = config | (((uint64_t)preTensor.GetPhyAddr() >> 7) << 8);
        }
        config = config | ((uint64_t)isUnitFlag << 63);
        set_fpc(config);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipeNz2ndFlagImpl(uint16_t ndNum, uint16_t srcNdStride, uint16_t dstNdStride)
{
    if constexpr(g_coreType == AscendC::AIC) {
                                                                                                  ;
        uint64_t config = 0;
        config = config | ((uint64_t)ndNum);
        config = config | ((uint64_t)srcNdStride << 16);
        config = config | ((uint64_t)dstNdStride << 32);
        set_nd_para(config);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipePreQuantFlagImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_quant_pre(config);
    }
}






struct FixpipeTiling {
    uint16_t nIterNum = 0;
    uint16_t nSize = 0;
    bool isDb = false;
    uint16_t tailNSize = 0;
};


[aicore] __inline__ __attribute__((always_inline)) FixpipeTiling GenFixpipeTiling(uint16_t n)
{
    FixpipeTiling tiling;

    uint16_t maxDeqNums = 256;
    if (n <= maxDeqNums) {
        tiling.nIterNum = 1;
        tiling.nSize = n;
        tiling.isDb = false;
        tiling.tailNSize = 0;
    } else {
        tiling.isDb = true;
        uint16_t dbMaxDeqNums = maxDeqNums / 2;
        tiling.nIterNum = n / dbMaxDeqNums;
        tiling.nSize = dbMaxDeqNums;
        tiling.tailNSize = n % dbMaxDeqNums;
    }
    return tiling;
}

template <typename SrcT> struct FixpipeInfoParams {
    [aicore] __inline__ __attribute__((always_inline)) FixpipeInfoParams() {}

    [aicore] __inline__ __attribute__((always_inline)) FixpipeInfoParams(const FixpipeParams<SrcT>& intriParams, const uint8_t dstByteSize)
    {
        dstTypeSize = dstByteSize;
        srcTypeSize = B32_BYTE_SIZE;
        howo = (intriParams.burstLen * ONE_BLK_SIZE / srcTypeSize) / BLOCK_CUBE;
        roundHowo = DivCeil(howo, BLOCK_CUBE) * BLOCK_CUBE;
        fracLen = BLOCK_CUBE;
        c0 = fracLen;




        n = intriParams.cburstNum * BLOCK_CUBE;
        m = howo;



        srcStride = intriParams.srcStride * BLOCK_CUBE + roundHowo;




        if (intriParams.nz2ndParams.nz2ndEn) {

            dstStride = intriParams.dstStride;



              ;
            n = intriParams.nz2ndParams.originalNSize;
        } else {


            dstStride = intriParams.dstStride + intriParams.burstLen * dstTypeSize / srcTypeSize;
        }

        sid = 0;
        quantPre = intriParams.quantParams.quantPre;
        reluEn = intriParams.reluEn;
        nz2ndEn = intriParams.nz2ndParams.nz2ndEn;
        ndNum = intriParams.nz2ndParams.ndNum;
        srcNdStride = intriParams.nz2ndParams.srcNdStride;
        dstNdStride = intriParams.nz2ndParams.dstNdStride;


        if (intriParams.quantParams.quantPre == QuantMode_t::DEQF16 ||
            intriParams.quantParams.quantPre == QuantMode_t::QF322B8_PRE ||
            intriParams.quantParams.quantPre == QuantMode_t::REQ8) {
            deqScalar = intriParams.quantParams.deqScalar;
        }

        unitFlag = intriParams.unitFlag;
    }


    uint8_t dstTypeSize = 0;
    uint8_t srcTypeSize = 0;
    uint16_t howo = 0;
    uint16_t roundHowo = 0;
    uint8_t fracLen = 0;
    uint8_t c0 = 0;
    uint16_t n = 0;
    uint16_t m = 0;
    uint16_t srcStride = 0;
    uint32_t dstStride = 0;
    uint16_t burstLen = 0;
    uint8_t sid = 0;
    bool channelSplit = false;
    uint8_t unitFlag = 0;


    QuantMode_t quantPre = QuantMode_t::NoQuant;
    __attribute__((cce_cube_buff)) uint64_t* cbufWorkspace;
    uint64_t deqScalar = 0;

    bool reluEn = false;

    bool nz2ndEn = false;
    uint16_t ndNum = 1;
    uint16_t srcNdStride = 0;
    uint16_t dstNdStride = 0;

    FixpipeTiling tiling;
};


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) T *src, FixpipeInfoParams<T> &fixpipeInfo)
{
                                                                                                          ;
}

template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src, FixpipeInfoParams<SrcT>& fixpipeInfo)
{
                                                                                                         ;






    if (fixpipeInfo.quantPre == QuantMode_t::VDEQF16 || fixpipeInfo.quantPre == QuantMode_t::VQF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::VREQ8) {
        fixpipeInfo.tiling = GenFixpipeTiling(fixpipeInfo.n);
        for (uint16_t i = 0; i < fixpipeInfo.tiling.nIterNum; ++i) {
            FixpipeL0C2L1ImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.nSize, i);
        }

        if (fixpipeInfo.tiling.tailNSize > 0) {
            FixpipeL0C2L1ImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.tailNSize, fixpipeInfo.tiling.nIterNum);
        }
        return;
    }





    if (fixpipeInfo.quantPre == QuantMode_t::DEQF16 || fixpipeInfo.quantPre == QuantMode_t::QF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::REQ8) {

        SetQuantPreImpl(fixpipeInfo.deqScalar);
    }
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToL1(dst, src, fixpipeInfo, fixpipeInfo.n);
}

template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2GMImpl(__attribute__((cce_global)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src, FixpipeInfoParams<SrcT>& fixpipeInfo)
{
    if (fixpipeInfo.nz2ndEn) {
        uint64_t ndPara = static_cast<uint64_t>(fixpipeInfo.dstNdStride) << 32;
        ndPara |= static_cast<uint64_t>(fixpipeInfo.srcNdStride) << 16;
        ndPara |= static_cast<uint64_t>(fixpipeInfo.ndNum);
        SetNdParaImpl(ndPara);
    }






    if (fixpipeInfo.quantPre == QuantMode_t::VDEQF16 || fixpipeInfo.quantPre == QuantMode_t::VQF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::VREQ8) {
        fixpipeInfo.tiling = GenFixpipeTiling(fixpipeInfo.n);
        for (uint16_t i = 0; i < fixpipeInfo.tiling.nIterNum; ++i) {
            FixpipeL0C2GMImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.nSize, i);
        }

        if (fixpipeInfo.tiling.tailNSize > 0) {
            FixpipeL0C2GMImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.tailNSize, fixpipeInfo.tiling.nIterNum);
        }
        return;
    }






    if (fixpipeInfo.quantPre == QuantMode_t::DEQF16 || fixpipeInfo.quantPre == QuantMode_t::QF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::REQ8) {
        SetQuantPreImpl(fixpipeInfo.deqScalar);
    }
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut(dst, src, fixpipeInfo, fixpipeInfo.n);
}

template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1ImplN(__attribute__((cce_cube_buff)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src,
    const FixpipeInfoParams<SrcT>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(fixpipeInfo, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToL1(dst, src, fixpipeInfo, calNSize, nIterIndex);
}

template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2GMImplN(__attribute__((cce_global)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src,
    const FixpipeInfoParams<SrcT>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(fixpipeInfo, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut(dst, src, fixpipeInfo, calNSize, nIterIndex);
}



template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0cToL1(__attribute__((cce_cube_buff)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src,
    const FixpipeInfoParams<SrcT>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t cburstNum = fixpipeInfo.tiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * fixpipeInfo.srcStride * fixpipeInfo.c0;
    uint32_t dstOffset = 0;
    if (fixpipeInfo.nz2ndEn) {
        dstOffset = nIterIndex * fixpipeInfo.tiling.nSize;
    } else {
        dstOffset = cburstNum * nIterIndex * fixpipeInfo.dstStride * 32 / sizeof(DstT);
    }



    return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), fixpipeInfo.sid,
        calNSize, fixpipeInfo.m, fixpipeInfo.dstStride, fixpipeInfo.srcStride, fixpipeInfo.unitFlag,
        fixpipeInfo.quantPre, static_cast<uint8_t>(fixpipeInfo.reluEn), fixpipeInfo.channelSplit, fixpipeInfo.nz2ndEn);
}

template <typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) uint64_t GetGMLen(const FixpipeInfoParams<SrcT>& fixpipeInfo,
                                    const uint16_t& calNSize, const uint16_t& dstEleSize)
{
    constexpr uint16_t dstStrideUnit = 32;
    constexpr uint16_t fractalNsize = 16;
    uint64_t cburstNum = calNSize / fractalNsize;
    uint64_t gmLen = (cburstNum - 1) * fixpipeInfo.dstStride * dstStrideUnit +
        fixpipeInfo.m * fractalNsize * dstEleSize;
    if (fixpipeInfo.nz2ndEn) {

        gmLen = (static_cast<uint64_t>(fixpipeInfo.ndNum) - 1) * dstEleSize * fixpipeInfo.dstNdStride +
            (fixpipeInfo.m - 1) * fixpipeInfo.dstStride * dstEleSize + cburstNum * fractalNsize * dstEleSize;
    }
    return gmLen;
}



template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0cToOut(__attribute__((cce_global)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src,
    const FixpipeInfoParams<SrcT>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t cburstNum = fixpipeInfo.tiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * fixpipeInfo.srcStride * fixpipeInfo.c0;
    uint32_t dstOffset = 0;
    if (fixpipeInfo.nz2ndEn) {
        dstOffset = nIterIndex * fixpipeInfo.tiling.nSize;
    } else {
        dstOffset = cburstNum * nIterIndex * fixpipeInfo.dstStride * 32 / sizeof(DstT);
    }
    if constexpr (g_gm_overflow_check) {
        bool isSrc = false;
        uint16_t dstEleSize = sizeof(DstT);
        uint64_t gmLen = GetGMLen(fixpipeInfo, calNSize, dstEleSize);
        AscendCUtils::CheckGmMemOverflow((__attribute__((cce_global)) DstT*)(dst + dstOffset), isSrc, gmLen);
    }


    return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), fixpipeInfo.sid,
        calNSize, fixpipeInfo.m, fixpipeInfo.dstStride, fixpipeInfo.srcStride, fixpipeInfo.unitFlag,
        fixpipeInfo.quantPre, static_cast<uint8_t>(fixpipeInfo.reluEn), fixpipeInfo.channelSplit, fixpipeInfo.nz2ndEn);
}

template <typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void CopyDeqTensorToFbuf(const FixpipeInfoParams<SrcT>& fixpipeInfo, uint16_t calNSize,
    uint16_t nIterIndex)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t deqDataSize = DivCeil(calNSize * sizeof(uint64_t), 128) * 128;
    __attribute__((cce_fixpipe_buff)) uint64_t* deqTensorTempBuf =
        AscendCUtils::GetTemporaryFbBufferAddr<uint64_t>(0, deqDataSize / sizeof(uint64_t));
    uint32_t deqValueOffset = nIterIndex * fixpipeInfo.tiling.nSize;

    uint16_t fbufBurstLen = deqDataSize / 128;
    copy_cbuf_to_fbuf(deqTensorTempBuf, fixpipeInfo.cbufWorkspace + deqValueOffset, 1, fbufBurstLen, 0, 0);

    uint64_t deqTensorAddr = (((uint64_t)deqTensorTempBuf) >> (uint64_t)7) << 8;
    set_fpc(deqTensorAddr);
    AscendCUtils::FreeTemporaryFbBuffer<uint64_t>(deqTensorTempBuf);
}

template <typename DstT, typename SrcT, typename ParamT = PrimT<SrcT>,
    typename std::enable_if<IsSameType<PrimT<SrcT>, ParamT>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT>& dstLocal, const LocalTensor<SrcT>& srcLocal,
    const FixpipeParams<ParamT>& intriParams)
{
    FixpipeInfoParams<PrimT<SrcT>> fixpipeInfo(intriParams, sizeof(PrimT<DstT>));
    FixpipeL0C2L1Impl((__attribute__((cce_cube_buff)) PrimT<DstT>*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<SrcT>*)srcLocal.GetPhyAddr(), fixpipeInfo);
}

template <typename DstT, typename SrcT, typename BufT, typename ParamT = PrimT<SrcT>,
    typename std::enable_if<IsSameType<PrimT<SrcT>, ParamT>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT>& dstLocal, const LocalTensor<SrcT>& srcLocal,
    const LocalTensor<BufT>& cbufWorkspace, const FixpipeParams<ParamT>& intriParams)
{
    FixpipeInfoParams<PrimT<SrcT>> fixpipeInfo(intriParams, sizeof(PrimT<DstT>));
    fixpipeInfo.cbufWorkspace = (__attribute__((cce_cube_buff)) uint64_t*)cbufWorkspace.GetPhyAddr();
    FixpipeL0C2L1Impl((__attribute__((cce_cube_buff)) PrimT<DstT>*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<SrcT>*)srcLocal.GetPhyAddr(), fixpipeInfo);
}


template <typename DstT, typename SrcT, typename ParamT = PrimT<SrcT>,
    typename std::enable_if<IsSameType<PrimT<SrcT>, ParamT>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT>& dstGlobal, const LocalTensor<SrcT>& srcLocal,
    const FixpipeParams<ParamT>& intriParams)
{







    FixpipeInfoParams<PrimT<SrcT>> fixpipeInfo(intriParams, sizeof(PrimT<DstT>));

    FixpipeL0C2GMImpl((__attribute__((cce_global)) PrimT<DstT>*)dstGlobal.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<SrcT>*)srcLocal.GetPhyAddr(), fixpipeInfo);






}


template <typename DstT, typename SrcT, typename BufT, typename ParamT = PrimT<SrcT>,
    typename std::enable_if<IsSameType<PrimT<SrcT>, ParamT>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT> &dstGlobal, const LocalTensor<SrcT> &srcLocal,
    const LocalTensor<BufT> &cbufWorkspace, const FixpipeParams<ParamT> &intriParams)
{
    FixpipeInfoParams<PrimT<SrcT>> fixpipeInfo(intriParams, sizeof(PrimT<DstT>));
    fixpipeInfo.cbufWorkspace = (__attribute__((cce_cube_buff)) uint64_t *)cbufWorkspace.GetPhyAddr();
    FixpipeL0C2GMImpl((__attribute__((cce_global)) PrimT<DstT>*)dstGlobal.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<SrcT>*)srcLocal.GetPhyAddr(), fixpipeInfo);
}
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_v2_impl.h" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_v2_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeClipReluImpl(uint64_t config)
{
    (void)(config);
                                                           ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeAddrImpl(const LocalTensor<T> &eleWiseTensor, uint16_t c0ChStride)
{
                                                       ;
}




const uint32_t L0C_SRC_ALIGN = 16 * sizeof(float);

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void CheckCommonFixpipeParam(__attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &params)
{
# 95 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_v2_impl.h"
}

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void CheckFixpipeL0C2L1Param(__attribute__((cce_cube_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &params)
{
    CheckCommonFixpipeParam<DstT, SrcT, config>(src, params);
                                                                                                                   ;

                                                                              ;

                                                                             ;





                                     ;
}

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void CheckFixpipeL0C2GMParam(__attribute__((cce_global)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &params)
{
    CheckCommonFixpipeParam<DstT, SrcT, config>(src, params);





                                                                                                                ;
    if constexpr(IsSameType<SrcT, float>::value && IsSameType<DstT, float>::value) {

                                                                                               ;
    } else if constexpr(IsSameType<SrcT, int32_t>::value && IsSameType<DstT, int32_t>::value) {

                                                                                                   ;
    }
    if (params.isChannelSplit) {

                                                                                                                      ;

                                                                           ;
    }
}


struct FixpipeTilingV220 {
    uint16_t nIterNum = 0;
    uint16_t nSize = 0;
    bool isDb = false;
    uint16_t tailNSize = 0;
};


[aicore] __inline__ __attribute__((always_inline)) FixpipeTilingV220 GenFixpipeTilingV220(uint16_t n)
{
    FixpipeTilingV220 tiling;

    uint16_t maxDeqNums = 256;
    if (n <= maxDeqNums) {
        tiling.nIterNum = 1;
        tiling.nSize = n;
        tiling.isDb = false;
        tiling.tailNSize = 0;
    } else {
        tiling.isDb = true;
        uint16_t dbMaxDeqNums = maxDeqNums / 2;
        tiling.nIterNum = n / dbMaxDeqNums;
        tiling.nSize = dbMaxDeqNums;
        tiling.tailNSize = n % dbMaxDeqNums;
    }
    return tiling;
}

[aicore] __inline__ __attribute__((always_inline)) void CopyDeqTensorToFbuf(
    __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize, uint16_t nIterIndex)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t deqDataSize = DivCeil(calNSize * sizeof(uint64_t), 128) * 128;
    __attribute__((cce_fixpipe_buff)) uint64_t *deqTensorTempBuf =
        AscendCUtils::GetTemporaryFbBufferAddr<uint64_t>(0, deqDataSize / sizeof(uint64_t));
    uint32_t deqValueOffset = nIterIndex * fixpipeTiling.nSize;

    uint16_t fbufBurstLen = deqDataSize / 128;
    copy_cbuf_to_fbuf(deqTensorTempBuf, cbufWorkspace + deqValueOffset, 1, fbufBurstLen, 0, 0);

    uint64_t deqTensorAddr = (((uint64_t)deqTensorTempBuf) >> (uint64_t)7) << 8;
    set_fpc(deqTensorAddr);
    AscendCUtils::FreeTemporaryFbBuffer<uint64_t>(deqTensorTempBuf);
}

template <typename T, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) T *src, const FixpipeParamsV220 &intriParams)
{


                                    ;
}

template <typename T, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(
    __attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) T *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{


                                    ;
}

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2UBImpl(__attribute__((cce_unif_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &intriParams)
{
                                                                                        ;
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2UBImpl(
    __attribute__((cce_unif_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
                                                                                        ;
}

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2L1Param<DstT, SrcT, config>(dst, src, intriParams);





    if (intriParams.quantPre == QuantMode_t::DEQF16 || intriParams.quantPre == QuantMode_t::QF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::REQ8) {

        SetQuantPreImpl(intriParams.deqScalar);
    }
    PipeBarrier<PIPE_FIX>();
    FixpipeTilingV220 fixpipeTiling;

    FixpipeL0cToL1<DstT, SrcT, config>(dst, src, intriParams, fixpipeTiling, intriParams.nSize);
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(
    __attribute__((cce_cube_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2L1Param<DstT, SrcT, config>(dst, src, intriParams);






    FixpipeTilingV220 fixpipeTiling = GenFixpipeTilingV220(intriParams.nSize);
    if (intriParams.quantPre == QuantMode_t::VDEQF16 || intriParams.quantPre == QuantMode_t::VQF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::VREQ8) {
        for (uint16_t i = 0; i < fixpipeTiling.nIterNum; ++i) {
            FixpipeL0C2L1ImplN<DstT, SrcT, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.nSize, i);
        }

        if (fixpipeTiling.tailNSize > 0) {
            FixpipeL0C2L1ImplN<DstT, SrcT, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.tailNSize, fixpipeTiling.nIterNum);
        }
        return;
    }
}

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2GMImpl(__attribute__((cce_global)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2GMParam<DstT, SrcT, config>(dst, src, intriParams);
    if constexpr (config.format == CO2Layout::ROW_MAJOR) {
        uint64_t ndPara = static_cast<uint64_t>(intriParams.dstNdStride) << 32;
        ndPara |= static_cast<uint64_t>(intriParams.srcNdStride) << 16;
        ndPara |= static_cast<uint64_t>(intriParams.ndNum);
        SetNdParaImpl(ndPara);
    }
    FixpipeTilingV220 fixpipeTiling;





    if (intriParams.quantPre == QuantMode_t::DEQF16 || intriParams.quantPre == QuantMode_t::QF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::REQ8) {
        SetQuantPreImpl(intriParams.deqScalar);
    }
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut<DstT, SrcT, config>(dst, src, intriParams, fixpipeTiling, intriParams.nSize);
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2GMImpl(
    __attribute__((cce_global)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2GMParam<DstT, SrcT, config>(dst, src, intriParams);
    if constexpr (config.format == CO2Layout::ROW_MAJOR) {
        uint64_t ndPara = static_cast<uint64_t>(intriParams.dstNdStride) << 32;
        ndPara |= static_cast<uint64_t>(intriParams.srcNdStride) << 16;
        ndPara |= static_cast<uint64_t>(intriParams.ndNum);
        SetNdParaImpl(ndPara);
    }






    FixpipeTilingV220 fixpipeTiling = GenFixpipeTilingV220(intriParams.nSize);
    if (intriParams.quantPre == QuantMode_t::VDEQF16 || intriParams.quantPre == QuantMode_t::VQF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::VREQ8) {
        for (uint16_t i = 0; i < fixpipeTiling.nIterNum; ++i) {
            FixpipeL0C2GMImplN<DstT, SrcT, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.nSize, i);
        }

        if (fixpipeTiling.tailNSize > 0) {
            FixpipeL0C2GMImplN<DstT, SrcT, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.tailNSize, fixpipeTiling.nIterNum);
        }
        return;
    }
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1ImplN(__attribute__((cce_cube_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace,
    const FixpipeParamsV220 &intriParams, const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize,
    uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(cbufWorkspace, fixpipeTiling, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToL1<DstT, SrcT, config>(dst, src, intriParams, fixpipeTiling, calNSize, nIterIndex);
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2GMImplN(__attribute__((cce_global)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace,
    const FixpipeParamsV220 &intriParams, const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize,
    uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(cbufWorkspace, fixpipeTiling, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut<DstT, SrcT, config>(dst, src, intriParams, fixpipeTiling, calNSize, nIterIndex);
}



template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0cToL1(__attribute__((cce_cube_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &intriParams,
    const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    uint16_t cburstNum = fixpipeTiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * intriParams.srcStride * BLOCK_CUBE;
    uint32_t dstOffset = cburstNum * nIterIndex * intriParams.dstStride * 32 / sizeof(DstT);


    if constexpr(g_coreType == AscendC::AIC) {
        switch (intriParams.quantPre) {
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322F16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322BF16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::DEQF16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VDEQF16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::QF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VQF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::REQ8, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VREQ8, static_cast<uint8_t>(intriParams.reluEn), false, false);
            default:
                                                                                            ;
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) uint64_t GetGMLength(
    const FixpipeParamsV220 &intriParams, const uint16_t &calNSize, const uint16_t &dstEleSize, const bool &nz2ndEn)
{
    constexpr uint16_t dstStrideUnit = 32;
    constexpr uint16_t fractalNsize = 16;
    uint64_t cburstNum = calNSize / fractalNsize;
    uint64_t gmLen =
        (cburstNum - 1) * intriParams.dstStride * dstStrideUnit + intriParams.mSize * fractalNsize * dstEleSize;
    if (nz2ndEn) {
        gmLen = (static_cast<uint64_t>(intriParams.ndNum) - 1) * dstEleSize * intriParams.dstNdStride +
                (intriParams.mSize - 1) * intriParams.dstStride * dstEleSize + cburstNum * fractalNsize * dstEleSize;
    }
    return gmLen;
}



template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0cToOut(__attribute__((cce_global)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &intriParams,
    const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    uint16_t cburstNum = fixpipeTiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * intriParams.srcStride * BLOCK_CUBE;
    uint32_t dstOffset = 0;
    bool nz2ndEn = false;
    if constexpr (config.format == CO2Layout::ROW_MAJOR) {
        dstOffset = nIterIndex * fixpipeTiling.nSize;
        nz2ndEn = true;
    } else {
        dstOffset = cburstNum * nIterIndex * intriParams.dstStride * 32 / sizeof(DstT);
    }

    if constexpr (g_gm_overflow_check) {
        uint64_t gmLen = GetGMLength(intriParams, calNSize, sizeof(DstT), nz2ndEn);
        AscendCUtils::CheckGmMemOverflow((__attribute__((cce_global)) DstT *)(dst + dstOffset), false, gmLen);
    }
    if constexpr(g_coreType == AscendC::AIC) {
        switch (intriParams.quantPre) {
            case QuantMode_t::NoQuant:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::NoQuant, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322F16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322BF16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::DEQF16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VDEQF16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::QF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VQF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::REQ8, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VREQ8, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            default:
                                                                                            ;
        }
    }
}
}
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h" 2







namespace AscendC {
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfig(const LocalTensor<T> &reluPre, const LocalTensor<T> &quantPre,
    bool isUnitFlag = false);

template <typename T, bool setRelu = false>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfig(const LocalTensor<T> &preTensor, bool isUnitFlag = false);

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipeNz2ndFlag(uint16_t ndNum, uint16_t srcNdStride, uint16_t dstNdStride);

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipePreQuantFlag(uint64_t config);

[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeClipRelu(uint64_t config);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeAddr(const LocalTensor<T> &eleWiseTensor, uint16_t c0ChStride);



template <typename DstT, typename SrcT, const FixpipeConfig& config = CFG_ROW_MAJOR>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT>& dstLocal, const LocalTensor<SrcT>& srcLocal,
    const FixpipeParamsV220& intriParams);


template <typename DstT, typename SrcT, const FixpipeConfig& config = CFG_ROW_MAJOR, typename BufT = uint64_t,
    typename std::enable_if<IsSameType<PrimT<BufT>, uint64_t>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT>& dstLocal, const LocalTensor<SrcT>& srcLocal,
    const LocalTensor<BufT>& cbufWorkspace, const FixpipeParamsV220& intriParams);


template <typename DstT, typename SrcT, const FixpipeConfig& config = CFG_ROW_MAJOR>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT>& dstGlobal, const LocalTensor<SrcT>& srcLocal,
    const FixpipeParamsV220& intriParams);


template <typename DstT, typename SrcT, const FixpipeConfig& config = CFG_ROW_MAJOR, typename BufT = uint64_t,
    typename std::enable_if<IsSameType<PrimT<BufT>, uint64_t>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT>& dstGlobal, const LocalTensor<SrcT>& srcLocal,
    const LocalTensor<BufT>& cbufWorkspace, const FixpipeParamsV220& intriParams);
# 142 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h"
}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_conv2d_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_conv2d_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_conv2d_base_impl.h" 1
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_conv2d_base_impl.h"
namespace AscendC {
# 159 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_conv2d_base_impl.h"
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void GetTypeforC0(Conv2dParams& conv2dParams, Conv2dTilling& tilling)
{
    if (IsSameType<PrimT<T>, int8_t>::value) {
        tilling.c0Size = 32;
        tilling.dTypeSize = 1;
    } else if (IsSameType<PrimT<T>, __cce_half>::value) {
        tilling.c0Size = 16;
        tilling.dTypeSize = 2;
    } else {
        tilling.c0Size = 0;
        tilling.dTypeSize = 0;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CalculateConv2dTiling(Conv2dTilling& tilling)
{
    tilling.mBlockNum = DivCeil(tilling.mNum, tilling.blockSize);
    tilling.nBlockNum = DivCeil(tilling.nNum, tilling.blockSize);
    tilling.kBlockNum = DivCeil(tilling.kNum, tilling.c0Size);

    tilling.roundM = DivCeil(tilling.mNum, tilling.blockSize) * tilling.blockSize;
    tilling.roundN = DivCeil(tilling.nNum, tilling.blockSize) * tilling.blockSize;
    tilling.roundK = DivCeil(tilling.kNum, tilling.c0Size) * tilling.c0Size;

    uint32_t k0a = TOTAL_L0A_SIZE / 2 / (tilling.roundM * tilling.dTypeSize);
    uint32_t k0b = TOTAL_L0B_SIZE / 2 / (tilling.roundN * tilling.dTypeSize);
    uint32_t k0 = k0a > k0b ? k0b : k0a;
    k0 = k0 > tilling.kNum ? tilling.kNum : k0;

    tilling.kTileBlock = k0 / tilling.c0Size;
    if (tilling.kTileBlock == 0) {
        tilling.kTileBlock = 1;
    }

    tilling.mIterNum = 1;
    tilling.nIterNum = 1;
    tilling.kIterNum = DivCeil(tilling.kBlockNum, tilling.kTileBlock);

    tilling.mTileBlock = DivCeil(tilling.mBlockNum, tilling.mIterNum);
    tilling.nTileBlock = DivCeil(tilling.nBlockNum, tilling.nIterNum);

    tilling.mTileNums = tilling.mTileBlock * tilling.blockSize;

    tilling.mHasTail = (tilling.howo != tilling.mIterNum * tilling.mTileBlock * tilling.blockSize) ? true : false;
    tilling.kHasTail = (tilling.kBlockNum < tilling.kIterNum * tilling.kTileBlock) ? true : false;
    tilling.nHasTail = (tilling.nBlockNum < tilling.nIterNum * tilling.nTileBlock) ? true : false;

    tilling.mTailBlock = tilling.mBlockNum - (tilling.mIterNum - 1) * tilling.mTileBlock;
    tilling.mTailNums = tilling.howo - (tilling.mIterNum - 1) * tilling.mTileBlock * tilling.blockSize;

    tilling.kTailBlock = tilling.kBlockNum - (tilling.kIterNum - 1) * tilling.kTileBlock;
    tilling.nTailBlock = tilling.nBlockNum - (tilling.nIterNum - 1) * tilling.nTileBlock;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0AForConv2DV1(uint32_t kBlocks, uint32_t indexK, uint32_t mBlocks, uint32_t indexM,
    Conv2dParams& conv2dParams, Conv2dTilling& tilling, const LocalTensor<T>& src0Local, const LocalTensor<T>& L0a)
{
    uint32_t cinPos = indexK * tilling.kTileBlock;

    for (size_t index = 0; index < tilling.mTileBlock; index++) {
        uint32_t hoWoPos = (indexM * tilling.mTileBlock + index) * tilling.blockSize;
        uint32_t hoIdx = hoWoPos / tilling.wo;
        uint32_t woIdx = hoWoPos % tilling.wo;
        uint32_t hiIdx = hoIdx * tilling.strideH;
        uint32_t wiIdx = woIdx * tilling.strideW;

        uint32_t c1Idx = cinPos / (tilling.height * tilling.width);
        uint32_t kHwIdx = cinPos % (tilling.height * tilling.width);
        uint32_t l0aIdx = index * kBlocks * tilling.blockSize * tilling.c0Size;
        uint32_t disableC1 = 0;
        uint32_t c1Offset = c1Idx * tilling.c0Size * tilling.hi * tilling.wi;

        LoadData3DParamsV1<PrimT<T>> params;

        for (size_t i = 0; i < PAD_SIZE; i++) {
            params.padList[i] = conv2dParams.padList[i];
        }

        params.l1H = tilling.hi;
        params.l1W = tilling.wi;
        params.c1Index = disableC1;
        params.fetchFilterW = kHwIdx % tilling.width;
        params.fetchFilterH = kHwIdx / tilling.width;
        params.leftTopW = wiIdx - params.padList[0];
        params.leftTopH = hiIdx - params.padList[2];
        params.strideW = tilling.strideW;
        params.strideH = tilling.strideH;
        params.filterW = tilling.width;
        params.filterH = tilling.height;
        params.dilationFilterW = tilling.dilationW;
        params.dilationFilterH = tilling.dilationH;
        params.jumpStride = 1;
        params.repeatMode = 0;
        params.repeatTime = kBlocks;
        params.cSize = 0;
        params.padValue = 0;

        LoadDataImpl(L0a[l0aIdx], src0Local[c1Offset], params);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0AForConv2DV2(uint32_t kBlocks, uint32_t indexK, uint32_t mBlocks, uint32_t indexM,
    Conv2dParams& conv2dParams, Conv2dTilling& tilling, const LocalTensor<T>& src0Local, const LocalTensor<T>& L0a)
{


    uint32_t kStartPt = indexK * kBlocks * tilling.c0Size;
    uint32_t mStartPt = indexM * mBlocks;
    uint32_t channelSize = conv2dParams.cin;

    LoadData3DParamsV2<PrimT<T>> params;

    for (size_t i = 0; i < PAD_SIZE; i++) {
        params.padList[i] = conv2dParams.padList[i];
    }

    params.l1H = tilling.hi;
    params.l1W = tilling.wi;
    params.channelSize = channelSize;
    params.kExtension = kBlocks * tilling.c0Size;
    params.mExtension = mBlocks;
    params.kStartPt = kStartPt;
    params.mStartPt = mStartPt;
    params.strideW = tilling.strideW;
    params.strideH = tilling.strideH;
    params.filterW = tilling.width;
    params.filterH = tilling.height;
    params.dilationFilterW = tilling.dilationW;
    params.dilationFilterH = tilling.dilationH;
    params.enTranspose = false;
    params.enSmallK = false;
    params.padValue = 0;
    params.filterSizeW = false;
    params.filterSizeH = false;
    params.fMatrixCtrl = false;

    LoadDataImpl(L0a, src0Local, params);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0AForConv2D(uint32_t kBlocks, uint32_t indexK, uint32_t mBlocks, uint32_t indexM,
    Conv2dParams& conv2dParams, Conv2dTilling& tilling, const LocalTensor<T>& src0Local, const LocalTensor<T>& L0a)
{



    LoadL0AForConv2DV2(kBlocks, indexK, mBlocks, indexM, conv2dParams, tilling, src0Local, L0a);

}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0BForConv2D(uint32_t kBlocks, uint32_t nBlocks, uint32_t indexK, uint32_t indexN,
    Conv2dTilling& tilling, const LocalTensor<T>& src1Local, const LocalTensor<T>& L0b)
{
    if (tilling.nIterNum == 1) {

        uint32_t wSize = tilling.blockSize * tilling.c0Size;
        uint32_t wIdx = (indexK * tilling.kTileBlock * tilling.nBlockNum + indexN * tilling.nTileBlock) * wSize;
        LoadData2DParams params;
        params.startIndex = 0;
        params.repeatTimes = kBlocks * nBlocks;
        params.srcStride = 1;
        LoadDataImpl(L0b, src1Local[wIdx], params);
    } else {

        for (size_t index = 0; index < kBlocks; index++) {
            uint32_t wSize = indexN * tilling.nTileBlock * tilling.blockSize * tilling.c0Size;
            uint32_t wIdx =
                (indexK * tilling.kTileBlock + index) * tilling.nBlockNum * tilling.blockSize * tilling.c0Size + wSize;
            uint32_t l0bIdx = index * nBlocks * tilling.blockSize * tilling.c0Size;
            LoadData2DParams params;
            params.startIndex = 0;
            params.repeatTimes = nBlocks;
            params.srcStride = 1;
            LoadDataImpl(L0b[l0bIdx], src1Local[wIdx], params);
        }
    }
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void MmadFuncForConv2D(const LocalTensor<src_T>& L0a, const LocalTensor<src_T>& L0b,
    const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias, Conv2dParams& conv2dParams, Conv2dTilling tilling,
    uint32_t kBlocks, uint32_t mBlocks, uint32_t nBlocks, uint32_t indexK, uint32_t indexM, uint32_t indexN)
{

    uint32_t bSize = tilling.blockSize * tilling.blockSize;
    uint32_t dstFlattenIdx = (indexN * tilling.mBlockNum * tilling.nTileBlock + indexM * tilling.mTileBlock) * bSize;
    uint32_t hwActualSize = mBlocks;




    if (hwActualSize == 1) {
        hwActualSize = 2;
    }

    MmadParams mmadParams;

    mmadParams.m = hwActualSize;
    mmadParams.k = kBlocks * tilling.c0Size;
    mmadParams.n = nBlocks * tilling.blockSize;
    mmadParams.isBias = 1;

    if ((indexK == 0) && (conv2dParams.initY == 0)) {
        mmadParams.isBias = 0;
    }

    if ((indexK == 0) && (conv2dParams.initY == 2)) {
        mmadParams.isBias = 0;
        uint32_t biasOffset = nBlocks * indexN * 16;

        uint32_t burstLenUnit = 64;
        uint32_t extent = sizeof(PrimT<dst_T>) * nBlocks * 16;
        uint32_t burstLen = extent / burstLenUnit;
        BroadCastVecToMM(L0c[dstFlattenIdx], bias[biasOffset], 1, burstLen, 0, 0);
        event_t eventIdVToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_M));
        SetFlag<HardEvent::V_M>(eventIdVToM);
        WaitFlag<HardEvent::V_M>(eventIdVToM);
    }

    MmadImpl(L0c[dstFlattenIdx], L0a, L0b, mmadParams);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecNmNopingpong(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    LocalTensor<src_T> L0b;
    LocalTensor<src_T> L0a;
    GetSingleThreadBuffer(L0a, L0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

            LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0b);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local, L0a);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFuncForConv2D(L0a, L0b, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                    tilling.nTileBlock, indexK, indexM, indexN);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

[aicore] __inline__ __attribute__((always_inline)) void SetWaitFlagMte1ToM()
{
    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
    PipeBarrier<PIPE_M>();
}

[aicore] __inline__ __attribute__((always_inline)) void PingPongRealeaseEvent(event_t eventId0, event_t eventId1)
{
    WaitFlag<HardEvent::M_MTE1>(eventId0);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId0);
    WaitFlag<HardEvent::M_MTE1>(eventId1);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId1);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecNmPingPong(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t ping = 1;
    LocalTensor<src_T> L0aPing;
    LocalTensor<src_T> L0bPing;
    LocalTensor<src_T> L0aPong;
    LocalTensor<src_T> L0bPong;
    GetPingPongBuffer(L0aPing, L0aPong, L0bPing, L0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());

    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0bPing);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local,
                        L0aPing);
                    SetWaitFlagMte1ToM();
                    MmadFuncForConv2D(L0aPing, L0bPing, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0bPong);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local,
                        L0aPong);
                    SetWaitFlagMte1ToM();
                    MmadFuncForConv2D(L0aPong, L0bPong, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }

    PingPongRealeaseEvent(eventId0, eventId1);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecNm(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t needL0Asize = tilling.roundM * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Bsize = tilling.roundN * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        Conv2DExecNmNopingpong(L0c, bias, src0Local, src1Local, conv2dParams, tilling);
        return;
    }
    Conv2DExecNmPingPong(L0c, bias, src0Local, src1Local, conv2dParams, tilling);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecMnNopingpong(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    LocalTensor<src_T> L0a;
    LocalTensor<src_T> L0b;
    GetSingleThreadBuffer(L0a, L0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

            LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local, L0a);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0b);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFuncForConv2D(L0a, L0b, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                    tilling.nTileBlock, indexK, indexM, indexN);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecMnPingPong(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t ping = 1;
    LocalTensor<src_T> L0aPing;
    LocalTensor<src_T> L0aPong;
    LocalTensor<src_T> L0bPing;
    LocalTensor<src_T> L0bPong;
    GetPingPongBuffer(L0aPing, L0aPong, L0bPing, L0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local, L0aPing);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0bPing);
                    SetWaitFlagMte1ToM();
                    MmadFuncForConv2D(L0aPing, L0bPing, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local, L0aPong);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0bPong);
                    SetWaitFlagMte1ToM();
                    MmadFuncForConv2D(L0aPong, L0bPong, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }

    PingPongRealeaseEvent(eventId0, eventId1);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecMn(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t needL0Bsize = tilling.roundN * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Asize = tilling.roundM * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        Conv2DExecMnNopingpong(L0c, bias, src0Local, src1Local, conv2dParams, tilling);
        return;
    }
    Conv2DExecMnPingPong(L0c, bias, src0Local, src1Local, conv2dParams, tilling);
}

}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_conv2d_intf.h" 2


namespace AscendC {

template <typename T> [aicore] __inline__ __attribute__((always_inline)) Conv2dTilling GetConv2dTiling(Conv2dParams& conv2dParams);
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_conv2d_intf.h"
template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("MTE2")))
    __attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<dst_T> &dstLocal, const LocalTensor<src_T> &featureMap,
    const LocalTensor<src_T> &weight, Conv2dParams &conv2dParams, Conv2dTilling &tilling);

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("MTE2")))__attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<dst_T> &dstLocal,
    const LocalTensor<dst_T> &bias, const LocalTensor<src_T> &featureMap, const LocalTensor<src_T> &weight,
    Conv2dParams &conv2dParams, Conv2dTilling &tilling);
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 1
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_sync_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_sync_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void ClcSyncCount(__attribute__((cce_global)) int32_t* localSyncGM, __attribute__((cce_unif_buff)) int32_t* ubWorkspaceAddr,
    const int32_t blockIdx, const int32_t totalBlocks, bool isFirst, int32_t& count)
{
    if (isFirst) {
        __attribute__((cce_unif_buff)) int32_t* localUbAddr = ubWorkspaceAddr + (blockIdx * DEFAULT_BLK_NUM);
        *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(localUbAddr)) = 1;
        event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
        copy_ubuf_to_gm(static_cast<__attribute__((cce_global)) void*>(localSyncGM), static_cast<__attribute__((cce_unif_buff)) void*>(localUbAddr), 0, 1, 1, 0,
            0);
        event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        count = 1;
        for (int32_t i = 0; i < totalBlocks; i++) {
            if (i != blockIdx) {
                count += *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(ubWorkspaceAddr) + i * ONE_BLK_FLOAT_NUM);
            }
        }
    } else {
        for (int32_t i = 0; i < totalBlocks; i++) {
            count += *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(ubWorkspaceAddr) + i * ONE_BLK_FLOAT_NUM);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockNum();
template <bool isAIVOnly = true>
[aicore] __inline__ __attribute__((always_inline)) void SoftSyncAllImpl(__attribute__((cce_global)) int32_t* gmWorkspaceAddr, __attribute__((cce_unif_buff)) int32_t* ubWorkspaceAddr,
    const int usedCores)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    __sync_all_stub(usedCores, isAIVOnly);
    PipeBarrier<PIPE_ALL>();

    int32_t totalBlocks = isAIVOnly ? GetBlockNum() : (GetTaskRationImpl() * GetBlockNum());
    totalBlocks = usedCores != 0 ? usedCores : totalBlocks;
    int32_t blockIdx = isAIVOnly ? get_block_idx() : GetBlockIdxImpl();

    __attribute__((cce_global)) int32_t* localSyncGM = gmWorkspaceAddr + (blockIdx * DEFAULT_BLK_NUM);
    __attribute__((cce_unif_buff)) int32_t* localUbAddr = ubWorkspaceAddr + (blockIdx * DEFAULT_BLK_NUM);

    copy_gm_to_ubuf(static_cast<__attribute__((cce_unif_buff)) void*>(localUbAddr), static_cast<__attribute__((cce_global)) void*>(localSyncGM), 0, 1, 1, 0, 0);
    event_t eventIdMte2ToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
    SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
    WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
    int32_t curValue = *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(localUbAddr)) + 1;
    bool isFirst = curValue == 1 ? true : false;
    *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(localUbAddr)) = curValue;
    event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
    copy_ubuf_to_gm(static_cast<__attribute__((cce_global)) void*>(localSyncGM), static_cast<__attribute__((cce_unif_buff)) void*>(localUbAddr), 0, 1, 1, 0, 0);
    event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
    SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    int32_t totalBlockCount = ONE_BLK_FLOAT_NUM * totalBlocks;
    uint16_t blockLen = totalBlockCount / AscendCUtils::GetC0Count(sizeof(int32_t));
    while (true) {
        copy_gm_to_ubuf(static_cast<__attribute__((cce_unif_buff)) void*>(ubWorkspaceAddr), static_cast<__attribute__((cce_global)) void*>(gmWorkspaceAddr), 0, 1,
            blockLen, 0, 0);
        SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        int32_t count = 0;
        ClcSyncCount(localSyncGM, ubWorkspaceAddr, blockIdx, totalBlocks, isFirst, count);
        event_t eventIdSToMte2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE2));
        SetFlag<HardEvent::S_MTE2>(eventIdSToMte2);
        WaitFlag<HardEvent::S_MTE2>(eventIdSToMte2);
        if (count >= (totalBlocks * curValue)) {
            break;
        }
    }
    __sync_all_stub(usedCores, isAIVOnly);
}

constexpr uint16_t SYNC_AIV_FLAG = 12;
constexpr uint16_t SYNC_AIC_FLAG = 11;
constexpr uint16_t SYNC_AIC_AIV_FLAG = 13;
constexpr uint16_t SYNC_AIV_ONLY_ALL = 14;
constexpr uint16_t SYNC_MODE_SHIFT_VALUE = 4;
constexpr uint16_t SYNC_FLAG_SHIFT_VALUE = 8;
constexpr uint32_t ASCENDC_SUPER_KERNEL_EARLY_START_AIC_TO_AIC = 0b00;
constexpr uint32_t ASCENDC_SUPER_KERNEL_EARLY_START_AIC_TO_AIV = 0b01;
constexpr uint32_t ASCENDC_SUPER_KERNEL_EARLY_START_AIC_TO_MIX = 0b10;
constexpr uint32_t ASCENDC_SUPER_KERNEL_EARLY_START_AIV_TO_AIC = 0b0100;
constexpr uint32_t ASCENDC_SUPER_KERNEL_EARLY_START_AIV_TO_AIV = 0b0101;
constexpr uint32_t ASCENDC_SUPER_KERNEL_EARLY_START_AIV_TO_MIX = 0b0110;
constexpr uint32_t ASCENDC_SUPER_KERNEL_EARLY_START_MIX_TO_AIC = 0b1000;
constexpr uint32_t ASCENDC_SUPER_KERNEL_EARLY_START_MIX_TO_AIV = 0b1001;
constexpr uint32_t ASCENDC_SUPER_KERNEL_EARLY_START_MIX_TO_MIX = 0b1010;

[aicore] __inline__ __attribute__((always_inline)) uint16_t GetffstMsg(uint16_t mode, uint16_t flagId)
{
    return (0x1 + ((mode & 0x3) << SYNC_MODE_SHIFT_VALUE) + ((flagId & 0xf) << SYNC_FLAG_SHIFT_VALUE));
}

template<pipe_t AIV_PIPE = PIPE_MTE3, pipe_t AIC_PIPE = PIPE_FIX>
[aicore] __inline__ __attribute__((always_inline)) void SetNextTaskStartImpl()
{
    if constexpr(g_coreType == AscendC::AIC) {
        ffts_cross_core_sync(AIC_PIPE, AscendC::GetffstMsg(0x0, AscendC::SYNC_AIC_FLAG));
    }
    if constexpr(g_coreType == AscendC::AIV) {
        ffts_cross_core_sync(AIV_PIPE, AscendC::GetffstMsg(0x0, AscendC::SYNC_AIV_ONLY_ALL));
    }
    return;
}

[aicore] __inline__ __attribute__((always_inline)) void WaitPreTaskEndImpl()
{
# 222 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_sync_impl.h"
}

template <bool isAIVOnly = true> [aicore] __inline__ __attribute__((always_inline)) void SyncAllImpl()
{
    PipeBarrier<PIPE_ALL>();
    if constexpr (isAIVOnly) {
        ffts_cross_core_sync(PIPE_MTE3, GetffstMsg(0x0, SYNC_AIV_ONLY_ALL));
        wait_flag_dev(SYNC_AIV_ONLY_ALL);
        return;
    }

    if constexpr(g_coreType == AscendC::AIC) {
        wait_flag_dev(SYNC_AIV_FLAG);
        ffts_cross_core_sync(PIPE_FIX, GetffstMsg(0x0, SYNC_AIC_FLAG));
        wait_flag_dev(SYNC_AIC_FLAG);
        ffts_cross_core_sync(PIPE_MTE3, GetffstMsg(0x02, SYNC_AIC_AIV_FLAG));
        return;
    }
    if constexpr(g_coreType == AscendC::AIV) {
        ffts_cross_core_sync(PIPE_MTE3, GetffstMsg(0x02, SYNC_AIV_FLAG));
        wait_flag_dev(SYNC_AIC_AIV_FLAG);
        return;
    }
}

template <uint8_t modeId, pipe_t pipe>
[aicore] __inline__ __attribute__((always_inline)) void NotifyEventImpl(uint16_t flagId)
{
    ffts_cross_core_sync(pipe, GetffstMsg(modeId, flagId));
}

template <uint8_t modeId, pipe_t pipe>
[aicore] __inline__ __attribute__((always_inline)) void WaitEventImpl(uint16_t flagId)
{
    (void)modeId;
    wait_flag_dev(flagId);
}

[aicore] __inline__ __attribute__((always_inline)) void SetSyncBaseAddrImpl(uint64_t config)
{
    set_ffts_base_addr(config);
}

[aicore] __inline__ __attribute__((always_inline)) void SetSyncBaseAddr(uint64_t config)
{
    SetSyncBaseAddrImpl(config);
}
}
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_duplicate_impl.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_duplicate_impl.h"
namespace AscendC {
template <typename T> constexpr [aicore] __inline__ __attribute__((always_inline)) void CheckDuplicateSupportedType()
{
    static_assert(SupportType<T, __cce_half, bfloat16_t, int16_t, uint16_t, int32_t, uint32_t, float>(), "Failed to check "
        "dtype in Duplicate, current api support dtype combination is src and dst both: half / bfloat16_t / int16_t / "
        "uint16_t / int32_t / uint32_t / float.");
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, T scalarValue, const uint8_t repeatTimes,
    const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    vector_dup(dstLocal, scalarValue, repeatTimes, dstBlockStride, 1, dstRepeatStride, 0);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateImpl(__attribute__((cce_unif_buff)) T* dstLocal, const T& scalarValue, uint64_t mask,
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        DuplicateIntrinsicsImpl(dstLocal, scalarValue, repeatTimes, dstBlockStride, dstRepeatStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateImpl(__attribute__((cce_unif_buff)) T* dstLocal, const T& scalarValue, uint64_t mask[],
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        DuplicateIntrinsicsImpl(dstLocal, scalarValue, repeatTimes, dstBlockStride, dstRepeatStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateImpl(__attribute__((cce_unif_buff)) T* dstLocal, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        DuplicateIntrinsicsImpl(dstLocal, scalarValue, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        ResetMask();
        SetMaskNorm();
    }
}
}
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_server.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_server.h"
namespace AscendC {
class KfcCommServer {
public:
    __attribute__((cce_global)) KfcMsg* msgSendHead;
    __attribute__((cce_global)) KfcMsg* msgSendStart;


    __attribute__((cce_global)) KfcMsg* msgRcvHead;
    __attribute__((cce_global)) KfcMsg* msgRcvStart;

    __attribute__((cce_global)) uint8_t* ubAvalidTail;

    uint8_t msgSendPos;
    uint8_t msgRcvPos;
    uint8_t subBlockID;

public:
    [aicore] __inline__ __attribute__((always_inline)) void Init(__attribute__((cce_global)) uint8_t* workspace, int i)
    {

        this->msgRcvStart = (__attribute__((cce_global)) KfcMsg*)GetMsgHead(workspace, i);
        this->msgSendStart = this->msgRcvStart + 64;

        this->msgSendHead = this->msgSendStart;
        this->msgSendPos = 0;
        this->msgRcvHead = this->msgRcvStart;
        this->msgRcvPos = 0;
        this->subBlockID = i;

                                                                             ;

                                                                            ;
        ubAvalidTail = GetUBAvaliedAddr(workspace, i);
    }

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg* AllocMessage()
    {
        return AllocMessageImpl(this->msgSendHead, this->msgSendPos, this->msgSendStart);
    }

    [aicore] __inline__ __attribute__((always_inline)) void FreeMessage(__attribute__((cce_global)) KfcMsg* msg)
    {
        FreeMessageImpl(msg);
    }

    [aicore] __inline__ __attribute__((always_inline)) void FreeUB(int32_t addr)
    {
        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventID);
        WaitFlag<HardEvent::MTE3_MTE2>(eventID);
        __attribute__((cce_cube_buff)) uint32_t* dst = (__attribute__((cce_cube_buff)) uint32_t*)(TOTAL_L1_SIZE);




        create_cbuf_matrix((__attribute__((cce_cube_buff)) uint32_t*)dst, 0x10001, (uint32_t)addr);

        eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
        SetFlag<HardEvent::MTE2_MTE3>(eventID);
        WaitFlag<HardEvent::MTE2_MTE3>(eventID);
# 96 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_server.h"
        copy_cbuf_to_gm((__attribute__((cce_global)) void*)ubAvalidTail, (__attribute__((cce_cube_buff)) void*)dst, 0, 1, 1, 1, 1);
    }

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg* RcvMessage()
    {
        auto msg = (__attribute__((cce_global)) KfcMsg*)RcvMessageImpl(this->msgRcvHead, this->msgRcvPos, this->msgRcvStart);
        return msg;
    }

    [aicore] __inline__ __attribute__((always_inline)) void RollBackMsg()
    {
        RollBackMsgImpl(this->msgRcvHead, this->msgRcvPos);
        return;
    }
};

typedef KfcCommServer* KFC_COMM_SERVER_PTR;

}
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_cube_group_handle_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_cube_group_handle_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/core_mng/roc/kernel_operator_cube_group_intf.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/core_mng/roc/kernel_operator_cube_group_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_cube_group_info.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_cube_group_info.h"
namespace AscendC {

constexpr uint32_t MAX_MSG_PER_AIV = 4;
constexpr uint32_t BARRIER_SIZE = 64;
constexpr uint32_t BARRIER_MAX_AIV = 50;
constexpr uint16_t CACHE_LINE_LEN = 512;
constexpr uint32_t UB_START_ADDR = TOTAL_UB_SIZE - ONE_BLK_SIZE * BARRIER_MAX_AIV;
constexpr uint16_t CACHELINE_BLKNUM = CACHE_LINE_LEN / ONE_BLK_SIZE;

enum class CubeMsgState : uint8_t {
    FREE = 0,
    VALID,
    QUIT,
    FAKE

};

struct CubeGroupMsgHead {
    volatile CubeMsgState msgState = CubeMsgState::FREE;
    volatile uint8_t aivID;
};

struct BarrierInfo {
    volatile uint32_t head;
    uint32_t buffer[15];
};


enum class PipeMode : uint8_t { SCALAR_MODE = 0, MTE3_MODE = 1, MAX };

template <int32_t ActualFuncId, int32_t ExpectFuncId>
struct IsEqual {};

template <int32_t FuncId>
struct IsEqual<FuncId, FuncId> {
    using Type = void;
};
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/core_mng/roc/kernel_operator_cube_group_intf.h" 2
namespace AscendC {
class KfcWorkspace {
public:
    [aicore] __inline__ __attribute__((always_inline)) KfcWorkspace(__attribute__((cce_global)) uint8_t* workspace);
    [aicore] __inline__ __attribute__((always_inline)) void UpdateKfcWorkspace(uint32_t offset);
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetKfcWorkspace();
    [aicore] __inline__ __attribute__((always_inline)) ~KfcWorkspace();

private:
    friend [aicore] __inline__ __attribute__((always_inline)) uint8_t GetEventId(KfcWorkspace &desc);
    [aicore] __inline__ __attribute__((always_inline)) KfcWorkspace() = delete;
    __attribute__((cce_global)) uint8_t* msgStart;
    uint8_t evtID;
};

template <typename CubeMsgType>
class CubeResGroupHandle {
public:
    [aicore] __inline__ __attribute__((always_inline)) CubeResGroupHandle() = default;

    [aicore] __inline__ __attribute__((always_inline)) CubeResGroupHandle(
        __attribute__((cce_global)) uint8_t* workspace, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, uint8_t evtIDIn);




    [aicore] __inline__ __attribute__((always_inline)) void AssignQueue(uint8_t queueIdIn);



    template <PipeMode pipeMode = PipeMode::SCALAR_MODE>
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) CubeMsgType *AllocMessage();


    template <PipeMode pipeMode = PipeMode::SCALAR_MODE>
    [aicore] __inline__ __attribute__((always_inline)) uint16_t PostMessage(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgType &msgInput);


    [aicore] __inline__ __attribute__((always_inline)) uint16_t FreeMessage(__attribute__((cce_global)) CubeMsgType *msg);


    [aicore] __inline__ __attribute__((always_inline)) uint16_t FreeMessage(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgState waitState);


    [aicore] __inline__ __attribute__((always_inline)) uint16_t PostFakeMsg(__attribute__((cce_global)) CubeMsgType *msg);


    [aicore] __inline__ __attribute__((always_inline)) void SetQuit(__attribute__((cce_global)) CubeMsgType *msg);



    [aicore] __inline__ __attribute__((always_inline)) void SetSkipMsg(uint8_t skipCnt);



    template <bool sync = true>
    [aicore] __inline__ __attribute__((always_inline)) bool Wait(uint16_t offset);

private:
    template <typename T>
    friend [aicore] __inline__ __attribute__((always_inline)) bool __IsRun(T handle);

    template <typename T>
    friend [aicore] __inline__ __attribute__((always_inline)) uint32_t __GetMsgAreaLen(T handle, uint32_t sizeOfCubeMsgStruct);


    template <typename T>
    friend [aicore] __inline__ __attribute__((always_inline)) void __SetAivQuit(T *handle, uint8_t aivID);


    template <typename U>
    friend [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) U *__RcvMessage(CubeResGroupHandle<U> handle);


    template <typename T>
    friend [aicore] __inline__ __attribute__((always_inline)) void __ReleaseMessage(T *handle);


    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* __GetAicMsgHead(__attribute__((cce_global)) uint8_t* workspace, uint8_t aicStart, uint16_t msgSizePerAic);


    [aicore] __inline__ __attribute__((always_inline)) void __AivUpdateMsgCurrent();


    [aicore] __inline__ __attribute__((always_inline)) void __AicUpdateMsgCurrent();


    [aicore] __inline__ __attribute__((always_inline)) bool __AivIsRun(uint8_t aivID);


    [aicore] __inline__ __attribute__((always_inline)) void __WriteGmCubeMsgByScalar(__attribute__((cce_global)) CubeMsgType *msg);


    [aicore] __inline__ __attribute__((always_inline)) void __WriteGmStateByScalar(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgState newState);

    [aicore] __inline__ __attribute__((always_inline)) void __WriteGmCubeMsgByDatacopy(__attribute__((cce_global)) CubeMsgType *msgPtr, CubeMsgType &cubeMsgInput);


    [aicore] __inline__ __attribute__((always_inline)) void __CopyCubeMsg(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgType &msgInput);

    uint8_t aicSize = 0;
    uint8_t aivSize = 0;
    uint8_t aivPerAic = 0;
    uint8_t aivNumForCurAic = 0;
    uint8_t queueId = 0;

    __attribute__((cce_global)) CubeMsgType *msgHead;

    __attribute__((cce_global)) CubeMsgType *msgCurrent;

    __attribute__((cce_unif_buff)) CubeMsgType *ubMsg;

    uint16_t msgPos = 0;

    uint16_t msgSize = 0;


    uint64_t aivWorkState = 0;
    uint8_t aivWork = 0;
    uint8_t eventID;
};

template <int groupID, class MatmulApiType, template <class, class> class CallBack, typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) CubeResGroupHandle<CubeMsgType> CreateCubeResGroup(
    KfcWorkspace &desc, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, __attribute__((cce_global)) uint8_t* tiling);
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_cube_group_handle_impl.h" 2
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) KfcWorkspace::KfcWorkspace(__attribute__((cce_global)) uint8_t* workspace)
{
    msgStart = workspace;
    if constexpr(g_coreType == AscendC::AIV) {
        evtID = GetTPipePtr()->AllocEventID<HardEvent::MTE3_S>();
        SetFlag<HardEvent::MTE3_S>(evtID);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void KfcWorkspace::UpdateKfcWorkspace(uint32_t offset)
{
    msgStart += offset;
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* KfcWorkspace::GetKfcWorkspace()
{
    return msgStart;
}

[aicore] __inline__ __attribute__((always_inline)) KfcWorkspace::~KfcWorkspace()
{
    if constexpr(g_coreType == AscendC::AIV) {
        WaitFlag<HardEvent::MTE3_S>(evtID);
        GetTPipePtr()->ReleaseEventID<HardEvent::MTE3_S>(evtID);
    }
}

[aicore] __inline__ __attribute__((always_inline)) uint8_t GetEventId(KfcWorkspace &kfcWorkspace)
{
    return kfcWorkspace.evtID;
}

template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) CubeResGroupHandle<CubeMsgType>::CubeResGroupHandle(
    __attribute__((cce_global)) uint8_t* workspace, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, uint8_t evtIDIn)
{
                                                                                                             ;
                                                                                                          ;
    aicSize = blockSize / MIX_NUM;
    aivSize = msgQueueSize;

    aivPerAic = Ceil(aivSize, aicSize);
    int8_t aivInLastAic = aivSize - (aicSize - 1) * aivPerAic;
                                                                                    ;

    aivNumForCurAic = (GetBlockIdxImpl() == blockStart / MIX_NUM + aicSize - 1) ? aivInLastAic : aivPerAic;
    aivWorkState = (static_cast<uint64_t>(1) << aivNumForCurAic) - 1;

    msgSize = aivPerAic * aicSize * MAX_MSG_PER_AIV;
    msgHead = (__attribute__((cce_global)) CubeMsgType *)__GetAicMsgHead(workspace, blockStart / MIX_NUM, aivPerAic * MAX_MSG_PER_AIV);
    msgCurrent = msgHead;
    if constexpr(g_coreType == AscendC::AIV) {
        eventID = evtIDIn;
        uint32_t ubMsgAddr = TOTAL_UB_SIZE - ONE_BLK_SIZE * AIV_CORE_NUM - sizeof(CubeMsgType);





        ubMsg = reinterpret_cast<__attribute__((cce_unif_buff)) CubeMsgType *>(ubMsgAddr);

    }
}




template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::AssignQueue(uint8_t queueIdIn)
{
                                                                                                                      ;
    if constexpr(g_coreType == AscendC::AIV) {
        uint8_t aicSubgroupID = queueIdIn / aivPerAic;
        uint8_t aivSubgroupID = queueIdIn % aivPerAic;
        queueId = queueIdIn;

                                                                                                                   ;
        if (aicSubgroupID != aicSize - 1) {
            aivNumForCurAic = aivPerAic;
        } else {
            aivNumForCurAic = aivSize - (aicSize - 1) * aivPerAic;
        }



                            ;
        msgHead += aicSubgroupID * aivPerAic * MAX_MSG_PER_AIV + aivSubgroupID;
        msgCurrent = msgHead;
    }
}



template <typename CubeMsgType>
template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) CubeMsgType *CubeResGroupHandle<CubeMsgType>::AllocMessage()
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (pipeMode == PipeMode::MTE3_MODE) {
            WaitFlag<HardEvent::MTE3_S>((event_t)eventID);
        }
        dcci(
            reinterpret_cast<__attribute__((cce_global)) int64_t *>(msgCurrent), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while ((msgCurrent->head).msgState != CubeMsgState::FREE) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msgCurrent),
                cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
        }
        __attribute__((cce_global)) CubeMsgType *msgReturn = msgCurrent;
        __AivUpdateMsgCurrent();
        return msgReturn;
    }
    return msgCurrent;
}


template <typename CubeMsgType>
template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) uint16_t CubeResGroupHandle<CubeMsgType>::PostMessage(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgType &msgInput)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (pipeMode == PipeMode::SCALAR_MODE) {
            __CopyCubeMsg(msg, msgInput);
            __WriteGmCubeMsgByScalar(msg);
        } else if constexpr (pipeMode == PipeMode::MTE3_MODE) {
            __WriteGmCubeMsgByDatacopy(msg, msgInput);
            SetFlag<HardEvent::MTE3_S>((event_t)eventID);
        } else {
                                                                                                 ;
        }
    }
    return msg - msgHead;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) uint16_t CubeResGroupHandle<CubeMsgType>::FreeMessage(__attribute__((cce_global)) CubeMsgType *msg)
{
    if constexpr(g_coreType == AscendC::AIC) {
        __WriteGmStateByScalar(msg, CubeMsgState::FREE);
    }
    return msg - msgHead;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) uint16_t CubeResGroupHandle<CubeMsgType>::FreeMessage(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgState waitState)
{
    if constexpr(g_coreType == AscendC::AIC) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while ((msg->head).msgState != waitState) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        }
        __WriteGmStateByScalar(msg, CubeMsgState::FREE);
    }
    return msg - msgHead;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) uint16_t CubeResGroupHandle<CubeMsgType>::PostFakeMsg(__attribute__((cce_global)) CubeMsgType *msg)
{
    if constexpr(g_coreType == AscendC::AIV) {
        __WriteGmStateByScalar(msg, CubeMsgState::FAKE);
    }
    return msg - msgHead;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::SetQuit(__attribute__((cce_global)) CubeMsgType *msg)
{
    uint8_t aivID = queueId % aivPerAic;
    if constexpr(g_coreType == AscendC::AIV) {



                            ;
        (msg->head).aivID = aivID;
        __WriteGmStateByScalar(msg, CubeMsgState::QUIT);
    }
}



template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::SetSkipMsg(uint8_t skipCnt)
{
    if constexpr(g_coreType == AscendC::AIC) {
        aivWork += skipCnt;




                            ;
        msgPos += skipCnt;
    }
}



template <typename CubeMsgType>
template <bool sync>
[aicore] __inline__ __attribute__((always_inline)) bool CubeResGroupHandle<CubeMsgType>::Wait(uint16_t offset)
{
    if constexpr(g_coreType == AscendC::AIV) {
        __attribute__((cce_global)) CubeMsgType *cubeMsgCur = msgHead + offset;
        dcci(
            reinterpret_cast<__attribute__((cce_global)) int64_t *>(cubeMsgCur), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        if constexpr (sync) {
            while ((cubeMsgCur->head).msgState != CubeMsgState::FREE) {
                dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(cubeMsgCur),
                    cache_line_t::SINGLE_CACHE_LINE,
                    dcci_dst_t::CACHELINE_OUT);
            }
            return true;
        } else {
            return (cubeMsgCur->head).msgState == CubeMsgState::FREE;
        }
    }
    return true;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) bool __IsRun(T handle)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return handle.aivWorkState;
    }
    return false;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t __GetMsgAreaLen(T handle, uint32_t sizeOfCubeMsgStruct)
{
    return handle.msgSize * sizeOfCubeMsgStruct;
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void __SetAivQuit(T *handle, uint8_t aivID)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t mask = ~(static_cast<uint64_t>(1) << aivID);
        handle->aivWorkState &= mask;
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void __ReleaseMessage(T *handle)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if (handle->aivWork + 1 >= handle->aivNumForCurAic) {
            uint8_t lineCnt = handle->msgPos / handle->aivPerAic;
            handle->msgPos = (lineCnt == MAX_MSG_PER_AIV - 1)
                                 ? 0
                                 : handle->msgPos + (handle->aivPerAic - handle->aivNumForCurAic) + 1;
            handle->aivWork = 0;
        } else {
            handle->msgPos += 1;
            handle->aivWork += 1;
        }
        handle->msgCurrent = handle->msgHead + handle->msgPos;
        if (handle->aivWorkState == 0) {
            return;
        }
        while (!(handle->aivWorkState & (static_cast<uint64_t>(1) << handle->aivWork))) {
            if (handle->aivWork + 1 >= handle->aivNumForCurAic) {
                uint8_t lineCnt = handle->msgPos / handle->aivPerAic;
                handle->msgPos = (lineCnt == MAX_MSG_PER_AIV - 1)
                                     ? 0
                                     : handle->msgPos + (handle->aivPerAic - handle->aivNumForCurAic) + 1;
                handle->aivWork = 0;
            } else {
                handle->msgPos += 1;
                handle->aivWork += 1;
            }
            handle->msgCurrent = handle->msgHead + handle->msgPos;
        }
    }
}

template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) CubeMsgType *__RcvMessage(CubeResGroupHandle<CubeMsgType> handle)
{
    if constexpr(g_coreType == AscendC::AIC) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(handle.msgCurrent),
            cache_line_t::SINGLE_CACHE_LINE,
            dcci_dst_t::CACHELINE_OUT);
        while ((handle.msgCurrent->head).msgState == CubeMsgState::FREE) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(handle.msgCurrent),
                cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
        }
    }
    return handle.msgCurrent;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* CubeResGroupHandle<CubeMsgType>::__GetAicMsgHead(
    __attribute__((cce_global)) uint8_t* workspace, uint8_t aicStart, uint16_t msgSizePerAic)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint8_t aicIndex = GetBlockIdxImpl() - aicStart;
        auto ptr = reinterpret_cast<__attribute__((cce_global)) CubeMsgType *>(workspace);
        return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&ptr[aicIndex * msgSizePerAic]);
    }
    return workspace;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__AivUpdateMsgCurrent()
{
    msgPos = (msgPos == 3) ? 0 : msgPos + 1;
    msgCurrent = msgHead + msgPos * aivPerAic;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__AicUpdateMsgCurrent()
{
    if (aivWork + 1 >= aivNumForCurAic) {
        uint8_t lineCnt = msgPos / aivPerAic;
        msgPos = (lineCnt == MAX_MSG_PER_AIV - 1) ? 0 : msgPos + (aivPerAic - aivNumForCurAic) + 1;
        aivWork = 0;
    } else {
        msgPos += 1;
        aivWork += 1;
    }
    msgCurrent = msgHead + msgPos;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) bool CubeResGroupHandle<CubeMsgType>::__AivIsRun(uint8_t aivID)
{
    return aivWorkState & (static_cast<uint64_t>(1) << aivID);
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__WriteGmCubeMsgByScalar(__attribute__((cce_global)) CubeMsgType *msg)
{
                                                                                               ;

    for (uint32_t i = 1; i < sizeof(CubeMsgType) / sizeof(int64_t); i++) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg) + sizeof(int64_t) * i,
            cache_line_t::SINGLE_CACHE_LINE,
            dcci_dst_t::CACHELINE_OUT);
    }
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__WriteGmStateByScalar(
    __attribute__((cce_global)) CubeMsgType *msg, CubeMsgState newState)
{
                                                                                               ;
    (msg->head).msgState = newState;
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
}

template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__WriteGmCubeMsgByDatacopy(
    __attribute__((cce_global)) CubeMsgType *msgPtr, CubeMsgType &cubeMsgInput)
{
                                                                                                      ;
    auto ubData = reinterpret_cast<__attribute__((cce_unif_buff)) uint64_t *>(ubMsg);
    auto msgData = reinterpret_cast<uint64_t *>(&cubeMsgInput);
    for (uint32_t i = 0; i < sizeof(CubeMsgType) / sizeof(uint64_t); i++, ubData++, msgData++) {
        *ubData = *msgData;
    }
    event_t evtID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_MTE3>(evtID);
    WaitFlag<HardEvent::S_MTE3>(evtID);
    PipeBarrier<PIPE_MTE3>();
    copy_ubuf_to_gm((__attribute__((cce_global)) void *)msgPtr, (__attribute__((cce_unif_buff)) void *)ubMsg, 0, 1, sizeof(CubeMsgType) / ONE_BLK_SIZE, 0, 0);
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__CopyCubeMsg(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgType &msgInput)
{
    auto gmPtr = reinterpret_cast<__attribute__((cce_global)) uint64_t *>(msg);
    auto msgDataPtr = reinterpret_cast<uint64_t *>(&msgInput);
    for (uint32_t i = 0; i < sizeof(CubeMsgType) / sizeof(int64_t); i++, gmPtr++, msgDataPtr++) {
        *gmPtr = *msgDataPtr;
    }
}

template <int groupID, class MatmulApiType, template <class, class> class CallBack, typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) CubeResGroupHandle<CubeMsgType> CreateCubeResGroup(
    KfcWorkspace &desc, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, __attribute__((cce_global)) uint8_t* tiling)
{

                                                                                                               ;
    CubeResGroupHandle handle =
        CubeResGroupHandle<CubeMsgType>(desc.GetKfcWorkspace(), blockStart, blockSize, msgQueueSize, GetEventId(desc));
    desc.UpdateKfcWorkspace(__GetMsgAreaLen(handle, sizeof(CubeMsgType)));

    if constexpr(g_coreType == AscendC::AIV) {
        return handle;
    }


    auto aicId = GetBlockIdxImpl();
    if ((aicId < blockStart / MIX_NUM) || (aicId >= (blockStart / MIX_NUM + blockSize / MIX_NUM))) {
        return handle;
    }
    MatmulApiType mm;
    CallBack<MatmulApiType, CubeMsgType> obj;
    obj.Init(obj, mm, tiling);
    while (__IsRun(handle)) {
        auto rcvMsg = __RcvMessage(handle);
        if ((rcvMsg->head).msgState == CubeMsgState::QUIT) {
            __SetAivQuit(&handle, (rcvMsg->head).aivID);
        } else {
            obj.Call(mm, rcvMsg, handle);
        }
        __ReleaseMessage(&handle);
    }
    return handle;
}

}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_group_barrier_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_group_barrier_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/core_mng/roc/kernel_operator_group_barrier_intf.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/core_mng/roc/kernel_operator_group_barrier_intf.h"
namespace AscendC {
template <PipeMode pipeMode>
class GroupBarrier {
public:
    [aicore] __inline__ __attribute__((always_inline)) GroupBarrier(__attribute__((cce_global)) uint8_t* groupWorkspace, uint32_t arriveSizeIn, uint32_t waitSizeIn);
    [aicore] __inline__ __attribute__((always_inline)) void Arrive(uint32_t arriveIndex);

    [aicore] __inline__ __attribute__((always_inline)) void Wait(uint32_t waitIndex);
    [aicore] __inline__ __attribute__((always_inline)) uint64_t GetWorkspaceLen();

private:
    [aicore] __inline__ __attribute__((always_inline)) void __WriteCurrentValue(__attribute__((cce_global)) BarrierInfo *BarrierInfoAddr);
    [aicore] __inline__ __attribute__((always_inline)) GroupBarrier() = delete;
    __attribute__((cce_global)) BarrierInfo *barrierInfoArrive;
    __attribute__((cce_global)) BarrierInfo *barrierInfoWait;
    uint32_t arriveSize;
    uint32_t waitSize;
    uint32_t counter;
    bool hasArrive;
};
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_group_barrier_impl.h" 2
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockNum();
template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) GroupBarrier<pipeMode>::GroupBarrier(
    __attribute__((cce_global)) uint8_t* groupWorkspace, uint32_t arriveSizeIn, uint32_t waitSizeIn)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                         ;
                                                                                                                   ;
                                                                                                             ;



                          ;



                          ;
        this->barrierInfoArrive = reinterpret_cast<__attribute__((cce_global)) BarrierInfo *>(groupWorkspace);
        this->barrierInfoWait = reinterpret_cast<__attribute__((cce_global)) BarrierInfo *>(groupWorkspace + BARRIER_SIZE);
        this->arriveSize = arriveSizeIn;
        this->waitSize = waitSizeIn;
        this->counter = 1;
        this->hasArrive = false;






        __attribute__((cce_unif_buff)) int32_t *dst = reinterpret_cast<__attribute__((cce_unif_buff)) int32_t *>(UB_START_ADDR);

        for (uint32_t i = 0; i < BARRIER_MAX_AIV; i++) {
            *(__attribute__((cce_unif_buff)) uint32_t *)(dst + DEFAULT_BLK_NUM * i) = 1;
        }
    }
}

template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) void GroupBarrier<pipeMode>::Arrive(uint32_t arriveIndex)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (counter > 1) {
            uint32_t expectedWaitNum = (counter - 1) * waitSize;
            GlobalTensor<uint32_t> barrierInfoWaitGlobal;
            __attribute__((cce_global)) BarrierInfo *BarrierInfoAddr =
                barrierInfoWait + (CACHE_LINE_LEN / sizeof(BarrierInfo)) * arriveIndex;
            dcci((__attribute__((cce_global)) uint64_t *)BarrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
            while (BarrierInfoAddr->head != expectedWaitNum) {
                dcci((__attribute__((cce_global)) uint64_t *)BarrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
            }
        }
        __WriteCurrentValue(barrierInfoArrive);
        counter += 1;
        hasArrive = true;
    }
}


template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) void GroupBarrier<pipeMode>::Wait(uint32_t waitIndex)
{



    if constexpr(g_coreType == AscendC::AIV) {
        uint32_t waitCounter = (hasArrive) ? counter - 1 : counter;
        uint32_t expectedArriveNum = waitCounter * arriveSize;
        __attribute__((cce_global)) BarrierInfo *BarrierInfoAddr = barrierInfoArrive + (CACHE_LINE_LEN / sizeof(BarrierInfo)) * waitIndex;
        dcci((__attribute__((cce_global)) uint64_t *)BarrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while (BarrierInfoAddr->head < expectedArriveNum) {
            dcci((__attribute__((cce_global)) uint64_t *)BarrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        }
        __WriteCurrentValue(barrierInfoWait);
        counter =
            (hasArrive) ? counter : counter + 1;
        hasArrive = false;
    }
}

template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) uint64_t GroupBarrier<pipeMode>::GetWorkspaceLen()
{
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                        ;
                                                                                                  ;

                                                                                                            ;

                                                                                                                ;
        return (arriveSize > waitSize) ? arriveSize * CACHE_LINE_LEN : waitSize * CACHE_LINE_LEN;
    }
}

template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) void GroupBarrier<pipeMode>::__WriteCurrentValue(__attribute__((cce_global)) BarrierInfo *BarrierInfoAddr)
{
    if constexpr(g_coreType == AscendC::AIV) {
        uint32_t num = (arriveSize >= waitSize) ? arriveSize : waitSize;
        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventID);
        WaitFlag<HardEvent::S_MTE3>(eventID);
        SetAtomicAddImpl<int32_t>();







        __attribute__((cce_unif_buff)) int32_t *dst = (__attribute__((cce_unif_buff)) int32_t *)(UB_START_ADDR);

        copy_ubuf_to_gm((__attribute__((cce_global)) void *)BarrierInfoAddr, (__attribute__((cce_unif_buff)) void *)dst, 0, num, 1, 0, CACHELINE_BLKNUM - 1);

        SetAtomicNoneImpl();
    }
}
}
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2





# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_pop_stack_buffer.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_pop_stack_buffer.h"
namespace AscendC {
template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) uint64_t GetEndAddress()
{
    Hardware hardType = GetPhyType(pos);
                                                                                                      ;


    return TOTAL_UB_SIZE - sizeof(KfcMsg);



}

template <typename T, TPosition pos> [aicore] __inline__ __attribute__((always_inline)) bool PopStackBuffer(LocalTensor<T>& popLocal)
{
    TBuffAddr addr;
    addr.logicPos = (int8_t)pos;
                                                                                                       ;
    uint64_t endAddress = GetEndAddress<pos>();
    uint64_t queEndAddress = GetTPipePtr()->GetQueueEndAddress<pos>();

                                                                                                                   ;
    addr.dataLen = (uint32_t)(endAddress - queEndAddress);
    addr.bufferAddr = queEndAddress;
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_pop_stack_buffer.h"
    popLocal.SetAddr(addr);
    return true;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) bool PopStackBuffer(TBuf<pos>& popBuffer, TBufType& bufStart)
{
    uint64_t endAddress = GetEndAddress<pos>();
    uint64_t queEndAddress = GetTPipePtr()->GetQueueEndAddress<pos>();

                                                                                                                   ;
    uint32_t dataLen = (uint32_t)(endAddress - queEndAddress);
    bufStart.address = queEndAddress;
    bufStart.dataLen = dataLen;
    popBuffer.SetTpipeBuf(&bufStart, dataLen);







    return true;
}
}
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2

namespace AscendC {
# 66 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h"
template <bool isAIVOnly = true>
[aicore] __inline__ __attribute__((always_inline)) void IBSet(const GlobalTensor<int32_t>& gmWorkspace, const LocalTensor<int32_t>& ubWorkspace,
                                  int32_t blockIdx, int32_t eventID);

template <bool isAIVOnly = true>
[aicore] __inline__ __attribute__((always_inline)) void IBWait(const GlobalTensor<int32_t>& gmWorkspace, const LocalTensor<int32_t>& ubWorkspace,
                                   int32_t blockIdx, int32_t eventID);
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h"
template<pipe_t AIV_PIPE = PIPE_MTE3, pipe_t AIC_PIPE = PIPE_FIX>

[aicore] __inline__ __attribute__((always_inline)) void SetNextTaskStart();

[aicore] __inline__ __attribute__((always_inline)) void WaitPreTaskEnd();







template <bool isAIVOnly = true>
[aicore] __inline__ __attribute__((always_inline)) void SyncAll(const GlobalTensor<int32_t>& gmWorkspace, const LocalTensor<int32_t>& ubWorkspace,
                                 const int32_t usedCores = 0);

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockIdx();

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockNum();

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockIdx();

[aicore] __inline__ __attribute__((always_inline)) int64_t GetTaskRation();

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("V")))
    __attribute__((out_pipe("MTE3"))) void InitOutput(GlobalTensor<T> gmWorkspaceAddr, uint32_t size, T value = 0);

template <bool isAIVOnly = true>
[aicore] __inline__ __attribute__((always_inline)) void SyncAll();

enum class AtomicDtype { ATOMIC_NONE = 0, ATOMIC_F32, ATOMIC_F16, ATOMIC_S16, ATOMIC_S32, ATOMIC_S8, ATOMIC_BF16 };

enum class AtomicOp { ATOMIC_SUM = 0 };

template <AtomicDtype type, AtomicOp op>
[aicore] __inline__ __attribute__((always_inline)) void SetStoreAtomicConfig();

[aicore] __inline__ __attribute__((always_inline)) void GetStoreAtomicConfig(uint16_t& atomicType, uint16_t& atomicOp);

template <uint8_t modeId, pipe_t pipe>
[aicore] __inline__ __attribute__((always_inline)) void CrossCoreSetFlag(uint16_t flagId);

template <uint8_t modeId = 0, pipe_t pipe = PIPE_S>
[aicore] __inline__ __attribute__((always_inline)) void CrossCoreWaitFlag(uint16_t flagId);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCachePreload(const GlobalTensor<uint64_t>& srcTensor, const T cacheOffset);

[aicore] __inline__ __attribute__((always_inline)) int64_t GetICachePreloadStatus();

[aicore] __inline__ __attribute__((always_inline)) void ICachePreLoad(const int64_t preFetchLen);

[aicore] __inline__ __attribute__((always_inline)) void CheckLocalMemoryIA(const CheckLocalMemoryIAParam& checkParams);
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_proposal.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_proposal.h"
namespace AscendC {
struct MrgSort4Info {
    [aicore] MrgSort4Info() {}

    [aicore] MrgSort4Info(const uint16_t elementLengthsIn[MRG_SORT_ELEMENT_LEN], const bool ifExhaustedSuspensionIn,
        const uint16_t validBitIn, const uint16_t repeatTimesIn)
        : ifExhaustedSuspension(ifExhaustedSuspensionIn),
          validBit(validBitIn),
          repeatTimes(repeatTimesIn)
    {
        for (int32_t i = 0; i < MRG_SORT_ELEMENT_LEN; ++i) {
            elementLengths[i] = elementLengthsIn[i];
        }
    }

    uint16_t elementLengths[MRG_SORT_ELEMENT_LEN] = { 0 };
    bool ifExhaustedSuspension = false;
    uint16_t validBit = 0;
    uint8_t repeatTimes = 1;
};

template <typename T> struct MrgSortSrcList {
    [aicore] MrgSortSrcList() {}

    [aicore] MrgSortSrcList(const LocalTensor<T>& src1In, const LocalTensor<T>& src2In, const LocalTensor<T>& src3In,
        const LocalTensor<T>& src4In)
    {
        src1 = src1In[0];
        src2 = src2In[0];
        src3 = src3In[0];
        src4 = src4In[0];
    }

    LocalTensor<T> src1;
    LocalTensor<T> src2;
    LocalTensor<T> src3;
    LocalTensor<T> src4;
};
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h" 2








# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_proposal_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_proposal_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_proposal_base_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_proposal_base_impl.h"
namespace AscendC {
constexpr uint32_t SORT_LEN = 4;
constexpr uint32_t SORT_NUM_TWO = 2;
constexpr uint32_t SORT_NUM_THREE = 3;

[aicore] __inline__ __attribute__((always_inline)) void ComSortInnerLoopTail(uint32_t& offset0Tail, uint32_t& offset1Tail, uint32_t& offset2Tail,
    uint32_t& offset3Tail, uint16_t& validBitTail, uint16_t (&elementCountListTail)[SORT_LEN],
    const uint32_t baseOffset, const uint32_t elementCountTail, int32_t mergeTmpTailQueNum)
{
    if (mergeTmpTailQueNum == SORT_NUM_TWO) {
        offset1Tail = offset0Tail + baseOffset;
        elementCountListTail[1] = elementCountTail;
        offset2Tail = 0;
        elementCountListTail[2] = 0;
        offset3Tail = 0;
        elementCountListTail[3] = 0;
        validBitTail = 0b0011;
    } else if (mergeTmpTailQueNum == SORT_NUM_THREE) {
        offset1Tail = offset0Tail + baseOffset;
        offset2Tail = offset0Tail + SORT_NUM_TWO * baseOffset;
        elementCountListTail[2] = elementCountTail;
        offset3Tail = 0;
        elementCountListTail[3] = 0;
        validBitTail = 0b0111;
    } else {
        offset1Tail = offset0Tail + baseOffset;
        offset2Tail = offset0Tail + SORT_NUM_TWO * baseOffset;
        offset3Tail = offset0Tail + SORT_NUM_THREE * baseOffset;
        elementCountListTail[3] = elementCountTail;
        validBitTail = 0b1111;
    }
}

}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_proposal_impl.h" 2


namespace AscendC {
constexpr uint32_t singleSortElementCountV220 = 32;
constexpr uint32_t singleSortElementCountV200 = 16;
constexpr uint32_t regionProposalDataSize = 8;

template <typename T>
[[deprecated("NOTICE: MrgSort4 is not deprecated. Currently, MrgSort4 is an unsupported API on current device."
             "Please check your code!")]]
[aicore] __inline__ __attribute__((always_inline)) void Vmrgsort4Cal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* addrArray[MRG_SORT_ELEMENT_LEN], uint64_t config)
{
                                                 ;
}

template <typename T>
[[deprecated("NOTICE: RpSort16 is not deprecated. Currently, RpSort16 is an unsupported API on current device."
             "Please check your code!")]]
[aicore] __inline__ __attribute__((always_inline)) void VbitsortCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const ProposalIntriParams& intriParams)
{
                                                 ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VbitsortCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* src0Local, __attribute__((cce_unif_buff)) uint32_t* src1Local,
    const ProposalIntriParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        vbitsort(dstLocal, src0Local, src1Local, intriParams.repeat);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Vmrgsort4Cal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* addrArray[MRG_SORT_ELEMENT_LEN], uint64_t src1,
    uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIV) {
        vmrgsort4(dstLocal, addrArray, src1, config);
    }
}

template <typename T>
[[deprecated(
    "NOTICE: ProposalConcat is not deprecated. Currently, ProposalConcat is an unsupported API on current device."
    "Please check your code!")]]
[aicore] __inline__ __attribute__((always_inline)) void VconcatCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const ProposalIntriParams& intriParams)
{
                                                       ;
}

template <typename T>
[[deprecated(
    "NOTICE: ProposalExtract is not deprecated. Currently, ProposalExtract is an unsupported API on current device."
    "Please check your code!")]]
[aicore] __inline__ __attribute__((always_inline)) void VextractCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const ProposalIntriParams& intriParams)
{
                                                        ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgSortCal(const LocalTensor<T> &dstLocal, const MrgSortSrcList<T> &sortList,
    const uint16_t elementCountList[4], uint32_t sortedNum[4], uint16_t validBit, const int32_t repeatTimes)
{
    MrgSort4Info mrgSortInfo(elementCountList, false, validBit, (uint16_t)repeatTimes);





    uint64_t config = 0;
    config |= (mrgSortInfo.repeatTimes & 0xFF);
    config |= (uint64_t(mrgSortInfo.validBit & 0xF) << 8);
    config |= (uint64_t(mrgSortInfo.ifExhaustedSuspension & 0x1) << 12);

    uint64_t src1 = 0;
    src1 |= (uint64_t(mrgSortInfo.elementLengths[0] & 0xFFFF));
    src1 |= (uint64_t(mrgSortInfo.elementLengths[1] & 0xFFFF) << 16);
    src1 |= (uint64_t(mrgSortInfo.elementLengths[2] & 0xFFFF) << 32);
    src1 |= (uint64_t(mrgSortInfo.elementLengths[3] & 0xFFFF) << 48);

    __attribute__((cce_unif_buff)) T *addrArray[MRG_SORT_ELEMENT_LEN] = {(__attribute__((cce_unif_buff)) T *)sortList.src1.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)sortList.src2.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)sortList.src3.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)sortList.src4.GetPhyAddr()};

    Vmrgsort4Cal((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), addrArray, src1, config);
}

[aicore] __inline__ __attribute__((always_inline)) void GetMrgSortResultImpl(
    uint16_t &mrgSortList1, uint16_t &mrgSortList2, uint16_t &mrgSortList3, uint16_t &mrgSortList4)
{
    int64_t mrgSortResult = get_vms4_sr();
    constexpr uint64_t resMask = 0xFFFF;

    mrgSortList1 = static_cast<uint64_t>(mrgSortResult) & resMask;
    constexpr uint64_t sortList2Bit = 16;

    mrgSortList2 = (static_cast<uint64_t>(mrgSortResult) >> sortList2Bit) & resMask;
    constexpr uint64_t sortList3Bit = 32;

    mrgSortList3 = (static_cast<uint64_t>(mrgSortResult) >> sortList3Bit) & resMask;
    constexpr uint64_t sortList4Bit = 48;

    mrgSortList4 = (static_cast<uint64_t>(mrgSortResult) >> sortList4Bit) & resMask;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FullSortInnerLoop(const LocalTensor<T> &dstLocal, const LocalTensor<T> &tmpLocal,
    const uint32_t baseOffset, const uint16_t singleMergeTmpElementCount, const int32_t mergeTmpRepeatTimes)
{
    if (mergeTmpRepeatTimes <= 0) {
        return;
    }
    MrgSortSrcList sortList =
        MrgSortSrcList(tmpLocal[0], tmpLocal[baseOffset], tmpLocal[2 * baseOffset], tmpLocal[3 * baseOffset]);
    const uint16_t elementCountList[MRG_SORT_ELEMENT_LEN] = {singleMergeTmpElementCount, singleMergeTmpElementCount,
        singleMergeTmpElementCount, singleMergeTmpElementCount};
    uint32_t sortedNum[MRG_SORT_ELEMENT_LEN];
    MrgSortCal<T>(dstLocal, sortList, elementCountList, sortedNum, 0b1111, mergeTmpRepeatTimes);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FullSortInnerLoopTail(const LocalTensor<T> &dstLocal, const LocalTensor<T> &tmpLocal,
    const uint32_t baseOffset, const uint16_t singleMergeTmpElementCount, const uint32_t elementCountTail,
    const int32_t mergeTmpRepeatTimes, int32_t mergeTmpTailQueNum)
{
    if (mergeTmpTailQueNum <= 0) {
        return;
    }
    uint16_t validBitTail;
    uint16_t elementCountListTail[MRG_SORT_ELEMENT_LEN] = {singleMergeTmpElementCount, singleMergeTmpElementCount,
        singleMergeTmpElementCount, singleMergeTmpElementCount};
    uint32_t offset1Tail, offset2Tail, offset3Tail;
    uint32_t offset0Tail = MRG_SORT_ELEMENT_LEN * baseOffset * mergeTmpRepeatTimes;

    ComSortInnerLoopTail(offset0Tail, offset1Tail, offset2Tail, offset3Tail, validBitTail, elementCountListTail,
        baseOffset, elementCountTail, mergeTmpTailQueNum);
    if (mergeTmpTailQueNum > 1) {
        MrgSortSrcList sortListTail = MrgSortSrcList(tmpLocal[offset0Tail], tmpLocal[offset1Tail],
            tmpLocal[offset2Tail], tmpLocal[offset3Tail]);
        uint32_t sortedNumTail[MRG_SORT_ELEMENT_LEN];
        MrgSortCal<T>(dstLocal[offset0Tail], sortListTail, elementCountListTail, sortedNumTail,
            validBitTail, 1);
    } else {
        if constexpr (IsSameType<T, __cce_half>::value) {
            DataCopy(dstLocal[offset0Tail], tmpLocal[offset0Tail], elementCountTail * 4);
        } else {
            DataCopy(dstLocal[offset0Tail], tmpLocal[offset0Tail], elementCountTail * 2);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetFullSortInnerLoopTimes(const int32_t repeatTimes)
{
    uint32_t loopi = 0;
    int32_t queNum = repeatTimes;
    while (queNum > 1) {
        queNum = Ceil(queNum, MRG_SORT_ELEMENT_LEN);
        loopi++;
    }
    return loopi;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DoFullSort(const LocalTensor<T> &dstLocal, const LocalTensor<T> &concatLocal,
    const LocalTensor<uint32_t> &indexLocal, LocalTensor<T> &tmpLocal, const int32_t repeatTimes)
{
    uint32_t elementCount = concatLocal.GetSize();
    uint32_t singleMergeElementCount = singleSortElementCountV220;
    uint32_t loopi = GetFullSortInnerLoopTimes(repeatTimes);
    uint16_t singleMergeTmpElementCount = singleMergeElementCount;
    uint32_t srcLocalElementCount = repeatTimes * singleMergeElementCount;
    uint32_t dstLocalElementCount = srcLocalElementCount * regionProposalDataSize / sizeof(T);
    int32_t mergeTmpTotalQueNum = repeatTimes;
    int32_t mergeTmpTailQueNum = repeatTimes % MRG_SORT_ELEMENT_LEN;
    int32_t mergeTmpQueNum = mergeTmpTotalQueNum - mergeTmpTailQueNum;
    int32_t mergeTmpRepeatTimes = repeatTimes / MRG_SORT_ELEMENT_LEN;
    DataCopy(tmpLocal, dstLocal, dstLocalElementCount);
    PipeBarrier<PIPE_V>();
    for (int i = 0; i < loopi; i++) {
        uint32_t baseOffset;
        baseOffset = singleMergeTmpElementCount * regionProposalDataSize / sizeof(T);
        FullSortInnerLoop(dstLocal, tmpLocal, baseOffset, singleMergeTmpElementCount, mergeTmpRepeatTimes);
        PipeBarrier<PIPE_V>();
        uint16_t elementCountTail = srcLocalElementCount % singleMergeTmpElementCount ?
            srcLocalElementCount % singleMergeTmpElementCount :
            singleMergeTmpElementCount;
        FullSortInnerLoopTail(dstLocal, tmpLocal, baseOffset, singleMergeTmpElementCount, elementCountTail,
            mergeTmpRepeatTimes, mergeTmpTailQueNum);
        PipeBarrier<PIPE_V>();
        DataCopy(tmpLocal, dstLocal, dstLocalElementCount);
        PipeBarrier<PIPE_V>();
        singleMergeTmpElementCount *= MRG_SORT_ELEMENT_LEN;
        mergeTmpTotalQueNum = mergeTmpTotalQueNum % MRG_SORT_ELEMENT_LEN ?
            mergeTmpTotalQueNum / MRG_SORT_ELEMENT_LEN + 1 :
            mergeTmpTotalQueNum / MRG_SORT_ELEMENT_LEN;
        mergeTmpTailQueNum = mergeTmpTotalQueNum % MRG_SORT_ELEMENT_LEN;
        if (mergeTmpTailQueNum == 0 && elementCountTail != singleMergeTmpElementCount) {
            mergeTmpTailQueNum = MRG_SORT_ELEMENT_LEN;
        }
        mergeTmpQueNum = mergeTmpTotalQueNum - mergeTmpTailQueNum;
        mergeTmpRepeatTimes = mergeTmpQueNum / MRG_SORT_ELEMENT_LEN;
    }
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_gather_mask_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_gather_mask_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_gather.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_gather.h"
namespace AscendC {
struct GatherRepeatParams {
    [aicore] GatherRepeatParams() {}

    [aicore] GatherRepeatParams(const uint8_t dstBlkStrideIn, const uint8_t dstRepStrideIn)
        : dstBlkStride(dstBlkStrideIn),
          dstRepStride(dstRepStrideIn)
    {}

    uint32_t blockNumber = DEFAULT_BLK_NUM;
    uint16_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
};

enum class GatherMaskMode : uint8_t {
    VERSION_V1 = 0,
    VERSION_V2 = 1
};


const GatherMaskMode defaultGahterMaskMode = GatherMaskMode::VERSION_V2;
const GatherMaskMode defaultGatherMaskMode = GatherMaskMode::VERSION_V2;





struct GatherMaskParams {
    [aicore] GatherMaskParams() {}

    [aicore] GatherMaskParams(const uint8_t src0BlockStrideIn, const uint16_t repeatTimesIn,
        const uint16_t src0RepeatStrideIn, const uint8_t src1RepeatStrideIn)
        : src0BlockStride(src0BlockStrideIn),
          repeatTimes(repeatTimesIn),
          src0RepeatStride(src0RepeatStrideIn),
          src1RepeatStride(src1RepeatStrideIn)
    {}

    uint8_t src0BlockStride = DEFAULT_BLK_STRIDE;
    uint16_t repeatTimes = 0;
    uint16_t src0RepeatStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepeatStride = DEFAULT_REPEAT_STRIDE;
};
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_gather_mask_impl.h" 2

namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskImpl(__attribute__((cce_unif_buff)) uint16_t* dst, __attribute__((cce_unif_buff)) uint16_t* src0, __attribute__((cce_unif_buff)) uint16_t* src1,
    const uint8_t PatternMode, const bool reduceMode, const uint32_t mask, const GatherMaskParams& reducev2Params,
    uint64_t& rsvdCnt)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (reduceMode) {
            SetMaskCount();
        } else {
            SetMaskNorm();
        }







        set_vector_mask(0, mask);

        vreducev2(dst, src0, src1, reducev2Params.repeatTimes, reducev2Params.src0BlockStride, PatternMode,
            reducev2Params.src0RepeatStride, reducev2Params.src1RepeatStride);
        rsvdCnt = AscendCUtils::GetRsvdCnt();
        SetMaskNorm();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GatherMaskImpl(__attribute__((cce_unif_buff)) uint32_t* dst, __attribute__((cce_unif_buff)) uint32_t* src0, __attribute__((cce_unif_buff)) uint32_t* src1,
    const uint8_t PatternMode, const bool reduceMode, const uint32_t mask, const GatherMaskParams& reducev2Params,
    uint64_t& rsvdCnt)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (reduceMode) {
            SetMaskCount();
        } else {
            SetMaskNorm();
        }







        set_vector_mask(0, mask);

        vreducev2(dst, src0, src1, reducev2Params.repeatTimes, reducev2Params.src0BlockStride, PatternMode,
            reducev2Params.src0RepeatStride, reducev2Params.src1RepeatStride);
        rsvdCnt = AscendCUtils::GetRsvdCnt();
        SetMaskNorm();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint8_t PatternMode,
    const GatherMaskParams& reducev2Params)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                                                                                        ;
        if (sizeof(T) == sizeof(uint16_t)) {
            vreducev2(reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src0),
                reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src1), reducev2Params.repeatTimes, reducev2Params.src0BlockStride,
                PatternMode, reducev2Params.src0RepeatStride, reducev2Params.src1RepeatStride);
        } else {
            vreducev2(reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src0),
                reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src1), reducev2Params.repeatTimes, reducev2Params.src0BlockStride,
                PatternMode, reducev2Params.src0RepeatStride, reducev2Params.src1RepeatStride);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskImpl(
    __attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, const uint8_t PatternMode, const GatherMaskParams& reducev2Params)
{
                                                                             ;
    __attribute__((cce_unif_buff)) T* nullsrc1 = ONE_REPEAT_BYTE_SIZE * sizeof(T) + src0;
    GatherMaskImpl(dst, src0, nullsrc1, PatternMode, reducev2Params);
}

template <typename T, GatherMaskMode mode = defaultGatherMaskMode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) uint16_t* src1, const bool reduceMode,
    const uint32_t mask, const GatherMaskParams& reducev2Params, uint64_t& rsvdCnt)
{
                                                                                                               ;


                                                           ;
    GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src0), src1, 0,
        reduceMode, mask, reducev2Params, rsvdCnt);
}

template <typename T, GatherMaskMode mode = defaultGatherMaskMode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) uint32_t* src1, const bool reduceMode,
    const uint32_t mask, const GatherMaskParams& reducev2Params, uint64_t& rsvdCnt)
{
                                                                                                               ;


                                         ;
    GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src0), src1, 0,
        reduceMode, mask, reducev2Params, rsvdCnt);
}

template <typename T, GatherMaskMode mode = defaultGatherMaskMode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, const uint8_t src1Pattern,
    const bool reduceMode, const uint32_t mask, const GatherMaskParams& reducev2Params, uint64_t& rsvdCnt)
{
                                                                                                               ;
                                                                             ;


                                                                                                    ;
    __attribute__((cce_unif_buff)) T* nullsrc1 = ONE_REPEAT_BYTE_SIZE * sizeof(T) + src0;
    if (sizeof(T) == sizeof(uint16_t)) {
        GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src0),
            reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(nullsrc1), src1Pattern, reduceMode, mask, reducev2Params, rsvdCnt);
    } else {
        GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src0),
            reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(nullsrc1), src1Pattern, reduceMode, mask, reducev2Params, rsvdCnt);
    }
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetGatherMaskRemainCountImpl()
{
    return get_rsvd_cnt();
}
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h" 2
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
namespace AscendC {
#pragma begin_pipe(V)
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort4(const LocalTensor<T>& dstLocal, const MrgSortSrcList<T>& srcLocal,
    const MrgSort4Info& params);
# 72 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void RpSort16(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes);
# 88 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort(const LocalTensor<T>& dstLocal, const MrgSortSrcList<T>& srcLocal,
    const MrgSort4Info& params);
# 101 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sort32(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<uint32_t>& src1Local, const int32_t repeatTimes);
# 115 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ProposalConcat(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes, const int32_t modeNumber);
# 128 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ProposalExtract(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes, const int32_t modeNumber);
# 141 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Concat(LocalTensor<T> &concatLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &tmpLocal, const int32_t repeatTimes);
# 154 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Extract(const LocalTensor<T> &dstValueLocal, const LocalTensor<uint32_t> &dstIndexLocal,
    const LocalTensor<T> &sortedLocal, const int32_t repeatTimes);
# 169 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T, bool isExhaustedSuspension = false>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort(const LocalTensor<T> &dstLocal, const MrgSortSrcList<T> &sortList,
    const uint16_t elementCountList[4], uint32_t sortedNum[4], uint16_t validBit, const int32_t repeatTimes);
# 183 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T, bool isFullSort>
[aicore] __inline__ __attribute__((always_inline)) void Sort(const LocalTensor<T> &dstLocal, const LocalTensor<T> &concatLocal,
    const LocalTensor<uint32_t> &indexLocal, LocalTensor<T> &tmpLocal, const int32_t repeatTimes);







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetSortOffset(const uint32_t elemOffset);







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetSortLen(const uint32_t elemCount);
#pragma end_pipe
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetMrgSortResult(
    uint16_t &mrgSortList1, uint16_t &mrgSortList2, uint16_t &mrgSortList3, uint16_t &mrgSortList4);
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_determine_compute_sync_intf.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_determine_compute_sync_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_determine_compute_sync_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_determine_compute_sync_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_duplicate_intf.h" 1
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_duplicate_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_duplicate_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, uint64_t mask,
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, uint64_t mask[],
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride);
# 70 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_duplicate_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, const int32_t& calCount);
}
#pragma end_pipe
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_determine_compute_sync_impl.h" 2



namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void InitDetermineComputeWorkspaceCalc(GlobalTensor<int32_t> &gmWorkspace,
    LocalTensor<int32_t> &ubWorkspace)
{
    if constexpr(g_coreType == AscendC::AIV) {
        PipeBarrier<PIPE_ALL>();
        event_t eventID;
        auto blockNum = GetBlockNum();
        auto blockIdx = GetBlockIdx();
        if (GetBlockIdx() == 0) {
            Duplicate(ubWorkspace, 0, B32_DATA_NUM_PER_BLOCK * blockNum);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
            SetFlag<HardEvent::V_MTE3>(eventID);
            WaitFlag<HardEvent::V_MTE3>(eventID);
            DataCopy(gmWorkspace, ubWorkspace, B32_DATA_NUM_PER_BLOCK * blockNum);
        }
        ubWorkspace.SetValue(blockNum * B32_DATA_NUM_PER_BLOCK, 1);
        PipeBarrier<PIPE_ALL>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) bool CheckUBWorkspace(LocalTensor<int32_t> &ubWorkspace, int64_t blockIdx, int64_t blockNum)
{
    int32_t repeatTime = ubWorkspace.GetValue(blockNum * B32_DATA_NUM_PER_BLOCK);
    int64_t offset = 0;


    for (; offset < blockIdx * B32_DATA_NUM_PER_BLOCK; offset += B32_DATA_NUM_PER_BLOCK) {
        if (ubWorkspace.GetValue(offset) != repeatTime) {
            return false;
        }
    }
    for (; offset < blockNum * B32_DATA_NUM_PER_BLOCK; offset += B32_DATA_NUM_PER_BLOCK) {
        if (ubWorkspace.GetValue(offset) != 0) {
            return false;
        }
    }
    return true;
}

[aicore] __inline__ __attribute__((always_inline)) void WaitPreBlockCalc(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    if constexpr(g_coreType == AscendC::AIV) {
        PipeBarrier<PIPE_ALL>();
        event_t eventID;
        auto blockIdx = GetBlockIdx();
        auto blockNum = GetBlockNum();
        bool matchFlag;
        do {
            DataCopy(ubWorkspace, gmWorkspace, blockNum * B32_DATA_NUM_PER_BLOCK);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
            SetFlag<HardEvent::MTE2_S>(eventID);
            WaitFlag<HardEvent::MTE2_S>(eventID);
            matchFlag = CheckUBWorkspace(ubWorkspace, blockIdx, blockNum);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE2));
            SetFlag<HardEvent::S_MTE2>(eventID);
            WaitFlag<HardEvent::S_MTE2>(eventID);
        } while (!matchFlag);
        PipeBarrier<PIPE_ALL>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void NotifyNextBlockCalc(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    if constexpr(g_coreType == AscendC::AIV) {
        PipeBarrier<PIPE_ALL>();
        event_t eventID;
        auto blockIdx = GetBlockIdx();
        auto blockNum = GetBlockNum();
        int32_t repeatTime = ubWorkspace.GetValue(blockNum * B32_DATA_NUM_PER_BLOCK);
        if (blockIdx + 1 == blockNum) {
            Duplicate(ubWorkspace, 0, blockNum * B32_DATA_NUM_PER_BLOCK);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
            SetFlag<HardEvent::V_MTE3>(eventID);
            WaitFlag<HardEvent::V_MTE3>(eventID);
            DataCopy(gmWorkspace, ubWorkspace, blockNum * B32_DATA_NUM_PER_BLOCK);
        } else {
            auto offset = blockIdx * B32_DATA_NUM_PER_BLOCK;
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
            SetFlag<HardEvent::S_V>(eventID);
            WaitFlag<HardEvent::S_V>(eventID);
            Duplicate(ubWorkspace[offset], repeatTime, B32_DATA_NUM_PER_BLOCK);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
            SetFlag<HardEvent::V_MTE3>(eventID);
            WaitFlag<HardEvent::V_MTE3>(eventID);
            DataCopy(gmWorkspace[offset], ubWorkspace[offset], B32_DATA_NUM_PER_BLOCK);
        }
        ubWorkspace.SetValue(blockNum * B32_DATA_NUM_PER_BLOCK, repeatTime + 1);
        PipeBarrier<PIPE_ALL>();
    }
}
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_determine_compute_sync_intf.h" 2






namespace AscendC {





[aicore] __inline__ __attribute__((always_inline)) void InitDetermineComputeWorkspace(GlobalTensor<int32_t>& gmWorkspace,
    LocalTensor<int32_t>& ubWorkspace);





[aicore] __inline__ __attribute__((always_inline)) void WaitPreBlock(GlobalTensor<int32_t>& gmWorkspace, LocalTensor<int32_t>& ubWorkspace);






[aicore] __inline__ __attribute__((always_inline)) void NotifyNextBlock(GlobalTensor<int32_t>& gmWorkspace, LocalTensor<int32_t>& ubWorkspace);
}
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_transpose.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_transpose.h"
namespace AscendC {
enum class TransposeType : uint8_t {

    TRANSPOSE_TYPE_NONE,


    TRANSPOSE_NZ2ND_0213,


    TRANSPOSE_NZ2NZ_0213,


    TRANSPOSE_NZ2NZ_012_WITH_N,


    TRANSPOSE_NZ2ND_012_WITH_N,

    TRANSPOSE_NZ2ND_012_WITHOUT_N,


    TRANSPOSE_NZ2NZ_012_WITHOUT_N,
    TRANSPOSE_ND2ND_ONLY,
    TRANSPOSE_ND_UB_GM,
    TRANSPOSE_GRAD_ND_UB_GM,
    TRANSPOSE_ND2ND_B16,
    TRANSPOSE_NCHW2NHWC,
    TRANSPOSE_NHWC2NCHW,
};

struct TransDataTo5HDParams {
    [aicore] TransDataTo5HDParams() {}

    [aicore] TransDataTo5HDParams(const bool dstHighHalfIn, const bool srcHighHalfIn, const uint8_t repeatTimesIn,
        const uint16_t dstRepStrideIn, const uint16_t srcRepStrideIn)
        : dstHighHalf(dstHighHalfIn),
          srcHighHalf(srcHighHalfIn),
          repeatTimes(repeatTimesIn),
          dstRepStride(dstRepStrideIn),
          srcRepStride(srcRepStrideIn)
    {}

    bool dstHighHalf = false;
    bool srcHighHalf = false;
    uint8_t repeatTimes = 1;
    uint16_t dstRepStride = 0;
    uint16_t srcRepStride = 0;
};

struct TransposeParamsExt {
    [aicore] TransposeParamsExt() {}

    [aicore] TransposeParamsExt(const uint16_t nSizeIn, const uint16_t cSizeIn, const uint16_t hSizeIn,
        const uint16_t wSizeIn, const TransposeType transposeTypeIn)
        : nSize(nSizeIn),
          cSize(cSizeIn),
          hSize(hSizeIn),
          wSize(wSizeIn),
          transposeType(transposeTypeIn)
    {}

    uint16_t nSize = 0;
    uint16_t cSize = 0;
    uint16_t hSize = 0;
    uint16_t wSize = 0;
    TransposeType transposeType = TransposeType::TRANSPOSE_ND2ND_B16;
};
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h" 2
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_transpose_impl.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_transpose_impl.h"
namespace AscendC {
constexpr int8_t TWO_NUM = 2;
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dstList[16], __attribute__((cce_unif_buff)) float* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dstList[16], __attribute__((cce_unif_buff)) int32_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) uint32_t* dstList[16], __attribute__((cce_unif_buff)) uint32_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dstList[16], __attribute__((cce_unif_buff)) int16_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) uint16_t* dstList[16], __attribute__((cce_unif_buff)) uint16_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) __cce_half* dstList[16], __attribute__((cce_unif_buff)) __cce_half* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDB8IntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstList[16], __attribute__((cce_unif_buff)) T* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == false)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, false);
    } else if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, true);
    } else if ((transDataTo5HDParams.dstHighHalf == true) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, true);
    } else {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, false);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) int8_t* dstList[16], __attribute__((cce_unif_buff)) int8_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl(dstList, srcList, transDataTo5HDParams);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dstList[16], __attribute__((cce_unif_buff)) uint8_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl(dstList, srcList, transDataTo5HDParams);
}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
                                                                                            ;
}

template<>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<float>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<int32_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<uint32_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<int16_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<uint16_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<__cce_half>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDB8IntrinsicsImpl(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == false)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, false);
    } else if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, true);
    } else if ((transDataTo5HDParams.dstHighHalf == true) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, true);
    } else {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, false);
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<int8_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl<int8_t>(dstList, srcList, transDataTo5HDParams);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<uint8_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl<uint8_t>(dstList, srcList, transDataTo5HDParams);
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void SetVaReg(__attribute__((cce_unif_buff)) T* dstList[16], __attribute__((cce_unif_buff)) T* srcList[16])
{
    uint64_t vaRegArray1[VA_REG_ARRAY_LEN];
    uint64_t vaRegArray2[VA_REG_ARRAY_LEN];
    uint64_t vaRegArray3[VA_REG_ARRAY_LEN];
    uint64_t vaRegArray4[VA_REG_ARRAY_LEN];

    for (int32_t i = 0; i < VA_REG_ARRAY_LEN; i++) {
        vaRegArray1[i] = (uint64_t)dstList[i];
        vaRegArray2[i] = (uint64_t)dstList[VA_REG_ARRAY_LEN + i];
        vaRegArray3[i] = (uint64_t)srcList[i];
        vaRegArray4[i] = (uint64_t)srcList[VA_REG_ARRAY_LEN + i];
    }

    set_va_reg_sb(VA0, vaRegArray1);
    set_va_reg_sb(VA1, vaRegArray2);
    set_va_reg_sb(VA2, vaRegArray3);
    set_va_reg_sb(VA3, vaRegArray4);
}

[aicore] __inline__ __attribute__((always_inline)) void SetVaReg(uint64_t dst[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t src[NCHW_CONV_ADDR_LIST_SIZE])
{
    set_va_reg_sb(VA0, dst);
    set_va_reg_sb(VA1, dst + VA_REG_ARRAY_LEN);
    set_va_reg_sb(VA2, src);
    set_va_reg_sb(VA3, src + VA_REG_ARRAY_LEN);
}

[aicore] __inline__ __attribute__((always_inline)) void VldVaReg(__attribute__((cce_unif_buff)) uint64_t* dst, __attribute__((cce_unif_buff)) uint64_t* src)
{
    vld_va_reg(VA0, dst, L128);
    vld_va_reg(VA1, dst, H128);
    vld_va_reg(VA2, src, L128);
    vld_va_reg(VA3, src, H128);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDImpl(__attribute__((cce_unif_buff)) T* dstList[16], __attribute__((cce_unif_buff)) T* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        SetVaReg(dstList, srcList);
        TransDataTo5HDIntrinsicsImpl(dstList, srcList, transDataTo5HDParams);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDImpl(uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& transDataTo5HDParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        SetVaReg(dstList, srcList);
        TransDataTo5HDIntrinsicsImpl<T>(dstList, srcList, transDataTo5HDParams);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDVldVaRegImpl(
    __attribute__((cce_unif_buff)) uint64_t* dst, __attribute__((cce_unif_buff)) uint64_t* src, const TransDataTo5HDParams& transDataTo5HDParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        VldVaReg(dst, src);
        uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE] = { 0 };
        uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE] = { 0 };
        TransDataTo5HDIntrinsicsImpl<T>(dstList, srcList, transDataTo5HDParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void TransposeIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src)
{
    vtranspose((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src);
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void TransposeImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src)
{
    if constexpr(g_coreType == AscendC::AIV) {
        TransposeIntrinsicsImpl((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src);
    }
}

template <typename T> struct Transpose4dParams {
    [aicore] Transpose4dParams(){};

    uint8_t blockSize = 1;
    uint16_t tmp1RemainRowCount = 0;
    uint16_t tmp1CopyCount = 0;
    uint32_t tmp1NeedRowCount = 0;
    uint16_t tmp2Count = 0;
    uint16_t tmp2NeedRowCount = NCHW_CONV_ADDR_LIST_SIZE;
    uint16_t tmp3RemainRowCount = 0;
    uint16_t copyCIndex = 0;
    uint16_t srcBlockIndex = 0;
    uint32_t preCinnerOffset = 0;
    uint32_t preCoffset = 0;
    uint16_t dstBlockNum = 0;
    uint16_t dstNeedBlockNum = 0;
    uint16_t imageSize = 0;
    uint32_t oneChwSize = 0;
    uint16_t transRowCount = NCHW_CONV_ADDR_LIST_SIZE;
    uint16_t copyColCount = NCHW_CONV_ADDR_LIST_SIZE;
    uint16_t transLen = 0;
    uint32_t preTmpLen = B16_TMP_ELE_LEN;
    uint32_t dstAllBlockNum = 0;
    uint32_t imageBlockNum = 0;


    TransDataTo5HDParams transDataParams1;

    __attribute__((cce_unif_buff)) T* dstList1[NCHW_CONV_ADDR_LIST_SIZE];
    __attribute__((cce_unif_buff)) T* srcList1[NCHW_CONV_ADDR_LIST_SIZE];

    DataCopyParams dataCopyParams1;
    DataCopyParams dataCopyParams2;


    TransDataTo5HDParams transDataParams2;

    __attribute__((cce_unif_buff)) T* dstList2[NCHW_CONV_ADDR_LIST_SIZE];
    __attribute__((cce_unif_buff)) T* srcList2[NCHW_CONV_ADDR_LIST_SIZE];
};


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransBroadCastForB8Cal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    Transpose4dParams<T> &params)
{
    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[params.srcBlockIndex * params.blockSize].GetPhyAddr();
    }
    params.transDataParams1.dstRepStride = params.transDataParams1.repeatTimes > 1 ? ONE_BLK_SIZE : 0;
    params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;
    params.transDataParams1.dstHighHalf = false;
    params.transDataParams1.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    PipeBarrier<PIPE_V>();
    params.transDataParams1.dstHighHalf = true;
    params.transDataParams1.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[B8_TRANS_FRACTAL + m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[params.srcBlockIndex * params.blockSize].GetPhyAddr();
    }
    params.transDataParams1.dstRepStride = params.transDataParams1.repeatTimes > 1 ? ONE_BLK_SIZE : 0;
    params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;
    params.transDataParams1.dstHighHalf = false;
    params.transDataParams1.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    PipeBarrier<PIPE_V>();
    params.transDataParams1.dstHighHalf = true;
    params.transDataParams1.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransFracForB8Cal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    Transpose4dParams<T> &params)
{
    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = false;
    params.transDataParams2.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[B8_TRANS_FRACTAL + m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = false;
    params.transDataParams2.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[B8_TRANS_FRACTAL + n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = true;
    params.transDataParams2.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[B8_TRANS_FRACTAL + m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[B8_TRANS_FRACTAL + n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = true;
    params.transDataParams2.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransBroadCastCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    Transpose4dParams<T> &params)
{
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
            params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[m * params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[params.srcBlockIndex * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    } else if constexpr (sizeof(T) == sizeof(uint8_t)) {
        TransBroadCastForB8Cal(dstLocal, srcLocal, params);
    } else {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + TWO_NUM) {
            params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM)].GetPhyAddr();
            params.dstList1[m + 1] =
                (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM) + params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[params.srcBlockIndex * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransFracCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    Transpose4dParams<T> &params)
{
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
            params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[m * params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    } else if constexpr (sizeof(T) == sizeof(uint8_t)) {
        TransFracForB8Cal(dstLocal, srcLocal, params);
    } else {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + TWO_NUM) {
            params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM)].GetPhyAddr();
            params.dstList2[m + 1] =
                (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM) + params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransLastFracCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    Transpose4dParams<T> &params)
{
    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + TWO_NUM) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM)].GetPhyAddr();
        params.dstList2[m + 1] =
            (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM) + params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / TWO_NUM; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
        params.srcList2[n + NCHW_CONV_ADDR_LIST_SIZE / TWO_NUM] =
            (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
    }
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CopyFirstBlockCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params, DataCopyParams &dataCopyParams)
{
    if ((params.dstNeedBlockNum != 0) && (params.tmp3RemainRowCount != 0)) {
        DataCopy(dstLocal[params.dstBlockNum * (params.blockSize)], srcLocal, dataCopyParams);
        params.dstNeedBlockNum -= params.tmp3RemainRowCount;
        params.dstBlockNum += params.tmp3RemainRowCount;
        params.tmp3RemainRowCount = 0;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UpdataCopyToTmp2ParamCal(Transpose4dParams<T> &params, const TransposeParamsExt &transposeParams)
{
    params.copyCIndex += 1;
    params.tmp2NeedRowCount -= 1;
    params.tmp1CopyCount += 1;
    params.tmp2Count += 1;
    params.tmp1RemainRowCount -= 1;
    if (params.copyCIndex == transposeParams.cSize) {
        params.copyCIndex = 0;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UpdataTransToTmp3ParamCal(Transpose4dParams<T> &params, const uint16_t cSize,
    const TransposeType transposeType)
{
    if (transposeType == TransposeType::TRANSPOSE_NCHW2NHWC) {
        params.tmp3RemainRowCount = 1;
        params.tmp2Count = 0;
        if (params.dstNeedBlockNum == 1) {
            params.tmp2NeedRowCount = params.imageSize % params.transRowCount == 0 ?
                params.transRowCount :
                (params.imageSize % params.transRowCount);
        } else {
            params.tmp2NeedRowCount = params.transRowCount;
        }
        if ((params.dstNeedBlockNum > 1) && (sizeof(T) == sizeof(float))) {
            params.tmp3RemainRowCount = TWO_NUM;
        }
    } else if (transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
        params.tmp3RemainRowCount = 1;
        params.tmp2Count = 0;
        params.tmp2NeedRowCount = cSize * params.transRowCount;

        if (sizeof(T) == sizeof(float) && (params.dstNeedBlockNum != 1)) {
            params.tmp3RemainRowCount = TWO_NUM;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UpdataTransToTmp1ParamCal(Transpose4dParams<T> &params)
{

    params.srcBlockIndex += 1;

    if (params.dstNeedBlockNum == 1) {

        params.tmp1RemainRowCount = params.oneChwSize % params.tmp1NeedRowCount == 0 ?
            params.tmp1NeedRowCount :
            (params.oneChwSize % params.tmp1NeedRowCount);
    } else {
        params.tmp1RemainRowCount = params.tmp1NeedRowCount;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CopyTodstForChwCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint16_t cSize, Transpose4dParams<T> &params)
{
    params.dataCopyParams2.blockCount = cSize;
    params.dataCopyParams2.blockLen = params.tmp3RemainRowCount;
    params.dataCopyParams2.srcStride =
        params.preTmpLen * sizeof(T) / ONE_BLK_SIZE - params.tmp3RemainRowCount;
    params.dataCopyParams2.dstStride = params.imageBlockNum - params.tmp3RemainRowCount;

    if ((params.dstNeedBlockNum != 0) && (params.tmp3RemainRowCount != 0)) {
        DataCopy(dstLocal[params.dstBlockNum * (params.blockSize)], srcLocal, params.dataCopyParams2);
        params.dstNeedBlockNum -= params.tmp3RemainRowCount;
        params.dstBlockNum += params.tmp3RemainRowCount;
        params.dstAllBlockNum = params.tmp3RemainRowCount * cSize;
        params.tmp3RemainRowCount = 0;
    }
    if (sizeof(T) == sizeof(float) && (params.dstNeedBlockNum == 1) && (params.dstAllBlockNum != 0) &&
        (params.srcBlockIndex % params.dstAllBlockNum == 0)) {
        params.tmp2NeedRowCount = cSize * (params.imageSize % NCHW_CONV_ADDR_LIST_SIZE);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Transpose2HwcCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &sharedTmpBuffer, const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params)
{
    LocalTensor<T> tempTensorA = sharedTmpBuffer;
    LocalTensor<T> tempTensorB = sharedTmpBuffer[transposeParams.cSize * params.preTmpLen];
    LocalTensor<T> tempTensorC = sharedTmpBuffer[(transposeParams.cSize + 1) * params.preTmpLen];

    while (params.dstNeedBlockNum != 0) {

        if (params.tmp1RemainRowCount == 0) {
            TransBroadCastCal(tempTensorA, srcLocal, params);

            params.srcBlockIndex += 1;

            if (params.dstNeedBlockNum == 1) {

                params.tmp1RemainRowCount = params.oneChwSize % params.tmp1NeedRowCount == 0 ?
                    transposeParams.cSize * params.tmp1NeedRowCount :
                    (params.oneChwSize % params.tmp1NeedRowCount);
            } else {
                params.tmp1RemainRowCount = transposeParams.cSize * params.tmp1NeedRowCount;
            }
        }
        PipeBarrier<PIPE_V>();

        while (params.tmp2NeedRowCount != 0) {
            params.dataCopyParams1.blockCount = 1;
            params.dataCopyParams1.blockLen = 1;

            params.preCinnerOffset = (params.copyColCount * (params.tmp1CopyCount / transposeParams.cSize));
            params.preCoffset = (params.transLen) * (params.copyCIndex % transposeParams.cSize);
            DataCopy(tempTensorB[(params.tmp2Count % params.transRowCount) * (params.blockSize)],
                tempTensorA[params.preCoffset + params.preCinnerOffset], params.dataCopyParams1);

            UpdataCopyToTmp2ParamCal(params, transposeParams);

            if (params.tmp1RemainRowCount == 0) {
                params.tmp1CopyCount = 0;
                break;
            }
            PipeBarrier<PIPE_V>();
        }

        if (params.tmp2NeedRowCount == 0) {
            TransFracCal(tempTensorC, tempTensorB, params);

            UpdataTransToTmp3ParamCal(params, transposeParams.cSize, TransposeType::TRANSPOSE_NCHW2NHWC);
        }
        PipeBarrier<PIPE_V>();

        params.dataCopyParams1.blockCount = 1;
        params.dataCopyParams1.blockLen = params.tmp3RemainRowCount;
        CopyFirstBlockCal(dstLocal, tempTensorC, transposeParams, params, params.dataCopyParams1);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Transpose2ChwCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &sharedTmpBuffer, const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params)
{
    LocalTensor<T> tempTensorA = sharedTmpBuffer;
    LocalTensor<T> tempTensorB = sharedTmpBuffer[params.preTmpLen];

    LocalTensor<T> tempTensorC = sharedTmpBuffer[(transposeParams.cSize + 1) * params.preTmpLen];
    while (params.dstNeedBlockNum != 0) {
        if (params.tmp1RemainRowCount == 0) {
            TransBroadCastCal(tempTensorA, srcLocal, params);

            UpdataTransToTmp1ParamCal(params);
        }
        PipeBarrier<PIPE_V>();

        while (params.tmp1RemainRowCount != 0) {
            params.dataCopyParams1.blockCount = 1;
            params.dataCopyParams1.blockLen = 1;
            params.preCinnerOffset = (params.copyColCount * (params.tmp2Count / transposeParams.cSize));
            params.preCoffset = params.transLen * (params.copyCIndex % transposeParams.cSize);
            DataCopy(tempTensorB[params.preCoffset + params.preCinnerOffset],
                tempTensorA[(params.tmp2Count % params.blockSize) * params.transRowCount], params.dataCopyParams1);

            UpdataCopyToTmp2ParamCal(params, transposeParams);

            if (params.tmp1RemainRowCount == 0) {
                params.tmp1CopyCount = 0;
                break;
            }
            PipeBarrier<PIPE_V>();
        }

        if (params.tmp2NeedRowCount == 0) {
            if ((params.imageSize % NCHW_CONV_ADDR_LIST_SIZE != 0) && (params.dstNeedBlockNum == 1)) {
                for (int16_t k = 0; k < transposeParams.cSize; k++) {
                    TransLastFracCal(tempTensorC[k * params.preTmpLen], tempTensorB[k * params.preTmpLen], params);
                }
            } else {
                for (int16_t k = 0; k < transposeParams.cSize; k++) {
                    TransFracCal(tempTensorC[k * params.preTmpLen], tempTensorB[k * params.preTmpLen], params);
                }
            }
            UpdataTransToTmp3ParamCal(params, transposeParams.cSize, TransposeType::TRANSPOSE_NHWC2NCHW);
        }
        PipeBarrier<PIPE_V>();

        CopyTodstForChwCal(dstLocal, tempTensorC, transposeParams.cSize, params);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetTransposeParamCal(const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params)
{
    params.imageSize = transposeParams.hSize * transposeParams.wSize;
    params.imageBlockNum = params.imageSize * sizeof(T) / ONE_BLK_SIZE;
    params.oneChwSize = params.imageSize * transposeParams.cSize;
    params.blockSize = ONE_BLK_SIZE / sizeof(T);
    params.tmp1NeedRowCount = sizeof(T) == 1 ? ONE_BLK_SIZE : params.blockSize;

    params.transDataParams1.repeatTimes = transposeParams.cSize;
    params.transDataParams1.dstRepStride = params.transDataParams1.repeatTimes > 1 ? NCHW_CONV_ADDR_LIST_SIZE : 0;
    params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;

    params.transLen = BYTE_PER_FRACTAL / sizeof(T);
    if constexpr (sizeof(T) == sizeof(uint8_t)) {
        params.transRowCount = B8_TRANS_ROW;
        params.copyColCount = B8_COPY_COL;
        params.preTmpLen = B8_TMP_ELE_LEN;
        params.transLen = B8_TRANS_LEN / sizeof(T);
    } else if constexpr (sizeof(T) == sizeof(float)) {
        params.preTmpLen = B32_TMP_ELE_LEN;
        if (transposeParams.transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
            params.copyColCount = ONE_BLK_SIZE / sizeof(T);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Transpose4DImpl(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const TransposeParamsExt &transposeParams)
{
    LocalTensor<T> stackBuffer = sharedTmpBuffer.ReinterpretCast<T>();
                                                                                                                    ;

    Transpose4dParams<T> params;
    GetTransposeParamCal(transposeParams, params);

    if (transposeParams.transposeType == TransposeType::TRANSPOSE_NCHW2NHWC) {
        for (int16_t i = 0; i < transposeParams.nSize; i++) {
            params.dstBlockNum = 0;
            params.srcBlockIndex = 0;
            params.copyCIndex = 0;
            params.tmp1RemainRowCount = 0;
            params.tmp1CopyCount = 0;
            params.dstNeedBlockNum = params.oneChwSize * sizeof(T) / ONE_BLK_SIZE;
            params.tmp2NeedRowCount =
                params.dstNeedBlockNum == 1 ? (params.imageSize % params.transRowCount) : params.transRowCount;
            Transpose2HwcCal(dstLocal[i * params.oneChwSize], srcLocal[i * params.oneChwSize], stackBuffer,
                transposeParams, params);
        }
    } else if (transposeParams.transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
        for (int16_t i = 0; i < transposeParams.nSize; i++) {
            params.transDataParams1.repeatTimes = 1;
            params.transDataParams1.dstRepStride =
                params.transDataParams1.repeatTimes > 1 ? NCHW_CONV_ADDR_LIST_SIZE : 0;
            params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;
            params.dstBlockNum = 0;
            params.dstAllBlockNum = 0;
            params.srcBlockIndex = 0;
            params.dstNeedBlockNum =
                params.imageBlockNum;

            params.tmp2NeedRowCount = params.oneChwSize * sizeof(T) / ONE_BLK_SIZE == 1 ?
                (params.imageSize % params.transRowCount) :
                transposeParams.cSize * params.transRowCount;


            if ((sizeof(T) == sizeof(float)) && (params.dstNeedBlockNum == 1)) {
                params.tmp2NeedRowCount = transposeParams.cSize * params.blockSize;
            }
            Transpose2ChwCal(dstLocal[i * params.oneChwSize], srcLocal[i * params.oneChwSize], stackBuffer,
                transposeParams, params);
        }
    }
}
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h"
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void Transpose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal);
# 68 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((check_sync_alias)) void TransDataTo5HD(const LocalTensor<T> (&dstLocalList)[NCHW_CONV_ADDR_LIST_SIZE],
    const LocalTensor<T> (&srcLocalList)[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((check_sync_alias)) void TransDataTo5HD(uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Transpose(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const TransposeParamsExt &transposeParams);
#pragma end_pipe
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((check_sync_alias)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void TransDataTo5HD(const LocalTensor<uint64_t>& dstLocal, const LocalTensor<uint64_t>& srcLocal,
    const TransDataTo5HDParams& nchwconvParams);
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h" 1
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_gather_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_gather_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherbImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) uint32_t* offset,
    const uint32_t srcLength, uint8_t repeatTimes, const GatherRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                        ;
        ResetMask();
        uint16_t dstRptStd = repeatParams.dstRepStride;
        uint8_t dstBlkStd = repeatParams.dstBlkStride;
        uint32_t offsetAddr = (uint64_t)src0;




        vgatherb(dst, offset, offsetAddr, dstRptStd, dstBlkStd, repeatTimes);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* srcOffsetLocal,
    const uint32_t srcLength, const uint32_t srcBaseAddr, const uint64_t mask, const uint8_t repeatTimes,
    const uint16_t dstRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        uint32_t offsetAddr = (uint64_t)srcLocal + srcBaseAddr;




        AscendCUtils::SetMask<T>(mask);
        if constexpr (sizeof(T) == sizeof(uint16_t)) {
            vgather((__attribute__((cce_unif_buff)) uint16_t *)dstLocal, srcOffsetLocal, offsetAddr, dstRepStride, repeatTimes);
        } else if constexpr (sizeof(T) == sizeof(uint32_t)) {
            vgather((__attribute__((cce_unif_buff)) uint32_t *)dstLocal, srcOffsetLocal, offsetAddr, dstRepStride, repeatTimes);
        } else {
                                                                                                            ;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* srcOffsetLocal,
    const uint32_t srcLength, const uint32_t srcBaseAddr, const uint64_t mask[], const uint8_t repeatTimes,
    const uint16_t dstRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        uint32_t offsetAddr = (uint64_t)srcLocal + srcBaseAddr;




        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        if constexpr (sizeof(T) == sizeof(uint16_t)) {
            vgather((__attribute__((cce_unif_buff)) uint16_t *)dstLocal, (__attribute__((cce_unif_buff)) uint32_t *)srcOffsetLocal, offsetAddr, dstRepStride,
                repeatTimes);
        } else if constexpr (sizeof(T) == sizeof(uint32_t)) {
            vgather((__attribute__((cce_unif_buff)) uint32_t *)dstLocal, (__attribute__((cce_unif_buff)) uint32_t *)srcOffsetLocal, offsetAddr, dstRepStride,
                repeatTimes);
        } else {
                                                                                                            ;
        }
    }
}
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h" 2


#pragma begin_pipe(V)
namespace AscendC {
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gatherb(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<uint32_t>& offsetLocal, const uint8_t repeatTimes, const GatherRepeatParams& repeatParams);
# 66 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint64_t mask,
    const uint8_t repeatTimes, const uint16_t dstRepStride);
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint64_t mask[],
    const uint8_t repeatTimes, const uint16_t dstRepStride);
# 96 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint32_t count);
}
#pragma end_pipe
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h" 1
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_scatter_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_scatter_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ScatterImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* dstOffsetLocal,
    const uint32_t dstLength, const uint32_t dstBaseAddr, const uint64_t mask, const uint8_t repeatTimes,
    const uint8_t srcRepStride)
{
                                                ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ScatterImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* dstOffsetLocal,
    const uint32_t dstLength, const uint32_t dstBaseAddr, const uint64_t mask[], const uint8_t repeatTimes,
    const uint8_t srcRepStride)
{
                                                ;
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h" 2






#pragma begin_pipe(V)
namespace AscendC {
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint64_t mask,
    const uint8_t repeatTimes, const uint8_t srcRepStride);
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint64_t mask[],
    const uint8_t repeatTimes, const uint8_t srcRepStride);
# 80 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint32_t count);
}
#pragma end_pipe
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_brcb_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_brcb_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_brcb.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_brcb.h"
namespace AscendC {
struct BrcbRepeatParams {
    [aicore] BrcbRepeatParams() {}

    [aicore] BrcbRepeatParams(const uint16_t dstBlkStrideIn, const uint16_t dstRepStrideIn)
        : dstBlkStride(dstBlkStrideIn), dstRepStride(dstRepStrideIn)
    {}

    uint32_t blockNumber = DEFAULT_BLK_NUM;
    uint16_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint16_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
};
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_brcb_intf.h" 2
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_brcb_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_brcb_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_brcb_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BrcbImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, const uint8_t repeatTimes,
    const BrcbRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        ResetMask();
        if constexpr(sizeof(T) == B16_BYTE_SIZE) {
            vbrcb((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src0, repeatParams.dstBlkStride,
                repeatParams.dstRepStride, repeatTimes);
        } else if constexpr(sizeof(T) == B32_BYTE_SIZE) {
            vbrcb((__attribute__((cce_unif_buff)) uint32_t*)dst, (__attribute__((cce_unif_buff)) uint32_t*)src0, repeatParams.dstBlkStride,
                repeatParams.dstRepStride, repeatTimes);
        } else {


                               ;
        }
    }
}
}
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_brcb_intf.h" 2



#pragma begin_pipe(V)
namespace AscendC {
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_brcb_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Brcb(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local, const uint8_t repeatTimes,
    const BrcbRepeatParams& repeatParams);
}
#pragma end_pipe
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h" 1
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 79 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 101 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 119 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 141 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 159 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 181 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 199 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 222 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                 const LocalTensor<U>& src1Local, const uint64_t mask[], const uint8_t repeatTimes,
                                 const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                 const LocalTensor<U>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                                 const BinaryRepeatParams& repeatParams);
# 240 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                 const LocalTensor<U>& src1Local, const int32_t& calCount);
# 262 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 280 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 302 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 320 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 342 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 360 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 382 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                          const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                          const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                          const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                          const BinaryRepeatParams& repeatParams);
# 400 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                          const LocalTensor<T>& src1Local, const int32_t& calCount);
# 422 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                               const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                               const BinaryRepeatParams& repeatParams);
# 440 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, const int32_t& calCount);
# 462 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<int32_t>& src0Local,
                                  const LocalTensor<int32_t>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                                  const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                  const LocalTensor<U>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                                  const BinaryRepeatParams& repeatParams);

template <bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<int32_t>& src0Local,
                                  const LocalTensor<int32_t>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                                  const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                  const LocalTensor<U>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                                  const BinaryRepeatParams& repeatParams);
# 489 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<int32_t>& src0Local,
                                  const LocalTensor<int32_t>& src1Local, const int32_t& calCount);

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                  const LocalTensor<U>& src1Local, const int32_t& calCount);
# 514 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                   const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                                   const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                   const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                                   const BinaryRepeatParams& repeatParams);
# 532 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                   const LocalTensor<T>& src1Local, const int32_t& calCount);
# 554 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                       const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                                       const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                       const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                                       const BinaryRepeatParams& repeatParams);
# 572 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                       const LocalTensor<T>& src1Local, const int32_t& calCount);
# 594 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                               const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                               const BinaryRepeatParams& repeatParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, const int32_t& calCount);
}
#pragma end_pipe
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h" 1
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_binary_scalar_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_binary_scalar_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float, int16_t, int32_t>(), "Failed to check dtype in Adds, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vadds(dst, src, scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AddsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AddsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            AddsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        AddsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MulsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float, int16_t, int32_t>(), "Failed to check dtype in Muls, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmuls(dst, src, scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MulsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MulsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            MulsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        MulsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MaxsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float, int16_t, int32_t>(), "Failed to check dtype in Maxs, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmaxs(dst, src, scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride, false, false);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MaxsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MaxsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MaxsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MaxsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MaxsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            MaxsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        MaxsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MinsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float, int16_t, int32_t>(), "Failed to check dtype in Mins, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmins(dst, src, scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride, false, false);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MinsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MinsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MinsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MinsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MinsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            MinsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        MinsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeftIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, uint16_t, uint32_t, int16_t, int32_t>(), "Failed to check dtype in ShiftLeft, current "
        "api support dtype combination is src and dst both: uint16_t / uint32_t / int16_t / int32_t.");

                                                                                        ;
    vshl(dst, src, (uint32_t)scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride,
        (uint16_t)repeatParams.srcBlkStride, (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeftImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ShiftLeftIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeftImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ShiftLeftIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeftImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            ShiftLeftIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        ShiftLeftIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRightIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams, bool roundEn)
{


                                ;

                                                                                         ;

    if (roundEn) {
        if constexpr (SupportType<T, int16_t, int32_t>()) {
            vshr(dst, src, (int32_t)scalarValue, repeatTimes, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride, true);
        } else {
            vshr(dst, src, (uint32_t)scalarValue, repeatTimes, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride, true);
        }
    } else {
        if constexpr (SupportType<T, int16_t, int32_t>()) {
            vshr(dst, src, (int32_t)scalarValue, repeatTimes, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride, false);
        } else {
            vshr(dst, src, (uint32_t)scalarValue, repeatTimes, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride, false);
        }
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRightImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn = false)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ShiftRightIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams, roundEn);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRightImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn = false)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ShiftRightIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams, roundEn);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRightImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            ShiftRightIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE }, false);
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        ShiftRightIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE }, false);
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LeakyReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float>(), "Failed to check dtype in LeakyRelu, current api support dtype "
        "combination is src and dst both: half / float.");
    vlrelu(dst, src, scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        LeakyReluIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        LeakyReluIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            LeakyReluIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        LeakyReluIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h" 2





#pragma begin_pipe(V)
namespace AscendC {
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 84 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 109 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 135 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 160 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 186 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 211 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 237 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 262 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 288 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 313 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn = false);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn = false);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn);
# 339 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 364 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 390 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
}
#pragma end_pipe
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h" 1
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_cmpsel_impl.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_cmpsel_impl.h"
namespace AscendC {




template <typename U> [aicore] __inline__ __attribute__((always_inline)) void VselIntrinsicsImplPre(__attribute__((cce_unif_buff)) U* sel, SELMODE selMode)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    if (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        set_cmpmask(sel);
        PipeBarrier<PIPE_V>();
    } else if (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {







        uint32_t selAddr = static_cast<uint32_t>(reinterpret_cast<int64_t>(reinterpret_cast<__attribute__((cce_unif_buff)) int64_t*>(sel)));
        __attribute__((cce_unif_buff)) uint32_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint32_t>(TMP_UB_OFFSET, ONE_BLK_SIZE);

        AscendCUtils::SetMask<uint32_t>(ONE_BLK_SIZE);
        DuplicateIntrinsicsImpl(tempBuf, selAddr, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        set_cmpmask(tempBuf);
        PipeBarrier<PIPE_V>();

        AscendCUtils::FreeTemporaryBuffer<uint32_t>(tempBuf);

    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    SELMODE selMode, int32_t repeat, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    (void)sel;
    if (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        vsel(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
    } else if (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {
        vsel(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void VselIntrinsicsImplPre(T src1)
{
    if constexpr(g_coreType == AscendC::AIV) {
        __attribute__((cce_unif_buff)) T* tempBuf = AscendCUtils::GetTemporaryBufferAddr<T>(TMP_UB_OFFSET, ONE_BLK_SIZE);

        AscendCUtils::SetMask<T>(ONE_BLK_SIZE);
        DuplicateIntrinsicsImpl(tempBuf, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        set_cmpmask(tempBuf);
        PipeBarrier<PIPE_V>();

        AscendCUtils::FreeTemporaryBuffer<T>(tempBuf);
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, T src1, SELMODE selMode,
    int32_t repeat, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        (void)src1;
        vsel(dst, src0, sel, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
    }
}

template <typename T, SELMODE selMode = SELMODE::VSEL_CMPMASK_SPR>
[aicore] __inline__ __attribute__((always_inline)) void SelectCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, int32_t repeat,
    const BinaryRepeatParams& repeatParams)
{


                                                            ;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    if constexpr (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        vsel(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
    } else if constexpr (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {
        vsel(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SelectCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, int32_t repeat,
    const BinaryRepeatParams& repeatParams)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIV) {
        vsel(dst, src0, sel, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(SELMODE::VSEL_TENSOR_SCALAR_MODE));
    }
}






template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, SELMODE selMode,
    uint32_t calCount)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    BinaryRepeatParams repeatParams;
    SetMaskCount();

    if (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        set_cmpmask(sel);
        PipeBarrier<PIPE_V>();

        AscendCUtils::SetMask<U>(0, calCount);
        vsel(dst, src0, src1, 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
        PipeBarrier<PIPE_V>();
    } else if (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {







        uint32_t selAddr = static_cast<uint32_t>(reinterpret_cast<int64_t>(reinterpret_cast<__attribute__((cce_unif_buff)) int64_t*>(sel)));
        __attribute__((cce_unif_buff)) uint32_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint32_t>(TMP_UB_OFFSET, ONE_BLK_SIZE);

        AscendCUtils::SetMask<U>(0, ONE_BLK_SIZE);
        DuplicateIntrinsicsImpl(tempBuf, selAddr, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        set_cmpmask(tempBuf);
        PipeBarrier<PIPE_V>();

        AscendCUtils::FreeTemporaryBuffer<uint32_t>(tempBuf);

        AscendCUtils::SetMask<U>(0, calCount);
        vsel(dst, src0, src1, 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
        PipeBarrier<PIPE_V>();
    }

    ResetMask();
    SetMaskNorm();
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, T src1, SELMODE selMode,
    uint32_t calCount)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    SetMaskCount();
    __attribute__((cce_unif_buff)) T* tempBuf = AscendCUtils::GetTemporaryBufferAddr<T>(TMP_UB_OFFSET, ONE_BLK_SIZE);

    AscendCUtils::SetMask<U>(0, ONE_BLK_SIZE);
    DuplicateIntrinsicsImpl(tempBuf, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    set_cmpmask(tempBuf);
    PipeBarrier<PIPE_V>();

    AscendCUtils::FreeTemporaryBuffer<T>(tempBuf);

    AscendCUtils::SetMask<U>(0, calCount);
    BinaryRepeatParams repeatParams;
    vsel(dst, src0, sel, 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
        repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
        static_cast<uint8_t>(selMode));

    ResetMask();
    SetMaskNorm();
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, __attribute__((cce_unif_buff)) T* src1Local,
    SELMODE selMode, const uint64_t mask[], const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(selMask, selMode);

        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, __attribute__((cce_unif_buff)) T* src1Local,
    SELMODE selMode, const uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(selMask, selMode);

        AscendCUtils::SetMask<T>(mask);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, T src1Local,
    SELMODE selMode, const uint64_t mask[], const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(src1Local);

        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, T src1Local,
    SELMODE selMode, const uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(src1Local);

        AscendCUtils::SetMask<T>(mask);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTimes, repeatParams);
    }
}
}
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h" 2






#pragma begin_pipe(V)
namespace AscendC {
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask[],
    const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask,
    const BinaryRepeatParams& repeatParams);
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, uint32_t calCount);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetCmpMask(const LocalTensor<T>& dst);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetCmpMask(const LocalTensor<T>& src);
# 119 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams);
# 138 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, uint32_t calCount);
# 167 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint64_t mask[],
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams);


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint64_t mask,
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams);

template <typename T, SELMODE selMode>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    SelectCal<T, selMode>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), repeatTimes, repeatParams);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    SelectCal<T, U>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), repeatTimes, repeatParams);
}
# 205 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint32_t calCount);
# 228 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint64_t mask[], uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint64_t mask, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);
# 250 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint32_t calCount);
}
#pragma end_pipe
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_reduce_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_reduce_impl.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcgadd(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMaxIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcgmax(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMinIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcgmin(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcpadd(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void RepeatReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t srcBlkStride, const int32_t dstRepStride, const int32_t srcRepStride)
{
    vcadd(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride, 0);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        BlockReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        BlockReduceMaxIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        BlockReduceMinIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        PairReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        BlockReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        BlockReduceMaxIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        BlockReduceMinIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        PairReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void RepeatReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t elemsInOneRepeat, const int32_t dstBlkStride, const int32_t srcBlkStride, const int32_t dstRepStride,
    const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(elemsInOneRepeat);
        RepeatReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeat, srcBlkStride, dstRepStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint32_t count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            set_mask_count();
            set_vector_mask(0, count);
        }
        vcadd(dst, src, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, 1);
        auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        int64_t accVal = get_acc_val();
        *(dst) = *(reinterpret_cast<T*>(&accVal));
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        if constexpr (isSetMask) {
            set_mask_norm();
            set_vector_mask((uint64_t)-1, (uint64_t)-1);
        }
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, struct ReduceRepeatParams& params,
    const ReduceOrder order)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(params.highMask, params.lowMask);
        if (order == ReduceOrder::ORDER_VALUE_INDEX) {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::VALUE_INDEX);
        } else if (order == ReduceOrder::ORDER_INDEX_VALUE) {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::INDEX_VALUE);
        } else if (order == ReduceOrder::ORDER_ONLY_VALUE) {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_VALUE);
        } else {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_INDEX);
        }
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint64_t mask[],
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMaxImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t mask,
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMaxImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, struct ReduceRepeatParams& params,
    const ReduceOrder order)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(params.highMask, params.lowMask);
        if (order == ReduceOrder::ORDER_VALUE_INDEX) {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::VALUE_INDEX);
        } else if (order == ReduceOrder::ORDER_INDEX_VALUE) {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::INDEX_VALUE);
        } else if (order == ReduceOrder::ORDER_ONLY_VALUE) {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_VALUE);
        } else {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_INDEX);
        }
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint64_t mask[],
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    struct ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMinImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t mask,
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    struct ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMinImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, struct ReduceRepeatParams& params)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(params.highMask, params.lowMask);
        vcadd(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride, 0);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint64_t mask[],
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    struct ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceSumImpl<T, isSetMask>(dstLocal, srcLocal, params);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint32_t mask,
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    struct ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceSumImpl<T, isSetMask>(dstLocal, srcLocal, params);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxIntrinsicsImpl(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTimes,
    const int32_t srcRepStride)
{
    vcmax(workLocal, srcLocal, repeatTimes, 1, 1, srcRepStride, Order_t::VALUE_INDEX);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMinIntrinsicsImpl(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTimes,
    const int32_t srcRepStride)
{
    vcmin(workLocal, srcLocal, repeatTimes, 1, 1, srcRepStride, Order_t::VALUE_INDEX);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTimes,
    const int32_t srcRepStride)
{
    vcadd(workLocal, srcLocal, repeatTimes, 1, 1, srcRepStride, 0);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumSecondStep(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* workLocal,
    struct ReduceRepeatParams& params)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t newRepeatTimes = params.repeatTimes / elementNumPerRep;
    int32_t leftData = params.repeatTimes % elementNumPerRep;

    uint64_t highMask = 0;
    uint64_t lowMask = 0;


    lowMask = params.repeatTimes;



    SetMaskCount();

    AscendCUtils::SetMask<T>(highMask, lowMask);
    ReduceSumIntrinsicsImpl<T>(workLocal, workLocal, 1, DEFAULT_REPEAT_STRIDE);

    SetMaskNorm();
# 360 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_reduce_impl.h"
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateSpecialFormatMask(const int32_t& maskLen, uint64_t& highMask, uint64_t& lowMask)
{

    int32_t halfLen = HLAF_MASK_LEN / 2;
    for (int32_t i = 0; i < maskLen - halfLen; i++) {
        highMask = highMask << 2;
        highMask = highMask | 1;
    }
    int32_t lowMaskRange = maskLen >= halfLen ? halfLen : maskLen;
    for (int32_t i = 0; i < lowMaskRange; i++) {
        lowMask = lowMask << 2;
        lowMask = lowMask | 1;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceOperation(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTimes,
    const int32_t srcRepStride, const uint64_t& highMask, const uint64_t& lowMask, const ReduceMode& mode)
{
    AscendCUtils::SetMask<T>(highMask, lowMask);
    switch (mode) {
        case ReduceMode::REDUCE_MAX:
            ReduceMaxIntrinsicsImpl(workLocal, srcLocal, repeatTimes, srcRepStride);
            break;
        case ReduceMode::REDUCE_MIN:
            ReduceMinIntrinsicsImpl(workLocal, srcLocal, repeatTimes, srcRepStride);
            break;
        case ReduceMode::REDUCE_SUM:
            ReduceSumIntrinsicsImpl(workLocal, srcLocal, repeatTimes, srcRepStride);
            break;
        default:
            break;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplFirstStep(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal,
    struct ReduceRepeatParams& params, const ReduceMode& mode, int32_t& curData)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t range = params.repeatTimes / MAX_REPEAT_TIMES;

    for (int32_t index = 0; index < range; index++) {
        dstOffset = index * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT;
        srcOffset = index * MAX_REPEAT_TIMES * params.srcRepStride * ONE_BLK_SIZE / sizeof(T);
        ReduceOperation<T>(workLocal + dstOffset, srcLocal + srcOffset, MAX_REPEAT_TIMES, params.srcRepStride,
            params.highMask, params.lowMask, mode);
    }
    int32_t leftRepeatTimes = params.repeatTimes % MAX_REPEAT_TIMES;
    if (leftRepeatTimes > 0) {
        dstOffset = range * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT;
        srcOffset = range * MAX_REPEAT_TIMES * params.srcRepStride * ONE_BLK_SIZE / sizeof(T);
        ReduceOperation<T>(workLocal + dstOffset, srcLocal + srcOffset, leftRepeatTimes, params.srcRepStride,
            params.highMask, params.lowMask, mode);
    }
    curData = VREDUCE_PER_REP_OUTPUT * params.repeatTimes;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplSecondStep(__attribute__((cce_unif_buff)) T* workLocal, const ReduceMode& mode, int32_t& curData,
    int32_t preStartPos, int32_t secondStartPos)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t newMaskLen = 0;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t newRepeatTimes = curData / elementNumPerRep;
    int32_t leftData = curData % elementNumPerRep;
    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    int32_t bodyOutputCount = 0;
    int32_t tailOutputCount = 0;

    if (newRepeatTimes >= 1) {
        highMask = (sizeof(T) == sizeof(__cce_half)) ? 0x5555555555555555 : 0;
        lowMask = 0x5555555555555555;

        ReduceOperation<T>(workLocal + secondStartPos, workLocal + preStartPos, newRepeatTimes, DEFAULT_REPEAT_STRIDE,
            highMask, lowMask, mode);
        bodyOutputCount = newRepeatTimes * VREDUCE_PER_REP_OUTPUT;
    }
    highMask = 0;
    lowMask = 0;

    if (leftData > 0) {
        newMaskLen = leftData / VREDUCE_PER_REP_OUTPUT;

        CreateSpecialFormatMask<T>(newMaskLen, highMask, lowMask);

        dstOffset = secondStartPos + bodyOutputCount;
        srcOffset = preStartPos + newRepeatTimes * elementNumPerRep;
        ReduceOperation<T>(workLocal + dstOffset, workLocal + srcOffset, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask,
            mode);
        tailOutputCount = VREDUCE_PER_REP_OUTPUT;
    }

    curData = bodyOutputCount + tailOutputCount;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetIndex(__attribute__((cce_unif_buff)) T* workLocal, int32_t secondStartPos, int32_t& secondIndex,
    int32_t& thirdIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (sizeof(T) == sizeof(__cce_half)) {
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + secondStartPos + 1);
                                                                                                ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + thirdIndex + 1);
                                                                                                  ;
    } else {
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + secondStartPos + 1);
                                                                                                ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + thirdIndex + 1);
                                                                                                  ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetIndex(__attribute__((cce_unif_buff)) T* workLocal, int32_t secondStartPos, int32_t thirdStartPos,
    int32_t& firstIndex, int32_t& secondIndex, int32_t& thirdIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    using U = typename Conditional<sizeof(T) == B16_BYTE_SIZE, uint16_t, uint32_t>::type;
    thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) U*>(workLocal + thirdStartPos + 1);
                                                                                                            ;
    secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) U*>(workLocal + secondStartPos + thirdIndex + 1);
                                                                                                              ;
    firstIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) U*>(workLocal +
        elementNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT) + secondIndex + 1);
                                                                                                            ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetIndex(__attribute__((cce_unif_buff)) T* workLocal, int32_t secondStartPos, int32_t thirdStartPos,
    int32_t fourthStartPos, int32_t& firstIndex, int32_t& secondIndex, int32_t& thirdIndex, int32_t& fourthIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (sizeof(T) == sizeof(__cce_half)) {
        fourthIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + fourthStartPos + 1);



          ;
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + thirdStartPos + fourthIndex + 1);



          ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + secondStartPos +
            elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex + 1);



          ;
        firstIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal +
            elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT +
            secondIndex + 1);



          ;
    } else {
        fourthIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + fourthStartPos + 1);



          ;
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + thirdStartPos + fourthIndex + 1);



          ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + secondStartPos +
            elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex + 1);



          ;
        firstIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal +
            elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT +
            secondIndex + 1);



          ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplThirdStep(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* workLocal, const int32_t srcRepStride,
    const ReduceMode& mode, int32_t& curData, int32_t& secondStartPos, int32_t& thirdStartPos)
{
    int32_t preNum = 0;
    int32_t firstIndex = 0;
    int32_t secondIndex = 0;
    int32_t thirdIndex = 0;
    int32_t fourthIndex = 0;
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    uint64_t highMask = 0;
    uint64_t lowMask = 0;

    int32_t offsetNumPerRep = ONE_BLK_SIZE / sizeof(T) * srcRepStride;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (curData == VREDUCE_PER_REP_OUTPUT) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        GetIndex<T>(workLocal, secondStartPos, secondIndex, thirdIndex);
        preNum = offsetNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT);
        int32_t redultIndex = secondIndex + preNum;
        *dstLocal = *(workLocal + secondStartPos);
        *(dstLocal + 1) = *reinterpret_cast<T*>(&redultIndex);
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        return;
    }

    int32_t newMaskLen = curData / VREDUCE_PER_REP_OUTPUT;
    CreateSpecialFormatMask<T>(newMaskLen, highMask, lowMask);
    if (curData > elementNumPerRep) {
        ReduceImplSecondStep<T>(workLocal, mode, curData, secondStartPos, thirdStartPos);

        int32_t fourthStartPos =
            (((thirdStartPos + curData) * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
        dstOffset = fourthStartPos;
        srcOffset = thirdStartPos;
        PipeBarrier<PIPE_V>();
        ReduceOperation<T>(workLocal + dstOffset, workLocal + srcOffset, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask,
            mode);
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        *dstLocal = *(workLocal + dstOffset);

        GetIndex<T>(workLocal, secondStartPos, thirdStartPos, fourthStartPos, firstIndex, secondIndex, thirdIndex,
            fourthIndex);
        preNum = offsetNumPerRep *
            (elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT + secondIndex) / VREDUCE_PER_REP_OUTPUT;
    } else {
        dstOffset = thirdStartPos;
        srcOffset = secondStartPos;

        ReduceOperation<T>(workLocal + dstOffset, workLocal + srcOffset, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask,
            mode);
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        *dstLocal = *(workLocal + thirdStartPos);

        GetIndex<T>(workLocal, secondStartPos, thirdStartPos, firstIndex, secondIndex, thirdIndex);
        preNum = offsetNumPerRep * (elementNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT) + secondIndex) /
            VREDUCE_PER_REP_OUTPUT;
    }

    int32_t redultIndex = firstIndex + preNum;
    *(dstLocal + 1) = *reinterpret_cast<T*>(&redultIndex);
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumFirstStep(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal,
    struct ReduceRepeatParams& params)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t range = params.repeatTimes / MAX_REPEAT_TIMES;

    for (int32_t index = 0; index < range; index++) {
        dstOffset = index * MAX_REPEAT_TIMES;
        srcOffset = index * MAX_REPEAT_TIMES * (params.srcRepStride * ONE_BLK_SIZE / sizeof(T));
        ReduceOperation<T>(workLocal + dstOffset, srcLocal + srcOffset, MAX_REPEAT_TIMES, params.srcRepStride,
            params.highMask, params.lowMask, ReduceMode::REDUCE_SUM);
    }

    int32_t leftRepeatTimes = params.repeatTimes % MAX_REPEAT_TIMES;
    if (leftRepeatTimes > 0) {
        dstOffset = range * MAX_REPEAT_TIMES;
        srcOffset = range * MAX_REPEAT_TIMES * (params.srcRepStride * ONE_BLK_SIZE / sizeof(T));
        ReduceOperation<T>(workLocal + dstOffset, srcLocal + srcOffset, leftRepeatTimes, params.srcRepStride,
            params.highMask, params.lowMask, ReduceMode::REDUCE_SUM);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumFinalStep(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* workLocal, int32_t& secondResultNum)
{
    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    if (secondResultNum == 1) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        *(dstLocal) = *(workLocal);
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    } else {
        highMask = (secondResultNum > HLAF_MASK_LEN) ? ((((uint64_t)1) << (secondResultNum - HLAF_MASK_LEN)) - 1) : 0;
        lowMask = (secondResultNum > HLAF_MASK_LEN) ? FULL_MASK : ((((uint64_t)1) << secondResultNum) - 1);
        ReduceOperation<T>(dstLocal, workLocal, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask, ReduceMode::REDUCE_SUM);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* workLocal,
    struct ReduceRepeatParams& params)
{
    ReduceSumFirstStep<T>(workLocal, srcLocal, params);
    PipeBarrier<PIPE_V>();
    ReduceSumSecondStep<T>(dstLocal, workLocal, params);
    PipeBarrier<PIPE_V>();
    int32_t secondResultNum = DivCeil(params.repeatTimes, ONE_REPEAT_BYTE_SIZE / sizeof(T));
    ReduceSumFinalStep<T>(dstLocal, workLocal, secondResultNum);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplSecondStepNoIndex(__attribute__((cce_unif_buff)) T* workLocal, const ReduceMode& mode, int32_t& curData)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t newRepeatTimes = curData / elementNumPerRep;
    int32_t leftData = curData % elementNumPerRep;
    uint64_t highMask = 0, lowMask = 0;
    if (newRepeatTimes != 0) {
        CreateSpecialFormatMask<T>(elementNumPerRep / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
        ReduceOperation<T>(workLocal, workLocal, newRepeatTimes, DEFAULT_REPEAT_STRIDE, highMask, lowMask, mode);
    }
    highMask = 0;
    lowMask = 0;
    if (leftData > 0) {
        CreateSpecialFormatMask<T>(leftData / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
        ReduceOperation<T>(workLocal + newRepeatTimes * VREDUCE_PER_REP_OUTPUT,
            workLocal + newRepeatTimes * elementNumPerRep, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask, mode);
        newRepeatTimes += 1;
    }
    curData = newRepeatTimes * VREDUCE_PER_REP_OUTPUT;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplThirdStepNoIndex(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* workLocal, const ReduceMode& mode,
    int32_t& curData)
{
    uint64_t highMask = 0;
    uint64_t lowMask = 0;

    CreateSpecialFormatMask<T>(curData / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
    ReduceOperation<T>(workLocal, workLocal, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask, mode);
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    *dstLocal = *workLocal;
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplWithIndex(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* workLocal,
    struct ReduceRepeatParams& params, const ReduceMode& mode)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (params.repeatTimes == 1) {
            ReduceOperation<T>(dstLocal, srcLocal, 1, params.srcRepStride, params.highMask, params.lowMask, mode);
        } else {
            int32_t curData = 0;
            ReduceImplFirstStep<T>(workLocal, srcLocal, params, mode, curData);
            PipeBarrier<PIPE_V>();
            int32_t secondStartPos =
                ((curData * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
            ReduceImplSecondStep<T>(workLocal, mode, curData, 0, secondStartPos);
            PipeBarrier<PIPE_V>();
            int32_t thirdStartPos =
                (((secondStartPos + curData) * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
            ReduceImplThirdStep<T>(dstLocal, workLocal, params.srcRepStride, mode, curData, secondStartPos,
                thirdStartPos);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplNoIndex(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* workLocal,
    struct ReduceRepeatParams& params, const ReduceMode& mode)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (params.repeatTimes == 1) {
            ReduceOperation<T>(workLocal, srcLocal, 1, params.srcRepStride, params.highMask, params.lowMask, mode);
            event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);
            *dstLocal = *workLocal;
            event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
            event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
            SetFlag<HardEvent::S_V>(eventIdSToV);
            WaitFlag<HardEvent::S_V>(eventIdSToV);
            SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
            WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        } else {
            if (mode == ReduceMode::REDUCE_SUM) {
                ReduceSumImpl<T>(dstLocal, srcLocal, workLocal, params);
            } else {
                int32_t curData = 0;
                ReduceImplFirstStep<T>(workLocal, srcLocal, params, mode, curData);
                PipeBarrier<PIPE_V>();
                ReduceImplSecondStepNoIndex<T>(workLocal, mode, curData);

                int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
                if (curData <= elementNumPerRep) {
                    PipeBarrier<PIPE_V>();
                    ReduceImplThirdStepNoIndex<T>(dstLocal, workLocal, mode, curData);
                    return;
                }
                PipeBarrier<PIPE_V>();
                ReduceImplSecondStepNoIndex<T>(workLocal, mode, curData);
                if (curData <= elementNumPerRep) {
                    PipeBarrier<PIPE_V>();
                    ReduceImplThirdStepNoIndex<T>(dstLocal, workLocal, mode, curData);
                }
            }
        }
    }
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* workLocal,
    struct ReduceRepeatParams& params, bool calIndex, const ReduceMode& mode)
{
    if (calIndex) {
        ReduceImplWithIndex<T>(dstLocal, srcLocal, workLocal, params, mode);
    } else {
        ReduceImplNoIndex<T>(dstLocal, srcLocal, workLocal, params, mode);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceTailCompute(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count, bool calIndex, const ReduceMode& mode)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t repeatTimes = count / elementNumPerRep;
    int32_t tailCount = count % elementNumPerRep;

    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    T bodyValue = dstLocal.GetValue(0);
    T bodyIndex = dstLocal.GetValue(1);

    struct ReduceRepeatParams tailParams(tailCount, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(elementNumPerRep * repeatTimes), (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(),
        tailParams, calIndex, mode);
    eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    T tailValue = dstLocal.GetValue(0);
    T tailIndex = dstLocal.GetValue(1);


    struct ReduceRepeatParams lastParams(2, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    workLocal.SetValue(0, bodyValue);
    workLocal.SetValue(1, tailValue);
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), lastParams, calIndex, mode);
    if (calIndex) {
        eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        T lastIndexVal = dstLocal.GetValue(1);
        uint32_t newIndex = 0;
        uint32_t lastIndex = 0;
        if (sizeof(T) == sizeof(__cce_half)) {
            lastIndex = *reinterpret_cast<uint16_t*>(&lastIndexVal);
            newIndex = elementNumPerRep * repeatTimes + *reinterpret_cast<uint16_t*>(&tailIndex);
        } else {
            lastIndex = *reinterpret_cast<uint32_t*>(&lastIndexVal);
            newIndex = elementNumPerRep * repeatTimes + *reinterpret_cast<uint32_t*>(&tailIndex);
        }
        if (lastIndex == 1) {
            dstLocal.SetValue(1, *reinterpret_cast<T*>(&newIndex));
        } else {
            dstLocal.SetValue(1, bodyIndex);
        }
        eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCountImpl(uint32_t &maxMinValue, uint32_t &maxMinIndex)
{
    int64_t maxMinCnt = get_max_min_cnt();
    if constexpr (IsSameType<T, __cce_half>::value) {
        constexpr uint64_t valueMask = 0xffff;
        maxMinValue = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    } else {
        constexpr uint64_t valueMask = 0xffffffff;
        maxMinValue = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    }
    constexpr uint64_t indexBit = 32;
    maxMinIndex = (static_cast<uint64_t>(maxMinCnt) >> indexBit);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCountImpl(uint32_t &maxMinValue)
{
                                                                                   ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCountImpl(T &maxMinValue, T &maxMinIndex)
{
    int64_t maxMinCnt = get_max_min_cnt();
    uint32_t maxVal = 0;
    uint32_t maxIdx = 0;
    if constexpr (IsSameType<T, __cce_half>::value) {
        constexpr uint64_t valueMask = 0xffff;
        maxVal = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    } else {
        constexpr uint64_t valueMask = 0xffffffff;
        maxVal = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    }
    maxMinValue = *(reinterpret_cast<T*>(&maxVal));

    constexpr uint64_t indexBit = 32;
    maxIdx = (static_cast<uint64_t>(maxMinCnt) >> indexBit);
    maxMinIndex = *(reinterpret_cast<T*>(&maxIdx));
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCountImpl(T &maxMinValue)
{
                                                                                   ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) T GetAccValImpl()
{
    int64_t accVal = get_acc_val();
    return *(reinterpret_cast<T*>(&accVal));
}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h" 2
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
namespace AscendC {
#pragma begin_pipe(V)
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 71 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 87 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 103 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void RepeatReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t mask, const int32_t dstBlkStride, const int32_t srcBlkStride,
    const int32_t dstRepStride, const int32_t srcRepStride);
# 145 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 161 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);
# 177 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);
# 208 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex = 0);
# 224 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex = 0);
# 239 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex = 0);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex = 0);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride);
# 264 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count, bool calIndex = 0);
# 277 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count, bool calIndex = 0);
# 289 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count);
#pragma end_pipe
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue, T &maxMinIndex);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) T GetAccVal();
}
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_mask_intf.h" 1
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_mask_intf.h"
namespace AscendC {
#pragma begin_pipe(V)
template <typename T, typename U, GatherMaskMode mode = defaultGatherMaskMode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<U>& src1Pattern, const bool reduceMode, const uint32_t mask,
    const GatherMaskParams& gatherMaskParams, uint64_t& rsvdCnt);

template <typename T, GatherMaskMode mode = defaultGatherMaskMode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const uint8_t src1Pattern, const bool reduceMode, const uint32_t mask, const GatherMaskParams& gatherMaskParams,
    uint64_t& rsvdCnt);
#pragma end_pipe

[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) int64_t GetGatherMaskRemainCount();
}
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_mulcast_intf.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_mulcast_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams);

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams &repeatParams);

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint32_t calCount);
}
#pragma end_pipe
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_bilinearinterpalation_intf.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_bilinearinterpalation_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolation(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask, uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolation(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask[], uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer);
}
#pragma end_pipe
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_createvecindex_intf.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_createvecindex_intf.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dstLocal, const T &firstValue,
    uint64_t mask, uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dstLocal, const T &firstValue,
    uint64_t mask[], uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> dstLocal, const T &firstValue,
    uint32_t calCount);
}
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h" 1
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_ternary_scalar_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_ternary_scalar_impl.h"
namespace AscendC {

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AxpyIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, U scalarValue, const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{


                                                        ;
    vaxpy(dst, src, scalarValue, repeatTimes, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
        repeatParams.dstRepStride, repeatParams.srcRepStride);
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AxpyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const U& scalarValue, const uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if (sizeof(T) > sizeof(U)) {
                AscendCUtils::SetMask<T>(mask);
            } else {
                AscendCUtils::SetMask<U>(mask);
            }
        }
        AxpyIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AxpyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const U& scalarValue, const uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AxpyIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AxpyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const U& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        SetMaskCount();
        AscendCUtils::SetMask<U>(0, calCount);
        if constexpr (sizeof(T) > sizeof(U)) {
            AxpyIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
        } else {
            AxpyIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        ResetMask();
        SetMaskNorm();
    }
}
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h" 2
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 72 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
}
#pragma end_pipe
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h" 1
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_unary_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_unary_impl.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float, int32_t>(), "Failed to check dtype in Relu, current api support dtype "
        "combination is src and dst both: half / float / int32_t.");
    vrelu(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ExpIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float>(), "Failed to check dtype in Exp, current api support dtype combination "
        "is src and dst both: half / float.");
    vexp(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LnIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float>(), "Failed to check dtype in Ln, current api support dtype combination "
        "is src and dst both: half / float.");
    vln(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AbsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float>(), "Failed to check dtype in Abs, current api support dtype combination "
        "is src and dst both: half / float.");
    vabs(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReciprocalIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float>(), "Failed to check dtype in Reciprocal, current api support dtype "
        "combination is src and dst both: half / float.");
    vrec(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void RsqrtIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float>(), "Failed to check dtype in Rsqrt, current api support dtype "
        "combination is src and dst both: half / float.");
    vrsqrt(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SqrtIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, __cce_half, float>(), "Failed to check dtype in Sqrt, current api support dtype "
        "combination is src and dst both: half / float.");
    vsqrt(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void NotIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, uint16_t>(), "Failed to check dtype in Not, current api support dtype "
        "combination is src and dst both: int16_t / uint16_t.");
    vnot(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ReluIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ReluIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void ReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vrelu(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ExpImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const uint64_t mask[], const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ExpIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ExpImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const uint64_t mask, const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ExpIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void ExpImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vexp(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LnImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        LnIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LnImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        LnIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LnImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vln(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AbsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AbsIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AbsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AbsIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void AbsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vabs(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReciprocalImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ReciprocalIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReciprocalImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ReciprocalIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void ReciprocalImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vrec(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void RsqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        RsqrtIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void RsqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        RsqrtIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void RsqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vrsqrt(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        SqrtIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        SqrtIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void SqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vsqrt(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void NotImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        NotIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void NotImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        NotIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void NotImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vnot(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h" 2






#pragma begin_pipe(V)
namespace AscendC {
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 74 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 90 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 104 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 120 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 134 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 150 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 164 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 180 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 194 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t& calCount);
# 211 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 225 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 241 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 255 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 271 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 285 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
}
#pragma end_pipe
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_vdeq.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_struct_vdeq.h"
namespace AscendC {
struct VdeqInfo {
    [aicore] VdeqInfo() {}

    [aicore] VdeqInfo(const float vdeqScaleIn[VDEQ_TENSOR_SIZE], const int16_t vdeqOffsetIn[VDEQ_TENSOR_SIZE],
        const bool vdeqSignModeIn[VDEQ_TENSOR_SIZE])
    {
        for (int32_t i = 0; i < VDEQ_TENSOR_SIZE; ++i) {
            vdeqScale[i] = vdeqScaleIn[i];
            vdeqOffset[i] = vdeqOffsetIn[i];
            vdeqSignMode[i] = vdeqSignModeIn[i];
        }
    }

    float vdeqScale[VDEQ_TENSOR_SIZE] = { 0 };
    int16_t vdeqOffset[VDEQ_TENSOR_SIZE] = { 0 };
    bool vdeqSignMode[VDEQ_TENSOR_SIZE] = { 0 };
};
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h" 2






# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_vconv_impl.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_vconv_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) __cce_half* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    vconv_deq(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride, repeatParams.dstRepStride,
        repeatParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) __cce_half* dst, __attribute__((cce_unif_buff)) int8_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s82f16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) __cce_half* dst, __attribute__((cce_unif_buff)) uint8_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_u82f16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_s322f32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_s322f32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_s322f32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_s322f32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_s322f32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_s322f32(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) __cce_half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_f162f32(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) __cce_half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int8_t* dst, __attribute__((cce_unif_buff)) __cce_half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s8r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s8f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s8c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s8a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s8z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f162s8(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) __cce_half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162u8r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162u8f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162u8c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162u8a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162u8z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f162u8(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) __cce_half* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322f16r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322f16f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322f16c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322f16a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322f16z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:
            vconv_f322f16o(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f322f16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322s32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322s32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322s32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322s32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322s32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dst, __attribute__((cce_unif_buff)) __cce_half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s16r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s16f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s16c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s16a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s16z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
                                                                          ;
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int8_t* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
                                                                         ;
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) __cce_half* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_s162f16r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_s162f16f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_s162f16c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_s162f16a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_s162f16z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_s162f16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322f32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322f32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322f32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322f32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322f32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) bfloat16_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322bf16r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322bf16f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322bf16c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322bf16a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322bf16z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:


              ;
            break;
        case RoundMode::CAST_NONE:


              ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int64_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322s64r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322s64f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322s64c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322s64a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322s64z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) bfloat16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_bf162f32(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) bfloat16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_bf162s32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_bf162s32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_bf162s32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_bf162s32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_bf162s32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:


              ;
            break;
        case RoundMode::CAST_NONE:


              ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322s16r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322s16f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322s16c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322s16a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322s16z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s162f32(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s322s16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int64_t* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s322s64(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) int64_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_s642f32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_s642f32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_s642f32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_s642f32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_s642f32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) int64_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s642s32(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int4b_t* dst, __attribute__((cce_unif_buff)) __cce_half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s4r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s4f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s4c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s4a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s4z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f162s4(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                        ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) __cce_half* dst, __attribute__((cce_unif_buff)) int4b_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s42f16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}


template <typename U, typename T>
[aicore] static __inline__ __attribute__((always_inline)) void CheckCastDatatype() {
# 813 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_vconv_impl.h"
                                                       ;
}


template <typename U, typename T>
[aicore] __inline__ __attribute__((always_inline)) void CastImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src, const RoundMode& roundMode,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckCastDatatype<U, T>();
        set_mask_count();
        set_vector_mask(0, calCount);
        if constexpr (sizeof(U) > sizeof(T)) {
            if constexpr (IsSameType<T, int4b_t>::value) {
                CastIntrinsicsImpl(
                    dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE, ONE_FOURTH_DEFAULT_REPEAT_STRIDE});
            } else {
                CastIntrinsicsImpl(dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2});
            }
        } else if constexpr (sizeof(U) < sizeof(T)) {
            if constexpr (IsSameType<U, int4b_t>::value) {
                CastIntrinsicsImpl(
                    dst, src, roundMode, 1, {1, 1, ONE_FOURTH_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            } else {
                CastIntrinsicsImpl(dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE});
            }
        } else {
            CastIntrinsicsImpl(dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        }
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}


template <typename U, typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CastImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src, const RoundMode& roundMode,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckCastDatatype<U, T>();
        if constexpr (isSetMask) {
            if (sizeof(U) >= sizeof(T)) {
                AscendCUtils::SetMask<T>(mask[1], mask[0]);
            } else {
                AscendCUtils::SetMask<U>(mask[1], mask[0]);
            }
        }
        CastIntrinsicsImpl(dst, src, roundMode, repeatTimes, repeatParams);
    }
}


template <typename U, typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CastImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src, const RoundMode& roundMode,
    const uint64_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckCastDatatype<U, T>();
        if constexpr (isSetMask) {
            if (sizeof(U) >= sizeof(T)) {
                AscendCUtils::SetMask<T>(mask);
            } else {
                AscendCUtils::SetMask<U>(mask);
            }
        }
        CastIntrinsicsImpl(dst, src, roundMode, repeatTimes, repeatParams);
    }
}

template <typename T, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeqIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) int16_t* src, uint8_t repeat,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr (halfBlock) {
        vconv_deqs162b8h(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    } else {
        vconv_deqs162b8l(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    }
}

template <typename T, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastVDeqIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) int16_t* src, uint8_t repeat,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr (halfBlock) {
        vconv_vdeqs162b8h(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    } else {
        vconv_vdeqs162b8l(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    }
}


template <typename U, typename T, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeqImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        struct UnaryRepeatParams repeatParams;
        if constexpr (isVecDeq) {
            CastVDeqIntrinsicsImpl<U, halfBlock>(dst, src, 1, repeatParams);
        } else {
            CastDeqIntrinsicsImpl<U, halfBlock>(dst, src, 1, repeatParams);
        }
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}


template <typename U, typename T, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeqImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        if constexpr (isVecDeq) {
            CastVDeqIntrinsicsImpl<U, halfBlock>(dst, src, repeatTimes, repeatParams);
        } else {
            CastDeqIntrinsicsImpl<U, halfBlock>(dst, src, repeatTimes, repeatParams);
        }
    }
}


template <typename U, typename T, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeqImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src,
    const int32_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        if constexpr (isVecDeq) {
            CastVDeqIntrinsicsImpl<U, halfBlock>(dst, src, repeatTimes, repeatParams);
        } else {
            CastDeqIntrinsicsImpl<U, halfBlock>(dst, src, repeatTimes, repeatParams);
        }
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCastIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, uint8_t repeat,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<Tuple<U, T>, Tuple<__cce_half, int8_t>, Tuple<float, __cce_half>, Tuple<int16_t, int8_t>>(),
        "Failed to check dtype in AddReluCast, current api support dtype combination is src: half, dst: int8_t; src: "
        "float, dst: half; src: int16_t, dst: int8_t.");
    if constexpr (SupportType<Tuple<U, T>, Tuple<__cce_half, int8_t>>()) {
        vaddreluconv_f162s8(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else if constexpr (SupportType<Tuple<U, T>, Tuple<float, __cce_half>>()) {
        vaddreluconv_f322f16(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else {
        vaddreluconv_s162s8(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    }
}


template <typename DST_TYPE, typename SRC_TYPE, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint64_t mask, uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(DST_TYPE) >= sizeof(SRC_TYPE)) {
                AscendCUtils::SetMask<SRC_TYPE>(mask);
            } else {
                AscendCUtils::SetMask<DST_TYPE>(mask);
            }
        }
        AddReluCastIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename DST_TYPE, typename SRC_TYPE, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint64_t mask[], uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(DST_TYPE) >= sizeof(SRC_TYPE)) {
                AscendCUtils::SetMask<SRC_TYPE>(mask[1], mask[0]);
            } else {
                AscendCUtils::SetMask<DST_TYPE>(mask[1], mask[0]);
            }
        }
        AddReluCastIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename DST_TYPE, typename SRC_TYPE>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        if constexpr (sizeof(DST_TYPE) > sizeof(SRC_TYPE)) {
            AddReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE / HALF_FACTOR, DEFAULT_REPEAT_STRIDE / HALF_FACTOR});
        } else if constexpr (sizeof(DST_TYPE) < sizeof(SRC_TYPE)) {
            AddReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE / HALF_FACTOR,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        } else {
            AddReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE});
        }
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCastIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, uint8_t repeat,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<Tuple<U, T>, Tuple<__cce_half, int8_t>, Tuple<float, __cce_half>, Tuple<int16_t, int8_t>>(),
        "Failed to check dtype in SubReluCast, current api support dtype combination is src: half, dst: int8_t; src: "
        "float, dst: half; src: int16_t, dst: int8_t.");
    if constexpr (SupportType<Tuple<U, T>, Tuple<__cce_half, int8_t>>()) {
        vsubreluconv_f162s8(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else if constexpr (SupportType<Tuple<U, T>, Tuple<float, __cce_half>>()) {
        vsubreluconv_f322f16(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else {
        vsubreluconv_s162s8(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    }
}


template <typename DST_TYPE, typename SRC_TYPE>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        if constexpr (sizeof(DST_TYPE) > sizeof(SRC_TYPE)) {
            SubReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE / HALF_FACTOR, DEFAULT_REPEAT_STRIDE / HALF_FACTOR});
        } else if constexpr (sizeof(DST_TYPE) < sizeof(SRC_TYPE)) {
            SubReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE / HALF_FACTOR,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        } else {
            SubReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE});
        }
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}


template <typename DST_TYPE, typename SRC_TYPE, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint64_t mask, uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(DST_TYPE) >= sizeof(SRC_TYPE)) {
                AscendCUtils::SetMask<SRC_TYPE>(mask);
            } else {
                AscendCUtils::SetMask<DST_TYPE>(mask);
            }
        }
        SubReluCastIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename DST_TYPE, typename SRC_TYPE, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint64_t mask[], uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(DST_TYPE) >= sizeof(SRC_TYPE)) {
                AscendCUtils::SetMask<SRC_TYPE>(mask[1], mask[0]);
            } else {
                AscendCUtils::SetMask<DST_TYPE>(mask[1], mask[0]);
            }
        }
        SubReluCastIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

[aicore] __inline__ __attribute__((always_inline)) uint64_t MakeDeqScaleConfig(float scale, int16_t offset, bool signMode)
{
    constexpr uint64_t signModeBit = 46;
    constexpr uint64_t offsetMask = 0x1ff;
    constexpr uint64_t offsetBit = 37;
    uint64_t config = ((static_cast<uint64_t>(signMode) << signModeBit) | ((offset & offsetMask) << offsetBit) |
                       *(reinterpret_cast<uint32_t *>(&scale)));
    return config;
}

[aicore] __inline__ __attribute__((always_inline)) void SetDeqScaleImpl(float scale, int16_t offset, bool signMode)
{
    set_deqscale(MakeDeqScaleConfig(scale, offset, signMode));
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScaleImpl(const LocalTensor<T>& vdeqTensor, const VdeqInfo& vdeqInfo)
{
    for (uint8_t i = 0; i < VDEQ_TENSOR_SIZE; i++) {
        float scale = vdeqInfo.vdeqScale[i];
        int16_t offset = vdeqInfo.vdeqOffset[i];
        bool signMode = vdeqInfo.vdeqSignMode[i];
        vdeqTensor.SetValue(i, static_cast<T>(MakeDeqScaleConfig(scale, offset, signMode)));
    }



    constexpr uint64_t deqAddr = 5;
    set_deqscale(((uint64_t)vdeqTensor.GetPhyAddr()) >> deqAddr);

}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScaleImpl(T config)
{
    set_deqscale(config);
    g_deqValue = config;
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint64_t mask[], const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams);


template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 77 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint32_t calCount);
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2, bool isSetMask = true, bool isVecDeq = true, bool halfBlock = true>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T1, typename T2, bool isSetMask = true, bool isVecDeq = true, bool halfBlock = true>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const int32_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 107 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2, bool isVecDeq = true, bool halfBlock = true>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const uint32_t calCount);
# 130 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams);


template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);
# 148 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, const uint32_t calCount);
# 171 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams);


template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);
# 189 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, const uint32_t calCount);

#pragma end_pipe
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(__cce_half scale);

[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(float scale, int16_t offset, bool signMode);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(const LocalTensor<T>& vdeqTensor, const VdeqInfo& vdeqInfo);
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vpadding_intf.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vpadding_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint64_t mask, const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint64_t mask[], const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint32_t calCount);
}
#pragma end_pipe
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_scalar_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_scalar_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_scalar.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_scalar.h"
namespace AscendC {
template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetCountOfValueImpl(uint64_t valueIn)
{
    if constexpr (countValue == 1) {
        return bcnt1(valueIn);
    } else if constexpr (countValue == 0) {
        return bcnt0(valueIn);
    } else {
        static_assert(((countValue == 0) || (countValue == 1)) && "countValue must be 1 or 0");
        return 0;
    }
}

[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarCountLeadingZeroImpl(uint64_t valueIn)
{
    return clz(valueIn);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t CountBitsCntSameAsSignBitImpl(int64_t valueIn)
{
    return sflbits(valueIn);
}

template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetSFFValueImpl(uint64_t valueIn)
{
    if constexpr (countValue == 1) {
        return sff1(valueIn);
    } else if constexpr (countValue == 0) {
        return sff0(valueIn);
    } else {
        static_assert(((countValue == 0) || (countValue == 1)) && "countValue must be 1 or 0");
        return 0;
    }
}

template <RoundMode roundMode>
[aicore] __inline__ __attribute__((always_inline)) __cce_half ScalarCastF322F16Impl(float valueIn)
{
    switch (roundMode) {
        case RoundMode::CAST_ODD:
            return conv_f322f16o(valueIn);
        default:

                                                                                                                      ;
            return 0;
    }
}

template <RoundMode roundMode>
[aicore] __inline__ __attribute__((always_inline)) int32_t ScalarCastF322S32Impl(float valueIn)
{
    switch (roundMode) {
        case RoundMode::CAST_ROUND:
            return conv_f322s32a(valueIn);
        case RoundMode::CAST_CEIL:
            return conv_f322s32c(valueIn);
        case RoundMode::CAST_FLOOR:
            return conv_f322s32f(valueIn);
        case RoundMode::CAST_RINT:
            return conv_f322s32r(valueIn);
        default:

                                                                                                                      ;
            return 0;
    }
}

template <typename srcT, typename dstT, RoundMode roundMode>
[aicore] __inline__ __attribute__((always_inline)) dstT ScalarCastImpl(srcT valueIn)
{

    if constexpr (std::is_same<dstT, __cce_half>::value) {
        return ScalarCastF322F16Impl<roundMode>(valueIn);
    } else if constexpr (std::is_same<dstT, int32_t>::value) {
        return ScalarCastF322S32Impl<roundMode>(valueIn);
    } else {
        static_assert(((sizeof(dstT) == sizeof(__cce_half)) || (sizeof(dstT) == sizeof(int32_t))),
            "dstT only support half or int32_t");
        return 0;
    }




}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_scalar_intf.h" 2

namespace AscendC {
template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetCountOfValue(uint64_t valueIn);

[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarCountLeadingZero(uint64_t valueIn);

[aicore] __inline__ __attribute__((always_inline)) int64_t CountBitsCntSameAsSignBit(int64_t valueIn);

template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetSFFValue(uint64_t valueIn);

template <typename srcT, typename dstT, RoundMode roundMode>
[aicore] __inline__ __attribute__((always_inline)) dstT ScalarCast(srcT valueIn);
}
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_sys_var_intf.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_sys_var_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_sys_var_impl.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_sys_var_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockNumImpl()
{



    return get_subblockdim();

}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSystemCycleImpl()
{







    uint64_t sysCnt = 0;
    asm volatile("MOV %0, SYS_CNT\n" : "+l"(sysCnt));
    return (int64_t)(sysCnt);

}

[aicore] __inline__ __attribute__((always_inline)) void GetArchVersionImpl(uint32_t& coreVersion)
{
    const int32_t coreVersionOffset = 32;
    coreVersion = (uint32_t)((uint64_t)(get_arch_ver() >> coreVersionOffset) & 0xFFF);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetProgramCounterImpl()
{
    int64_t pc = get_pc() & 0xFFFFFFFFFFFF;
    return pc;
}

[aicore] __inline__ __attribute__((always_inline)) void TrapImpl()
{
    trap();
}
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_sys_var_intf.h" 2





namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void GetArchVersion(uint32_t& coreVersion);

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockNum();

[aicore] __inline__ __attribute__((always_inline)) int64_t GetProgramCounter();

[aicore] __inline__ __attribute__((always_inline)) void Trap();

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSystemCycle();
}
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/dropout.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/dropout.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_c220_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_c220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_membase_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_membase_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_membase_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/../../common/check.h" 1
# 13 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/../../common/check.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CheckTensorPosition(const LocalTensor<T> &checkTensor, __attribute__((cce_global)) const char* tensorInfo,
    __attribute__((cce_global)) const char* supportPosInfo)
{
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/../../common/check.h"
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CheckCalCount(const uint32_t calCount, __attribute__((cce_global)) const char* calCountInfo,
    const LocalTensor<T> &checkTensor, __attribute__((cce_global)) const char* tensorInfo, __attribute__((cce_global)) const char* apiInfo)
{







}

[aicore] __inline__ __attribute__((always_inline)) void CheckTmpBufferSize(const uint32_t checkBufferSize, const uint32_t compBufferSize,
    const uint32_t tmpBufferSize)
{



      ;
}

}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_membase_impl.h" 2

namespace AscendC {
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void DropOutBitModeInit(const LocalTensor<T>& sharedTmpBuffer)
{

                                                                                        ;

    ResetMask();
    LocalTensor<int16_t> stackBuffer = sharedTmpBuffer.template ReinterpretCast<int16_t>();


    UnaryRepeatParams unaryParams;
    Muls<int16_t, false>(stackBuffer, stackBuffer, 0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetCmpMask<int16_t>(stackBuffer);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DropOutBitModeInit()
{
    LocalTensor<int16_t> stackBuffer;
    bool ans = PopStackBuffer<int16_t, TPosition::LCM>(stackBuffer);

                                                                                                                      ;
    DropOutBitModeInit(stackBuffer);
}

template <typename T, bool isInitBitMode = false>
[aicore] __inline__ __attribute__((always_inline)) void DropOutBitMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const uint32_t dataSize)
{
    if constexpr (isInitBitMode == false) {
        DropOutBitModeInit(sharedTmpBuffer);
    }

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, dataSize);

    const BinaryRepeatParams binaryParams;
    Select<T, uint8_t>(dstLocal, maskLocal, srcLocal, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    const UnaryRepeatParams unaryParams;
    Muls<T, false>(dstLocal, dstLocal, static_cast<T>(divValue), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void DropOutByteModeCalc(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<__cce_half>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const __cce_half divValue, const DropOutParams<__cce_half, float>& params)
{
    const LocalTensor<__cce_half>& stackBuffer = params.firstLocal;

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);

    Cast<__cce_half, uint8_t, false>(stackBuffer, maskLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, params.repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Mul<__cce_half, false>(dstLocal, stackBuffer, srcLocal, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    Muls<__cce_half, false>(dstLocal, dstLocal, divValue, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DropOutByteModeCalc(const LocalTensor<float>& dstLocal, const LocalTensor<float>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const float divValue, const DropOutParams<__cce_half, float>& params)
{
    const LocalTensor<__cce_half>& firstLocal = params.firstLocal;
    const LocalTensor<float>& secondLocal = params.secondLocal;

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);

    Cast<__cce_half, uint8_t, false>(firstLocal, maskLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, params.repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, __cce_half, false>(secondLocal, firstLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, params.repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Mul<float, false>(dstLocal, secondLocal, srcLocal, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    Muls<float, false>(dstLocal, dstLocal, divValue, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DropOutByteModeSetTmpBuffer(LocalTensor<__cce_half>& firstLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, DropOutParams<__cce_half, float>& params)
{
    firstLocal = sharedTmpBuffer.ReinterpretCast<__cce_half>();

    params.stackBufferSize = firstLocal.GetSize();
    params.stackBufferSize = params.stackBufferSize / ONE_BLK_SIZE * ONE_BLK_SIZE;

    params.maxRepeatSize = MAX_REPEAT_HALF_SIZE;
    params.oneRepeatSize = ONE_REPEAT_HALF_SIZE;
}

[aicore] __inline__ __attribute__((always_inline)) void DropOutByteModeSetTmpBuffer(LocalTensor<__cce_half>& firstLocal, LocalTensor<float>& secondLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, DropOutParams<__cce_half, float>& params)
{
    uint32_t popBufferLen = sharedTmpBuffer.GetSize();
    constexpr uint32_t cutBufLen = sizeof(float) + sizeof(__cce_half);
    params.stackBufferSize = popBufferLen / cutBufLen / ONE_BLK_SIZE * ONE_BLK_SIZE;

    firstLocal = sharedTmpBuffer.ReinterpretCast<__cce_half>();
    firstLocal.SetSize(params.stackBufferSize);

    secondLocal = sharedTmpBuffer[params.stackBufferSize * sizeof(__cce_half)].ReinterpretCast<float>();
    secondLocal.SetSize(params.stackBufferSize);

    params.maxRepeatSize = MAX_REPEAT_FLOAT_SIZE;
    params.oneRepeatSize = ONE_REPEAT_FLOAT_SIZE;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DropOutByteMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const uint32_t dataSize)
{
    DropOutParams<__cce_half, float> params;
    params.dataSize = dataSize;

    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        DropOutByteModeSetTmpBuffer(params.firstLocal, sharedTmpBuffer, params);
    } else {
        DropOutByteModeSetTmpBuffer(params.firstLocal, params.secondLocal, sharedTmpBuffer, params);
    }



    const uint32_t round = params.dataSize / params.stackBufferSize;
    const uint32_t tail = params.dataSize % params.stackBufferSize;

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, params.stackBufferSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        DropOutByteModeCalc(dstLocal[offset], srcLocal[offset], maskLocal[offset], divValue, params);
        offset = offset + params.stackBufferSize;
    }

    if (tail != 0) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tail);
        DropOutByteModeCalc(dstLocal[offset], srcLocal[offset], maskLocal[offset], divValue, params);
    }

    SetMaskNorm();
    ResetMask();
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_c220_impl.h" 2

namespace AscendC {
template <typename T, bool isInitBitMode = false>
[aicore] __inline__ __attribute__((always_inline)) void DropOutBitMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const DropOutShapeInfo& info)
{
    if constexpr (isInitBitMode == false) {
        DropOutBitModeInit(sharedTmpBuffer);
    }

    GatherMaskParams reducev2Params;
    reducev2Params.repeatTimes = info.firstAxis;
    reducev2Params.src0RepeatStride = info.maskLastAxis / ONE_BLK_SIZE;

    LocalTensor<uint16_t> maskTmpLocal = maskLocal.ReinterpretCast<uint16_t>();

    const uint32_t mask = info.srcLastAxis / ONE_BYTE_BIT_SIZE / sizeof(uint16_t);
    uint64_t rsvdCnt = 0;

    GatherMask<uint16_t>(maskTmpLocal, maskTmpLocal, REDUCEV2_MODE_SEVEN, true, mask, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();

    DropOutBitMode<T, true>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, divValue,
        info.firstAxis * info.srcLastAxis);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DropOutByteMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const DropOutShapeInfo& info)
{
    GatherMaskParams reducev2Params;
    reducev2Params.repeatTimes = info.firstAxis;
    reducev2Params.src0RepeatStride = info.maskLastAxis / ONE_BLK_SIZE;

    LocalTensor<uint16_t> maskTmpLocal = maskLocal.ReinterpretCast<uint16_t>();

    const uint32_t mask = info.srcLastAxis / sizeof(uint16_t);
    uint64_t rsvdCnt = 0;

    GatherMask<uint16_t>(maskTmpLocal, maskTmpLocal, REDUCEV2_MODE_SEVEN, true, mask, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();

    DropOutByteMode(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, divValue, info.firstAxis * info.srcLastAxis);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_impl.h" 2




namespace AscendC {
#pragma begin_pipe(V)
template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DropOutOpt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const float keepProb,
    const DropOutShapeInfo& info)
{
    float divValue = 1.0;
    divValue = divValue / keepProb;

    const uint32_t dataSize = info.firstAxis * info.srcLastAxis;
    T actualVal;
    actualVal = static_cast<T>(divValue);
    if constexpr (dropOutMode == DROPOUT_MODE_BYTE_MISALIGN) {
        DropOutByteMode(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, actualVal, info);
    } else if constexpr (dropOutMode == DROPOUT_MODE_BYTE_ALIGN) {
        DropOutByteMode(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, actualVal, dataSize);
    } else if constexpr (dropOutMode == DROPOUT_MODE_BIT_ALIGN) {
        DropOutBitMode<T, isInitBitMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, actualVal,
            dataSize);
    } else if constexpr (dropOutMode == DROPOUT_MODE_BIT_MISALIGN) {
        DropOutBitMode<T, isInitBitMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, actualVal,
            info);
    }
}

template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DropOutImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const float keepProb,
    const DropOutShapeInfo& info)
{
                                 ;
                                                                                                   ;
                                                                                                       ;
                                                                                                         ;

                                                                                        ;

    if constexpr (dropOutMode != 0) {
        DropOutOpt<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb, info);
    } else if (info.srcLastAxis < info.maskLastAxis) {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BYTE_MISALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer,
            keepProb, info);
    } else if (info.srcLastAxis == info.maskLastAxis) {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BYTE_ALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb,
            info);
    } else if (info.srcLastAxis == (info.maskLastAxis * ONE_BYTE_BIT_SIZE)) {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BIT_ALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb,
            info);
    } else {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BIT_MISALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer,
            keepProb, info);
    }
                                ;
}

template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DropOutImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const float keepProb, const DropOutShapeInfo& info)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    DropOutImpl<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb, info);
}
#pragma end_pipe
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/dropout.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/dropout.h"
template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DropOut(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const float keepProb,
    const DropOutShapeInfo& info)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DropOutImpl<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb, info);
}
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/dropout.h"
template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DropOut(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const float keepProb, const DropOutShapeInfo& info)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DropOutImpl<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, keepProb, info);
}
#pragma end_pipe
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_tiling/kernel_tiling.h" 1








#pragma pack(push, 8)
struct LogSoftMaxTiling {
    uint32_t srcM = 0;
    uint32_t srcK = 0;
    uint32_t srcSize = 0;
    uint32_t outMaxM = 0;
    uint32_t outMaxK = 0;
    uint32_t outMaxSize = 0;
    uint32_t splitM = 0;
    uint32_t splitK = 0;
    uint32_t splitSize = 0;
    uint32_t reduceM = 0;
    uint32_t reduceK = 0;
    uint32_t reduceSize = 0;
    uint32_t rangeM = 0;
    uint32_t tailM = 0;
    uint32_t tailSplitSize = 0;
    uint32_t tailReduceSize = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct SoftMaxTiling {
    uint32_t srcM = 0;
    uint32_t srcK = 0;
    uint32_t srcSize = 0;
    uint32_t outMaxM = 0;
    uint32_t outMaxK = 0;
    uint32_t outMaxSize = 0;
    uint32_t splitM = 0;
    uint32_t splitK = 0;
    uint32_t splitSize = 0;
    uint32_t reduceM = 0;
    uint32_t reduceK = 0;
    uint32_t reduceSize = 0;
    uint32_t rangeM = 0;
    uint32_t tailM = 0;
    uint32_t tailSplitSize = 0;
    uint32_t tailReduceSize = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Mc2ServerCfg {
    uint32_t version = 0;
    uint8_t debugMode = 0;
    uint8_t sendArgIndex = 0;
    uint8_t recvArgIndex = 0;
    uint8_t commOutArgIndex = 0;
    uint8_t reserved[8] = {};
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Mc2HcommCfg {
    uint8_t skipLocalRankCopy = 0;
    uint8_t skipBufferWindowCopy = 0;
    uint8_t stepSize = 0;
    char reserved[13] = {};
    char groupName[128] = {};
    char algConfig[128] = {};
    uint32_t opType = 0;
    uint32_t reduceType = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Mc2InitTiling {
    uint8_t reserved[64] = {};
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Mc2CcTiling {
    uint8_t reserved[280] = {};
};
#pragma pack(pop)
#pragma pack(push, 8)
struct TCubeTiling {
    int32_t usedCoreNum = 0;
    int32_t M = 0;
    int32_t N = 0;
    int32_t Ka = 0;
    int32_t Kb = 0;
    int32_t singleCoreM = 0;
    int32_t singleCoreN = 0;
    int32_t singleCoreK = 0;
    int32_t baseM = 0;
    int32_t baseN = 0;
    int32_t baseK = 0;
    int32_t depthA1 = 0;
    int32_t depthB1 = 0;
    int32_t stepM = 0;
    int32_t stepN = 0;
    int32_t isBias = 0;
    int32_t transLength = 0;
    int32_t iterateOrder = 0;
    int32_t shareMode = 0;
    int32_t shareL1Size = 0;
    int32_t shareL0CSize = 0;
    int32_t shareUbSize = 0;
    int32_t batchM = 0;
    int32_t batchN = 0;
    int32_t singleBatchM = 0;
    int32_t singleBatchN = 0;
    int32_t stepKa = 0;
    int32_t stepKb = 0;
    int32_t depthAL1CacheUB = 0;
    int32_t depthBL1CacheUB = 0;
    int32_t dbL0A = 0;
    int32_t dbL0B = 0;
    int32_t dbL0C = 0;
    int32_t ALayoutInfoB = 0;
    int32_t ALayoutInfoS = 0;
    int32_t ALayoutInfoN = 0;
    int32_t ALayoutInfoG = 0;
    int32_t ALayoutInfoD = 0;
    int32_t BLayoutInfoB = 0;
    int32_t BLayoutInfoS = 0;
    int32_t BLayoutInfoN = 0;
    int32_t BLayoutInfoG = 0;
    int32_t BLayoutInfoD = 0;
    int32_t CLayoutInfoB = 0;
    int32_t CLayoutInfoS1 = 0;
    int32_t CLayoutInfoN = 0;
    int32_t CLayoutInfoG = 0;
    int32_t CLayoutInfoS2 = 0;
    int32_t BatchNum = 0;
    int32_t mxTypePara = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct BatchNormTiling {
    uint32_t originalBLength = 0;
    uint32_t meanVarSize = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t shCurLength = 0;
    float firstDimValueBack = 0;
    uint32_t castHalfRepStride = 0;
    uint32_t shCurLengthBlockNum = 0;
    uint32_t castHalfOutRepStride = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct DeepNormTiling {
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t inputXSize = 0;
    uint32_t meanVarSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t meanTmpTensorSize = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t varianceTmpTensorSize = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputRoundSize = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarRoundSize = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    float lastDimValueBack = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct GroupNormTiling {
    uint32_t n = 0;
    uint32_t c = 0;
    uint32_t hw = 0;
    uint32_t g = 0;
    uint32_t d = 0;
    uint32_t hwAlignSize = 0;
    uint32_t dhwAlignSize = 0;
    uint32_t inputXSize = 0;
    uint32_t meanVarSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t meanTmpTensorSize = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t varianceTmpTensorSize = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputRoundSize = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarRoundSize = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    float factor = 0;
    bool smallShape = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormGradBetaTiling {
    uint32_t stackBufferSize = 0;
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t bshLength = 0;
    uint32_t bsLength = 0;
    uint32_t oneCalSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t loopRound = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t bsTailSize = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    uint32_t gammaTempTensorPos = 0;
    uint32_t betaTempTensorPos = 0;
    uint32_t inputDyTmpTensorPos = 0;
    uint32_t resForGammaTmpTensorPos = 0;
    uint32_t reserved = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormGradTiling {
    uint32_t stackBufferSize = 0;
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t oneCalSize = 0;
    uint32_t nohCalSize = 0;
    uint32_t loopNum = 0;
    uint32_t tailSize = 0;
    uint32_t nohTailSize = 0;
    uint32_t tmpTensorBSHPos = 0;
    uint32_t tmpTensorBSHSize = 0;
    uint32_t pdVarTensorPos = 0;
    uint32_t pdVarTensorSize = 0;
    uint32_t pdMeanTensorPos = 0;
    uint32_t pdMeanTensorSize = 0;
    uint32_t x1TensorPos = 0;
    uint32_t x1TensorSize = 0;
    uint32_t x2TensorPos = 0;
    uint32_t x2TensorSize = 0;
    uint32_t x3TensorPos = 0;
    uint32_t x3TensorSize = 0;
    uint32_t tmpTensorPos = 0;
    uint32_t tmpTensorSize = 0;
    uint32_t tmpTensor1Pos = 0;
    uint32_t tmpTensor1Size = 0;
    uint32_t tmpTensor2Pos = 0;
    uint32_t tmpTensor2Size = 0;
    uint32_t lastDimValueBack = 0;
    uint32_t lastDimValueBackMulTwo = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormTiling {
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t inputXSize = 0;
    uint32_t meanVarSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t meanTmpTensorSize = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t varianceTmpTensorSize = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputRoundSize = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarRoundSize = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    float lastDimValueBack = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormSeparateTiling {
    uint32_t aLength = 0;
    uint32_t rLength = 0;
    uint32_t halfAddRepeatTimes = 0;
    uint32_t rHeadLength = 0;
    float k2Rec = 0;
    float k2RRec = 0;
    uint32_t inputXSize = 0;
    uint32_t meanVarSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t varianceTmpTensorSize = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputRoundSize = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarRoundSize = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t arCurLength = 0;
    uint32_t aCurLength = 0;
    float rValueBack = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct RmsNormTiling {
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    float reciprocalOfHLength = 0;
    uint32_t mainBshLength = 0;
    uint32_t mainBsLength = 0;
    uint32_t mainBsLengthAlign = 0;
    uint32_t loopRound = 0;
    uint32_t inputTailPos = 0;
    uint32_t tailBshLength = 0;
    uint32_t tailBsLength = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct UnPadTiling {
    uint32_t srcHeight = 0;
    uint32_t srcWidth = 0;
    uint32_t tmpBuffer1BlockNum = 0;
    uint32_t tmpBuffer1RowNum = 0;
    uint32_t tmpBuffer2Offset = 0;
    uint32_t widthTiling = 0;
    uint32_t widthFractal = 0;
    uint32_t widthFractalTail = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct PadTiling {
    uint32_t srcHeight = 0;
    uint32_t srcWidth = 0;
    uint32_t srcOriWidth = 0;
    uint32_t widthWithoutLastBlock = 0;
    uint32_t blocksPerRow = 0;
    uint32_t heightTiling = 0;
    uint32_t heightFractal = 0;
    uint32_t heightFractalTail = 0;
    uint32_t mainLoopOffset = 0;
    uint32_t tailBlockOffset = 0;
    uint32_t tmpBuffer1BlockNum = 0;
    uint32_t tmpBuffer1RowNum = 0;
    uint32_t tmpBuffer2Offset = 0;
    uint32_t widthTiling = 0;
    uint32_t widthFractal = 0;
    uint32_t widthFractalTail = 0;
    uint32_t widthFractalTailAlingned = 0;
    uint32_t brcbTiling = 0;
    uint32_t brcbFractal = 0;
    uint32_t brcbFractalTail = 0;
    uint32_t maxRepeatTimes = 0;
    uint32_t brcbTilingRepeatTimes = 0;
    uint32_t brcbTilingRepeatTimesTail = 0;
    uint32_t brcbFractalTailRepeatTimes = 0;
    uint32_t brcbFractalTailRepeatTimesTail = 0;
    uint32_t reserved = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct TopkTiling {
    int32_t tmpLocalSize = 0;
    int32_t allDataSize = 0;
    int32_t innerDataSize = 0;
    uint32_t sortRepeat = 0;
    int32_t mrgSortRepeat = 0;
    int32_t kAlignFourBytes = 0;
    int32_t kAlignTwoBytes = 0;
    int32_t maskOffset = 0;
    int32_t maskVreducev2FourBytes = 0;
    int32_t maskVreducev2TwoBytes = 0;
    int32_t mrgSortSrc1offset = 0;
    int32_t mrgSortSrc2offset = 0;
    int32_t mrgSortSrc3offset = 0;
    int32_t mrgSortTwoQueueSrc1Offset = 0;
    int32_t mrgFourQueueTailPara1 = 0;
    int32_t mrgFourQueueTailPara2 = 0;
    int32_t srcIndexOffset = 0;
    uint32_t copyUbToUbBlockCount = 0;
    int32_t topkMrgSrc1MaskSizeOffset = 0;
    int32_t topkNSmallSrcIndexOffset = 0;
    uint32_t vreduceValMask0 = 0;
    uint32_t vreduceValMask1 = 0;
    uint32_t vreduceIdxMask0 = 0;
    uint32_t vreduceIdxMask1 = 0;
    uint16_t vreducehalfValMask0 = 0;
    uint16_t vreducehalfValMask1 = 0;
    uint16_t vreducehalfValMask2 = 0;
    uint16_t vreducehalfValMask3 = 0;
    uint16_t vreducehalfValMask4 = 0;
    uint16_t vreducehalfValMask5 = 0;
    uint16_t vreducehalfValMask6 = 0;
    uint16_t vreducehalfValMask7 = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct ConfusionTransposeTiling {
    uint32_t param0 = 0;
    uint32_t param1 = 0;
    uint32_t param2 = 0;
    uint32_t param3 = 0;
    uint32_t param4 = 0;
    uint32_t param5 = 0;
    uint32_t param6 = 0;
    uint32_t param7 = 0;
    uint32_t param8 = 0;
    uint32_t param9 = 0;
    uint32_t param10 = 0;
    uint32_t param11 = 0;
    uint32_t param12 = 0;
    uint32_t param13 = 0;
    uint32_t param14 = 0;
    uint32_t param15 = 0;
    uint32_t param16 = 0;
    uint32_t param17 = 0;
};
#pragma pack(pop)
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_common_impl.h" 2



# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_impl.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SigmoidIntrinsicsImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& stackBuffer)
{
    struct UnaryRepeatParams repeatParams;
    struct BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, src, static_cast<T>(-1.0), MASK_PLACEHOLDER, 1, repeatParams);
    PipeBarrier<PIPE_V>();
    Exp<T, false>(dst, dst, MASK_PLACEHOLDER, 1, repeatParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(dst, dst, static_cast<T>(1), MASK_PLACEHOLDER, 1, repeatParams);
    Duplicate<T, false>(stackBuffer, static_cast<T>(1.0), MASK_PLACEHOLDER, 1,
        DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, stackBuffer, dst, MASK_PLACEHOLDER, 1, binaryRepeatParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SigmoidCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& sharedTmpBuffer, const uint32_t splitSize, const uint32_t loopCount, const uint32_t calcTail)
{
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
    for (uint32_t i = 0; i < loopCount; ++i) {
        SigmoidIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        SigmoidIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_common_impl.h" 2




namespace AscendC {
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SigmoidImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


                                                                      ;

                                                                          ;

    uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(T);
                                                                                               ;

    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    SigmoidCompute<T, isReuseSource>(dstTensor, srcTensor, tmpBuffer, splitSize, loopCount, calcTail);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SigmoidImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> stackBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackBuffer);
                                                                                 ;
    SigmoidImpl<T, isReuseSource>(dstTensor, srcTensor, stackBuffer, calCount);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SigmoidImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    SigmoidImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 78 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sigmoid<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Sigmoid<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}

#pragma end_pipe
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax_utils.h"
namespace AscendC {

enum class SoftmaxMode {
    SOFTMAX_NORMAL = 0,
    SOFTMAX_OUTPUT_WITHOUT_BRC = 1,
};

struct SoftmaxConfig {
    [aicore] constexpr SoftmaxConfig(const bool isCheckTilingIn)
    {
        isCheckTiling = isCheckTilingIn;
    }
    [aicore] constexpr SoftmaxConfig(const bool isCheckTilingIn, const uint32_t oriSrcMIn, const uint32_t oriSrcKIn)
    {
        isCheckTiling = isCheckTilingIn;
        oriSrcM = oriSrcMIn;
        oriSrcK = oriSrcKIn;
    }
    [aicore] constexpr SoftmaxConfig(const bool isCheckTilingIn, const uint32_t oriSrcMIn, const uint32_t oriSrcKIn, const enum SoftmaxMode modeIn)
    {
        isCheckTiling = isCheckTilingIn;
        oriSrcM = oriSrcMIn;
        oriSrcK = oriSrcKIn;
        mode = modeIn;
    }

    bool isCheckTiling = true;
    uint32_t oriSrcM = 0;
    uint32_t oriSrcK = 0;
    SoftmaxMode mode = SoftmaxMode::SOFTMAX_NORMAL;
};

constexpr SoftmaxConfig SOFTMAX_DEFAULT_CFG = { true, 0, 0 , SoftmaxMode::SOFTMAX_NORMAL};

struct SoftMaxParams {
    uint32_t srcM{ 0 };
    uint32_t srcK{ 0 };
    uint32_t oriSrcM{ 0 };
    uint32_t oriSrcK{ 0 };
    uint32_t loopCnt{ 1 };
    uint32_t splitMeanCnt{ 8 };
    float alpha{ 0.9375 };
};

struct SoftMaxShapeInfo {
    uint32_t srcM{ 0 };
    uint32_t srcK{ 0 };
    uint32_t oriSrcM{ 0 };
    uint32_t oriSrcK{ 0 };
};

};
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_common_utils.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_common_utils.h"
namespace AscendC {

constexpr uint8_t SOFTMAX_BASIC_TILE_NUM = 8;
constexpr uint8_t SOFTMAX_COMPUTE_DIM = 2;
constexpr uint8_t SOFTMAXGRAD_COMPUTE_DIM = 3;
constexpr uint8_t SOFTMAXFLASH_COMPUTE_DIM = 4;
constexpr uint8_t SOFTMAX_INNER_SHAPE_DIM = 2;
constexpr uint32_t FLOAT_NUM_PER_BLK = ONE_BLK_SIZE / B32_BYTE_SIZE;
constexpr uint32_t HALF_NUM_PER_BLK = ONE_BLK_SIZE / B16_BYTE_SIZE;
constexpr uint32_t HALF_REPEAT_STRIDE = DEFAULT_REPEAT_STRIDE / B16_BYTE_SIZE;
constexpr uint32_t SCALAR_STACK_DEPTH = 8;
constexpr uint32_t SOFTMAX_SHAPE_NZ_BASIC_COUNT = 16;
constexpr uint32_t SOFTMAX_NZ_TILING_NEEDBLOCK = 3;
constexpr uint32_t SOFTMAX_MAX_REPEAT_STRIDE = MAX_REPEAT_TIMES * DEFAULT_REPEAT_STRIDE;
constexpr uint32_t SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM = MAX_REPEAT_TIMES * FLOAT_REPEAT_SIZE;
constexpr uint32_t SOFTMAX_MAX_REPEAT_CLC_HALF_NUM = MAX_REPEAT_TIMES * HALF_REPEAT_SIZE;
constexpr uint32_t SOFTMAX_SPECIAL_BASICBLOCK_LEN = FLOAT_REPEAT_SIZE * SOFTMAX_BASIC_TILE_NUM * SOFTMAX_COMPUTE_DIM;
constexpr uint32_t SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE = 192;
constexpr uint32_t SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN = DEFAULT_BLOCK_SIZE * HALF_FACTOR;

struct LastAxisShapeND {
    uint32_t m;
    uint32_t k;
};

struct ReduceLastND {
    uint32_t originalSrcM;
    uint32_t originalSrcK;
    uint32_t srcM;
    uint32_t srcK;
    uint32_t dstM;
    uint32_t dstK;
};

struct BroadCastLastND {
    uint32_t dstM;
    uint32_t dstK;
    uint32_t srcM;
    uint32_t srcK;
};

};
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_common_shape_process.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_common_shape_process.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) LastAxisShapeND GetLastAxisShapeND(const ShapeInfo& shapeInfo)
{
    uint32_t calculateSize = 1;
    LastAxisShapeND ndinfo;
    for (uint32_t i = 0; i < shapeInfo.shapeDim; i++) {
        calculateSize *= shapeInfo.shape[i];
    }


                                                                             ;
    ndinfo.k = shapeInfo.shape[shapeInfo.shapeDim - 1];

                                                                   ;
    ndinfo.m = calculateSize / ndinfo.k;
    return ndinfo;
}

[aicore] __inline__ __attribute__((always_inline)) LastAxisShapeND GetLastAxisOriginShapeND(const ShapeInfo& srcShapeInfo)
{
    uint32_t calculateSize = 1;
    LastAxisShapeND ndinfo;
    for (uint32_t i = 0; i < srcShapeInfo.originalShapeDim; i++) {
        calculateSize *= srcShapeInfo.originalShape[i];
    }


                                                                                                    ;
    ndinfo.k = srcShapeInfo.originalShape[srcShapeInfo.originalShapeDim - 1];

                                                                   ;
    ndinfo.m = calculateSize / ndinfo.k;
    return ndinfo;
}
[aicore] __inline__ __attribute__((always_inline)) constexpr uint32_t CalculateNDSplitM(const uint32_t workLocalSize, const uint32_t dataTypeSize,
    const uint32_t reduceK, const LastAxisShapeND& ndinfo, bool isBasicBlock = false)
{
    uint32_t splitM = 0;
    if (dataTypeSize == B16_BYTE_SIZE) {
        splitM = workLocalSize / (reduceK + ndinfo.k + FLOAT_REPEAT_SIZE);
    } else {
        splitM = workLocalSize / (reduceK + FLOAT_REPEAT_SIZE);
    }

    splitM = splitM < ndinfo.m ? splitM : ndinfo.m;

    if (isBasicBlock && (splitM > SOFTMAX_BASIC_TILE_NUM) && (ndinfo.m % SOFTMAX_BASIC_TILE_NUM == 0)) {
        splitM = splitM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
        while (ndinfo.m % splitM != 0) {
            splitM -= SOFTMAX_BASIC_TILE_NUM;
        }

        while (splitM * ndinfo.k >= FLOAT_REPEAT_SIZE * DEFAULT_BLOCK_SIZE) {
            splitM = splitM / HALF_FACTOR;
        }
    }
    return splitM;
}

};
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_tiling_func.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_tiling_func.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) bool SoftMaxTilingFunc(const uint32_t workLocalSize, const SoftMaxShapeInfo& ndinfo,
    SoftMaxTiling& softmaxTiling, const uint32_t dataTypeSize1, const uint32_t dataTypeSize2, bool isBasicBlock = false,
    bool isDataFormatNZ = false)
{

                                                                                              ;
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / dataTypeSize2;
    const uint32_t srcM = ndinfo.srcM;
    const uint32_t srcK = ndinfo.srcK;
    const uint32_t oriSrcM = ndinfo.oriSrcM;

    softmaxTiling.srcM = srcM;
    softmaxTiling.srcK = srcK;
    softmaxTiling.srcSize = srcM * srcK;
    softmaxTiling.outMaxM = srcM;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = srcM * elementNumPerBlk;
    if (isDataFormatNZ) {
        softmaxTiling.reduceM = workLocalSize / (SOFTMAX_SHAPE_NZ_BASIC_COUNT + srcK);
    } else {
        softmaxTiling.reduceM = CalculateNDSplitM(workLocalSize, dataTypeSize1, elementNumPerBlk, {srcM, srcK},
            isBasicBlock);
    }

    if (softmaxTiling.reduceM < oriSrcM && softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < oriSrcM ? softmaxTiling.reduceM : oriSrcM;
    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = srcK;
    softmaxTiling.splitSize = softmaxTiling.reduceM * srcK;

                                                                                              ;
    softmaxTiling.rangeM = oriSrcM / softmaxTiling.reduceM;
    softmaxTiling.tailM = oriSrcM % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * srcK;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

[aicore] __inline__ __attribute__((always_inline)) bool SoftMaxFlashTilingFunc(const uint32_t workLocalSize, const LastAxisShapeND& ndinfo,
    SoftMaxTiling& softmaxTiling, const uint32_t elementNumPerBlk, bool isUpdate = false, bool isBasicBlock = false)
{
    softmaxTiling.srcM = ndinfo.m;
    softmaxTiling.srcK = ndinfo.k;
    softmaxTiling.srcSize = ndinfo.m * ndinfo.k;

    softmaxTiling.outMaxM = ndinfo.m;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = ndinfo.m * elementNumPerBlk;

    if (!isUpdate) {
        softmaxTiling.reduceM =
            workLocalSize / (elementNumPerBlk * SOFTMAX_COMPUTE_DIM + ndinfo.k * SOFTMAX_COMPUTE_DIM);
    } else {
        softmaxTiling.reduceM =
            workLocalSize / (elementNumPerBlk * SOFTMAXFLASH_COMPUTE_DIM + ndinfo.k * SOFTMAX_COMPUTE_DIM);
    }

    softmaxTiling.reduceM = softmaxTiling.reduceM < ndinfo.m ? softmaxTiling.reduceM : ndinfo.m;

    if (isBasicBlock && (softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) &&
        (softmaxTiling.srcM % SOFTMAX_BASIC_TILE_NUM == 0)) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
        while (softmaxTiling.srcM % softmaxTiling.reduceM != 0) {
            softmaxTiling.reduceM -= SOFTMAX_BASIC_TILE_NUM;
        }
    }

    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = ndinfo.k;
    softmaxTiling.splitSize = softmaxTiling.reduceM * ndinfo.k;

                                                                                                   ;
    softmaxTiling.rangeM = ndinfo.m / softmaxTiling.reduceM;
    softmaxTiling.tailM = ndinfo.m % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * ndinfo.k;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

[aicore] __inline__ __attribute__((always_inline)) bool SoftMaxGradTilingFunc(const uint32_t workLocalSize, const LastAxisShapeND& ndinfo,
    SoftMaxTiling& softmaxTiling, const uint32_t elementNumPerBlk, bool isFront = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false)
{
    softmaxTiling.srcM = ndinfo.m;
    softmaxTiling.srcK = ndinfo.k;
    softmaxTiling.srcSize = ndinfo.m * ndinfo.k;

    softmaxTiling.outMaxM = ndinfo.m;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = ndinfo.m * elementNumPerBlk;

    if (elementNumPerBlk != ONE_BYTE_BIT_SIZE) {
        softmaxTiling.reduceM = workLocalSize /
            (elementNumPerBlk * SOFTMAX_COMPUTE_DIM + ndinfo.k * SOFTMAXGRAD_COMPUTE_DIM + FLOAT_REPEAT_SIZE);
    } else {
        if (isFront && !isDataFormatNZ) {
            softmaxTiling.reduceM = workLocalSize / (elementNumPerBlk + ndinfo.k + FLOAT_REPEAT_SIZE);
        } else {
            softmaxTiling.reduceM =
                workLocalSize / (ndinfo.k + elementNumPerBlk * SOFTMAX_COMPUTE_DIM + FLOAT_REPEAT_SIZE);
        }
    }
    if (softmaxTiling.reduceM < ndinfo.m && softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < ndinfo.m ? softmaxTiling.reduceM : ndinfo.m;

    if (isBasicBlock && isFront && (softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) &&
        (softmaxTiling.srcM % SOFTMAX_BASIC_TILE_NUM == 0)) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
        while (softmaxTiling.srcM % softmaxTiling.reduceM != 0) {
            softmaxTiling.reduceM -= SOFTMAX_BASIC_TILE_NUM;
        }

        while (softmaxTiling.reduceM * ndinfo.k >= FLOAT_REPEAT_SIZE * DEFAULT_BLOCK_SIZE) {
            softmaxTiling.reduceM = softmaxTiling.reduceM / B16_BYTE_SIZE;
        }
    }

    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = ndinfo.k;
    softmaxTiling.splitSize = softmaxTiling.reduceM * ndinfo.k;

                                                                                                  ;
    softmaxTiling.rangeM = ndinfo.m / softmaxTiling.reduceM;
    softmaxTiling.tailM = ndinfo.m % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * ndinfo.k;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

};
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_common_broadcast.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_common_broadcast.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AlignedBrcbImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t brcbCount)
{
    T scalarList[SCALAR_STACK_DEPTH] = {0};

    SetVectorMask<T>(brcbCount);
    for (uint32_t j = 0; j < SCALAR_STACK_DEPTH; j++) {
        scalarList[j] = srcLocal.GetValue(j);
    }
    for (uint32_t k = 0; k < SCALAR_STACK_DEPTH; k++) {
        Duplicate<T, false>(dstLocal[k * brcbCount], scalarList[k], MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void ContinusColumnBrcbImpl(const LocalTensor<float>& dstLocal, const LocalTensor<float>& srcLocal,
    const uint32_t& repeat, const uint32_t& brcbCount)
{
    float scalarList[SCALAR_STACK_DEPTH] = {0};
    SetVectorMask<float>(brcbCount);
    const uint32_t rangeM = repeat / SCALAR_STACK_DEPTH;
    const uint32_t tailM = repeat % SCALAR_STACK_DEPTH;
    uint32_t offset = 0;

    for (uint32_t i = 0; i < rangeM; i++) {
        offset = i * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < SCALAR_STACK_DEPTH; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j);
        }
        for (uint32_t k = 0; k < SCALAR_STACK_DEPTH; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
    if (tailM != 0) {
        offset = rangeM * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j);
        }
        for (uint32_t k = 0; k < tailM; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void AlignedColumnBrcbImpl(const LocalTensor<float>& dstLocal, const LocalTensor<float>& srcLocal,
    const uint32_t& repeat, const uint32_t& brcbCount)
{
    float scalarList[SCALAR_STACK_DEPTH] = {0};
    SetVectorMask<float>(brcbCount);
    const uint32_t rangeM = repeat / SCALAR_STACK_DEPTH;
    const uint32_t tailM = repeat % SCALAR_STACK_DEPTH;
    uint32_t offset = 0;

    for (uint32_t i = 0; i < rangeM; i++) {
        offset = i * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < SCALAR_STACK_DEPTH; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j * brcbCount);
        }
        for (uint32_t k = 0; k < SCALAR_STACK_DEPTH; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
    if (tailM != 0) {
        offset = rangeM * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j * brcbCount);
        }
        for (uint32_t k = 0; k < tailM; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BroadCastNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t srcM)
{
    uint8_t repeat = srcM / DEFAULT_REPEAT_STRIDE;
    for (uint8_t i = 0; i < repeat; i++) {
        Muls<float, false>(dst[i * B16_BYTE_SIZE * FLOAT_REPEAT_SIZE], src[i * B16_BYTE_SIZE * FLOAT_REPEAT_SIZE], 1.0,
            MASK_PLACEHOLDER, B16_BYTE_SIZE, { 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
    }
    PipeBarrier<PIPE_V>();

    uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE];
    for (int32_t i = 0; i < NCHW_CONV_ADDR_LIST_SIZE; i++) {
        dstList[i] = (uint64_t)dst[i * FLOAT_NUM_PER_BLK].GetPhyAddr();
        srcList[i] = (uint64_t)src[i * FLOAT_NUM_PER_BLK].GetPhyAddr();
    }
    TransDataTo5HDParams transDataParams;
    transDataParams.repeatTimes = repeat;
    if (transDataParams.repeatTimes > 1) {
        transDataParams.dstRepStride = B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE;
        transDataParams.srcRepStride = B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE;
    }
    TransDataTo5HD<float>(dstList, srcList, transDataParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BroadCastLastCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const BroadCastLastND& brcParam, const uint32_t scalarStackDepth, const uint32_t index)
{
    const uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    const uint32_t rangeK = brcParam.dstK / elementNumPerRep;
    const uint32_t tailK = brcParam.dstK % elementNumPerRep;
    T scalarList[SCALAR_STACK_DEPTH] = {0};

    for (uint32_t j = 0; j < scalarStackDepth; j++) {
        scalarList[j] = src[(index * SCALAR_STACK_DEPTH + j) * brcParam.srcK].GetValue(0);
    }
    for (uint32_t j = 0; j < rangeK; j++) {
        for (uint32_t k = 0; k < scalarStackDepth; k++) {
            Duplicate(dst[j * elementNumPerRep + (index * SCALAR_STACK_DEPTH + k) * brcParam.dstK], scalarList[k],
                elementNumPerRep, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
    if (tailK != 0) {
        for (uint32_t k = 0; k < scalarStackDepth; k++) {
            Duplicate(dst[rangeK * elementNumPerRep + (index * SCALAR_STACK_DEPTH + k) * brcParam.dstK], scalarList[k],
                tailK, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

};
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_common_reduce.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_common_reduce.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);

    Max<float, false>(dst, src, src[FLOAT_NUM_PER_BLK], 1, 1,
        { B16_BYTE_SIZE, B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });
    PipeBarrier<PIPE_V>();
    BlockReduceMax<float, false>(dst, dst, reduceParam.srcM / FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE);

    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);

    Add<float, false>(dst, src, src[FLOAT_NUM_PER_BLK], 1, 1,
        { B16_BYTE_SIZE, B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });
    PipeBarrier<PIPE_V>();
    BlockReduceSum<float, false>(dst, dst, reduceParam.srcM / FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE);

    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void BigBlockReduceMax(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    for (uint32_t i = 0; i < splitM; i++) {
        BlockReduceMax<float, false>(dst[FLOAT_REPEAT_SIZE * i], src[i * splitK], FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
            1, 1, DEFAULT_REPEAT_STRIDE);
    }
    uint8_t remainRepeat = splitBlock - FLOAT_NUM_PER_BLK;
    if (remainRepeat == 0) {
        return;
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitM; ++j) {
        Max<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN + j * splitK],
            dst[j * FLOAT_REPEAT_SIZE], 1, remainRepeat, { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BigBlockReduceSum(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    for (uint32_t i = 0; i < splitM; i++) {
        BlockReduceSum<float, false>(dst[FLOAT_REPEAT_SIZE * i], src[i * splitK], FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
            1, 1, DEFAULT_REPEAT_STRIDE);
    }
    uint8_t remainRepeat = splitBlock - FLOAT_NUM_PER_BLK;
    if (remainRepeat == 0) {
        return;
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitM; ++j) {
        Add<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN + j * splitK],
            dst[j * FLOAT_REPEAT_SIZE], 1, remainRepeat, { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
    }
}

};
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_common_arithmetic.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common/softmax_common_arithmetic.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void TailMaxImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& src,
    const ReduceLastND& reduceParam, const uint64_t mask, const uint8_t srcRepeatStride, const uint32_t splitCount)
{
    const uint32_t tailStartOffset = HALF_REPEAT_SIZE * splitCount;
    if (reduceParam.srcK > SOFTMAX_MAX_REPEAT_STRIDE) {
        for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
            Max(dst[i * HALF_REPEAT_SIZE], dst[i * HALF_REPEAT_SIZE], src[tailStartOffset + i * reduceParam.srcK], mask,
                1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
    } else {
        const uint32_t range = reduceParam.originalSrcM / MAX_REPEAT_TIMES;
        const uint32_t tail = reduceParam.originalSrcM % MAX_REPEAT_TIMES;
        for (uint32_t i = 0; i < range; i++) {
            Max(dst[i * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM], dst[i * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM],
                src[tailStartOffset + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
        if (tail != 0) {
            Max(dst[range * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM], dst[range * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM],
                src[tailStartOffset + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
    }
}
[aicore] __inline__ __attribute__((always_inline)) void TailMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam, const uint64_t mask, const uint8_t srcRepeatStride, const uint32_t splitCount)
{
    const uint32_t tailStartOffset = FLOAT_REPEAT_SIZE * splitCount;
    if (reduceParam.srcK > SOFTMAX_MAX_REPEAT_STRIDE) {
        for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
            Max(dst[i * FLOAT_REPEAT_SIZE], dst[i * FLOAT_REPEAT_SIZE], src[tailStartOffset + i * reduceParam.srcK],
                mask, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
    } else {
        const uint32_t range = reduceParam.originalSrcM / MAX_REPEAT_TIMES;
        const uint32_t tail = reduceParam.originalSrcM % MAX_REPEAT_TIMES;
        for (uint32_t i = 0; i < range; i++) {
            Max(dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
        if (tail != 0) {
            Max(dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
    }
}
[aicore] __inline__ __attribute__((always_inline)) void TailAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam, const uint64_t mask, const uint8_t srcRepeatStride, const uint32_t splitCount)
{
    const uint32_t tailStartOffset = FLOAT_REPEAT_SIZE * splitCount;
    if (reduceParam.srcK > SOFTMAX_MAX_REPEAT_STRIDE) {
        for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
            Add(dst[i * FLOAT_REPEAT_SIZE], dst[i * FLOAT_REPEAT_SIZE], src[tailStartOffset + i * reduceParam.srcK],
                mask, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
    } else {
        const uint32_t range = reduceParam.originalSrcM / MAX_REPEAT_TIMES;
        const uint32_t tail = reduceParam.originalSrcM % MAX_REPEAT_TIMES;
        for (uint32_t i = 0; i < range; i++) {
            Add(dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
        if (tail != 0) {
            Add(dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void NextBlockMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint8_t splitM, const uint8_t srcRepstride, const uint32_t splitBlock, const uint32_t srcK)
{
    if (splitM > splitBlock) {
        for (uint32_t i = HALF_FACTOR; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepstride });
        }
    } else {





        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Max<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[HALF_REPEAT_SIZE + j * srcK], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void NextBlockAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint8_t splitM, const uint8_t srcRepstride, const uint32_t splitBlock, const uint32_t srcK)
{
    if (splitM > splitBlock) {
        for (uint32_t i = HALF_FACTOR; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepstride });
        }
    } else {





        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Add<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[HALF_REPEAT_SIZE + j * srcK], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BasicBlockMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src, uint8_t splitM,
    uint8_t offset, const uint32_t splitBlock)
{
    Max<float, false>(dst, src, src[FLOAT_REPEAT_SIZE], 1, splitM, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
    if (splitM > splitBlock) {
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
    } else {
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Max<float, false>(dst[j * FLOAT_REPEAT_SIZE],
                src[HALF_FACTOR * FLOAT_REPEAT_SIZE + splitBlock * FLOAT_REPEAT_SIZE * j], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BasicBlockAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src, uint8_t splitM,
    uint8_t offset, const uint32_t splitBlock)
{
    Add<float, false>(dst, src, src[FLOAT_REPEAT_SIZE], 1, splitM, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
    if (splitM > splitBlock) {
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
    } else {
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Add<float, false>(dst[j * FLOAT_REPEAT_SIZE],
                src[HALF_FACTOR * FLOAT_REPEAT_SIZE + splitBlock * FLOAT_REPEAT_SIZE * j], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GenericSubNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t dstBlockStride = srcK / FLOAT_NUM_PER_BLK;
        const uint8_t src1BlockStride = srcReduceK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < dstBlockStride; j++) {
            Sub<float, false>(dst[j * FLOAT_NUM_PER_BLK], src0[j * FLOAT_NUM_PER_BLK], src1, 1, 1,
                { dstBlockStride, dstBlockStride, src1BlockStride, (uint8_t)srcK, (uint8_t)srcK, (uint8_t)srcReduceK });
        }
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
        for (uint32_t j = 0; j < originalSrcM; j++) {
            Sub<float, false>(dst[j * srcK], src0[j * srcK], src1[j * srcReduceK], 1, 1,
                { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0 });
        }
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GenericDivNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t dstBlockStride = srcK / FLOAT_NUM_PER_BLK;
        const uint8_t src1BlockStride = srcReduceK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < dstBlockStride; j++) {
            Div<float, false>(dst[j * FLOAT_NUM_PER_BLK], src0[j * FLOAT_NUM_PER_BLK], src1, 1, 1,
                { dstBlockStride, dstBlockStride, src1BlockStride, (uint8_t)srcK, (uint8_t)srcK, (uint8_t)srcReduceK });
        }
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
        for (uint32_t j = 0; j < originalSrcM; j++) {
            Div<float, false>(dst[j * srcK], src0[j * srcK], src1[j * srcReduceK], 1, 1,
                { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0 });
        }
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GenericMulNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t dstBlockStride = srcK / FLOAT_NUM_PER_BLK;
        const uint8_t src1BlockStride = srcReduceK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < dstBlockStride; j++) {
            Mul<float, false>(dst[j * FLOAT_NUM_PER_BLK], src0[j * FLOAT_NUM_PER_BLK], src1, 1, 1,
                { dstBlockStride, dstBlockStride, src1BlockStride, (uint8_t)srcK, (uint8_t)srcK, (uint8_t)srcReduceK });
        }
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
        for (uint32_t j = 0; j < originalSrcM; j++) {
            Mul<float, false>(dst[j * srcK], src0[j * srcK], src1[j * srcReduceK], 1, 1,
                { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0 });
        }
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void TransDivToMulImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpbuffer, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    const uint32_t curReduceSize = originalSrcM * srcReduceK;
    Duplicate(tmpbuffer, (float)1.0, curReduceSize);
    PipeBarrier<PIPE_V>();
    Div(tmpbuffer, tmpbuffer, src, curReduceSize);
    PipeBarrier<PIPE_V>();
    GenericMulNDImpl(dst, dst, tmpbuffer, originalSrcM, srcK, srcReduceK);
}

};
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common.h" 2

namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void CreateSpecialFormatMask(uint64_t& lowMask, const uint32_t& maskLen, const uint32_t& nzBlockCount,
    const uint32_t& totalLen = SOFTMAX_SHAPE_NZ_BASIC_COUNT)
{



                                                                                       ;
    if (totalLen == SOFTMAX_SHAPE_NZ_BASIC_COUNT) {

                                                                                                          ;
    }
    if (totalLen >= B32_DATA_NUM_PER_BLOCK) {

                                                                                                                         ;
    }

                                                                                      ;
    uint16_t originalMask = totalLen == SOFTMAX_SHAPE_NZ_BASIC_COUNT ? 0xFFFF : 0xFF;
    uint64_t defaultMask = originalMask >> (totalLen - maskLen);
    lowMask = defaultMask;

    for (uint32_t i = 0; i < nzBlockCount - 1; i++) {
        lowMask = lowMask << totalLen;
        lowMask = lowMask | defaultMask;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BinaryComputeWithSpecialMask(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, uint64_t mask[2], const uint32_t& lastBlockMaskLen, const uint32_t& splitCount,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, const LocalTensor<float>&, uint64_t*,
    const uint8_t, const BinaryRepeatParams&))
{
    uint32_t repeat = splitCount / FLOAT_REPEAT_SIZE;
    uint32_t tail = splitCount % FLOAT_REPEAT_SIZE;

    uint32_t repeatRange = repeat / MAX_REPEAT_TIMES;
    uint32_t repeatTail = repeat % MAX_REPEAT_TIMES;
    const auto offsetCount = MAX_REPEAT_TIMES * FLOAT_REPEAT_SIZE;
    uint32_t dstOffset = 0;
    uint32_t src0Offset = 0;
    uint32_t src1Offset = 0;

    for (uint32_t i = 0; i < repeatRange; i++) {
        func(dst[i * offsetCount], src0[i * offsetCount], src1[i * offsetCount], mask, MAX_REPEAT_TIMES,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    if (repeatTail != 0) {
        func(dst[repeatRange * offsetCount], src0[repeatRange * offsetCount], src1[repeatRange * offsetCount], mask,
            repeatTail, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    if (tail != 0) {
        uint64_t tailMask[2] = { 0, 0 };
        CreateSpecialFormatMask(tailMask[0], lastBlockMaskLen, tail / SOFTMAX_SHAPE_NZ_BASIC_COUNT);
        func(dst[repeat * FLOAT_REPEAT_SIZE], src0[repeat * FLOAT_REPEAT_SIZE], src1[repeat * FLOAT_REPEAT_SIZE],
            tailMask, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void UnaryComputeWithSpecialMask(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    uint64_t mask[2], const uint32_t& lastBlockMaskLen, const uint32_t& splitCount,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, uint64_t*, const uint8_t,
    const UnaryRepeatParams&))
{
    uint32_t repeat = splitCount / FLOAT_REPEAT_SIZE;
    uint32_t tail = splitCount % FLOAT_REPEAT_SIZE;

    uint32_t repeatRange = repeat / MAX_REPEAT_TIMES;
    uint32_t repeatTail = repeat % MAX_REPEAT_TIMES;
    const auto offsetCount = MAX_REPEAT_TIMES * FLOAT_REPEAT_SIZE;
    uint32_t dstOffset = 0;
    uint32_t src0Offset = 0;
    uint32_t src1Offset = 0;

    for (uint32_t i = 0; i < repeatRange; i++) {
        func(dst[i * offsetCount], src[i * offsetCount], mask, MAX_REPEAT_TIMES,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    if (repeatTail != 0) {
        func(dst[repeatRange * offsetCount], src[repeatRange * offsetCount], mask, repeatTail,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    if (tail != 0) {
        uint64_t tailMask[2] = { 0, 0 };
        CreateSpecialFormatMask(tailMask[0], lastBlockMaskLen, tail / SOFTMAX_SHAPE_NZ_BASIC_COUNT);
        func(dst[repeat * FLOAT_REPEAT_SIZE], src[repeat * FLOAT_REPEAT_SIZE], tailMask, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

};
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_base_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_common_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_common_impl/softmax_common_broadcast.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_common_impl/softmax_common_broadcast.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BroadCastLastImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const struct BroadCastLastND& brcParam)
{


    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    uint64_t lowMask =
        brcParam.srcM * elementNumPerBlk;
    uint16_t repeat = brcParam.dstK / elementNumPerBlk;
    uint16_t srcBlkStride = brcParam.srcK / elementNumPerBlk;
    uint64_t mask[2] = { lowMask, 0 };

    uint32_t range = repeat / MAX_REPEAT_TIMES;
    uint32_t tail = repeat % MAX_REPEAT_TIMES;

    SetMaskCount();
    for (uint32_t i = 0; i < range; i++) {
        Copy<T>(dst[i * elementNumPerBlk * MAX_REPEAT_TIMES], src, mask, MAX_REPEAT_TIMES,
            { repeat, srcBlkStride, 1, 0 });
    }
    if (tail != 0) {
        Copy<T>(dst[range * elementNumPerBlk * MAX_REPEAT_TIMES], src, mask, tail, { repeat, srcBlkStride, 1, 0 });
    }

    SetMaskNorm();
    ResetMask();
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_common_impl/softmax_common_broadcast.h"
}

[aicore] __inline__ __attribute__((always_inline)) void SingleBlockBroadCastImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const struct ReduceLastND& reduceParam)
{
    BrcbRepeatParams brcbParams;
    brcbParams.dstBlkStride = 1;
    brcbParams.dstRepStride = BRCB_BROADCAST_NUMBER;
    const uint32_t range = reduceParam.originalSrcM / BRCB_BROADCAST_NUMBER;
    const uint32_t tail = reduceParam.originalSrcM % BRCB_BROADCAST_NUMBER;

    if (range != 0) {
        if (reduceParam.dstK == BRCB_BROADCAST_NUMBER * HALF_FACTOR) {
            brcbParams.dstBlkStride = HALF_FACTOR;
            brcbParams.dstRepStride = BRCB_BROADCAST_NUMBER * HALF_FACTOR;
            Brcb(dst[0], src, range, brcbParams);
            Brcb(dst[BRCB_BROADCAST_NUMBER], src, range, brcbParams);
        } else {
            Brcb(dst, src, range, brcbParams);
        }
    }

    if (tail != 0) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        float scalarList[SCALAR_STACK_DEPTH] = {0};
        for (uint32_t j = 0; j < tail; j++) {
            scalarList[j] = src[(range * BRCB_BROADCAST_NUMBER + j)].GetValue(0);
        }

        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        for (uint32_t k = 0; k < tail; k++) {
            Duplicate(dst[(range * SCALAR_STACK_DEPTH + k) * reduceParam.dstK], scalarList[k], reduceParam.dstK, 1,
                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void AlignedBroadCastImpl(const LocalTensor<float>& dst, const LocalTensor<float>& tmpbuffer,
    const struct ReduceLastND& reduceParam)
{
    const uint32_t repeat = (reduceParam.originalSrcM + BRCB_BROADCAST_NUMBER - 1) / BRCB_BROADCAST_NUMBER;

    if (reduceParam.dstK == BRCB_BROADCAST_NUMBER * HALF_FACTOR) {
        if (reduceParam.originalSrcM != 1) {
            Brcb(tmpbuffer, dst, (uint8_t)repeat, { HALF_FACTOR, BRCB_BROADCAST_NUMBER * HALF_FACTOR });
            Brcb(tmpbuffer[BRCB_BROADCAST_NUMBER], dst, (uint8_t)repeat,
                { HALF_FACTOR, BRCB_BROADCAST_NUMBER * HALF_FACTOR });
        } else {
            Brcb(tmpbuffer, dst, (uint8_t)repeat, { 1, BRCB_BROADCAST_NUMBER });
            PipeBarrier<PIPE_V>();
            DataCopy(tmpbuffer[DEFAULT_REPEAT_STRIDE], tmpbuffer, { 1, 1, 0, 0 });
        }
    } else {
        Brcb(tmpbuffer, dst, (uint8_t)repeat, { 1, BRCB_BROADCAST_NUMBER });
    }
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.originalSrcM * reduceParam.dstK);
    Copy<float, false>(dst, tmpbuffer, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

};
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_common_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_common_impl/softmax_common_nd_reduce.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_common_impl/softmax_common_nd_reduce.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void FirstBlockCopyImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t srcM, const uint32_t srcK, const uint16_t dstRepeatStride, const uint16_t srcRepeatStride)
{

    const uint32_t range = srcM / MAX_REPEAT_TIMES;
    const uint32_t tail = srcM % MAX_REPEAT_TIMES;
    for (uint32_t i = 0; i < range; i++) {
        Copy<float, false>(dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], src[i * MAX_REPEAT_TIMES * srcK],
            MASK_PLACEHOLDER, MAX_REPEAT_TIMES, { 1, 1, dstRepeatStride, srcRepeatStride });
    }
    if (tail != 0) {
        Copy<float, false>(dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], src[range * MAX_REPEAT_TIMES * srcK],
            MASK_PLACEHOLDER, tail, { 1, 1, dstRepeatStride, srcRepeatStride });
    }





}

[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxLastNDSplitImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const struct ReduceLastND& reduceParam, uint64_t mask, uint32_t splitNum)
{
    uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;

    for (uint32_t i = 0; i < range; i++) {
        WholeReduceMax(dst[i * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES, 1, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK, ReduceOrder::ORDER_ONLY_VALUE);
    }
    if (tail != 0) {
        WholeReduceMax(dst[range * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail, 1, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK, ReduceOrder::ORDER_ONLY_VALUE);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void AlignedReduceMaxNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceMaxParam, const uint32_t splitCount)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_REPEAT_SIZE);
    BlockReduceMax<float, false>(tmpTensor, src, 1, MASK_PLACEHOLDER, 1, 1, reduceMaxParam.srcK / FLOAT_NUM_PER_BLK);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    DataCopy(dst, tmpTensor, { 1, (uint16_t)reduceMaxParam.srcM, 0, 0 });
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    for (uint32_t i = 1; i < splitCount; i++) {
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_REPEAT_SIZE);
        BlockReduceMax<float, false>(tmpTensor, src[i * FLOAT_REPEAT_SIZE], 1, MASK_PLACEHOLDER, 1, 1,
            reduceMaxParam.srcK / FLOAT_NUM_PER_BLK);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_NUM_PER_BLK);
        Max<float, false>(dst, dst, tmpTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_NUM_PER_BLK);
    BlockReduceMax<float, false>(dst, dst, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void AlignedReduceSumNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam, const uint32_t splitCount)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_REPEAT_SIZE);
    BlockReduceSum<float, false>(tmpTensor, src, 1, MASK_PLACEHOLDER, 1, 1, reduceParam.srcK / FLOAT_NUM_PER_BLK);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    DataCopy(dst, tmpTensor, { 1, (uint16_t)reduceParam.srcM, 0, 0 });
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    for (uint32_t i = 1; i < splitCount; i++) {
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_REPEAT_SIZE);
        BlockReduceSum<float, false>(tmpTensor, src[i * FLOAT_REPEAT_SIZE], 1, MASK_PLACEHOLDER, 1, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);
        Add<float, false>(dst, dst, tmpTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);
    BlockReduceSum<float, false>(dst, dst, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxLastNDImpl(const LocalTensor<float>& dstMax, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceMaxParam)
{
    const uint32_t splitCount = reduceMaxParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceMaxParam.originalSrcK % FLOAT_REPEAT_SIZE;
    if (splitCount > 0) {
        AlignedReduceMaxNDImpl(tmpTensor, src, dstMax, reduceMaxParam, splitCount);
    }
    if (tailSrcK != 0) {
        ReduceMaxLastNDSplitImpl(dstMax, src, reduceMaxParam, tailSrcK, splitCount);
        PipeBarrier<PIPE_V>();
        if (splitCount == 0) {
            DataCopy(tmpTensor, dstMax, { 1, (uint16_t)reduceMaxParam.srcM, 0, 0 });
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_NUM_PER_BLK);
            Max<float, false>(tmpTensor, tmpTensor, dstMax, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            SetMaskNorm();
            ResetMask();
        }
    }

    PipeBarrier<PIPE_V>();
    SingleBlockBroadCastImpl(dstMax, tmpTensor, reduceMaxParam);
}

[aicore] __inline__ __attribute__((always_inline)) void BasicBlockReduceMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t originalSrcM, const uint32_t reduceK)
{
    if (originalSrcM == 1) {
        WholeReduceMax<float, false>(dst, src, MASK_PLACEHOLDER, DEFAULT_REPEAT_STRIDE, 1, 1, 0,
            ReduceOrder::ORDER_ONLY_VALUE);
        if (reduceK == DEFAULT_REPEAT_STRIDE * HALF_FACTOR) {
            PipeBarrier<PIPE_V>();
            DataCopy(dst[DEFAULT_REPEAT_STRIDE], dst, { 1, 1, 0, 0 });
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_REPEAT_SIZE);
        BlockReduceMax<float, false>(src, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        BlockReduceMax<float, false>(dst, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        SetMaskNorm();
        ResetMask();
    }
}

template <bool isBroadCast = true>
[aicore] __inline__ __attribute__((always_inline)) void NewReduceMaxLastNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    const uint16_t srcRepeatStride = reduceParam.srcK / FLOAT_NUM_PER_BLK;

    if (reduceParam.originalSrcK < FLOAT_REPEAT_SIZE) {
        ReduceMaxLastNDSplitImpl(dst, src, reduceParam, reduceParam.originalSrcK, 0);
        ResetMask();
    } else {
        if (reduceParam.originalSrcK >= SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN) {
            BigBlockReduceMax(tmpTensor, src, splitCount, reduceParam.originalSrcM, reduceParam.srcK);
        } else if (reduceParam.originalSrcK >= HALF_REPEAT_SIZE) {
            Max<float, false>(tmpTensor, src, src[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(reduceParam.originalSrcM),
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, (uint8_t)srcRepeatStride, (uint8_t)srcRepeatStride });
            NextBlockMaxImpl(tmpTensor, src, (uint8_t)(reduceParam.originalSrcM), srcRepeatStride, splitCount,
                reduceParam.srcK);
        } else {
            FirstBlockCopyImpl(tmpTensor, src, reduceParam.originalSrcM, reduceParam.srcK, DEFAULT_REPEAT_STRIDE,
                srcRepeatStride);
        }

        if (tailSrcK != 0) {
            PipeBarrier<PIPE_V>();
            TailMaxImpl(tmpTensor, src, reduceParam, tailSrcK, srcRepeatStride, splitCount);
            ResetMask();
        }
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(dst, tmpTensor, reduceParam.originalSrcM, reduceParam.dstK);
    }
    if constexpr (isBroadCast) {
        if (reduceParam.originalSrcM != 1 || reduceParam.originalSrcK <= FLOAT_REPEAT_SIZE) {
            PipeBarrier<PIPE_V>();
            AlignedBroadCastImpl(dst, tmpTensor, reduceParam);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumLastNDSplitImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const struct ReduceLastND& reduceParam, uint64_t mask, uint32_t dstRepStride, uint32_t splitNum)
{
    uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;

    for (uint32_t i = 0; i < range; i++) {
        WholeReduceSum(dst[i * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
            dstRepStride, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK);
    }
    if (tail != 0) {
        WholeReduceSum(dst[range * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail, dstRepStride,
            1, reduceParam.srcK / FLOAT_NUM_PER_BLK);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumLastNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    if (splitCount > 0) {
        AlignedReduceSumNDImpl(tmpTensor, src, dst, reduceParam, splitCount);
    }

    if (tailSrcK != 0) {
        ReduceSumLastNDSplitImpl(dst, src, reduceParam, tailSrcK, 1, splitCount);
        PipeBarrier<PIPE_V>();
        if (splitCount == 0) {
            DataCopy(tmpTensor, dst, { 1, (uint16_t)reduceParam.srcM, 0, 0 });
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);
            Add<float, false>(tmpTensor, tmpTensor, dst, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            SetMaskNorm();
            ResetMask();
        }
    }

    PipeBarrier<PIPE_V>();
    SingleBlockBroadCastImpl(dst, tmpTensor, reduceParam);
}

[aicore] __inline__ __attribute__((always_inline)) void BasicBlockReduceSumImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t originalSrcM, const uint32_t reduceK)
{
    if (originalSrcM == 1) {
        WholeReduceSum<float, false>(dst, src, MASK_PLACEHOLDER, DEFAULT_REPEAT_STRIDE, 1, 1, 0);
        if (reduceK == DEFAULT_REPEAT_STRIDE * HALF_FACTOR) {
            PipeBarrier<PIPE_V>();
            DataCopy(dst[DEFAULT_REPEAT_STRIDE], dst, { 1, 1, 0, 0 });
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_REPEAT_SIZE);
        BlockReduceSum<float, false>(src, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        BlockReduceSum<float, false>(dst, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        SetMaskNorm();
        ResetMask();
    }
}

template <bool isBroadCast = true>
[aicore] __inline__ __attribute__((always_inline)) void NewReduceSumLastNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    const uint16_t srcRepeatStride = reduceParam.srcK / FLOAT_NUM_PER_BLK;

    if (reduceParam.originalSrcK < FLOAT_REPEAT_SIZE) {
        ReduceSumLastNDSplitImpl(dst, src, reduceParam, reduceParam.originalSrcK, 1, 0);
        ResetMask();
    } else {
        if (reduceParam.originalSrcK >= SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN) {
            BigBlockReduceSum(tmpTensor, src, splitCount, reduceParam.originalSrcM, reduceParam.srcK);
        } else if (reduceParam.originalSrcK >= HALF_REPEAT_SIZE) {
            Add<float, false>(tmpTensor, src, src[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(reduceParam.originalSrcM),
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, (uint8_t)srcRepeatStride, (uint8_t)srcRepeatStride });
            NextBlockAddImpl(tmpTensor, src, (uint8_t)(reduceParam.originalSrcM), srcRepeatStride, splitCount,
                reduceParam.srcK);
        } else {
            FirstBlockCopyImpl(tmpTensor, src, reduceParam.originalSrcM, reduceParam.srcK, DEFAULT_REPEAT_STRIDE,
                srcRepeatStride);
        }

        if (tailSrcK != 0) {
            PipeBarrier<PIPE_V>();
            TailAddImpl(tmpTensor, src, reduceParam, tailSrcK, srcRepeatStride, splitCount);
            ResetMask();
        }
        PipeBarrier<PIPE_V>();
        BasicBlockReduceSumImpl(dst, tmpTensor, reduceParam.originalSrcM, reduceParam.dstK);
    }
    if constexpr (isBroadCast) {
        if (reduceParam.originalSrcM != 1 || reduceParam.originalSrcK <= FLOAT_REPEAT_SIZE) {
            PipeBarrier<PIPE_V>();
            AlignedBroadCastImpl(dst, tmpTensor, reduceParam);
        }
    }
}

};
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_common_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_common_nz_reduce.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_common_nz_reduce.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxSingleBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint64_t& mask, const ReduceLastND& reduceParam)
{
    const uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    const uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;
    for (uint32_t j = 0; j < range; j++) {
        WholeReduceMax(dst[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, MAX_REPEAT_TIMES, DEFAULT_REPEAT_STRIDE, 1,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
    if (tail != 0) {
        WholeReduceMax(dst[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, tail, DEFAULT_REPEAT_STRIDE, 1,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SingleUnAlignedReduceMaxNZImpl(const LocalTensor<float>& tmpBuffer1,
    const LocalTensor<float>& tmpBuffer0, const uint32_t lastBlockMaskLen, const ReduceLastND& reduceParam)
{
    ReduceMaxSingleBlockNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);

    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    AlignedColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxLastNZImpl(const LocalTensor<float>& tmpBuffer1, const LocalTensor<float>& tmpBuffer0,
    uint64_t mask[2], const ReduceLastND& reduceParam)
{
    const uint32_t splitNZBlockCount = reduceParam.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = reduceParam.dstM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitCount = reduceParam.originalSrcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    if (__builtin_expect(!!(splitNZBlockCount == 1 && lastBlockMaskLen != SOFTMAX_SHAPE_NZ_BASIC_COUNT), 0)) {
        SingleUnAlignedReduceMaxNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);
    } else {
        if (__builtin_expect(!!(splitNZBlockCount == 1), 0)) {
            ReduceMaxBlockNZImpl(tmpBuffer1, tmpBuffer0, reduceParam);
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
            Muls<float, false>(tmpBuffer1, tmpBuffer0, 1.0, MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            PipeBarrier<PIPE_V>();
            for (uint32_t j = 1; j < splitNZBlockCount - 1; j++) {
                Max<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
                PipeBarrier<PIPE_V>();
            }
            SetMaskNorm();
            ResetMask();

            BinaryComputeWithSpecialMask(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * (splitNZBlockCount - 1)],
                mask, lastBlockMaskLen, splitCount, Max<float>);

            PipeBarrier<PIPE_V>();
            ReduceMaxBlockNZImpl(tmpBuffer1, tmpBuffer1, reduceParam);
        }

        if (reduceParam.originalSrcM % DEFAULT_REPEAT_STRIDE == 0) {
            PipeBarrier<PIPE_V>();
            BroadCastNZImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM);
        } else {
            event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);

            ContinusColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);
            ResetMask();
        }
    }
}
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumSingleBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint64_t& mask, const ReduceLastND& reduceParam)
{
    const uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    const uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;
    for (uint32_t j = 0; j < range; j++) {
        WholeReduceSum(dst[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, MAX_REPEAT_TIMES,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT, 1, SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
    if (tail != 0) {
        WholeReduceSum(dst[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, tail, SOFTMAX_SHAPE_NZ_BASIC_COUNT, 1,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SingleUnAlignedReduceSumNZImpl(const LocalTensor<float>& tmpBuffer1,
    const LocalTensor<float>& tmpBuffer0, const uint32_t lastBlockMaskLen, const ReduceLastND& reduceParam)
{
    ReduceSumSingleBlockNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);

    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    AlignedColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumLastNZImpl(const LocalTensor<float>& tmpBuffer1, const LocalTensor<float>& tmpBuffer0,
    uint64_t mask[2], const struct ReduceLastND& reduceParam)
{
    const uint32_t splitOffset = reduceParam.dstM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitCount = reduceParam.originalSrcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = reduceParam.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    if (__builtin_expect(!!(splitNZBlockCount == 1 && lastBlockMaskLen != SOFTMAX_SHAPE_NZ_BASIC_COUNT), 0)) {
        SingleUnAlignedReduceSumNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);
    } else {
        if (__builtin_expect(!!(splitNZBlockCount == 1), 0)) {
            ReduceSumBlockNZImpl(tmpBuffer1, tmpBuffer0, reduceParam);
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
            Muls<float, false>(tmpBuffer1, tmpBuffer0, 1.0, MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            PipeBarrier<PIPE_V>();
            for (uint32_t j = 1; j < splitNZBlockCount - 1; j++) {
                Add<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
                PipeBarrier<PIPE_V>();
            }
            SetMaskNorm();
            ResetMask();

            BinaryComputeWithSpecialMask(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * (splitNZBlockCount - 1)],
                mask, lastBlockMaskLen, splitCount, Add<float>);

            PipeBarrier<PIPE_V>();
            ReduceSumBlockNZImpl(tmpBuffer1, tmpBuffer1, reduceParam);
        }

        if (reduceParam.originalSrcM % DEFAULT_REPEAT_STRIDE == 0) {
            PipeBarrier<PIPE_V>();
            BroadCastNZImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM);
        } else {
            event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);

            ContinusColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);
            ResetMask();
        }
    }
}

};
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_common_impl.h" 2

namespace AscendC {
constexpr RoundMode FLOAT2HALF_ROUND_MODE = RoundMode::CAST_ROUND;

};
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxBasicBlock(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& sumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / HALF_FACTOR;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        Cast<float, __cce_half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_BLK_NUM, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockMaxImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_BLK_NUM);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_BLK_NUM], reduceSumBuffer, splitCeilM,
            { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();

        Cast<__cce_half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_BLK_NUM);
        } else {
            BasicBlockAddImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_BLK_NUM);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
# 111 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_BLK_NUM], reduceSumBuffer, splitCeilM,
            { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(sumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_BLK_NUM });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxBasicBlock(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / HALF_FACTOR;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();

        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer1, src[offset1], (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockMaxImpl(tmpBuffer1, src[offset1], (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer2, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
# 175 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(maxTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(dst[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                maxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(dst[offset1], dst[offset1], MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer1, dst[offset1], (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockAddImpl(tmpBuffer1, dst[offset1], (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer2, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
# 209 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(sumTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(dst[offset1 + FLOAT_REPEAT_SIZE * j], dst[offset1 + FLOAT_REPEAT_SIZE * j],
                sumTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxBasicBlock(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    uint8_t stride = (uint8_t)(tiling.splitK / FLOAT_NUM_PER_BLK);
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        Cast<float, __cce_half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockMaxImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
# 267 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(maxTensor[offset2], reduceSumBuffer, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();

        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], maxTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockAddImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
# 303 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(sumTensor[offset2], reduceSumBuffer, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], sumTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_generic_nz_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_generic_nz_impl.h"
namespace AscendC {

template <bool isFlashV2 = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNZImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& sumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, __cce_half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<__cce_half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<__cce_half, float, false>(sumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    if constexpr (!isFlashV2) {
        for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
            Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        SetMaskNorm();
        ResetMask();
        BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
            mask, lastBlockMaskLen, splitCount, Div<float>);

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    }
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<__cce_half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <bool isFlashV2 = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        DataCopy(tmpBuffer0[splitOffset * j], src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            splitCount);
    }
    PipeBarrier<PIPE_V>();

    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    if constexpr (isFlashV2) {
        for (uint32_t j = 0; j < splitNZBlockCount; j++) {
            DataCopy(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0[splitOffset * j],
                splitCount);
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
        for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
            Div<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
                tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        SetMaskNorm();
        ResetMask();
        BinaryComputeWithSpecialMask(
            dst[offset1 + (splitNZBlockCount - 1) * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1, mask, lastBlockMaskLen, splitCount, Div<float>);
    }
}

template <bool isFlashV2 = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNZImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, __cce_half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();

    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();
    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();
    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    if constexpr (!isFlashV2) {
        for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
            Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        SetMaskNorm();
        ResetMask();
        BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
            mask, lastBlockMaskLen, splitCount, Div<float>);

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    }
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<__cce_half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNZImpl(const LocalTensor<T1>& dst, const LocalTensor<T1>& sumTensor,
    const LocalTensor<T1>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNZImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            tailReduceParam);
    }
}

}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_generic_nd_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_impl/softmax_generic_nd_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;

    NewReduceMaxLastNDImpl(maxTensor[offset2], src[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(dst[offset1], src[offset1], maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], dst[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();




    GenericDivNDImpl(dst[offset1], dst[offset1], sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);

}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNDImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& sumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer3 = workLocal[tiling.splitSize + tiling.reduceSize];
    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);

    PipeBarrier<PIPE_V>();
    Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();

    NewReduceSumLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);
    PipeBarrier<PIPE_V>();

    Cast(sumTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();



    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNDImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(maxTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);

    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();




    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNDExtImpl(const LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceSize, reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNDExtImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_impl.h" 2

namespace AscendC {

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNDImpl(const LocalTensor<T1>& dst, const LocalTensor<T1>& sumTensor,
    const LocalTensor<T1>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    PipeBarrier<PIPE_V>();
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftMaxNDExtImpl<T1>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            ReduceLastND reduceParam;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            if constexpr (SupportType<T1, __cce_half>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE * HALF_FACTOR };
            } else if constexpr (SupportType<T1, float>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE };
            }
            SoftMaxNDExtImpl<T1>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNDImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    PipeBarrier<PIPE_V>();
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftMaxNDExtImpl(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            ReduceLastND reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE };
            SoftMaxNDExtImpl(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    }
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SingleSoftMaxImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const uint32_t& offset, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];

    Cast(tmpBuffer0, src[offset], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer1, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer1, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(tmpBuffer1, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer1, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SingleSoftMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const uint32_t& offset, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.reduceSize];

    NewReduceMaxLastNDImpl(tmpBuffer0, src[offset], tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(dst[offset], src[offset], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    if constexpr(isReuseSource) {
        Exp(src[offset], dst[offset], splitSize);
        PipeBarrier<PIPE_V>();
        NewReduceSumLastNDImpl(tmpBuffer0, src[offset], tmpBuffer1, reduceParam);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(dst[offset], src[offset], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    } else {
        Exp(dst[offset], dst[offset], splitSize);
        PipeBarrier<PIPE_V>();
        NewReduceSumLastNDImpl(tmpBuffer0, dst[offset], tmpBuffer1, reduceParam);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(dst[offset], dst[offset], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    }
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNDImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    uint32_t offset = 0;
    uint32_t splitSize = tiling.splitSize;
    ReduceLastND reduceParam;
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM, tiling.splitK, tiling.reduceM,
            tiling.reduceK };
    } else {
        uint32_t splitK = 0;
        if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
            splitK = config.oriSrcK;
        } else {
            splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
        }
        if constexpr (SupportType<T, __cce_half>()) {
            reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE * HALF_FACTOR };
        } else if constexpr (SupportType<T, float>()) {
            reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE };
        }
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SingleSoftMaxImpl<isReuseSource>(dst, src, workLocal, tiling, offset, splitSize, reduceParam);
        offset += tiling.splitSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_base_impl.h" 2




namespace AscendC {
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    ShapeInfo srcShape = src.GetShapeInfo();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }

    if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
        SoftMaxTiling newTiling = tiling;
        SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
            newTiling, sizeof(T), sizeof(float), isBasicBlock);
        SoftMaxNDImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, originalSrcShape, newTiling);
    } else {
        SoftMaxNDImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, originalSrcShape, tiling);
    }
}
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, tiling, softmaxShapeInfo);
}
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, tiling, softmaxShapeInfo);
}

template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    ShapeInfo srcShape = src.GetShapeInfo();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {

        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || originalSrcShape.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), false, isDataFormatNZ);
            SoftMaxNZImpl<T1, T2, isBasicBlock>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, newTiling);
        } else {
            SoftMaxNZImpl<T1, T2, isBasicBlock>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling);
        }
    } else {

        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), isBasicBlock);
            SoftMaxNDImpl<T1, T2, isBasicBlock, config>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape,
                newTiling);
        } else {
            SoftMaxNDImpl<T1, T2, isBasicBlock, config>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape,
                tiling);
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftMaxImpl<T1, T2, isBasicBlock, isDataFormatNZ, config>(dst, sumTensor, maxTensor, src, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftMaxImpl<T1, T2, isBasicBlock, isDataFormatNZ, config>(dst, sumTensor, maxTensor, src, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T1, typename T2, uint8_t stepSizeMode = 0>
[aicore] __inline__ __attribute__((always_inline)) bool AdjustSoftMaxResNZImpl(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    uint32_t floatStepSize = ONE_BLK_FLOAT_NUM;
    uint32_t halfStepSize = ONE_BLK_HALF_NUM;

    bool isUpdateNeedCheck = false;
    const uint32_t splitNZBlockCount = softmaxShapeInfo.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    SetVectorMask<float>(SOFTMAX_SHAPE_NZ_BASIC_COUNT);
    for (uint32_t j = 0; j < softmaxShapeInfo.srcM; j++) {
        uint32_t offset = j * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        uint32_t splitCount = softmaxShapeInfo.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if constexpr (sizeof(T2) == sizeof(float)) {
            T2 maxValue = maxTensor.GetValue(j * floatStepSize);
            uint32_t checkValue = *reinterpret_cast<uint32_t*>(&maxValue);
            if (checkValue == from) {
                for (uint32_t k = 0; k < splitNZBlockCount; k++) {
                    Duplicate<T1, false>(softMaxRes[offset + splitCount * k], to, MASK_PLACEHOLDER, 1, 1,
                        DEFAULT_REPEAT_STRIDE);
                }
                isUpdateNeedCheck = true;
            }
        } else {
            T2 maxValue = maxTensor.GetValue(j * halfStepSize);
            uint16_t checkValue = *reinterpret_cast<uint16_t*>(&maxValue);
            if (checkValue == (uint16_t)from) {
                for (uint32_t k = 0; k < splitNZBlockCount; k++) {
                    Duplicate<T1, false>(softMaxRes[offset + splitCount * k], to, MASK_PLACEHOLDER, 1, 1,
                        DEFAULT_REPEAT_STRIDE);
                }
                isUpdateNeedCheck = true;
            }
        }
    }
    ResetMask();
    return isUpdateNeedCheck;
}

template <typename T1, typename T2, uint8_t stepSizeMode = 0>
[aicore] __inline__ __attribute__((always_inline)) bool AdjustSoftMaxResNDImpl(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    uint32_t floatStepSize = ONE_BLK_FLOAT_NUM;
    uint32_t halfStepSize = ONE_BLK_HALF_NUM;
    if constexpr (stepSizeMode) {
        floatStepSize = 1;
        halfStepSize = 1;
    }

    bool isUpdateNeedCheck = false;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, softmaxShapeInfo.srcK);
    for (uint32_t j = 0; j < softmaxShapeInfo.srcM; j++) {
        if constexpr (sizeof(T2) == sizeof(float)) {
            T2 maxValue = maxTensor.GetValue(j * floatStepSize);
            uint32_t checkValue = *reinterpret_cast<uint32_t*>(&maxValue);
            if (checkValue == from) {
                Duplicate<T1, false>(softMaxRes[j * softmaxShapeInfo.srcK], to, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                isUpdateNeedCheck = true;
            }
        } else {
            T2 maxValue = maxTensor.GetValue(j * halfStepSize);
            uint16_t checkValue = *reinterpret_cast<uint16_t*>(&maxValue);
            if (checkValue == (uint16_t)from) {
                Duplicate<T1, false>(softMaxRes[j * softmaxShapeInfo.srcK], to, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                isUpdateNeedCheck = true;
            }
        }
    }
    SetMaskNorm();
    ResetMask();
    return isUpdateNeedCheck;
}

template <typename T1, typename T2, bool isDataFormatNZ = false, uint8_t stepSizeMode = 0>
[aicore] __inline__ __attribute__((always_inline)) bool AdjustSoftMaxResImpl(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    bool isUpdateNeedCheck = false;
    if constexpr (isDataFormatNZ) {
        isUpdateNeedCheck = AdjustSoftMaxResNZImpl<T1, T2, stepSizeMode>(softMaxRes, maxTensor, from, to, softmaxShapeInfo);
    } else {
        isUpdateNeedCheck = AdjustSoftMaxResNDImpl<T1, T2, stepSizeMode>(softMaxRes, maxTensor, from, to, softmaxShapeInfo);
    }
    return isUpdateNeedCheck;
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h" 2
#pragma begin_pipe(V)

namespace AscendC {
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, T, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor, srcTensor, tiling,
        softmaxShapeInfo);
                                ;
}
# 71 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<__cce_half, float, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor, srcTensor, tiling,
        softmaxShapeInfo);
                                ;
}
# 97 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dstTensor, srcTensor, tiling, softmaxShapeInfo);
                                ;
}
# 123 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dstTensor, srcTensor, sharedTmpBuffer, tiling,
        softmaxShapeInfo);
                                ;
}
# 155 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, T, isBasicBlock, isDataFormatNZ,config>(dstTensor, sumTensor, maxTensor, srcTensor, sharedTmpBuffer,
        tiling, softmaxShapeInfo);
                                ;
}
# 186 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<__cce_half, float, isBasicBlock, isDataFormatNZ,config>(dstTensor, sumTensor, maxTensor, srcTensor,
        sharedTmpBuffer, tiling, softmaxShapeInfo);
                                ;
}
# 214 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T1, typename T2, bool isDataFormatNZ = false, uint8_t stepSizeMode = 0>
[aicore] __inline__ __attribute__((always_inline)) bool AdjustSoftMaxRes(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return false;
    }
    return AdjustSoftMaxResImpl<T1, T2, isDataFormatNZ, stepSizeMode>(softMaxRes, maxTensor, from, to, softmaxShapeInfo);
}
}

#pragma end_pipe
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/simple_softmax_base_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/simple_softmax_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/simple_softmax_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/simple_softmax_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/simple_softmax_common_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/simple_softmax_common_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount)
{
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.reduceSize + tiling.reduceSize];
    const uint16_t originalSrcM = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);

    DataCopyParams copyParams = { originalSrcM, 1, 0, 1 };
    DataCopy(tmpBuffer0, inMaxTensor[offset2], copyParams);
    DataCopy(tmpBuffer0[FLOAT_NUM_PER_BLK], inMaxTensor[offset2], copyParams);
    DataCopy(tmpBuffer1, inSumTensor[offset2], copyParams);
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inSumTensor[offset2], copyParams);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Exp<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Div<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer1, MASK_PLACEHOLDER,
            1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNZImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& inSumTensor,
    const LocalTensor<__cce_half>& inMaxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    Cast<float, __cce_half, false>(tmpBuffer1, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, __cce_half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    Cast<float, __cce_half, false>(tmpBuffer1, inSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<__cce_half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNZImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t originalSrcM = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);

    DataCopyParams copyParams = { originalSrcM, 1, 0, 1 };
    DataCopy(tmpBuffer1, inMaxTensor[offset2], copyParams);
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inMaxTensor[offset2], copyParams);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, __cce_half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, inSumTensor[offset2], copyParams);
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inSumTensor[offset2], copyParams);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<__cce_half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxNZImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const LocalTensor<float> workLocal,
    const SoftMaxTiling& tiling)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceM * ONE_BLK_SIZE / sizeof(T2);
        SimpleSoftMaxGenericNZImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1, offset2, splitCount);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceM * ONE_BLK_SIZE / sizeof(T2);
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SimpleSoftMaxGenericNZImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1, offset2, splitCount);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxBasicBlock(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& expSumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;

        Cast<float, __cce_half, false>(tmpBuffer1, maxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        Cast<float, __cce_half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        Cast<float, __cce_half, false>(tmpBuffer1, expSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxBasicBlock(const LocalTensor<float>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
# 261 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/simple_softmax_common_impl.h"
    const uint32_t splitBlock = tiling.srcK / FLOAT_REPEAT_SIZE;
    const uint8_t repstride = tiling.srcK / FLOAT_NUM_PER_BLK;
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.srcM * FLOAT_REPEAT_SIZE);
    for (uint32_t j = 0; j < splitBlock; ++j) {
        Sub<float, false>(dst[FLOAT_REPEAT_SIZE * j], src[FLOAT_REPEAT_SIZE * j], maxTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 0, repstride, repstride, 1 });
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.srcSize);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(dst, dst, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.srcM * FLOAT_REPEAT_SIZE);
    for (uint32_t j = 0; j < splitBlock; ++j) {
        Div<float, false>(dst[FLOAT_REPEAT_SIZE * j], dst[FLOAT_REPEAT_SIZE * j], expSumTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 0, repstride, repstride, 1 });
    }
    SetMaskNorm();
    ResetMask();

}

template <const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNDImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& inSumTensor,
    const LocalTensor<__cce_half>& inMaxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t offset1, const uint32_t offset2, const uint32_t curSplitM)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const uint32_t splitSize = curSplitM * tiling.splitK;
    const uint32_t reduceSize = curSplitM * tiling.reduceK;
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
        Cast(tmpBuffer2, inMaxTensor[offset2], RoundMode::CAST_NONE, reduceSize);
        PipeBarrier<PIPE_V>();

        GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, curSplitM, tiling.srcK, tiling.reduceK);

        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer0, tmpBuffer0, splitSize);

        Cast(tmpBuffer2, inSumTensor[offset2], RoundMode::CAST_NONE, reduceSize);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, curSplitM, tiling.srcK, tiling.reduceK);

        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    } else {
        uint32_t splitK = 0;
        if constexpr (config.oriSrcK % HALF_NUM_PER_BLK == 0) {
            splitK = config.oriSrcK;
        } else {
            splitK = AlignUp(config.oriSrcK, HALF_NUM_PER_BLK);
        }
        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
        Cast(tmpBuffer2, inMaxTensor[offset2], RoundMode::CAST_NONE, reduceSize);
        PipeBarrier<PIPE_V>();

        GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, curSplitM, splitK,
            DEFAULT_REPEAT_STRIDE * HALF_FACTOR);

        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer0, tmpBuffer0, splitSize);

        Cast(tmpBuffer2, inSumTensor[offset2], RoundMode::CAST_NONE, reduceSize);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, curSplitM, splitK,
            DEFAULT_REPEAT_STRIDE * HALF_FACTOR);

        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    }
}

template <const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t offset1, const uint32_t offset2, const uint32_t curSplitM)
{
    const uint32_t splitSize = curSplitM * tiling.splitK;

    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        GenericSubNDImpl(dst[offset1], src[offset1], inMaxTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);
        PipeBarrier<PIPE_V>();
        Exp(dst[offset1], dst[offset1], splitSize);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(dst[offset1], dst[offset1], inSumTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);
    } else {
        uint32_t splitK = 0;
        if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
            splitK = config.oriSrcK;
        } else {
            splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
        }
        GenericSubNDImpl(dst[offset1], src[offset1], inMaxTensor[offset2], curSplitM, splitK,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Exp(dst[offset1], dst[offset1], splitSize);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(dst[offset1], dst[offset1], inSumTensor[offset2], curSplitM, splitK,
            DEFAULT_REPEAT_STRIDE);
    }
}

template <typename T, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxNDImpl(const LocalTensor<T>& dst, const LocalTensor<T>& inSumTensor,
    const LocalTensor<T>& inMaxTensor, const LocalTensor<T>& src, const LocalTensor<float> workLocal,
    const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock) {
        SimpleSoftMaxBasicBlock(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
    } else {
        if constexpr (sizeof(T) == sizeof(float)) {
            SimpleSoftMaxGenericNDImpl<config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, 0, 0,
                tiling.srcM);
        } else {
            uint32_t offset1 = 0;
            uint32_t offset2 = 0;
            PipeBarrier<PIPE_V>();
            for (uint32_t i = 0; i < tiling.rangeM; i++) {
                offset1 = i * tiling.splitSize;
                offset2 = i * tiling.reduceSize;
                SimpleSoftMaxGenericNDImpl<config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1,
                    offset2, tiling.splitM);
            }
            PipeBarrier<PIPE_V>();
            if (tiling.tailM != 0) {
                offset1 = tiling.rangeM * tiling.splitSize;
                offset2 = tiling.rangeM * tiling.reduceSize;
                SimpleSoftMaxGenericNDImpl<config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1,
                    offset2, tiling.tailM);
            }
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxBasicBlock(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceM * FLOAT_NUM_PER_BLK;
        offset1 = i * tiling.splitSize;

        Cast<float, __cce_half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j],
                inMaxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j],
                inSumTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

template <const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNDImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t offset1, const uint32_t offset2, const uint32_t curSplitM)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const uint32_t splitSize = curSplitM * tiling.splitK;
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
        PipeBarrier<PIPE_V>();




        GenericSubNDImpl(tmpBuffer0, tmpBuffer0, inMaxTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);

        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer0, tmpBuffer0, tiling.splitSize);
        PipeBarrier<PIPE_V>();




        GenericDivNDImpl(tmpBuffer0, tmpBuffer0, inSumTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);

        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    } else {
        uint32_t splitK = 0;
        if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
            splitK = config.oriSrcK;
        } else {
            splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
        }
        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
        PipeBarrier<PIPE_V>();
        GenericSubNDImpl(tmpBuffer0, tmpBuffer0, inMaxTensor[offset2], curSplitM, splitK,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer0, tmpBuffer0, tiling.splitSize);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(tmpBuffer0, tmpBuffer0, inSumTensor[offset2], curSplitM, splitK,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    }
}

template <typename T, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxNDImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock) {
        SimpleSoftMaxBasicBlock(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
    } else {
        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset1 = i * tiling.splitSize;
            offset2 = i * tiling.reduceM * FLOAT_NUM_PER_BLK;
            SimpleSoftMaxGenericNDImpl<config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1,
                offset2, tiling.splitM);
        }
        PipeBarrier<PIPE_V>();
        if (tiling.tailM != 0) {
            offset1 = tiling.rangeM * tiling.splitSize;
            offset2 = tiling.rangeM * tiling.reduceM * FLOAT_NUM_PER_BLK;
            SimpleSoftMaxGenericNDImpl<config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1,
                offset2, tiling.tailM);
        }
    }
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/simple_softmax_impl.h" 2
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/simple_softmax_base_impl.h" 2




namespace AscendC {
template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    ShapeInfo srcShape = src.GetShapeInfo();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), false, isDataFormatNZ);
            SimpleSoftMaxNZImpl<T1, T2>(dst, inSumTensor, inMaxTensor, src, workLocal, newTiling);
        } else {
            SimpleSoftMaxNZImpl<T1, T2>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
        }
    } else {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), isBasicBlock);
            SimpleSoftMaxNDImpl<T1, isBasicBlock, config>(dst, inSumTensor, inMaxTensor, src, workLocal, newTiling);
        } else {
            SimpleSoftMaxNDImpl<T1, isBasicBlock, config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SimpleSoftMaxImpl<T1, T2, isBasicBlock, isDataFormatNZ, config>(dst, inSumTensor, inMaxTensor, src, workLocal,
        tiling, softmaxShapeInfo);
}

template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SimpleSoftMaxImpl<T1, T2, isBasicBlock, isDataFormatNZ, config>(dst, inSumTensor, inMaxTensor, src, workLocal,
        tiling, softmaxShapeInfo);
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& inSumTensor,
    const LocalTensor<T>& inMaxTensor, const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<T, T, isBasicBlock, isDataFormatNZ, config>(dstTensor, inSumTensor, inMaxTensor, srcTensor,
        tiling, softmaxShapeInfo);
}
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMax(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<__cce_half>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<__cce_half, float, isBasicBlock, isDataFormatNZ, config>(dstTensor, inSumTensor, inMaxTensor, srcTensor,
        tiling, softmaxShapeInfo);
}
# 99 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& inSumTensor,
    const LocalTensor<T>& inMaxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<T, T, isBasicBlock, isDataFormatNZ, config>(dstTensor, inSumTensor, inMaxTensor, srcTensor,
        sharedTmpBuffer, tiling, softmaxShapeInfo);
}
# 128 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMax(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<__cce_half, float, isBasicBlock, isDataFormatNZ, config>(dstTensor, inSumTensor, inMaxTensor, srcTensor,
        sharedTmpBuffer, tiling, softmaxShapeInfo);
}
}

#pragma end_pipe
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flashv2_base_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flashv2_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_flashv2_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_flashv2_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_block_reduce_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_block_reduce_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SpecialBasicBlockMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpbuffer, const uint8_t splitM, const uint8_t splitCeilM, const uint32_t splitK)
{
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
        BlockReduceMax<float, false>(tmpbuffer, src, splitM * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpbuffer, tmpbuffer, splitM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(dst, tmpbuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    } else {
        BlockReduceMax<float, false>(tmpbuffer, src, HALF_FACTOR * FLOAT_REPEAT_SIZE, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpbuffer, tmpbuffer, HALF_FACTOR * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpbuffer, tmpbuffer, tmpbuffer[FLOAT_NUM_PER_BLK], 1, 1,
            { 1, HALF_FACTOR, HALF_FACTOR, DEFAULT_REPEAT_STRIDE, B16_DATA_NUM_PER_BLOCK, B16_DATA_NUM_PER_BLOCK });
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(dst, tmpbuffer, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SpecialBasicBlockAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpbuffer, const uint8_t splitM, const uint8_t splitCeilM, const uint32_t splitK)
{
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
        BlockReduceSum<float, false>(tmpbuffer, src, splitM * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpbuffer, tmpbuffer, splitM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(dst, tmpbuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    } else {
        BlockReduceSum<float, false>(tmpbuffer, src, HALF_FACTOR * FLOAT_REPEAT_SIZE, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpbuffer, tmpbuffer, HALF_FACTOR * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpbuffer, tmpbuffer, tmpbuffer[FLOAT_NUM_PER_BLK], 1, 1,
            { 1, HALF_FACTOR, HALF_FACTOR, DEFAULT_REPEAT_STRIDE, B16_DATA_NUM_PER_BLOCK, B16_DATA_NUM_PER_BLOCK });
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(dst, tmpbuffer, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BasicBlockReduceMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer, const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    const uint8_t splitCeilM = (uint8_t)(DivCeil(splitM, FLOAT_NUM_PER_BLK));
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR || splitK == SOFTMAX_SPECIAL_BASICBLOCK_LEN) {
        SpecialBasicBlockMaxImpl(dst, src, tmpBuffer, (uint8_t)splitM, splitCeilM, splitK);
    } else {
        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer, src, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else if (splitK > DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
            BigBlockReduceMax(tmpBuffer, src, splitBlock, splitM, splitK);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (splitK / FLOAT_REPEAT_SIZE));
            BasicBlockMaxImpl(tmpBuffer, src, (uint8_t)splitM, offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(dst, tmpBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}
[aicore] __inline__ __attribute__((always_inline)) void BasicBlockReduceSumImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer, const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    const uint8_t splitCeilM = (uint8_t)(DivCeil(splitM, FLOAT_NUM_PER_BLK));
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR || splitK == SOFTMAX_SPECIAL_BASICBLOCK_LEN) {
        SpecialBasicBlockAddImpl(dst, src, tmpBuffer, (uint8_t)splitM, splitCeilM, splitK);
    } else {
        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer, src, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else if (splitK > DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
            BigBlockReduceSum(tmpBuffer, src, splitBlock, splitM, splitK);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (splitK / FLOAT_REPEAT_SIZE));
            BasicBlockAddImpl(tmpBuffer, src, (uint8_t)splitM, offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(dst, tmpBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h" 2

namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SetWaitFlagVToS()
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2BasicBlockImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& expSumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<__cce_half>& inExpSumTensor, const LocalTensor<__cce_half>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& tmpBuffer3 =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE / B16_BYTE_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;

    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        Cast<float, __cce_half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer3, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], tmpBuffer3, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();
        Cast<float, __cce_half, false>(tmpBuffer2, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer3, tmpBuffer2, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Cast<__cce_half, float, false>(maxTensor[offset2], tmpBuffer3, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });


        Sub<float, false>(tmpBuffer2, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer2, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(expMaxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, B16_BYTE_SIZE });
        }

        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        BasicBlockReduceSumImpl(tmpBuffer3, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 110 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], tmpBuffer3, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();

        Cast<float, __cce_half, false>(tmpBuffer3, inExpSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Mul<float, false>(tmpBuffer3, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(expSumTensor[offset2], tmpBuffer3, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2BasicBlockImpl(const LocalTensor<float>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<float>& src,
    const LocalTensor<float>& expMaxTensor, const LocalTensor<float>& inExpSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];
    const LocalTensor<float>& tmpBuffer3 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE / B16_BYTE_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, src[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 161 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Copy<float, false>(tmpBuffer2, inMaxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Max<float, false>(maxTensor[offset2], tmpBuffer2, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(dst[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                maxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }


        Sub<float, false>(tmpBuffer2, tmpBuffer2, maxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(expMaxTensor[offset2], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Exp<float, false>(dst[offset1], dst[offset1], MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        BasicBlockReduceSumImpl(tmpBuffer3, dst[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();







        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();

        Mul<float, false>(inExpSumTensor[offset2], expMaxTensor[offset2], inExpSumTensor[offset2], MASK_PLACEHOLDER,
            reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(expSumTensor[offset2], inExpSumTensor[offset2], tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2BasicBlock(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];

    const LocalTensor<float>& inMaxTmp = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];

    const LocalTensor<float>& inSumTmp =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE / B16_BYTE_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        Cast<float, __cce_half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 247 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Copy<float, false>(inMaxTmp, inMaxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Max<float, false>(maxTensor[offset2], inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], maxTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }

        Sub<float, false>(inMaxTmp, inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
# 281 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Copy<float, false>(tmpBuffer1, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { B16_BYTE_SIZE, 1, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE });
        Copy<float, false>(tmpBuffer1[DEFAULT_REPEAT_STRIDE], inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { B16_BYTE_SIZE, 1, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer1, FLOAT2HALF_ROUND_MODE,
            MASK_PLACEHOLDER, reduceCeilValue * B16_BYTE_SIZE, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceSumImpl(inSumTmp, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 308 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, inSumTmp, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();

        Mul<float, false>(inSumTmp, inMaxTmp, inExpSumTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(expSumTensor[offset2], inSumTmp, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
    }
}
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateBasicBlock(const LocalTensor<__cce_half>& dst,
    const LocalTensor<__cce_half>& expSumTensor, const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& reduceSumBuffer = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        Cast<float, __cce_half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        BasicBlockReduceMaxImpl(reduceSumBuffer, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 355 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], reduceSumBuffer, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, B16_BYTE_SIZE });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        BasicBlockReduceSumImpl(reduceSumBuffer, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 386 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], reduceSumBuffer, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(expSumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateBasicBlock(const LocalTensor<__cce_half>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        Cast<float, __cce_half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 431 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(maxTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();

        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], maxTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }

        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        BasicBlockReduceSumImpl(tmpBuffer2, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 458 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(expSumTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateBasicBlock(const LocalTensor<float>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<float>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, src[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 493 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(maxTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                maxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(dst[offset1], src[offset1], MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        BasicBlockReduceSumImpl(tmpBuffer2, dst[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 516 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(expSumTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

    }
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_flashv2_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_update_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_update_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2UpdateImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& expSumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<__cce_half>& inExpSumTensor, const LocalTensor<__cce_half>& inMaxTensor, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& tmpBuffer3 =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE];

    Cast(tmpBuffer1, inMaxTensor[offset2], RoundMode::CAST_NONE, reduceSize);
    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);

    Max<float, false>(tmpBuffer3, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Sub<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    Cast<__cce_half, float, false>(maxTensor[offset2], tmpBuffer3, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer3, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitSize);

    Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    NewReduceSumLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);
    Exp<float, false>(tmpBuffer1, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<__cce_half, float, false>(expMaxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    Cast<float, __cce_half, false>(tmpBuffer2, inExpSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer2, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<__cce_half, float, false>(expSumTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2UpdateImpl(const LocalTensor<float>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling,
    const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.reduceSize];

    NewReduceMaxLastNDImpl(tmpBuffer0, src[offset1], tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);

    Max<float, false>(tmpBuffer0, inMaxTensor[offset2], tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Sub<float, false>(tmpBuffer1, inMaxTensor[offset2], tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Exp<float, false>(expMaxTensor[offset2], tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
    GenericSubNDImpl(dst[offset1], src[offset1], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);



    DataCopy(maxTensor[offset2], tmpBuffer0, reduceSize);

    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(tmpBuffer0, dst[offset1], tmpBuffer1, reduceParam);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);

    Mul<float, false>(expSumTensor[offset2], expMaxTensor[offset2], inExpSumTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(expSumTensor[offset2], expSumTensor[offset2], tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2UpdateImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling,
    const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& tmpBuffer3 =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE];



    DataCopy(tmpBuffer1, inMaxTensor[offset2], reduceSize);

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();
    Max(maxTensor[offset2], inMaxTensor[offset2], tmpBuffer3, reduceSize);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);

    NewReduceSumLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);

    Sub(tmpBuffer1, tmpBuffer1, maxTensor[offset2], reduceSize);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer1, tmpBuffer1, reduceSize);
    PipeBarrier<PIPE_V>();

    BroadCastLastImpl(tmpBuffer0, tmpBuffer1,
        { tiling.reduceM, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE, tiling.reduceM, tiling.reduceK });
    PipeBarrier<PIPE_V>();
    Cast(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer0, FLOAT2HALF_ROUND_MODE, reduceSize * B16_BYTE_SIZE);

    Mul(tmpBuffer1, tmpBuffer1, inExpSumTensor[offset2], reduceSize);
    PipeBarrier<PIPE_V>();
    Add(expSumTensor[offset2], tmpBuffer1, tmpBuffer3, reduceSize);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_flashv2_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_no_update_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_no_update_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& expSumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& reduceBuffer = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(reduceBuffer, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, reduceBuffer, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Cast(maxTensor[offset2], reduceBuffer, FLOAT2HALF_ROUND_MODE, reduceSize);
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(reduceBuffer, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();
    Cast(expSumTensor[offset2], reduceBuffer, FLOAT2HALF_ROUND_MODE, reduceSize);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateImpl(const LocalTensor<float>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;

    NewReduceMaxLastNDImpl(maxTensor[offset2], src[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(dst[offset1], src[offset1], maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(expSumTensor[offset2], dst[offset1], tmpBuffer0, reduceParam);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateExtImpl(const LocalTensor<T>& dst, const LocalTensor<T>& expSumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftmaxFlashV2NoUpdateImpl(dst, expSumTensor, maxTensor, src, workLocal, reduceParam, tiling, offset1,
            offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(maxTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(expSumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateExtImpl(const LocalTensor<__cce_half>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling,
    ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftmaxFlashV2NoUpdateImpl(dst, expSumTensor, maxTensor, src, workLocal, reduceParam, tiling, offset1,
            offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_flashv2_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_nz_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_nz_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void FlashV2NZUpdateGenericImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> inMaxTmp = workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    LocalTensor<float> inSumTmp =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    DataCopy(inSumTmp, inSumTensor[offset2], { 1, copyBlockCount, 0, 0 });
    DataCopy(inMaxTmp, inMaxTensor[offset2], { 1, copyBlockCount, 0, 0 });

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        DataCopy(tmpBuffer0[splitOffset * j], src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            splitCount);
    }
    PipeBarrier<PIPE_V>();
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);
    Max<float, false>(maxTensor[offset2], inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inMaxTmp, inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Exp<float, false>(expMaxTensor[offset2], inMaxTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, maxTensor[offset2], { copyBlockCount, 1, 0, 1 });
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], maxTensor[offset2], { copyBlockCount, 1, 0, 1 });

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();

    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        DataCopy(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0[splitOffset * j],
            splitCount);
    }

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);

    Mul<float, false>(inMaxTmp, expMaxTensor[offset2], inSumTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(sumTensor[offset2], inMaxTmp, sumTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void FlashV2NZUpdateGenericImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& sumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<__cce_half>& inSumTensor, const LocalTensor<__cce_half>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> inMaxTmp = workLocal[tiling.splitSize + tiling.reduceSize];
    LocalTensor<float> inSumTmp = workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, __cce_half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    Cast<float, __cce_half, false>(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    Cast<float, __cce_half, false>(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    Max<float, false>(tmpBuffer1, inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<__cce_half, float, false>(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    Cast<__cce_half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<__cce_half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<__cce_half, float, false>(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void FlashV2NZUpdateGenericImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> inMaxTmp = workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    LocalTensor<float> inSumTmp =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    DataCopy(inMaxTmp, inMaxTensor[offset2], { 1, copyBlockCount, 0, 0 });
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, __cce_half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);
    Max<float, false>(maxTensor[offset2], inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inMaxTmp, inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, inMaxTmp, { copyBlockCount, 1, 0, 1 });
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inMaxTmp, { copyBlockCount, 1, 0, 1 });

    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<__cce_half, float, false>(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
        1, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, maxTensor[offset2], { copyBlockCount, 1, 0, 1 });
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], maxTensor[offset2], { copyBlockCount, 1, 0, 1 });

    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    DataCopy(inSumTmp, inSumTensor[offset2], { 1, copyBlockCount, 0, 0 });

    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<__cce_half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);

    Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(sumTensor[offset2], inMaxTmp, sumTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NZUpdateImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1 && splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if (splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NZUpdateImpl(const LocalTensor<__cce_half>& dstTensor,
    const LocalTensor<float>& sumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<__cce_half>& expMaxTensor, const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1 && splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if (splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NZNoUpdateImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1 && splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if (splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NZNoUpdateImpl(const LocalTensor<__cce_half>& dstTensor,
    const LocalTensor<float>& sumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1 && splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if (splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, tailReduceParam);
    }
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_flashv2_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_common_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_common_impl.h"
namespace AscendC {

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxFlashV2NZImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (!isUpdate) {
        SoftmaxFlashV2NZNoUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal,
            originalSrcShape, tiling);
    } else {
        SoftmaxFlashV2NZUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor,
            inSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
    }
}

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxFlashV2NZImpl(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcTensor, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (!isUpdate) {
        SoftmaxFlashV2NZNoUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal,
            originalSrcShape, tiling);
    } else {
        SoftmaxFlashV2NZUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor,
            inSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdate(const LocalTensor<T1>& dst, const LocalTensor<T1>& expSumTensor,
    const LocalTensor<T1>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftmaxFlashV2NoUpdateBasicBlock(dst, expSumTensor, maxTensor, src, workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftmaxFlashV2NoUpdateExtImpl<T1>(dst, expSumTensor, maxTensor, src, workLocal, originalSrcShape, tiling,
                reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftmaxFlashV2NoUpdateBasicBlock(dst, expSumTensor, maxTensor, src, workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            ReduceLastND reduceParam;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            if constexpr (SupportType<T1, __cce_half>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE * HALF_FACTOR };
            } else if constexpr (SupportType<T1, float>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE };
            }
            SoftmaxFlashV2NoUpdateExtImpl<T1>(dst, expSumTensor, maxTensor, src, workLocal, originalSrcShape, tiling,
                reduceParam);
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdate(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftmaxFlashV2NoUpdateBasicBlock(dst, expSumTensor, maxTensor, src, workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftmaxFlashV2NoUpdateExtImpl(dst, expSumTensor, maxTensor, src, workLocal, originalSrcShape, tiling,
                reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftmaxFlashV2NoUpdateBasicBlock(dst, expSumTensor, maxTensor, src, workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            ReduceLastND reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE };
            SoftmaxFlashV2NoUpdateExtImpl(dst, expSumTensor, maxTensor, src, workLocal, originalSrcShape, tiling,
                reduceParam);
        }
    }
}

template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NDExtImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& expSumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftmaxFlashV2UpdateImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
            workLocal, reduceParam, tiling, offset1, offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NDImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& expSumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftmaxFlashV2BasicBlockImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftmaxFlashV2NDExtImpl<T1, T2>(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor,
                inMaxTensor, workLocal, originalSrcShape, tiling, reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftmaxFlashV2BasicBlockImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            ReduceLastND reduceParam;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            if constexpr (SupportType<T2, __cce_half>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE * HALF_FACTOR };
            } else if constexpr (SupportType<T2, float>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE };
            }
            SoftmaxFlashV2NDExtImpl<T1, T2>(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor,
                inMaxTensor, workLocal, originalSrcShape, tiling, reduceParam);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NDExtImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape,
    const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftmaxFlashV2UpdateImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
            workLocal, reduceParam, tiling, offset1, offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NDImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftmaxFlashV2BasicBlock(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftmaxFlashV2NDExtImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, originalSrcShape, tiling, reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftmaxFlashV2BasicBlock(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            ReduceLastND reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE };
            SoftmaxFlashV2NDExtImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, originalSrcShape, tiling, reduceParam);
        }
    }
}

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2PostProcess(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& expSumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (!isUpdate) {
        SoftmaxFlashV2NoUpdate<T1, T2, isBasicBlock, config>(dstTensor, expSumTensor, maxTensor, srcTensor, workLocal,
            originalSrcShape, tiling);
    } else {
        SoftmaxFlashV2NDImpl<T1, T2, isBasicBlock, config>(dstTensor, expSumTensor, maxTensor, srcTensor, expMaxTensor,
            inExpSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
    }
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_flashv2_impl.h" 2

namespace AscendC {
    [aicore] __inline__ __attribute__((always_inline)) void SoftMaxFlashV2M1CastIntrinsicsImpl(const LocalTensor<float>& dstLocal, const LocalTensor<__cce_half>& srcLocal,
        const uint32_t calCount)
    {
        UnaryRepeatParams unaryParams;
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);

        SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
        Cast<float, __cce_half, false>(dstLocal, srcLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftMaxFlashV2M1CastIntrinsicsImpl(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<float>& srcLocal,
        const uint32_t calCount)
    {
        UnaryRepeatParams unaryParams;
        unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);

        SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
        Cast<__cce_half, float, false>(dstLocal, srcLocal, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }

    template <bool isBasicBlock = false>
    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1BrcbSubImpl(const LocalTensor<float>& dstLocal, const LocalTensor<float>& src0Local,
        const LocalTensor<float>& src1Local, const LocalTensor<float>& tmpBuffer, const uint32_t srcM, const uint32_t srcK)
    {

        uint32_t splitCeilM = DivCeil(srcM, FLOAT_NUM_PER_BLK);
        Brcb(tmpBuffer, src1Local, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if constexpr (isBasicBlock) {
            uint32_t splitBlock = srcK / FLOAT_REPEAT_SIZE;
            uint8_t offset = srcK / FLOAT_NUM_PER_BLK;
            SetVectorMask<float, MaskMode::COUNTER>(0, srcM * FLOAT_REPEAT_SIZE);
            for (uint32_t j = 0; j < splitBlock; ++j) {
                Sub<float, false>(dstLocal[FLOAT_REPEAT_SIZE * j], src0Local[FLOAT_REPEAT_SIZE * j], tmpBuffer,
                    MASK_PLACEHOLDER, 1, { 1, 1, 0, offset, offset, 1 });
            }
            PipeBarrier<PIPE_V>();
        } else if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
            uint8_t blockStride = srcK / FLOAT_NUM_PER_BLK;
            SetVectorMask<float, MaskMode::COUNTER>(0, srcM * FLOAT_NUM_PER_BLK);
            for (uint8_t j = 0; j < blockStride; j++) {
                Sub<float, false>(dstLocal[j * FLOAT_NUM_PER_BLK], src0Local[j * FLOAT_NUM_PER_BLK], tmpBuffer,
                    MASK_PLACEHOLDER, 1, { blockStride, blockStride, DEFAULT_BLK_STRIDE, (uint8_t)srcK, (uint8_t)srcK, DEFAULT_REPEAT_STRIDE});
            }
            PipeBarrier<PIPE_V>();
        } else {
            SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
            for (uint32_t j = 0; j < srcM; j++) {
                Sub<float, false>(dstLocal[j * srcK], src0Local[j * srcK], tmpBuffer[j * FLOAT_NUM_PER_BLK],
                    MASK_PLACEHOLDER, 1, { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0});
            }
            PipeBarrier<PIPE_V>();
        }
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1NoUpdateBasicBlockProcess(const LocalTensor<float>& dstLocal,
        const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal,
        const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
    {
        UnaryRepeatParams unaryParams;
        const LocalTensor<float>& reduceBuffer = workLocal;
        uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;

        SetMaskNorm();
        ResetMask();


        BasicBlockReduceMaxImpl(maxTensor, srcLocal, reduceBuffer, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        SetMaskCount();


        SoftmaxFlashV2M1BrcbSubImpl<true>(srcLocal, srcLocal, maxTensor, reduceBuffer, tiling.splitM, tiling.splitK);

        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.splitSize);
        Exp<float, false>(dstLocal, srcLocal, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();

        SetMaskNorm();
        ResetMask();


        BasicBlockReduceSumImpl(expSumTensor, dstLocal, reduceBuffer, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        SetMaskCount();
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1NoUpdateBasicBlock(const LocalTensor<__cce_half>& dstLocal,
        const LocalTensor<__cce_half>& expSumTensor, const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& srcLocal,
        const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
    {
        const LocalTensor<float>& srcBuffer = workLocal;
        const LocalTensor<float>& sumBuffer = srcBuffer[tiling.splitSize];
        const LocalTensor<float>& maxBuffer = sumBuffer[tiling.splitM];
        const LocalTensor<float>& tmpBuffer = maxBuffer[tiling.splitM];

        SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocal, tiling.splitSize);
        SoftmaxFlashV2M1NoUpdateBasicBlockProcess(srcBuffer, sumBuffer, maxBuffer, srcBuffer, tmpBuffer, tiling);
        SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocal, srcBuffer, tiling.splitSize);
        SoftMaxFlashV2M1CastIntrinsicsImpl(expSumTensor, sumBuffer, tiling.splitM);
        SoftMaxFlashV2M1CastIntrinsicsImpl(maxTensor, maxBuffer, tiling.splitM);
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1NoUpdateBasicBlock(const LocalTensor<float>& dstLocal,
        const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal,
        const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
    {
        SoftmaxFlashV2M1NoUpdateBasicBlockProcess(dstLocal, expSumTensor, maxTensor, srcLocal, workLocal, tiling);
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1NoUpdateBasicBlock(const LocalTensor<__cce_half>& dstLocal,
        const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcLocal,
        const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
    {
        const LocalTensor<float>& srcBuffer = workLocal;
        const LocalTensor<float>& tmpBuffer = srcBuffer[tiling.splitSize];

        SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocal, tiling.splitSize);
        SoftmaxFlashV2M1NoUpdateBasicBlockProcess(srcBuffer, expSumTensor, maxTensor, srcBuffer, tmpBuffer, tiling);
        SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocal, srcBuffer, tiling.splitSize);
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1BasicBlockImplProcess(const LocalTensor<float>& dstLocal, const LocalTensor<float>& expSumTensor,
        const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal, const LocalTensor<float>& expMaxTensor,
        const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
        const SoftMaxTiling& tiling)
    {
        UnaryRepeatParams unaryParams;
        BinaryRepeatParams binaryParams;
        CopyRepeatParams copyParams(1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);

        const LocalTensor<float>& reduceBuffer = workLocal;
        const LocalTensor<float>& tmpBufferM1 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];

        uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;

        SetMaskNorm();
        ResetMask();


        BasicBlockReduceMaxImpl(tmpBufferM1, srcLocal, reduceBuffer, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.splitM);


        Copy<float, false>(reduceBuffer, inMaxTensor, MASK_PLACEHOLDER, 1, copyParams);
        PipeBarrier<PIPE_V>();


        Max<float, false>(maxTensor, inMaxTensor, tmpBufferM1, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();


        Sub<float, false>(tmpBufferM1, reduceBuffer, maxTensor, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();

        Exp<float, false>(expMaxTensor, tmpBufferM1, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();


        SoftmaxFlashV2M1BrcbSubImpl<true>(dstLocal, srcLocal, maxTensor, tmpBufferM1, tiling.splitM, tiling.splitK);

        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.splitSize);
        Exp<float, false>(dstLocal, dstLocal, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();

        SetMaskNorm();
        ResetMask();

        BasicBlockReduceSumImpl(tmpBufferM1, dstLocal, reduceBuffer, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.splitM);


        Mul<float, false>(inExpSumTensor, expMaxTensor, inExpSumTensor, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(expSumTensor, inExpSumTensor, tmpBufferM1, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1BasicBlockImpl(const LocalTensor<float>& dstLocal, const LocalTensor<float>& expSumTensor,
        const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal, const LocalTensor<float>& expMaxTensor,
        const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
        const SoftMaxTiling& tiling)
    {
        SoftmaxFlashV2M1BasicBlockImplProcess(dstLocal, expSumTensor, maxTensor, srcLocal, expMaxTensor,
            inExpSumTensor, inMaxTensor, workLocal, tiling);
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1BasicBlockImpl(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<__cce_half>& expSumTensor,
        const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& srcLocal, const LocalTensor<__cce_half>& expMaxTensor,
        const LocalTensor<__cce_half>& inExpSumTensor, const LocalTensor<__cce_half>& inMaxTensor, const LocalTensor<float>& workLocal,
        const SoftMaxTiling& tiling)
    {
        const LocalTensor<float>& srcBuffer = workLocal;
        const LocalTensor<float>& sumBuffer = srcBuffer[tiling.splitSize];
        const LocalTensor<float>& maxBuffer = sumBuffer[tiling.splitM];
        const LocalTensor<float>& expMaxBuffer = maxBuffer[tiling.splitM];
        const LocalTensor<float>& tmpBuffer = expMaxBuffer[tiling.splitM];

        SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocal, tiling.splitSize);
        SoftMaxFlashV2M1CastIntrinsicsImpl(sumBuffer, inExpSumTensor, tiling.splitM);
        SoftMaxFlashV2M1CastIntrinsicsImpl(maxBuffer, inMaxTensor, tiling.splitM);

        SoftmaxFlashV2M1BasicBlockImplProcess(srcBuffer, sumBuffer, maxBuffer, srcBuffer, expMaxBuffer, sumBuffer, maxBuffer, tmpBuffer, tiling);

        SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocal, srcBuffer, tiling.splitSize);
        SoftMaxFlashV2M1CastIntrinsicsImpl(expSumTensor, sumBuffer, tiling.splitM);
        SoftMaxFlashV2M1CastIntrinsicsImpl(maxTensor, maxBuffer, tiling.splitM);
        SoftMaxFlashV2M1CastIntrinsicsImpl(expMaxTensor, expMaxBuffer, tiling.splitM);
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1BasicBlockImpl(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<float>& expSumTensor,
        const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcLocal, const LocalTensor<__cce_half>& expMaxTensor,
        const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
        const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
    {
        const LocalTensor<float>& srcBuffer = workLocal;
        const LocalTensor<float>& expMaxBuffer = srcBuffer[tiling.splitSize];
        const LocalTensor<float>& tmpBuffer = expMaxBuffer[tiling.splitM];

        SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocal, tiling.splitSize);
        SoftmaxFlashV2M1BasicBlockImplProcess(srcBuffer, expSumTensor, maxTensor, srcBuffer,
            expMaxBuffer, inExpSumTensor, inMaxTensor, tmpBuffer, tiling);
        SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocal, srcBuffer, tiling.splitSize);
        SoftMaxFlashV2M1CastIntrinsicsImpl(expMaxTensor, expMaxBuffer, tiling.splitM);
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1NoUpdateImplProcess(const LocalTensor<float>& dstLocal, const LocalTensor<float>& expSumTensor,
        const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal, const LocalTensor<float>& workLocal,
        const ReduceLastND& reduceParam)
    {
        UnaryRepeatParams unaryParams;
        const LocalTensor<float>& tmpBuffer = workLocal;

        SetMaskNorm();
        ResetMask();

        NewReduceMaxLastNDImpl<false>(maxTensor, srcLocal, tmpBuffer, reduceParam);
        PipeBarrier<PIPE_V>();

        SetMaskCount();

        SoftmaxFlashV2M1BrcbSubImpl(dstLocal, srcLocal, maxTensor, tmpBuffer, reduceParam.srcM, reduceParam.srcK);
        PipeBarrier<PIPE_V>();

        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * reduceParam.srcK);
        Exp<float, false>(dstLocal, dstLocal, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();

        SetMaskNorm();
        ResetMask();

        NewReduceSumLastNDImpl<false>(expSumTensor, dstLocal, tmpBuffer, reduceParam);
        PipeBarrier<PIPE_V>();

        SetMaskCount();
    }

    template <typename T1, typename T2>
    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1NoUpdateImplPreCast(LocalTensor<float>& dstLocalOut,
        LocalTensor<float>& expSumTensorOut, LocalTensor<float>& maxTensorOut, LocalTensor<float>& srcLocalOut,
        LocalTensor<float>& workLocalOut, const LocalTensor<T1>& dstLocalIn, const LocalTensor<T2>& expSumTensorIn,
        const LocalTensor<T2>& maxTensorIn, const LocalTensor<T1>& srcLocalIn, const LocalTensor<float>& workLocalIn,
        const ReduceLastND& param)
    {
        uint32_t splitM = DivCeil(param.srcM, FLOAT_NUM_PER_BLK) * FLOAT_NUM_PER_BLK;
        uint32_t splitSize = param.srcM * param.srcK;
        if constexpr (SupportType<T1, float>() && SupportType<T2, float>()) {
            dstLocalOut = dstLocalIn;
            expSumTensorOut = expSumTensorIn;
            maxTensorOut = maxTensorIn;
            srcLocalOut = srcLocalIn;
            workLocalOut = workLocalIn;
        } else if constexpr (SupportType<T1, __cce_half>() && SupportType<T2, __cce_half>()) {
            LocalTensor<float> srcBuffer = workLocalIn;
            LocalTensor<float> sumBuffer = srcBuffer[splitSize];
            LocalTensor<float> maxBuffer = sumBuffer[splitM];
            LocalTensor<float> tmpBuffer = maxBuffer[splitM];
            SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocalIn, splitSize);
            dstLocalOut = srcBuffer;
            expSumTensorOut = sumBuffer;
            maxTensorOut = maxBuffer;
            srcLocalOut = srcBuffer;
            workLocalOut = tmpBuffer;
        } else if constexpr (SupportType<T1, __cce_half>() && SupportType<T2, float>()) {
            LocalTensor<float> srcBuffer = workLocalIn;
            LocalTensor<float> tmpBuffer = srcBuffer[splitM];
            SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocalIn, splitSize);
            dstLocalOut = srcBuffer;
            expSumTensorOut = expSumTensorIn;
            maxTensorOut = maxTensorIn;
            srcLocalOut = srcBuffer;
            workLocalOut = tmpBuffer;
        }
    }

    template <typename T1, typename T2>
    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1NoUpdateImplPostCast(const LocalTensor<T1>& dstLocalOut,
        const LocalTensor<T2>& expSumTensorOut, const LocalTensor<T2>& maxTensorOut,
        const LocalTensor<float>& dstLocalIn, const LocalTensor<float>& expSumTensorIn, const LocalTensor<float>& maxTensorIn,
        const ReduceLastND& param)
    {
        uint32_t splitM = param.srcM;
        uint32_t splitSize = param.srcM * param.srcK;
        if constexpr (SupportType<T1, __cce_half>() && SupportType<T2, __cce_half>()) {
            SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocalOut, dstLocalIn, splitSize);
            SoftMaxFlashV2M1CastIntrinsicsImpl(expSumTensorOut, expSumTensorIn, splitM);
            SoftMaxFlashV2M1CastIntrinsicsImpl(maxTensorOut, maxTensorIn, splitM);
        } else if constexpr (SupportType<T1, __cce_half>() && SupportType<T2, float>()) {
            SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocalOut, dstLocalIn, splitSize);
        }
    }

    template <typename T1, typename T2>
    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1NoUpdateImpl(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& expSumTensor,
        const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcLocal, const LocalTensor<float>& workLocal,
        const ReduceLastND& reduceParam, const SoftMaxTiling& tiling)
    {
        LocalTensor<float> dstLocalOut;
        LocalTensor<float> expSumTensorOut;
        LocalTensor<float> maxTensorOut;
        LocalTensor<float> srcLocalOut;
        LocalTensor<float> tmpBuffer;

        SoftmaxFlashV2M1NoUpdateImplPreCast<T1, T2>(dstLocalOut, expSumTensorOut, maxTensorOut, srcLocalOut, tmpBuffer,
            dstLocal, expSumTensor, maxTensor, srcLocal, workLocal, reduceParam);
        SoftmaxFlashV2M1NoUpdateImplProcess(dstLocalOut, expSumTensorOut, maxTensorOut, srcLocalOut, tmpBuffer, reduceParam);
        SoftmaxFlashV2M1NoUpdateImplPostCast<T1, T2>(dstLocal, expSumTensor, maxTensor, dstLocalOut, expSumTensorOut, maxTensorOut, reduceParam);
    }

    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1UpdateImplProcess(const LocalTensor<float>& dstLocal, const LocalTensor<float>& expSumTensor,
        const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal, const LocalTensor<float>& expMaxTensor,
        const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
        const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling)
    {
        UnaryRepeatParams unaryParams;
        BinaryRepeatParams binaryParams;
        CopyRepeatParams copyParams(1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        const LocalTensor<float>& tmpBuffer0 = workLocal;
        const LocalTensor<float>& reduceBuffer = workLocal[tiling.reduceSize];

        SetMaskNorm();
        ResetMask();

        NewReduceMaxLastNDImpl<false>(tmpBuffer0, srcLocal, reduceBuffer, reduceParam);
        PipeBarrier<PIPE_V>();

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM);

        Max<float, false>(tmpBuffer0, inMaxTensor, tmpBuffer0, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();

        Sub<float, false>(reduceBuffer, inMaxTensor, tmpBuffer0, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();

        Exp<float, false>(expMaxTensor, reduceBuffer, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();

        SoftmaxFlashV2M1BrcbSubImpl(dstLocal, srcLocal, tmpBuffer0, reduceBuffer, reduceParam.srcM, reduceParam.srcK);
        PipeBarrier<PIPE_V>();

        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * reduceParam.srcK);
        Exp<float, false>(dstLocal, dstLocal, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();

        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM);
        Copy<float, false>(maxTensor, tmpBuffer0, MASK_PLACEHOLDER, 1, copyParams);
        PipeBarrier<PIPE_V>();

        SetMaskNorm();
        ResetMask();

        NewReduceSumLastNDImpl<false>(tmpBuffer0, dstLocal, reduceBuffer, reduceParam);

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM);

        Mul<float, false>(expSumTensor, expMaxTensor, inExpSumTensor, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(expSumTensor, expSumTensor, tmpBuffer0, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }

    template <typename T1, typename T2>
    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1UpdateImplPreCast(LocalTensor<float>& dstLocalOut, LocalTensor<float>& expSumTensorOut,
        LocalTensor<float>& maxTensorOut, LocalTensor<float>& srcLocalOut, LocalTensor<float>& expMaxTensorOut, LocalTensor<float>& inExpSumTensorOut,
        LocalTensor<float>& inMaxTensorOut, LocalTensor<float>& workLocalOut, const LocalTensor<T1>& dstLocalIn, const LocalTensor<T2>& expSumTensorIn,
        const LocalTensor<T2>& maxTensorIn, const LocalTensor<T1>& srcLocalIn, const LocalTensor<T1>& expMaxTensorIn,
        const LocalTensor<T2>& inExpSumTensorIn, const LocalTensor<T2>& inMaxTensorIn, const LocalTensor<float>& workLocalIn, const ReduceLastND& param)
    {
        uint32_t splitM = DivCeil(param.srcM, FLOAT_NUM_PER_BLK) * FLOAT_NUM_PER_BLK;
        uint32_t splitSize = param.srcM * param.srcK;
        if constexpr (SupportType<T1, float>() && SupportType<T2, float>()) {
            dstLocalOut = dstLocalIn;
            expSumTensorOut = expSumTensorIn;
            maxTensorOut = maxTensorIn;
            srcLocalOut = srcLocalIn;
            expMaxTensorOut = expMaxTensorIn;
            inExpSumTensorOut = inExpSumTensorIn;
            inMaxTensorOut = inMaxTensorIn;
            workLocalOut = workLocalIn;
        } else if constexpr (SupportType<T1, __cce_half>() && SupportType<T2, __cce_half>()) {
            const LocalTensor<float>& srcBuffer = workLocalIn;
            const LocalTensor<float>& sumBuffer = srcBuffer[splitSize];
            const LocalTensor<float>& maxBuffer = sumBuffer[splitM];
            const LocalTensor<float>& expMaxBuffer = maxBuffer[splitM];
            const LocalTensor<float>& tmpBuffer = expMaxBuffer[splitM];
            SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocalIn, splitSize);
            SoftMaxFlashV2M1CastIntrinsicsImpl(sumBuffer, inExpSumTensorIn, param.srcM);
            SoftMaxFlashV2M1CastIntrinsicsImpl(maxBuffer, inMaxTensorIn, param.srcM);
            dstLocalOut = srcBuffer;
            expSumTensorOut = sumBuffer;
            maxTensorOut = maxBuffer;
            srcLocalOut = srcBuffer;
            expMaxTensorOut = expMaxBuffer;
            inExpSumTensorOut = sumBuffer;
            inMaxTensorOut = maxBuffer;
            workLocalOut = tmpBuffer;
        } else if constexpr (SupportType<T1, __cce_half>() && SupportType<T2, float>()) {
            const LocalTensor<float>& srcBuffer = workLocalIn;
            const LocalTensor<float>& expMaxBuffer = srcBuffer[splitSize];
            const LocalTensor<float>& tmpBuffer = expMaxBuffer[splitM];
            SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocalIn, splitSize);
            dstLocalOut = srcBuffer;
            expSumTensorOut = expSumTensorIn;
            maxTensorOut = maxTensorIn;
            srcLocalOut = srcBuffer;
            expMaxTensorOut = expMaxBuffer;
            inExpSumTensorOut = inExpSumTensorIn;
            inMaxTensorOut = inMaxTensorIn;
            workLocalOut = tmpBuffer;
        }
    }

    template <typename T1, typename T2>
    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1UpdateImplPostCast(const LocalTensor<T1>& dstLocalOut, const LocalTensor<T2>& expSumTensorOut,
        const LocalTensor<T2>& maxTensorOut, const LocalTensor<T1>& expMaxTensorOut,
        const LocalTensor<float>& dstLocalIn, const LocalTensor<float>& expSumTensorIn, const LocalTensor<float>& maxTensorIn,
        const LocalTensor<float>& expMaxTensorIn, const ReduceLastND& param)
    {
        uint32_t splitM = param.srcM;
        uint32_t splitSize = param.srcM * param.srcK;
        if constexpr (SupportType<T1, __cce_half>() && SupportType<T2, __cce_half>()) {
            SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocalOut, dstLocalIn, splitSize);
            SoftMaxFlashV2M1CastIntrinsicsImpl(expSumTensorOut, expSumTensorIn, splitM);
            SoftMaxFlashV2M1CastIntrinsicsImpl(maxTensorOut, maxTensorIn, splitM);
            SoftMaxFlashV2M1CastIntrinsicsImpl(expMaxTensorOut, expMaxTensorIn, splitM);
        } else if constexpr (SupportType<T1, __cce_half>() && SupportType<T2, float>()) {
            SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocalOut, dstLocalIn, splitSize);
            SoftMaxFlashV2M1CastIntrinsicsImpl(expMaxTensorOut, expMaxTensorIn, splitM);
        }
    }

    template <typename T1, typename T2>
    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1UpdateImpl(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& expSumTensor,
        const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcLocal, const LocalTensor<T1>& expMaxTensor,
        const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor,
        const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling)
    {
        LocalTensor<float> dstLocalOut;
        LocalTensor<float> expSumTensorOut;
        LocalTensor<float> maxTensorOut;
        LocalTensor<float> srcLocalOut;
        LocalTensor<float> expMaxTensorOut;
        LocalTensor<float> inExpSumTensorOut;
        LocalTensor<float> inMaxTensorOut;
        LocalTensor<float> tmpBuffer;

        SoftmaxFlashV2M1UpdateImplPreCast<T1, T2>(dstLocalOut, expSumTensorOut, maxTensorOut, srcLocalOut,
            expMaxTensorOut, inExpSumTensorOut, inMaxTensorOut, tmpBuffer,
            dstLocal, expSumTensor, maxTensor, srcLocal, expMaxTensor, inExpSumTensor, inMaxTensor, workLocal, reduceParam);

        SoftmaxFlashV2M1UpdateImplProcess(dstLocalOut, expSumTensorOut, maxTensorOut, srcLocalOut,
            expMaxTensorOut, inExpSumTensorOut, inMaxTensorOut, tmpBuffer, reduceParam, tiling);

        SoftmaxFlashV2M1UpdateImplPostCast<T1, T2>(dstLocal, expSumTensor, maxTensor, expMaxTensor,
            dstLocalOut, expSumTensorOut, maxTensorOut, expMaxTensorOut, reduceParam);
    }

    template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false>
    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1ImplProcess(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& expSumTensor,
        const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
        const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
        const ReduceLastND& reduceParam, const SoftMaxTiling& tiling)
    {
        if constexpr (isBasicBlock && !isUpdate) {
            SoftmaxFlashV2M1NoUpdateBasicBlock(dstTensor, expSumTensor, maxTensor, srcTensor, workLocal, tiling);
        } else if constexpr (isBasicBlock && isUpdate) {
            SoftmaxFlashV2M1BasicBlockImpl(dstTensor, expSumTensor, maxTensor, srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, workLocal, tiling);
        } else if constexpr (!isBasicBlock && !isUpdate) {
            SoftmaxFlashV2M1NoUpdateImpl<T1, T2>(dstTensor, expSumTensor, maxTensor, srcTensor, workLocal, reduceParam, tiling);
        } else if constexpr (!isBasicBlock && isUpdate) {
            SoftmaxFlashV2M1UpdateImpl<T1, T2>(dstTensor, expSumTensor, maxTensor, srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, reduceParam, tiling);
        }
    }

    template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false>
    [aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2M1PostProcess(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& expSumTensor,
        const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
        const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
        const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
    {
        ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK};
        ReduceLastND tailParam = { tiling.tailM, originalSrcShape.k, tiling.tailM, tiling.splitK, tiling.tailM, tiling.reduceK};

        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        PipeBarrier<PIPE_V>();
        SetMaskCount();
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            SoftmaxFlashV2M1ImplProcess<T1, T2, isUpdate, isBasicBlock>(dstTensor[offset1], expSumTensor[offset2], maxTensor[offset2], srcTensor[offset1],
                expMaxTensor[offset2], inExpSumTensor[offset2], inMaxTensor[offset2], workLocal, reduceParam, tiling);
            offset1 += tiling.splitSize;
            offset2 += tiling.reduceM;
        }

        if constexpr (!isBasicBlock) {
            if (tiling.tailM != 0) {
                offset1 = tiling.rangeM * tiling.splitSize;
                offset2 = tiling.rangeM * tiling.reduceM;
                SoftmaxFlashV2M1ImplProcess<T1, T2, isUpdate, isBasicBlock>(dstTensor[offset1], expSumTensor[offset2], maxTensor[offset2], srcTensor[offset1],
                    expMaxTensor[offset2], inExpSumTensor[offset2], inMaxTensor[offset2], workLocal, tailParam, tiling);
            }
        }
        SetMaskNorm();
        ResetMask();
    }
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flashv2_base_impl.h" 2




namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) constexpr SoftMaxTiling SoftMaxFlashV2TilingFuncImpl(const uint32_t srcM, const uint32_t srcK,
    const uint32_t dataTypeSize1, const uint32_t dataTypeSize2, const uint32_t localWorkSpaceSize,
    const bool isUpdate = false, const bool isBasicBlock = false, const bool isDataFormatNZ = false,
    const bool isFlashOutputBrc = false)
{
    SoftMaxTiling softmaxTiling;
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / dataTypeSize2;
    softmaxTiling.srcM = srcM;
    softmaxTiling.srcK = srcK;
    softmaxTiling.srcSize = srcM * srcK;

    softmaxTiling.outMaxM = srcM;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = srcM * elementNumPerBlk;

    if (isDataFormatNZ) {
        softmaxTiling.reduceM = localWorkSpaceSize / (SOFTMAX_SHAPE_NZ_BASIC_COUNT * SOFTMAX_NZ_TILING_NEEDBLOCK + srcK);
    } else {
        if (isBasicBlock && srcK % FLOAT_REPEAT_SIZE == 0 && srcM % SOFTMAX_BASIC_TILE_NUM == 0) {
            softmaxTiling.reduceM =
                CalculateNDSplitM(localWorkSpaceSize, dataTypeSize1, elementNumPerBlk, { srcM, srcK }, isBasicBlock);
        } else {
            softmaxTiling.reduceM = (dataTypeSize1 == B16_BYTE_SIZE) ?
                localWorkSpaceSize / (SOFTMAX_COMPUTE_DIM * elementNumPerBlk + srcK + FLOAT_REPEAT_SIZE) :
                localWorkSpaceSize / (elementNumPerBlk + FLOAT_REPEAT_SIZE);
        }
    }

    uint32_t softmaxBasicTileNum = SOFTMAX_BASIC_TILE_NUM;
    if (isFlashOutputBrc && dataTypeSize1 == B16_BYTE_SIZE) {
        softmaxBasicTileNum = HALF_NUM_PER_BLK;
    }

    if (softmaxTiling.reduceM < srcM && softmaxTiling.reduceM > softmaxBasicTileNum) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / softmaxBasicTileNum * softmaxBasicTileNum;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < srcM ? softmaxTiling.reduceM : srcM;

    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = srcK;
    softmaxTiling.splitSize = softmaxTiling.reduceM * srcK;

    softmaxTiling.rangeM = srcM / softmaxTiling.reduceM;
    softmaxTiling.tailM = srcM % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * srcK;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;

    if (isFlashOutputBrc && (softmaxTiling.rangeM > MIN_BLOCK_LEN || softmaxTiling.tailM != 0)) {


                                                                                                           ;
    }
    return softmaxTiling;
}

template <typename T1, typename T2, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] __inline__ __attribute__((always_inline)) SoftMaxTiling SoftmaxFlashV2UpdateTilingImpl(const LocalTensor<T1>& srcTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    if constexpr (isDataFormatNZ) {
        LastAxisShapeND srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
            ShapeInfo srcShape = srcTensor.GetShapeInfo();
            srcNDinfo = GetLastAxisShapeND(srcShape);
        }
        uint32_t workLocalSize = workLocal.GetSize();
        return SoftMaxFlashV2TilingFuncImpl(srcNDinfo.m, srcNDinfo.k, sizeof(T1), sizeof(T2), workLocalSize, isUpdate, false, true);
    } else {
        if constexpr (!config.isCheckTiling) {
            return tiling;
        }

        LastAxisShapeND srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
            ShapeInfo srcShape = srcTensor.GetShapeInfo();
            srcNDinfo = GetLastAxisShapeND(srcShape);
        }

        if (srcNDinfo.m == tiling.srcM && srcNDinfo.k == tiling.srcK) {
            return tiling;
        }

        SoftMaxTiling softmaxTiling;
        uint32_t workLocalSize = workLocal.GetSize();

        if constexpr (config.mode == SoftmaxMode::SOFTMAX_OUTPUT_WITHOUT_BRC) {
            softmaxTiling = SoftMaxFlashV2TilingFuncImpl(srcNDinfo.m, srcNDinfo.k, sizeof(T1), sizeof(T2), workLocalSize, isUpdate, isBasicBlock, false, true);
        } else {
            softmaxTiling = SoftMaxFlashV2TilingFuncImpl(srcNDinfo.m, srcNDinfo.k, sizeof(T1), sizeof(T2), workLocalSize, isUpdate, isBasicBlock);
        }
        return softmaxTiling;
    }
}

template <typename T1, typename T2, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2Impl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();

    SoftMaxTiling newTiling = SoftmaxFlashV2UpdateTilingImpl<T1, T2, isUpdate, isBasicBlock, isDataFormatNZ, config>(
        srcTensor, workLocal, tiling, softmaxShapeInfo);

    LastAxisShapeND originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    }

    if constexpr (isDataFormatNZ) {
        SoftMaxFlashV2NZImpl<T1, T2, isUpdate, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
            expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, newTiling);
    } else if constexpr (config.mode == SoftmaxMode::SOFTMAX_OUTPUT_WITHOUT_BRC) {
        SoftmaxFlashV2M1PostProcess<T1, T2, isUpdate, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
            expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, newTiling);
    } else {
        SoftmaxFlashV2PostProcess<T1, T2, isUpdate, isBasicBlock, config>(dstTensor, sumTensor, maxTensor, srcTensor,
            expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, newTiling);
    }
}

template <typename T1, typename T2, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2Impl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftmaxFlashV2Impl<T1, T2, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor,
        srcTensor, expMaxTensor, inSumTensor, inMaxTensor, workLocal, tiling, softmaxShapeInfo);
}

template <typename T1, typename T2, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2Impl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    if constexpr (isDataFormatNZ) {
        LastAxisShapeND srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
            ShapeInfo srcShape = srcTensor.GetShapeInfo();
            srcNDinfo = GetLastAxisShapeND(srcShape);
        }
        uint32_t workLocalSize = workLocal.GetSize();
        if (workLocalSize < SOFTMAX_SHAPE_NZ_BASIC_COUNT * SOFTMAX_NZ_TILING_NEEDBLOCK + srcNDinfo.k) {
            PopStackBuffer<float, TPosition::LCM>(workLocal);
        }
    }

    SoftmaxFlashV2Impl<T1, T2, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor,
        srcTensor, expMaxTensor, inSumTensor, inMaxTensor, workLocal, tiling, softmaxShapeInfo);
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
[aicore] __inline__ __attribute__((always_inline)) constexpr SoftMaxTiling SoftMaxFlashV2TilingFunc(const SoftMaxShapeInfo& shapeInfo,
    const uint32_t dataTypeSize1, const uint32_t dataTypeSize2, const uint32_t localWorkSpaceSize,
    const bool isUpdate = false, const bool isBasicBlock = false, const bool isDataFormatNZ = false, const bool isFlashOutputBrc = false)
{
    return SoftMaxFlashV2TilingFuncImpl(shapeInfo.srcM, shapeInfo.srcK, dataTypeSize1, dataTypeSize2,
        localWorkSpaceSize / B32_BYTE_SIZE, isUpdate, isBasicBlock, isDataFormatNZ, isFlashOutputBrc);
}
# 73 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& expSumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<T>& expMaxTensor,
    const LocalTensor<T>& inExpSumTensor, const LocalTensor<T>& inMaxTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<T, T, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, tiling, softmaxShapeInfo);
                                       ;
}
# 113 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcTensor, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<__cce_half, float, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, tiling, softmaxShapeInfo);
                                       ;
}
# 156 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& expSumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<T>& expMaxTensor,
    const LocalTensor<T>& inExpSumTensor, const LocalTensor<T>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<T, T, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, sharedTmpBuffer, tiling, softmaxShapeInfo);
                                       ;
}
# 199 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcTensor, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<__cce_half, float, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, sharedTmpBuffer, tiling, softmaxShapeInfo);
                                       ;
}
}
#pragma end_pipe
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv3.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv3.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flashv3_base_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flashv3_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_flashv3_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_flashv3_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3ReduceSumImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& rowMeanLocal, const LocalTensor<float>& rowMeanGlobalTmp,
    const LocalTensor<float>& meanTmp, const struct ReduceLastND& reduceParam, const uint32_t& baseK,
    const uint32_t& reduceSize)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    const uint16_t srcRepeatStride = reduceParam.srcK / FLOAT_NUM_PER_BLK;

    for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
        BlockReduceSum<float, false>(dst[FLOAT_REPEAT_SIZE * i], src[i * reduceParam.srcK], FLOAT_NUM_PER_BLK,
            MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
    uint8_t remainRepeat = splitCount - FLOAT_NUM_PER_BLK;
    if (remainRepeat != 0) {
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < reduceParam.originalSrcM; j++) {
        Add<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN + j * reduceParam.srcK],
            dst[j * FLOAT_REPEAT_SIZE], 1, remainRepeat, { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
    if (tailSrcK != 0) {
      PipeBarrier<PIPE_V>();
      TailAddImpl(dst, src, reduceParam, tailSrcK, srcRepeatStride, splitCount);
      ResetMask();
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.originalSrcM * FLOAT_REPEAT_SIZE);
    BlockReduceSum<float, false>(rowMeanLocal, dst, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    UnaryRepeatParams unaryParams;
    if (baseK != 0) {
        Muls<float, false>(rowMeanLocal, rowMeanLocal, static_cast<float>(1.0f / baseK), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
    BlockReduceSum<float, false>(rowMeanGlobalTmp, rowMeanLocal, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    const uint32_t repeat = (reduceParam.originalSrcM + BRCB_BROADCAST_NUMBER - 1) / BRCB_BROADCAST_NUMBER;
    Brcb(dst, rowMeanGlobalTmp, (uint8_t)repeat, { 1, BRCB_BROADCAST_NUMBER });
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.originalSrcM * reduceParam.dstK);
    Copy<float, false>(rowMeanGlobalTmp, dst, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);
    Muls<float, false>(meanTmp, rowMeanGlobalTmp, static_cast<float>(1.0f / FLOAT_NUM_PER_BLK), MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void ModifyInputImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& meanTmp, const LocalTensor<float>& workLocal, const LocalTensor<float>& tmpBuffer2,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const SoftMaxParams& params, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& rowMeanLocal = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize];
    const LocalTensor<float>& rowMeanGlobalTmp = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 2];
    const uint32_t baseK = reduceParam.originalSrcK / params.splitMeanCnt;

    SoftmaxFlashV3ReduceSumImpl(tmpBuffer1, src, rowMeanLocal, rowMeanGlobalTmp, meanTmp, reduceParam, baseK, reduceSize);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);
    Sub<float, false>(rowMeanGlobalTmp, meanTmp, rowMeanLocal, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    UnaryRepeatParams unaryParams;
    Muls<float, false>(rowMeanGlobalTmp, rowMeanGlobalTmp, static_cast<float>(params.alpha / (1.0f - params.alpha)),
        MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
        Brcb(tmpBuffer1, rowMeanGlobalTmp[i * params.splitMeanCnt], 1, {1, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, baseK);
        const CopyRepeatParams copyRepeatParams = {1, 0, (uint16_t)(baseK / FLOAT_NUM_PER_BLK), 1};
        Copy<float, false>(tmpBuffer2, tmpBuffer1, MASK_PLACEHOLDER, params.splitMeanCnt, copyRepeatParams);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcK);
        Sub<float, false>(src[i * reduceParam.srcK], src[i * reduceParam.srcK], tmpBuffer2, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
}
[aicore] __inline__ __attribute__((always_inline)) void ModifyMaxImpl(const LocalTensor<float>& maxTensor, const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& src, const LocalTensor<float>& meanTmp, const LocalTensor<float>& maxTmp,
    const LocalTensor<float>& shiftVal, const SoftMaxParams& params, const uint32_t& reduceSize)
{
    float scalar = params.alpha / (1 - params.alpha);

    Sub(shiftVal, meanTmp, meanTensor, reduceSize);
    PipeBarrier<PIPE_V>();
    Muls(shiftVal, shiftVal, scalar, reduceSize);
    PipeBarrier<PIPE_V>();
    Add(maxTensor, maxTmp, shiftVal, reduceSize);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3NoUpdateImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const SoftMaxParams& params,
    const ReduceLastND& reduceParam, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& meanTmp = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& maxTmp = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize];
    const LocalTensor<float>& shiftCurr = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 2];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 3];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    ModifyInputImpl(tmpBuffer0, tmpBuffer0, meanTmp, workLocal, tmpBuffer2, reduceParam, tiling, params, reduceSize);
    PipeBarrier<PIPE_V>();
    Copy(meanTensor[offset2], meanTmp, reduceSize, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    ResetMask();

    NewReduceMaxLastNDImpl(maxTmp, tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    ModifyMaxImpl(maxTensor[offset2], meanTensor[offset2], tmpBuffer0, meanTmp, maxTmp, shiftCurr, params, reduceSize);
    PipeBarrier<PIPE_V>();

    Sub(shiftCurr, maxTensor[offset2], shiftCurr, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, shiftCurr, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(expSumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3TailImpl(const SoftMaxTiling& tiling, ReduceLastND& reduceParam,
    uint32_t& offset1, uint32_t& offset2, uint32_t& splitSize, uint32_t& reduceSize)
{
    offset2 = tiling.rangeM * tiling.reduceSize;
    offset1 = tiling.rangeM * tiling.splitSize;
    splitSize = tiling.tailSplitSize;
    reduceSize = tiling.tailReduceSize;
    reduceParam.originalSrcM = tiling.tailM;
    reduceParam.srcM = tiling.tailM;
    reduceParam.dstM = tiling.tailM;
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3NoUpdateExtImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxParams& params, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;

    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        SoftmaxFlashV3NoUpdateImpl(dst, meanTensor, expSumTensor, maxTensor, src, workLocal, tiling, params, reduceParam,
            offset1, offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        PipeBarrier<PIPE_V>();
    }

    if (tiling.tailM != 0) {
        SoftmaxFlashV3TailImpl(tiling, reduceParam, offset1, offset2, splitSize, reduceSize);
        SoftmaxFlashV3NoUpdateImpl(dst, meanTensor, expSumTensor, maxTensor, src, workLocal, tiling, params, reduceParam,
            offset1, offset2, splitSize, reduceSize);
        PipeBarrier<PIPE_V>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3UpdateMeanImpl(const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& inMeanTensor, const LocalTensor<float>& meanTmp,
    const SoftMaxParams& params, const uint32_t& reduceSize)
{

    Muls(meanTensor, inMeanTensor, params.loopCnt - 1.0f, reduceSize);
    PipeBarrier<PIPE_V>();
    Add(meanTensor, meanTensor, meanTmp, reduceSize);
    PipeBarrier<PIPE_V>();
    Muls(meanTensor, meanTensor, static_cast<float>(1.0f / params.loopCnt), reduceSize);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3UpdateImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src,
    const LocalTensor<__cce_half>& expMaxTensor, const LocalTensor<float>& inMeanTensor, const LocalTensor<float>& inExpSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxParams& params, const ReduceLastND& reduceParam, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& meanTmp = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& maxTmp = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize];
    const LocalTensor<float>& shiftCurr = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 2];
    const LocalTensor<float>& shiftPrev = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 3];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 4];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    ModifyInputImpl(tmpBuffer0, tmpBuffer0, meanTmp, workLocal, tmpBuffer2, reduceParam, tiling, params, reduceSize);
    PipeBarrier<PIPE_V>();
    SoftmaxFlashV3UpdateMeanImpl(meanTensor[offset2], inMeanTensor[offset2], meanTmp, params, reduceSize);
    PipeBarrier<PIPE_V>();

    NewReduceMaxLastNDImpl(maxTmp, tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    ModifyMaxImpl(maxTensor[offset2], meanTensor[offset2], tmpBuffer0, meanTmp, maxTmp, shiftCurr, params, reduceSize);
    PipeBarrier<PIPE_V>();

    ModifyMaxImpl(maxTmp, meanTensor[offset2], tmpBuffer0, inMeanTensor[offset2], inMaxTensor[offset2], shiftPrev,
        params, reduceSize);
    PipeBarrier<PIPE_V>();

    Max(maxTensor[offset2], maxTensor[offset2], maxTmp, reduceSize);
    PipeBarrier<PIPE_V>();

    Sub(maxTmp, inMaxTensor[offset2], maxTensor[offset2], reduceSize);
    PipeBarrier<PIPE_V>();

    Add(maxTmp, shiftPrev, maxTmp, reduceSize);
    PipeBarrier<PIPE_V>();
    Exp(maxTmp, maxTmp, reduceSize);
    PipeBarrier<PIPE_V>();
    Mul(expSumTensor[offset2], maxTmp, inExpSumTensor[offset2], reduceSize);
    PipeBarrier<PIPE_V>();

    Sub(shiftCurr, maxTensor[offset2], shiftCurr, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, shiftCurr, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(meanTmp, tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();
    Add(expSumTensor[offset2], expSumTensor[offset2], meanTmp, reduceSize);
    PipeBarrier<PIPE_V>();
    BroadCastLastImpl(tmpBuffer0, maxTmp,
        { tiling.reduceM, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE, tiling.reduceM, tiling.reduceK });
    PipeBarrier<PIPE_V>();
    Cast(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer0, FLOAT2HALF_ROUND_MODE, reduceSize * B16_BYTE_SIZE);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3NDExtImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src,
    const LocalTensor<__cce_half>& expMaxTensor, const LocalTensor<float>& inMeanTensor, const LocalTensor<float>& inExpSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxParams& params, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;

    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        SoftmaxFlashV3UpdateImpl(dst, meanTensor, expSumTensor, maxTensor, src, expMaxTensor, inMeanTensor, inExpSumTensor,
            inMaxTensor, workLocal, tiling, params, reduceParam, offset1, offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        PipeBarrier<PIPE_V>();
    }

    if (tiling.tailM != 0) {
        SoftmaxFlashV3TailImpl(tiling, reduceParam, offset1, offset2, splitSize, reduceSize);
        SoftmaxFlashV3UpdateImpl(dst, meanTensor, expSumTensor, maxTensor, src, expMaxTensor, inMeanTensor, inExpSumTensor,
            inMaxTensor, workLocal, tiling, params, reduceParam, offset1, offset2, splitSize, reduceSize);
        PipeBarrier<PIPE_V>();
    }
}
template <typename T, typename U, bool isUpdate = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3Process(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inExpSumTensor,
    const LocalTensor<U>& inMaxTensor, const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape,
    const SoftMaxTiling& tiling, const SoftMaxParams& params)
{

                                                                                                               ;
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM, tiling.splitK, tiling.reduceM,
        tiling.reduceK };
    if constexpr (!isUpdate) {
        SoftmaxFlashV3NoUpdateExtImpl(dstTensor, meanTensor, expSumTensor, maxTensor, srcTensor, workLocal,
            tiling, params, reduceParam);
    } else {
        SoftmaxFlashV3NDExtImpl(dstTensor, meanTensor, expSumTensor, maxTensor, srcTensor, expMaxTensor, inMeanTensor,
            inExpSumTensor, inMaxTensor, workLocal, tiling, params, reduceParam);
    }
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flashv3_base_impl.h" 2




namespace AscendC {
template <typename T, typename U, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3Impl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inexpSumTensor,
    const LocalTensor<U>& inMaxTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxParams& params)
{
    static_assert((SupportType<Tuple<T, U>, Tuple<__cce_half, float>>()), "Failed to check dtype in SoftmaxFlashV3, "
        "Current api support dtype combination is T : half, U : float");
    SetMaskNorm();
    ResetMask();
    LastAxisShapeND originalSrcShape = { params.oriSrcM, params.oriSrcK };
    if (params.srcM == 0 || params.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    }
    SoftmaxFlashV3Process<T, U, isUpdate, isBasicBlock, config>(dstTensor, meanTensor, expSumTensor, maxTensor, srcTensor,
        expMaxTensor, inMeanTensor, inexpSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling, params);
}

template <typename T, typename U, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3Impl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inexpSumTensor,
    const LocalTensor<U>& inMaxTensor, const SoftMaxTiling& tiling, const SoftMaxParams& params)
{
    LocalTensor<float> workLocal;
    bool ans = PopStackBuffer<float, TPosition::LCM>(workLocal);
                                                                                                ;
    SoftmaxFlashV3Impl<T, U, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, meanTensor, expSumTensor, maxTensor,
        srcTensor, expMaxTensor, inMeanTensor, inexpSumTensor, inMaxTensor, workLocal, tiling, params);
}

template <typename T, typename U, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3Impl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inexpSumTensor,
    const LocalTensor<U>& inMaxTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxParams& params)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftmaxFlashV3Impl<T, U, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, meanTensor, expSumTensor, maxTensor,
        srcTensor, expMaxTensor, inMeanTensor, inexpSumTensor, inMaxTensor, workLocal, tiling, params);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv3.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv3.h"
template <typename T, typename U, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inExpSumTensor,
    const LocalTensor<U>& inMaxTensor, const SoftMaxTiling& tiling, const SoftMaxParams& params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV3Impl<T, U, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, meanTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inMeanTensor, inExpSumTensor, inMaxTensor, tiling, params);
                                       ;
}
# 106 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv3.h"
template <typename T, typename U, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV3(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inExpSumTensor,
    const LocalTensor<U>& inMaxTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxParams& params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV3Impl<T, U, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, meanTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inMeanTensor, inExpSumTensor, inMaxTensor, sharedTmpBuffer, tiling, params);
                                       ;
}
}
#pragma end_pipe
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_grad_base_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_grad_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_grad_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_grad_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_grad_nd_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_grad_nd_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradNDGenericImpl(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& gradTensor,
    const LocalTensor<__cce_half>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const ReduceLastND& reduceSumParam, const BroadCastLastND& brcParam, const bool isFront, const uint32_t offset1,
    const uint32_t offset2, const uint32_t splitSize, const uint32_t reduceSize)
{
    LocalTensor<float> srcBuffer = workLocal;
    LocalTensor<float> gradBuffer = workLocal[tiling.splitSize];
    LocalTensor<float> dstBuffer = workLocal[tiling.splitSize + tiling.splitSize];

    LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
    LocalTensor<float> addBuffer =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize + tiling.reduceSize];

    Cast(srcBuffer, srcTensor[offset1], RoundMode::CAST_NONE, splitSize);
    Cast(gradBuffer, gradTensor[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    Mul(dstBuffer, srcBuffer, gradBuffer, splitSize);
    PipeBarrier<PIPE_V>();
    ReduceSumLastNDImpl(addBuffer, dstBuffer, reduceBuffer, reduceSumParam);
    PipeBarrier<PIPE_V>();
    if (isFront) {
        Cast(dstTensor[offset2], addBuffer, FLOAT2HALF_ROUND_MODE, reduceSize);
    } else {
        BroadCastLastND brcParam = { tiling.reduceM, tiling.srcK, tiling.reduceM, tiling.reduceK };
        BroadCastLastImpl(dstBuffer, addBuffer, brcParam);
        PipeBarrier<PIPE_V>();
        Sub(dstBuffer, gradBuffer, dstBuffer, splitSize);
        PipeBarrier<PIPE_V>();
        Mul(dstBuffer, dstBuffer, srcBuffer, splitSize);
        PipeBarrier<PIPE_V>();
        Cast(dstTensor[offset1], dstBuffer, FLOAT2HALF_ROUND_MODE, splitSize);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradNDImpl(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& gradTensor,
    const LocalTensor<__cce_half>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const LastAxisShapeND& originalSrcShape, bool isFront = false)
{
    const ReduceLastND reduceSumParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                                          tiling.splitK, tiling.reduceM, tiling.reduceK };
    const BroadCastLastND brcParam = { tiling.reduceM, tiling.srcK, tiling.reduceM, tiling.reduceK };
    LocalTensor<float> srcBuffer = workLocal;
    LocalTensor<float> gradBuffer = workLocal[tiling.splitSize];
    LocalTensor<float> dstBuffer = workLocal[tiling.splitSize + tiling.splitSize];

    LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
    LocalTensor<float> addBuffer =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        SoftmaxGradNDGenericImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, reduceSumParam, brcParam, isFront,
            offset1, offset2, tiling.splitSize, tiling.reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
    }
    if (tiling.tailM != 0) {
        const ReduceLastND tailReduceSumParam = { tiling.tailM, originalSrcShape.k, tiling.tailM,
                                                  tiling.splitK, tiling.tailM, tiling.reduceK };
        const BroadCastLastND tailBrcParam = { tiling.tailM, tiling.srcK, tiling.tailM, tiling.reduceK };
        SoftmaxGradNDGenericImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, tailReduceSumParam, tailBrcParam,
            isFront, offset1, offset2, tiling.tailSplitSize, tiling.tailReduceSize);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFrontNDImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const LastAxisShapeND& originalSrcShape)
{
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    ReduceLastND reduceSumParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };

    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        LocalTensor<float> srcBuffer = workLocal;
        LocalTensor<float> gradBuffer = workLocal[tiling.splitSize];
        LocalTensor<float> dstBuffer = workLocal[tiling.splitSize + tiling.splitSize];

        LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
        LocalTensor<float> addBuffer =
            workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize + tiling.reduceSize];
        const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
        const uint32_t elementNumPerBlk = DEFAULT_C0_SIZE / B32_BYTE_SIZE;
        uint8_t offset = (uint8_t)(splitBlock * elementNumPerBlk);
        const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
        const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
        const uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
        SetMaskNorm();
        ResetMask();
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            if constexpr (isBasicBlock) {
                Cast<float, __cce_half, false>(srcBuffer, srcTensor[i * tiling.splitSize], RoundMode::CAST_NONE,
                    MASK_PLACEHOLDER, repeatTimes, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
                Cast<float, __cce_half, false>(gradBuffer, gradTensor[i * tiling.splitSize], RoundMode::CAST_NONE,
                    MASK_PLACEHOLDER, repeatTimes, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
                PipeBarrier<PIPE_V>();
                Mul<float, false>(dstBuffer, srcBuffer, gradBuffer, MASK_PLACEHOLDER, repeatTimes,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
                for (uint32_t j = 1; j < splitBlock; ++j) {
                    PipeBarrier<PIPE_V>();
                    Add<float, false>(dstBuffer, dstBuffer, dstBuffer[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                        (uint8_t)(tiling.splitM), { 1, 1, 1, offset, offset, offset });
                }
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(dstBuffer, dstBuffer, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                    offset);
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(reduceBuffer, dstBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
# 141 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_grad_nd_impl.h"
                Brcb(dstBuffer, reduceBuffer, splitCeilM, { B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });
                Brcb(dstBuffer[DEFAULT_BLK_NUM], reduceBuffer, splitCeilM,
                    { B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });

                PipeBarrier<PIPE_V>();
                Cast<__cce_half, float, false>(dstTensor[i * tiling.reduceSize], dstBuffer, FLOAT2HALF_ROUND_MODE,
                    MASK_PLACEHOLDER, reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            } else {
                Cast(srcBuffer, srcTensor[i * tiling.splitSize], RoundMode::CAST_NONE, tiling.splitSize);
                Cast(gradBuffer, gradTensor[i * tiling.splitSize], RoundMode::CAST_NONE, tiling.splitSize);
                PipeBarrier<PIPE_V>();
                Mul(dstBuffer, srcBuffer, gradBuffer, tiling.splitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(addBuffer, dstBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
                Cast(dstTensor[i * tiling.reduceSize], addBuffer, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);
            }
        }
        if (tiling.tailM != 0) {
            Cast(srcBuffer, srcTensor[tiling.rangeM * tiling.splitSize], RoundMode::CAST_NONE, tiling.tailSplitSize);
            Cast(gradBuffer, gradTensor[tiling.rangeM * tiling.splitSize], RoundMode::CAST_NONE, tiling.tailSplitSize);
            PipeBarrier<PIPE_V>();
            Mul(dstBuffer, srcBuffer, gradBuffer, tiling.tailSplitSize);
            reduceSumParam.srcM = tiling.tailM;
            reduceSumParam.dstM = tiling.tailM;
            reduceSumParam.originalSrcM = tiling.tailM;
            PipeBarrier<PIPE_V>();
            ReduceSumLastNDImpl(addBuffer, dstBuffer, reduceBuffer, reduceSumParam);
            PipeBarrier<PIPE_V>();
            Cast(dstTensor[tiling.rangeM * tiling.reduceSize], addBuffer, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);
        }
    } else {
        LocalTensor<float> srcBuffer = workLocal;
        LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize];
        uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
        const uint32_t elementNumPerBlk = DEFAULT_C0_SIZE / B32_BYTE_SIZE;
        uint8_t offset = (uint8_t)(splitBlock * elementNumPerBlk);
        const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, elementNumPerBlk));
        SetMaskNorm();
        ResetMask();
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            if constexpr (isBasicBlock) {
                offset2 = i * tiling.reduceSize;
                offset1 = i * tiling.splitSize;
                PipeBarrier<PIPE_V>();
                Mul<float, false>(srcBuffer, srcTensor[offset1], gradTensor[offset1], MASK_PLACEHOLDER, repeatTimes,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

                for (uint32_t j = 1; j < splitBlock; ++j) {
                    PipeBarrier<PIPE_V>();
                    Add<float, false>(srcBuffer, srcBuffer, srcBuffer[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                        (uint8_t)(tiling.splitM), { 1, 1, 1, offset, offset, offset });
                }
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(srcBuffer, srcBuffer, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                    splitBlock * DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(reduceBuffer, srcBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
# 214 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_grad_nd_impl.h"
                Brcb(dstTensor[offset2], reduceBuffer, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

            } else {
                Mul(srcBuffer, srcTensor[i * tiling.splitSize], gradTensor[i * tiling.splitSize], tiling.splitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(dstTensor[i * tiling.reduceSize], srcBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
            }
        }

        if (tiling.tailM != 0) {
            Mul(srcBuffer, srcTensor[tiling.rangeM * tiling.splitSize], gradTensor[tiling.rangeM * tiling.splitSize],
                tiling.tailSplitSize);
            PipeBarrier<PIPE_V>();

            reduceSumParam.srcM = tiling.tailM;
            reduceSumParam.dstM = tiling.tailM;
            reduceSumParam.originalSrcM = tiling.tailM;
            ReduceSumLastNDImpl(dstTensor[tiling.rangeM * tiling.reduceSize], srcBuffer, reduceBuffer, reduceSumParam);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradPostProcess(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const LastAxisShapeND& originalSrcShape, bool isFront = false)
{
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    ReduceLastND reduceSumParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };

    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        SoftmaxGradNDImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, originalSrcShape, isFront);
    } else {
        if (isFront) {
            SoftmaxGradFrontNDImpl<float>(dstTensor, srcTensor, gradTensor, workLocal, tiling, originalSrcShape);
        } else {
            LocalTensor<float> splitBuffer = workLocal;
            LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize];
            LocalTensor<float> addBuffer = workLocal[tiling.splitSize + tiling.reduceSize];

            BroadCastLastND brcParam = { tiling.reduceM, tiling.srcK, tiling.reduceM, elementNumPerBlk };
            for (uint32_t i = 0; i < tiling.rangeM; i++) {
                Mul(splitBuffer, srcTensor[i * tiling.splitSize], gradTensor[i * tiling.splitSize], tiling.splitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(addBuffer, splitBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
                BroadCastLastImpl(splitBuffer, addBuffer, brcParam);
                PipeBarrier<PIPE_V>();
                Sub(splitBuffer, gradTensor[i * tiling.splitSize], splitBuffer, tiling.splitSize);
                PipeBarrier<PIPE_V>();
                Mul(dstTensor[i * tiling.splitSize], srcTensor[i * tiling.splitSize], splitBuffer, tiling.splitSize);
            }
            if (tiling.tailM != 0) {
                reduceSumParam.srcM = tiling.tailM;
                reduceSumParam.dstM = tiling.tailM;
                reduceSumParam.originalSrcM = tiling.tailM;
                Mul(splitBuffer, srcTensor[tiling.rangeM * tiling.splitSize],
                    gradTensor[tiling.rangeM * tiling.splitSize], tiling.tailSplitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(addBuffer, splitBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
                BroadCastLastImpl(splitBuffer, addBuffer, brcParam);
                PipeBarrier<PIPE_V>();
                Sub(splitBuffer, gradTensor[tiling.rangeM * tiling.splitSize], splitBuffer, tiling.tailSplitSize);
                PipeBarrier<PIPE_V>();
                Mul(dstTensor[tiling.rangeM * tiling.splitSize], srcTensor[tiling.rangeM * tiling.splitSize],
                    splitBuffer, tiling.tailSplitSize);
            }
        }
    }
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_grad_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_grad_nz_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/../common/softmax_grad_nz_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGradFrontGenericNZImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& gradTensor,
    const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, uint64_t mask[2],
    const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, __cce_half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        Cast<float, __cce_half, false>(tmpBuffer1[splitOffset * j],
            gradTensor[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1[splitOffset * j],
            MASK_PLACEHOLDER, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer2, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<__cce_half, float, false>(dst[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGradFrontGenericNZImpl(const LocalTensor<float>& dst,
    const LocalTensor<float>& gradTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer0[splitOffset * j], src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            gradTensor[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(dst[offset2], tmpBuffer1, { (uint16_t)reduceParam.originalSrcM, 1, 1, 0 });
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFrontNZImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape,
    const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, tiling.splitK, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, tiling.splitK, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };

    const uint32_t lastBlockMaskLen = tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        SoftMaxGradFrontGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset1, offset2,
            splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SoftMaxGradFrontGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset1, offset2,
            splitCount, tailReduceParam);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGradGenericNZImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& gradTensor,
    const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, uint64_t mask[2],
    const uint32_t& offset, const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    LocalTensor<float> tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, __cce_half, false>(tmpBuffer0[splitOffset * j],
            src[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        Cast<float, __cce_half, false>(tmpBuffer1[splitOffset * j],
            gradTensor[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer2[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1[splitOffset * j],
            MASK_PLACEHOLDER, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer3, tmpBuffer2, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer1[splitOffset * j], tmpBuffer1[splitOffset * j], tmpBuffer3, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer2[splitOffset * j], tmpBuffer1[splitOffset * j], tmpBuffer0[splitOffset * j],
            MASK_PLACEHOLDER, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<__cce_half, float, false>(dst[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer2[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGradGenericNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& gradTensor,
    const LocalTensor<float>& src, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, uint64_t mask[2],
    const uint32_t& offset, const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer0[splitOffset * j], src[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            gradTensor[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j],
            gradTensor[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(dst[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0[splitOffset * j],
            src[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradNZImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape,
    const SoftMaxTiling& tiling, bool isFront = false)
{
    if (isFront) {
        SoftmaxGradFrontNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, tiling);
    } else {
        const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
            tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
        const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
            tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
        uint32_t lastBlockMaskLen = tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
            tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
            SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        uint64_t mask[2] = { 0, 0 };
        CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);
        uint32_t offset = 0;
        uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset = i * splitCount;
            SoftMaxGradGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset, splitCount,
                mainReduceParam);
        }
        PipeBarrier<PIPE_V>();
        if (tiling.tailM != 0) {
            offset = tiling.rangeM * splitCount;
            splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
            SoftMaxGradGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset, splitCount,
                tailReduceParam);
        }
    }
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/membase/v220/softmax_grad_impl.h" 2
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_grad_base_impl.h" 2




namespace AscendC {
template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFrontImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    ShapeInfo srcShape = srcTensor.GetShapeInfo();
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, true, false, true);
            SoftmaxGradFrontNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, newTiling);
        } else {
            SoftmaxGradFrontNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, tiling);
        }
    } else {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, true, isBasicBlock);
            SoftmaxGradFrontNDImpl<T, isBasicBlock>(dstTensor, gradTensor, srcTensor, workLocal, newTiling,
                originalSrcShape);
        } else {
            SoftmaxGradFrontNDImpl<T, isBasicBlock>(dstTensor, gradTensor, srcTensor, workLocal, tiling,
                originalSrcShape);
        }
    }
}

template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFrontImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFrontImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, bool isFront,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    ShapeInfo srcShape = srcTensor.GetShapeInfo();
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);

    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, isFront, false, true);
            SoftmaxGradNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, newTiling, isFront);
        } else {
            SoftmaxGradNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, tiling, isFront);
        }
    } else {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, isFront, false);
            SoftmaxGradPostProcess<T>(dstTensor, gradTensor, srcTensor, workLocal, newTiling, originalSrcShape,
                isFront);
        } else {
            SoftmaxGradPostProcess<T>(dstTensor, gradTensor, srcTensor, workLocal, tiling, originalSrcShape, isFront);
        }
    }
}

template <typename T, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, bool isFront,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftmaxGradImpl<T, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling, isFront, softmaxShapeInfo);
}
template <typename T, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    bool isFront, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftmaxGradImpl<T, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling, isFront, softmaxShapeInfo);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h"
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGrad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, bool isFront = false,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                     ;
    SoftmaxGradImpl<T, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, tiling, isFront, softmaxShapeInfo);
                                    ;
}
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h"
template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFront(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                     ;
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, tiling, softmaxShapeInfo);
                                    ;
}
# 100 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h"
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGrad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    bool isFront = false, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                     ;
    SoftmaxGradImpl<T, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, sharedTmpBuffer, tiling, isFront,
        softmaxShapeInfo);
                                    ;
}
# 129 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h"
template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFront(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, sharedTmpBuffer, tiling,
        softmaxShapeInfo);
}
}
#pragma end_pipe
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_membase_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_membase_impl.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void XorCalcSimplified(const LocalTensor<T>& dstAddr, const LocalTensor<T> &src0Addr,
    const LocalTensor<T> &src1Addr, const LocalTensor<T>& tmpTensor)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    And<T, false>(dstAddr, src0Addr, src1Addr, MASK_PLACEHOLDER, 1, binaryParams);

    Or<T, false>(tmpTensor, src0Addr, src1Addr, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Not<T, false>(dstAddr, dstAddr, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    And<T, false>(dstAddr, tmpTensor, dstAddr, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void XorImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{




    uint32_t stackSize = sharedTmpBuffer.GetSize() / sizeof(T) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    const uint32_t loopCount = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    SetMaskCount();
    SetVectorMask<T>(0, stackSize);
    for (uint32_t i = 0; i < loopCount; i++) {
        XorCalcSimplified(dstTensor[i * stackSize],
            src0Tensor[i * stackSize],
            src1Tensor[i * stackSize],
            sharedTmpBuffer.ReinterpretCast<T>());
    }
    if (tail != 0) {
        SetVectorMask<T>(0, tail);
        XorCalcSimplified(dstTensor[loopCount * stackSize],
            src0Tensor[loopCount * stackSize],
            src1Tensor[loopCount * stackSize],
            sharedTmpBuffer.ReinterpretCast<T>());
    }
    SetMaskNorm();
    SetVectorMask<T>(FULL_MASK, FULL_MASK);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_common_impl.h" 2




namespace AscendC {
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_common_impl.h"
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Xor(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    static_assert((std::is_same<T, int16_t>::value || std::is_same<T, uint16_t>::value),
        "Failed to check the data types, current api support data types are int16_t/uint16_t.");

    XorImpl(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Xor(const LocalTensor<T>& dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer)
{






    Xor<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize());
}
# 84 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Xor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    Xor<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 103 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Xor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor)
{






    Xor<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, src0Tensor.GetSize());
}
#pragma end_pipe
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/floor.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/floor.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/floor/floor_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/floor/floor_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/floor/floor_v220_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/floor/floor_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void FloorProcess(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    (void)tmpTensor;
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void FloorProcess(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    const LocalTensor<float> floatTmpTensor = tmpTensor.ReinterpretCast<float>();



    Cast<float, __cce_half, false>(floatTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    FloorProcess(floatTmpTensor, floatTmpTensor, tmpTensor);

    Cast<__cce_half, float, false>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void FloorImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Floor");
    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Floor");



    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(float) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        FloorProcess(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<T>(0, calcTail);
        FloorProcess(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void FloorImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    FloorImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
[aicore] __inline__ __attribute__((always_inline)) void FloorImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const uint32_t calCount)
{
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Floor");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Floor");

    Cast<float, float>(dstTensor, srcTensor, RoundMode::CAST_FLOOR, calCount);
}
[aicore] __inline__ __attribute__((always_inline)) void FloorImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Floor");
    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Floor");

    (void)sharedTmpBuffer;
    Cast<float, float>(dstTensor, srcTensor, RoundMode::CAST_FLOOR, calCount);
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/floor/floor_common_impl.h" 2
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/floor.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/floor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Floor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    static_assert((std::is_same<T, float>::value || std::is_same<T, __cce_half>::value),
        "Failed to check the data types, current api support data types are half/float.");
    FloorImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/floor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Floor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    static_assert((std::is_same<T, float>::value || std::is_same<T, __cce_half>::value),
        "Failed to check the data types, current api support data types are half/float.");
    FloorImpl(dstTensor, srcTensor, calCount);
}

#pragma end_pipe
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/sort.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/sort.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/sort.h" 2
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2





# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/algorithm.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/algorithm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/algorithm/max.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/algorithm/max.h"
namespace AscendC {
namespace Std {
template <typename T, typename U>
[host,aicore] __inline__ __attribute__((always_inline)) T max(const T src0, const U src1)
{
    static_assert(Std::is_same<T, U>::value, "Only support compare with same type!");
    return (src0 > src1) ? src0 : src1;
}

}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/algorithm.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/algorithm/min.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/../../impl/std/algorithm/min.h"
namespace AscendC {
namespace Std {
template <typename T, typename U>
[host,aicore] __inline__ __attribute__((always_inline)) T min(const T src0, const U src1)
{
    static_assert(Std::is_same<T, U>::value, "Only support compare with same type!");
    return (src0 < src1) ? src0 : src1;
}
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/std/algorithm.h" 2

namespace AscendC {
namespace Std {
template <typename T, typename U> [host,aicore] __inline__ __attribute__((always_inline)) T min(const T src0, const U src1);
template <typename T, typename U> [host,aicore] __inline__ __attribute__((always_inline)) T max(const T src0, const U src1);
}
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/arithprogression.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/arithprogression.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/../../impl/index/arithprogression/arithprogression_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/../../impl/index/arithprogression/arithprogression_common_impl.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetBaseArithProgression(const LocalTensor<T> &dstLocal, const T firstValue, const T diffValue,
    const int32_t count)
{
    for (int i = 0; i < count; i++) {
        dstLocal.SetValue(i, static_cast<T>(firstValue) +
            static_cast<T>(diffValue) * static_cast<T>(i));
    }
}


template <>
[aicore] __inline__ __attribute__((always_inline)) void GetBaseArithProgression(const LocalTensor<__cce_half> &dstLocal, const __cce_half firstValue,
    const __cce_half diffValue, const int32_t count)
{
    for (int i = 0; i < count; i++) {
        dstLocal.SetValue(i, static_cast<float>(firstValue) +
            static_cast<float>(diffValue) * static_cast<float>(i));
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ArithProgressionImpl(const LocalTensor<T> &dstLocal, const T firstValue, const T diffValue,
    const int32_t count)
{

                                                                                                       ;

                                                                                  ;


    if (g_coreType == AIC) {
        return;
    }

    struct UnaryRepeatParams addsParamsStride1(1, 1, 1, 1);
    struct UnaryRepeatParams addsParamsStride8(1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);

    constexpr int32_t BLOCK_NUM = (ONE_BLK_SIZE / sizeof(T));
    constexpr int32_t REPEAT_NUM = (ONE_REPEAT_BYTE_SIZE / sizeof(T));
    if (count > BLOCK_NUM) {

        GetBaseArithProgression<T>(dstLocal, firstValue, diffValue, BLOCK_NUM);
        auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        if (count > REPEAT_NUM) {

            SetVectorMask<T>(0, (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(BLOCK_NUM)) - 1));
            PipeBarrier<PIPE_V>();
            for (int i = 0; i < DEFAULT_BLK_NUM - 1; i++) {
                Adds<T, false>(dstLocal[(i + 1) * BLOCK_NUM], dstLocal[i * BLOCK_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(BLOCK_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride1);
                PipeBarrier<PIPE_V>();
            }
            int32_t repeat = count / REPEAT_NUM;
            int32_t tail = count % REPEAT_NUM;
            ResetMask();
            PipeBarrier<PIPE_V>();

            for (int i = 0; i < repeat - 1; i++) {
                Adds<T, false>(dstLocal[(i + 1) * REPEAT_NUM], dstLocal[i * REPEAT_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(REPEAT_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride8);
                PipeBarrier<PIPE_V>();
            }
            if (tail > 0) {
                int32_t tail_aligned = (tail + BLOCK_NUM - 1) / BLOCK_NUM * BLOCK_NUM;
                SetVectorMask<T>(tail_aligned);
                PipeBarrier<PIPE_V>();
                Adds<T, false>(dstLocal[repeat * REPEAT_NUM], dstLocal[(repeat - 1) * REPEAT_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(REPEAT_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride8);
                PipeBarrier<PIPE_V>();
            }
        } else {

            int32_t countAligned = (count + BLOCK_NUM - 1) / BLOCK_NUM * BLOCK_NUM;
            int32_t repeat = countAligned / BLOCK_NUM;
            SetVectorMask<T>(0, (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(BLOCK_NUM)) - 1));
            PipeBarrier<PIPE_V>();
            for (int i = 0; i < repeat - 1; i++) {
                Adds<T, false>(dstLocal[(i + 1) * BLOCK_NUM], dstLocal[i * BLOCK_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(BLOCK_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride1);
                PipeBarrier<PIPE_V>();
            }
        }
    } else {

        GetBaseArithProgression<T>(dstLocal, firstValue, diffValue, count);
    }
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/arithprogression.h" 2

namespace AscendC {
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/arithprogression.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V, S"))) void ArithProgression(const LocalTensor<T> &dstLocal,
    const T firstValue, const T diffValue, const int32_t count)
{
    ArithProgressionImpl(dstLocal, firstValue, diffValue, count);
}
}
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad_utils.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad_utils.h"
namespace AscendC {

struct LayerNormGradShapeInfo {
    DataFormat dataFormat = DataFormat::ND;
};

};
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernormgrad/layernormgrad_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernormgrad/layernormgrad_common_impl.h"
const uint32_t LAYERNORM_GRAD_B32_BYTE_SIZE = 4;
const uint32_t LAYERNORM_GRAD_B16_BYTE_SIZE = 2;

namespace AscendC {

struct LayerNormGradParams {
    [aicore] LayerNormGradParams(LayerNormGradTiling &tiling, LocalTensor<float> &stackBuffer)
        : bLength(tiling.bLength),
          sLength(tiling.sLength),
          hLength(tiling.hLength),
          loopNum(tiling.loopNum),
          tailSize(tiling.tailSize),
          nohTailSize(tiling.nohTailSize),
          oneCalSize(tiling.oneCalSize),
          nohCalSize(tiling.nohCalSize),
          x1Tensor(stackBuffer[tiling.x1TensorPos]),
          x2Tensor(stackBuffer[tiling.x2TensorPos]),
          x3Tensor(stackBuffer[tiling.x3TensorPos]),
          pdVarTensor(stackBuffer[tiling.pdVarTensorPos]),
          pdMeanTensor(stackBuffer[tiling.pdMeanTensorPos]),
          tmpTensor(stackBuffer[tiling.tmpTensorPos]),
          tmpTensor1(stackBuffer[tiling.tmpTensor1Pos]),
          tmpTensor2(stackBuffer[tiling.tmpTensor2Pos]),
          tmpTensorBSH(stackBuffer[tiling.tmpTensorBSHPos]),
          lastDimValueBack(*(reinterpret_cast<float *>(&tiling.lastDimValueBack))),
          lastDimValueBackMulTwo(*(reinterpret_cast<float *>(&tiling.lastDimValueBackMulTwo)))
    {
        x1Tensor.SetSize(tiling.x1TensorSize);
        x2Tensor.SetSize(tiling.x2TensorSize);
        x3Tensor.SetSize(tiling.x3TensorSize);
        pdVarTensor.SetSize(tiling.pdVarTensorSize);
        pdMeanTensor.SetSize(tiling.pdMeanTensorSize);
        tmpTensor.SetSize(tiling.tmpTensorSize);
        tmpTensor1.SetSize(tiling.tmpTensor1Size);
        tmpTensor2.SetSize(tiling.tmpTensor2Size);
        tmpTensorBSH.SetSize(tiling.tmpTensorBSHSize);
    }

    [aicore] LayerNormGradParams(uint32_t b, uint32_t s, uint32_t h)
    {
        bLength = b;
        sLength = s;
        hLength = h;
    }

    uint32_t bLength;
    uint32_t sLength;
    uint32_t hLength;

    uint32_t loopNum;
    uint32_t tailSize;
    uint32_t nohTailSize;
    uint32_t oneCalSize;
    uint32_t nohCalSize;

    float lastDimValueBack;
    float lastDimValueBackMulTwo;

    LocalTensor<float> x1Tensor;
    LocalTensor<float> x2Tensor;
    LocalTensor<float> x3Tensor;
    LocalTensor<float> pdVarTensor;
    LocalTensor<float> pdMeanTensor;
    LocalTensor<float> tmpTensor;
    LocalTensor<float> tmpTensor1;
    LocalTensor<float> tmpTensor2;
    LocalTensor<float> tmpTensorBSH;
};

[aicore] __inline__ __attribute__((always_inline)) void DuplicateLastDimImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength)
{
    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    float scalarList[BRCB_BROADCAST_NUMBER] = {0};
    const uint32_t rangeM = bsLength / BRCB_BROADCAST_NUMBER;
    const uint32_t tailM = bsLength % BRCB_BROADCAST_NUMBER;

    for (uint32_t i = 0; i < rangeM; i++) {
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            scalarList[j] = src[i * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            Duplicate(dst[(i * BRCB_BROADCAST_NUMBER + j) * hLength], scalarList[j], hLength);
        }
    }
    if (tailM != 0) {
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = src[rangeM * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < tailM; j++) {
            Duplicate(dst[(rangeM * BRCB_BROADCAST_NUMBER + j) * hLength], scalarList[j], hLength);
        }
    }
}


[aicore] __inline__ __attribute__((always_inline)) void BrcbLastDimImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength)
{
    const uint32_t maxRepeatHSize = BRCB_MAX_REPEAT_SIZE * hLength;

    const uint32_t lineRound = hLength / BRCB_BROADCAST_NUMBER;

    const uint32_t rowRound = bsLength / BRCB_BROADCAST_NUMBER;
    const uint32_t rowTail = bsLength % BRCB_BROADCAST_NUMBER;
    const uint32_t rowRoundLen = bsLength - rowTail;

    const uint32_t repeatTimes = rowRound / MAX_REPEAT_TIMES;
    const uint32_t tailTimes = rowRound % MAX_REPEAT_TIMES;

    BrcbRepeatParams repeatParams;
    repeatParams.dstBlkStride = lineRound;
    repeatParams.dstRepStride = hLength;

    for (uint32_t i = 0; i < lineRound; i++) {
        for (uint32_t j = 0; j < repeatTimes; j++) {
            Brcb(dst[i * BRCB_BROADCAST_NUMBER + j * maxRepeatHSize], src[j * BRCB_MAX_REPEAT_SIZE], MAX_REPEAT_TIMES,
                repeatParams);
        }

        if (tailTimes > 0) {
            Brcb(dst[i * BRCB_BROADCAST_NUMBER + repeatTimes * maxRepeatHSize], src[repeatTimes * BRCB_MAX_REPEAT_SIZE],
                tailTimes, repeatParams);
        }
        PipeBarrier<PIPE_V>();
    }

    if (rowTail != 0) {
        DuplicateLastDimImpl(dst[rowRoundLen * hLength], src[rowRoundLen], rowTail, hLength);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void BroadcastLastDimImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t dstSize, const uint32_t srcSize)
{

    BrcbLastDimImpl(dst, src, srcSize, dstSize / srcSize);



}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumImpl(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const uint32_t calSize, const uint32_t hLength)
{
                                                                                                   ;
    const uint32_t count = calSize / hLength;

    for (uint32_t i = 0; i < count; i++) {
        uint32_t totalNum = hLength;
        uint32_t iMulhLength = i * hLength;

        LocalTensor<float> srctmp = src;
        LocalTensor<float> dstTmp = dst;

        while (totalNum > 1) {
            uint32_t repeatTimes = totalNum / ONE_REPEAT_FLOAT_SIZE;
            uint32_t tailSize = totalNum % ONE_REPEAT_FLOAT_SIZE;

            uint32_t blockNum = repeatTimes / MAX_REPEAT_TIMES;
            uint32_t blockTail = repeatTimes % MAX_REPEAT_TIMES;

            for (uint32_t j = 0; j < blockNum; j++) {
                WholeReduceSum(dst[iMulhLength + j * MAX_REPEAT_TIMES], srctmp[iMulhLength + j * MAX_REPEAT_FLOAT_SIZE],
                    ONE_REPEAT_FLOAT_SIZE, MAX_REPEAT_TIMES, 1, 1, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();

            if (totalNum == ONE_REPEAT_FLOAT_SIZE) {
                dstTmp = dst[i];
            } else {
                dstTmp = dst[iMulhLength + blockNum * MAX_REPEAT_TIMES];
            }

            if (blockTail > 0) {
                WholeReduceSum(dstTmp, srctmp[iMulhLength + blockNum * MAX_REPEAT_FLOAT_SIZE], ONE_REPEAT_FLOAT_SIZE,
                    blockTail, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
            }

            if (totalNum < ONE_REPEAT_FLOAT_SIZE) {
                dstTmp = dst[i];
            } else {
                dstTmp = dst[iMulhLength + totalNum / ONE_REPEAT_FLOAT_SIZE];
            }

            if (tailSize > 0) {
                WholeReduceSum(dstTmp, srctmp[iMulhLength + repeatTimes * ONE_REPEAT_FLOAT_SIZE], tailSize,
                    DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
            }

            totalNum = DivCeil(totalNum, ONE_REPEAT_FLOAT_SIZE);
            srctmp = dst;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateTensor(const LocalTensor<T> &dst, const LocalTensor<T> &src, const uint32_t count,
    const uint32_t length)
{
    BroadcastLastDimImpl(dst, src, count * length, count);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void ComputePdX1(const LocalTensor<float> &inputDy, const LocalTensor<float> &inputGamma,
    LayerNormGradParams &param, const uint32_t nohSize, const uint32_t hLength)
{

    for (size_t i = 0; i < nohSize; ++i) {
        Mul(param.x1Tensor[i * hLength], inputDy[i * hLength], inputGamma, hLength);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ComputePdX2(const LocalTensor<T> &inputX, const LocalTensor<T> &inputMean,
    const LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize, const uint32_t hLength)
{

    DuplicateTensor(param.tmpTensorBSH, inputMean, nohSize, hLength);
    PipeBarrier<PIPE_V>();

    Sub(param.x2Tensor, inputX, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DoOneDiv(LocalTensor<float> &dstTensor, LocalTensor<float> &oneTensor,
    LocalTensor<float> &src1Tensor, const uint32_t nohSize)
{
    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, B32_DATA_NUM_PER_BLOCK);
    Duplicate<float, false>(oneTensor, 1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, nohSize);
    Div<float, false>(dstTensor, oneTensor, src1Tensor, MASK_PLACEHOLDER, 1,
        { 1, 0, 1, DEFAULT_REPEAT_STRIDE, 0, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
}

[aicore] __inline__ __attribute__((always_inline)) void ComputePdVar(const LocalTensor<float> &inputVariance, float epsilon, LayerNormGradParams &param,
    const uint32_t calSize, const uint32_t nohSize)
{
    const float multiplier1 = -1.5;
    const float multiplier2 = -0.5;

    Adds(param.tmpTensor, inputVariance, epsilon, nohSize);
    PipeBarrier<PIPE_V>();

    Mul(param.tmpTensorBSH, param.tmpTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();
    Mul(param.tmpTensor, param.tmpTensorBSH, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();
    Sqrt(param.tmpTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();

    DoOneDiv(param.tmpTensor, param.tmpTensorBSH, param.tmpTensor, nohSize);


    DuplicateTensor(param.tmpTensorBSH, param.tmpTensor, nohSize, param.hLength);
    PipeBarrier<PIPE_V>();


    Mul(param.tmpTensorBSH, param.x2Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
    Mul(param.tmpTensorBSH, param.x1Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
    Muls(param.tmpTensorBSH, param.tmpTensorBSH, static_cast<float>(multiplier2), calSize);
    PipeBarrier<PIPE_V>();



    ReduceSumImpl(param.pdVarTensor, param.tmpTensorBSH, calSize, param.hLength);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void ComputePdMean(const LocalTensor<float> &inputVariance, const LocalTensor<float> &resForGamma,
    float epsilon, LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize)
{
    constexpr float exponent = -0.5;
    constexpr float multiplier = -1.0;
    constexpr float multiplier2 = -2.0;

    Adds(param.tmpTensor, inputVariance, epsilon, nohSize);
    PipeBarrier<PIPE_V>();


    Sqrt(param.tmpTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();

    DoOneDiv(param.tmpTensor, param.tmpTensorBSH, param.tmpTensor, nohSize);


    DuplicateTensor(param.tmpTensorBSH, param.tmpTensor, nohSize, param.hLength);
    PipeBarrier<PIPE_V>();


    Mul(resForGamma, param.x2Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();


    Mul(param.x3Tensor, param.x1Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
    Muls(param.tmpTensorBSH, param.x3Tensor, static_cast<float>(multiplier), calSize);
    PipeBarrier<PIPE_V>();


    ReduceSumImpl(param.pdMeanTensor, param.tmpTensorBSH, calSize, param.hLength);


    Muls(param.tmpTensorBSH, param.x2Tensor, static_cast<float>(multiplier2), calSize);
    PipeBarrier<PIPE_V>();

    ReduceSumImpl(param.tmpTensor, param.tmpTensorBSH, calSize, param.hLength);
    PipeBarrier<PIPE_V>();


    Muls(param.tmpTensor, param.tmpTensor, static_cast<float>(param.lastDimValueBack), nohSize);
    PipeBarrier<PIPE_V>();
    Mul(param.tmpTensor, param.pdVarTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();


    Add(param.pdMeanTensor, param.pdMeanTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void ComputePdX(const LocalTensor<float> &inputVariance, const LocalTensor<float> &outputPdX,
    float epsilon, const LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize)
{


    Muls(param.pdVarTensor, param.pdVarTensor, static_cast<float>(param.lastDimValueBackMulTwo), nohSize);
    PipeBarrier<PIPE_V>();

    DuplicateTensor(param.tmpTensorBSH, param.pdVarTensor, nohSize, param.hLength);
    PipeBarrier<PIPE_V>();

    Mul(param.x1Tensor, param.tmpTensorBSH, param.x2Tensor, calSize);
    PipeBarrier<PIPE_V>();


    Muls(param.pdMeanTensor, param.pdMeanTensor, static_cast<float>(param.lastDimValueBack), nohSize);
    PipeBarrier<PIPE_V>();
    DuplicateTensor(param.tmpTensorBSH, param.pdMeanTensor, nohSize, param.hLength);


    Add(param.x1Tensor, param.x1Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();

    Add(outputPdX, param.x1Tensor, param.x3Tensor, calSize);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetTmpTensor(const LocalTensor<float> &outputPdX, const LocalTensor<float> &inputDy,
    const LocalTensor<float> &inputX, LayerNormGradParams &param, bool isReuseSource = false)
{
    param.tmpTensor = outputPdX;
    if (isReuseSource == true) {
        param.x1Tensor = inputDy;
        param.x2Tensor = inputX;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ComputeProcess(const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX,
    const LocalTensor<T> &inputVariance, const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma,
    const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma, T epsilon, LayerNormGradParams &param,
    const uint32_t calSize, const uint32_t nohSize, bool isReuseSource)
{}

template <>
[aicore] __inline__ __attribute__((always_inline)) void ComputeProcess<__cce_half>(const LocalTensor<__cce_half> &inputDy, const LocalTensor<__cce_half> &inputX,
    const LocalTensor<__cce_half> &inputVariance, const LocalTensor<__cce_half> &inputMean, const LocalTensor<__cce_half> &inputGamma,
    const LocalTensor<__cce_half> &outputPdX, const LocalTensor<__cce_half> &resForGamma, __cce_half epsilon, LayerNormGradParams &param,
    const uint32_t calSize, const uint32_t nohSize, bool isReuseSource)
{
    Cast(param.tmpTensor1, inputDy, RoundMode::CAST_NONE, calSize);
    Cast(param.tmpTensor2, inputGamma, RoundMode::CAST_NONE, param.hLength);
    PipeBarrier<PIPE_V>();

    ComputePdX1(param.tmpTensor1, param.tmpTensor2, param, nohSize, param.hLength);

    Cast(param.tmpTensor1, inputX, RoundMode::CAST_NONE, calSize);
    Cast(param.tmpTensor2, inputMean, RoundMode::CAST_NONE, nohSize);
    PipeBarrier<PIPE_V>();
    ComputePdX2(param.tmpTensor1, param.tmpTensor2, param, calSize, nohSize, param.hLength);

    Cast(param.tmpTensor1, inputVariance, RoundMode::CAST_NONE, nohSize);
    PipeBarrier<PIPE_V>();
    ComputePdVar(param.tmpTensor1, epsilon, param, calSize, nohSize);


    ComputePdMean(param.tmpTensor1, param.tmpTensor2, epsilon, param, calSize, nohSize);



    ComputePdX(param.tmpTensor1, param.tmpTensor, epsilon, param, calSize, nohSize);

    Cast(outputPdX, param.tmpTensor, RoundMode::CAST_NONE, calSize);
    Cast(resForGamma, param.tmpTensor2, RoundMode::CAST_NONE, calSize);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void ComputeProcess<float>(const LocalTensor<float> &inputDy, const LocalTensor<float> &inputX,
    const LocalTensor<float> &inputVariance, const LocalTensor<float> &inputMean, const LocalTensor<float> &inputGamma,
    const LocalTensor<float> &outputPdX, const LocalTensor<float> &resForGamma, float epsilon,
    LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize, bool isReuseSource)
{
    GetTmpTensor(outputPdX, inputDy, inputX, param, isReuseSource);

    ComputePdX1(inputDy, inputGamma, param, nohSize, param.hLength);

    ComputePdX2(inputX, inputMean, param, calSize, nohSize, param.hLength);

    ComputePdVar(inputVariance, epsilon, param, calSize, nohSize);


    ComputePdMean(inputVariance, resForGamma, epsilon, param, calSize, nohSize);
    PipeBarrier<PIPE_V>();



    ComputePdX(inputVariance, outputPdX, epsilon, param, calSize, nohSize);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradComputeND(const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX,
    const LocalTensor<T> &inputVariance, const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma,
    const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma, T epsilon, LayerNormGradParams &param,
    bool isReuseSource)
{
    int offset0 = 0;
    int offset1 = 0;

    for (size_t i = 0; i < param.loopNum; ++i) {
        ComputeProcess<T>(inputDy[offset0], inputX[offset0], inputVariance[offset1], inputMean[offset1], inputGamma,
            outputPdX[offset0], resForGamma[offset0], epsilon, param, param.oneCalSize, param.nohCalSize,
            isReuseSource);
        offset0 += param.oneCalSize;
        offset1 += param.nohCalSize;
    }

    if (param.tailSize != 0) {
        ComputeProcess<T>(inputDy[offset0], inputX[offset0], inputVariance[offset1], inputMean[offset1], inputGamma,
            outputPdX[offset0], resForGamma[offset0], epsilon, param, param.tailSize, param.nohTailSize, isReuseSource);
    }
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradImpl(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, LocalTensor<uint8_t> &sharedTmpBuffer, T epsilon,
    LayerNormGradTiling &tiling, const LayerNormGradShapeInfo &shapeInfo = {})
{
                                       ;


                                                                                  ;

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    LayerNormGradParams param(tiling, stackBuffer);

    if (shapeInfo.dataFormat == DataFormat::ND) {
        LayerNormGradComputeND(inputDy, inputX, inputVariance, inputMean, inputGamma, outputPdX, resForGamma, epsilon,
            param, isReuseSource);
    } else {
                                                                                           ;
    }
                                      ;
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradImpl(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, T epsilon, LayerNormGradTiling &tiling,
    const LayerNormGradShapeInfo &shapeInfo = {})
{
    LocalTensor<uint8_t> stackBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackBuffer);
                                                                                 ;

    LayerNormGradImpl<T, isReuseSource>(outputPdX, resForGamma, inputDy, inputX, inputVariance, inputMean, inputGamma,
        stackBuffer, epsilon, tiling);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGrad(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, LocalTensor<uint8_t> &sharedTmpBuffer, T epsilon,
    LayerNormGradTiling &tiling, const LayerNormGradShapeInfo &shapeInfo = {})
{
    LayerNormGradImpl<T, isReuseSource>(outputPdX, resForGamma, inputDy, inputX, inputVariance, inputMean, inputGamma,
        sharedTmpBuffer, epsilon, tiling, shapeInfo);
}
# 78 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGrad(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, T epsilon, LayerNormGradTiling &tiling,
    const LayerNormGradShapeInfo &shapeInfo = {})
{
    LayerNormGradImpl<T, isReuseSource>(outputPdX, resForGamma, inputDy, inputX, inputVariance, inputMean, inputGamma,
        epsilon, tiling, shapeInfo);
}
#pragma end_pipe
}
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgradbeta.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgradbeta.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernormgrad/layernormgradbeta_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernormgrad/layernormgradbeta_common_impl.h"
namespace AscendC {
struct LayerNormGradBetaParams {
    [aicore] LayerNormGradBetaParams(){};

    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;

    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    uint32_t hCurLength = 0;

    LocalTensor<float> gammaTempTensor;
    LocalTensor<float> betaTempTensor;
    LocalTensor<float> inputDyTmpTensor;
    LocalTensor<float> resForGammaTmpTensor;
};


template <bool isClearDst = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumFirstN(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const uint32_t bsLength, const uint32_t hLength)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, hLength);
    uint32_t startIndex = 0;
    if constexpr (isClearDst) {
        const UnaryRepeatParams unaryRepeatParams;
        Adds<float, false>(dst, src, static_cast<float>(0), MASK_PLACEHOLDER, 1, unaryRepeatParams);
        startIndex = 1;
        PipeBarrier<PIPE_V>();
    }

    const BinaryRepeatParams binaryParams;
    for (; startIndex < bsLength; startIndex++) {
        Add<float, false>(dst, src[startIndex * hLength], dst, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <bool isClearDst = false>
[aicore] __inline__ __attribute__((always_inline)) void ComputeProcess(const LocalTensor<float> &resForGamma, const LocalTensor<float> &inputDy,
    const LocalTensor<float> &outputPdGamma, const LocalTensor<float> &outputPdBeta,
    const LayerNormGradBetaParams &params)
{
    const LocalTensor<float> &resForGammaTmpTensor = params.resForGammaTmpTensor;

    SetVectorMask<float, MaskMode::COUNTER>(0, params.bshCurLength);

    const BinaryRepeatParams binaryParams;

    Mul<float, false>(resForGammaTmpTensor, inputDy, resForGamma, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    ReduceSumFirstN<isClearDst>(outputPdGamma, resForGammaTmpTensor, params.bsCurLength, params.hCurLength);

    ReduceSumFirstN<isClearDst>(outputPdBeta, inputDy, params.bsCurLength, params.hCurLength);
}

template <bool isClearDst = false>
[aicore] __inline__ __attribute__((always_inline)) void ComputeProcess(const LocalTensor<__cce_half> &resForGamma, const LocalTensor<__cce_half> &inputDy,
    const LocalTensor<__cce_half> &outputPdGamma, const LocalTensor<__cce_half> &outputPdBeta,
    const LayerNormGradBetaParams &params)
{
    const LocalTensor<float> &inputDyTmpTensor = params.inputDyTmpTensor;
    const LocalTensor<float> &resForGammaTmpTensor = params.resForGammaTmpTensor;

    const LocalTensor<float> &gammaTempTensor = params.gammaTempTensor;
    const LocalTensor<float> &betaTempTensor = params.betaTempTensor;

    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, params.bshCurLength);

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);


    Cast<float, __cce_half, false>(inputDyTmpTensor, inputDy, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, __cce_half, false>(resForGammaTmpTensor, resForGamma, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    ComputeProcess<isClearDst>(resForGammaTmpTensor, inputDyTmpTensor, gammaTempTensor, betaTempTensor, params);

    SetVectorMask<float, MaskMode::COUNTER>(0, params.hCurLength);

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);

    Cast<__cce_half, float, false>(outputPdGamma, gammaTempTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<__cce_half, float, false>(outputPdBeta, betaTempTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradBetaComputeND(const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy,
    const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta, const LayerNormGradBetaTiling &tiling,
    LayerNormGradBetaParams &params)
{
    ComputeProcess<true>(resForGamma, inputDy, outputPdGamma, outputPdBeta, params);

    uint32_t inputOffset = tiling.oneCalSize;

    for (uint32_t index = 1; index < tiling.loopRound; index++) {
        ComputeProcess(resForGamma[inputOffset], inputDy[inputOffset], outputPdGamma, outputPdBeta, params);
        inputOffset += tiling.oneCalSize;
    }

    if (tiling.inputTailSize > 0) {
        params.bshCurLength = tiling.inputTailSize;
        params.bsCurLength = tiling.bsTailSize;

        ComputeProcess(resForGamma[tiling.inputTailPos], inputDy[tiling.inputTailPos], outputPdGamma, outputPdBeta,
            params);
    }
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormGradBetaTensorInfo(const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy,
    const LocalTensor<float> &stackBuffer, const LayerNormGradBetaTiling &tiling, LayerNormGradBetaParams &params)
{
    params.bLength = tiling.bLength;
    params.sLength = tiling.sLength;
    params.hLength = tiling.hLength;
    params.originalHLength = tiling.originalHLength;

    params.bshCurLength = tiling.bshCurLength;
    params.bsCurLength = tiling.bsCurLength;
    params.hCurLength = tiling.originalHLength;

    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        params.gammaTempTensor = stackBuffer[tiling.gammaTempTensorPos];
        params.betaTempTensor = stackBuffer[tiling.betaTempTensorPos];
        params.inputDyTmpTensor = stackBuffer[tiling.inputDyTmpTensorPos];
        params.resForGammaTmpTensor = stackBuffer[tiling.resForGammaTmpTensorPos];




          ;
    }

    if constexpr (sizeof(T) == sizeof(float)) {
        if constexpr (isReuseSource) {
            params.resForGammaTmpTensor = resForGamma;
        } else {
            params.resForGammaTmpTensor = stackBuffer[tiling.resForGammaTmpTensorPos];





              ;
        }
    }




      ;
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradBetaImpl(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const LayerNormGradBetaTiling &tiling)
{
                                           ;
                                                                                                         ;

    if constexpr(g_coreType == AscendC::AIC) {
                                              ;
        return;
    }

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    LayerNormGradBetaParams params;
    GetLayerNormGradBetaTensorInfo<T, isReuseSource>(resForGamma, inputDy, stackBuffer, tiling, params);

    SetMaskCount();
    LayerNormGradBetaComputeND(resForGamma, inputDy, outputPdGamma, outputPdBeta, tiling, params);

    SetMaskNorm();
    ResetMask();
                                          ;
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradBetaImpl(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, LayerNormGradBetaTiling &tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    LayerNormGradBetaImpl<T, isReuseSource>(outputPdGamma, outputPdBeta, resForGamma, inputDy, sharedTmpBuffer, tiling);
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgradbeta.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgradbeta.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradBeta(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const LayerNormGradBetaTiling &tiling)
{
    LayerNormGradBetaImpl<T, isReuseSource>(outputPdGamma, outputPdBeta, resForGamma, inputDy, sharedTmpBuffer, tiling);
}
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgradbeta.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradBeta(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, LayerNormGradBetaTiling &tiling)
{
    LayerNormGradBetaImpl<T, isReuseSource>(outputPdGamma, outputPdBeta, resForGamma, inputDy, tiling);
}
#pragma end_pipe
}
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_v220_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h" 2
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateLastDimImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t srcSize, const uint32_t brcbSize)
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    T scalarList[BRCB_BROADCAST_NUMBER] = {0};
    const uint32_t rangeM = srcSize / BRCB_BROADCAST_NUMBER;
    const uint32_t tailM = srcSize % BRCB_BROADCAST_NUMBER;

    for (uint32_t i = 0; i < rangeM; i++) {
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            scalarList[j] = srcTensor[i * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            Duplicate(dstTensor[(i * BRCB_BROADCAST_NUMBER + j) * brcbSize], scalarList[j], brcbSize);
        }
    }
    if (tailM != 0) {
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = srcTensor[rangeM * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < tailM; j++) {
            Duplicate(dstTensor[(rangeM * BRCB_BROADCAST_NUMBER + j) * brcbSize], scalarList[j], brcbSize);
        }
    }

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AlignedPad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    PadParams& padParams, PadTiling& tiling)
{
    uint16_t leftPad = padParams.leftPad;
    uint16_t rightPad = padParams.rightPad;
    int32_t padValue = padParams.padValue;

    uint32_t height = tiling.srcHeight;
    uint32_t width = tiling.srcWidth;
    uint32_t oriWidth = tiling.srcOriWidth;




      ;


                                                                                                                ;


                                                                                                                       ;


    uint32_t elementsPerBlock = ONE_BLK_SIZE / sizeof(T);

    DataCopy(dstTensor, srcTensor, height * width);
    PipeBarrier<PIPE_V>();

    uint64_t mask[2];
    mask[0] = ((1 << rightPad) - 1) << (elementsPerBlock - (width - oriWidth));
    mask[1] = 0;

    uint32_t widthWithoutLastBlock = tiling.widthWithoutLastBlock;
    uint32_t blocksPerRow = tiling.blocksPerRow;

    uint32_t heightTiling = tiling.heightTiling;
    uint32_t heightFractal = tiling.heightFractal;
    uint32_t heightFractalTail = tiling.heightFractalTail;
    for (uint32_t i = 0; i < heightFractal; i++) {
        Duplicate<T, true>(dstTensor[i * tiling.mainLoopOffset + widthWithoutLastBlock], static_cast<T>(padValue), mask,
            heightTiling, 1, blocksPerRow);
    }
    if (heightFractalTail) {
        Duplicate<T, true>(dstTensor[tiling.tailBlockOffset], static_cast<T>(padValue), mask, heightFractalTail, 1,
            blocksPerRow);
    }
}
# 126 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UnAlignedPad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    PadParams& padParams, const LocalTensor<T>& tmpBuffer, PadTiling& tiling)
{
    uint16_t leftPad = padParams.leftPad;
    uint16_t rightPad = padParams.rightPad;
    int32_t padValue = padParams.padValue;

    uint32_t height = tiling.srcHeight;
    uint32_t width = tiling.srcWidth;
    uint32_t oriWidth = tiling.srcOriWidth;






      ;

    uint32_t tmp1BlockNum = tiling.tmpBuffer1BlockNum;

    LocalTensor<T> tmp1 = tmpBuffer;
    LocalTensor<T> tmp2 = tmpBuffer[tiling.tmpBuffer2Offset];

    uint32_t widthTiling = tiling.widthTiling;
    uint32_t widthFractal = tiling.widthFractal;
    uint32_t widthFractalTail = tiling.widthFractalTail;

    uint32_t widthFractalTailAlingned = tiling.widthFractalTailAlingned;

    uint32_t brcbTiling = tiling.brcbTiling;
    uint32_t brcbFractal = tiling.brcbFractal;
    uint32_t brcbFractalTail = tiling.brcbFractalTail;
    uint32_t brcbFractalCount = 0;

    uint32_t maxRepeatTimes = tiling.maxRepeatTimes;
    uint32_t brcbTilingRepeatTimes = tiling.brcbTilingRepeatTimes;
    uint32_t brcbTilingRepeatTimesTail = tiling.brcbTilingRepeatTimesTail;
    uint32_t brcbFractalTailRepeatTimes = tiling.brcbFractalTailRepeatTimes;
    uint32_t brcbFractalTailRepeatTimesTail = tiling.brcbFractalTailRepeatTimesTail;

    uint32_t tmp1RowFull = tiling.tmpBuffer1RowNum;
    uint32_t tmp1RowCount = tiling.tmpBuffer1RowNum;
    uint32_t tmp1RemainRow = 0;

    uint32_t tmp2RowFull = tiling.tmpBuffer1RowNum;
    uint32_t tmp2RowCount = 0;
    uint32_t tmp2NeedRow = tiling.tmpBuffer1RowNum;

    uint32_t tmpWidth = ONE_BLK_SIZE / sizeof(T);


    TransDataTo5HDParams transDataParams;
    transDataParams.repeatTimes = tmp1BlockNum;
    if (transDataParams.repeatTimes > 1) {
        transDataParams.dstRepStride = 16;
        transDataParams.srcRepStride = 16;
    }

    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE];


    for (uint16_t i = 0; i < NCHW_CONV_ADDR_LIST_SIZE; i++) {
        dstList[i] = (uint64_t)(tmp2[i * tmpWidth].GetPhyAddr());
        srcList[i] = (uint64_t)(tmp2[i * tmpWidth].GetPhyAddr());
    }




    for (uint32_t j = 0; j < height; j++) {
        for (uint32_t i = 0; i < (widthFractal + 1); i++) {
            tmp2RowCount = 0;

            if (i == 0 && leftPad != 0) {
                Duplicate<T, true>(tmp2, static_cast<T>(padValue), tmpWidth, leftPad, 1, 1);
                tmp2RowCount += leftPad;
            }


            if (i == widthFractal) {
                if (rightPad != 0) {
                    Duplicate<T, true>(tmp2[(widthFractalTailAlingned - rightPad) * tmpWidth], static_cast<T>(padValue),
                        tmpWidth, rightPad, 1, 1);
                }
                tmp2NeedRow = widthFractalTailAlingned - tmp2RowCount - rightPad;
            } else {
                tmp2NeedRow = tmp2RowFull - tmp2RowCount;
            }

            while (tmp2NeedRow != 0) {
                PipeBarrier<PIPE_V>();
                tmp1RemainRow = tmp1RowFull - tmp1RowCount;
                if (tmp2NeedRow > tmp1RemainRow) {
                    if (tmp1RemainRow != 0) {
                        DataCopy(tmp2[tmp2RowCount * tmpWidth], tmp1[tmp1RowCount * tmpWidth],
                            { 1, static_cast<uint16_t>(tmp1RemainRow), 0, 0 });
                        tmp1RowCount += tmp1RemainRow;
                        tmp2RowCount += tmp1RemainRow;
                        tmp2NeedRow -= tmp1RemainRow;
                        PipeBarrier<PIPE_V>();
                    }


                    if (brcbFractalCount == brcbFractal) {
                        for (uint32_t i = 0; i < brcbFractalTailRepeatTimes; i++) {
                            Brcb(tmp1[i * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling + i * maxRepeatTimes * 8], maxRepeatTimes,
                                { 1, 8 });
                        }
                        if (brcbFractalTailRepeatTimesTail) {
                            Brcb(tmp1[brcbFractalTailRepeatTimes * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling +
                                brcbFractalTailRepeatTimes * maxRepeatTimes * 8],
                                brcbFractalTailRepeatTimesTail, { 1, 8 });
                        }
                        tmp1RowFull = brcbFractalTail;
                    } else {
                        for (uint32_t i = 0; i < brcbTilingRepeatTimes; i++) {
                            Brcb(tmp1[i * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling + i * maxRepeatTimes * 8], maxRepeatTimes,
                                { 1, 8 });
                        }
                        if (brcbTilingRepeatTimesTail) {
                            Brcb(tmp1[brcbTilingRepeatTimes * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling + brcbTilingRepeatTimes * maxRepeatTimes * 8],
                                brcbTilingRepeatTimesTail, { 1, 8 });
                        }
                    }
# 266 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h"
                    brcbFractalCount += 1;
                    tmp1RowCount = 0;
                } else {
                    DataCopy(tmp2[tmp2RowCount * tmpWidth], tmp1[tmp1RowCount * tmpWidth],
                        { 1, static_cast<uint16_t>(tmp2NeedRow), 0, 0 });
                    tmp1RowCount += tmp2NeedRow;
                    tmp2RowCount += tmp2NeedRow;
                    tmp2NeedRow = 0;
                }
            }

            PipeBarrier<PIPE_V>();


            TransDataTo5HD<T>(dstList, srcList, transDataParams);
            PipeBarrier<PIPE_V>();
            if (i == widthFractal) {

                if (sizeof(T) == sizeof(__cce_half)) {
                    DataCopy(dstTensor[j * (width + leftPad + rightPad) + widthFractal * 16 * tmp1BlockNum], tmp2,
                        { static_cast<uint16_t>(widthFractalTailAlingned / 16), 1, 15, 0 });
                } else if (sizeof(T) == sizeof(float)) {
                    if (widthFractalTailAlingned / 16 != 0) {
                        DataCopy(dstTensor[j * (width + leftPad + rightPad) + widthFractal * 16 * tmp1BlockNum], tmp2,
                            { static_cast<uint16_t>(widthFractalTailAlingned / 16), 2, 14, 0 });
                    }
                    if (widthFractalTailAlingned % 16) {
                        DataCopy(dstTensor[j * (width + leftPad + rightPad) + widthFractal * 16 * tmp1BlockNum +
                            widthFractalTailAlingned / 16 * 16],
                            tmp2[widthFractalTailAlingned / 16 * 16 * 8], { 1, 1, 15, 0 });
                    }
                }
            } else {

                if (sizeof(T) == sizeof(__cce_half)) {
                    DataCopy(dstTensor[j * (width + leftPad + rightPad) + i * 16 * tmp1BlockNum], tmp2,
                        { static_cast<uint16_t>(tmp1BlockNum), 1, 15, 0 });
                } else if (sizeof(T) == sizeof(float)) {
                    DataCopy(dstTensor[j * (width + leftPad + rightPad) + i * 16 * tmp1BlockNum], tmp2,
                        { static_cast<uint16_t>(tmp1BlockNum), 2, 14, 0 });
                }
            }
        }
    }
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_v220_impl.h" 2

namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void PadCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    PadParams &padParams, const LocalTensor<uint8_t> &sharedTmpBuffer, PadTiling &tiling)
{
    uint32_t width = tiling.srcWidth;


    if (width * sizeof(T) % ONE_BLK_SIZE == 0) {
        AlignedPad(dstTensor, srcTensor, padParams, tiling);
    } else {
        LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
        UnAlignedPad(dstTensor, srcTensor, padParams, tmpBuffer, tiling);
    }
}
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_v220_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UnPadCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    UnPadParams &unPadParams, LocalTensor<uint8_t> &sharedTmpBuffer, UnPadTiling &tiling)
{
    uint16_t rightPad = unPadParams.rightPad;
    uint16_t height = tiling.srcHeight;
    uint16_t width = tiling.srcWidth;

    GatherMaskParams reducev2Params;
    reducev2Params.repeatTimes = height;
    reducev2Params.src0RepeatStride = static_cast<uint16_t>(width * sizeof(T) / ONE_BLK_SIZE);
    uint64_t rsvdCnt = 0;
    GatherMask(dstTensor, srcTensor, REDUCEV2_MODE_SEVEN, true, (width - rightPad), reducev2Params, rsvdCnt);
    ResetMask();
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_common_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void PadImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, PadParams &padParams,
    const LocalTensor<uint8_t> &sharedTmpBuffer, PadTiling &tiling)
{
    PadCompute<T>(dstTensor, srcTensor, padParams, sharedTmpBuffer, tiling);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UnPadImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    UnPadParams &unPadParams, LocalTensor<uint8_t> &sharedTmpBuffer, UnPadTiling &tiling)
{

                                                                                                               ;






      ;
    UnPadCompute<T>(dstTensor, srcTensor, unPadParams, sharedTmpBuffer, tiling);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h" 2

namespace AscendC {
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h"
#pragma begin_pipe(V)
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Pad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, PadParams &padParams,
    const LocalTensor<uint8_t> &sharedTmpBuffer, PadTiling &tiling)
{
                             ;
    PadImpl<T>(dstTensor, srcTensor, padParams, sharedTmpBuffer, tiling);
                            ;
}
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Pad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, PadParams &padParams,
    PadTiling &tiling)
{
    LocalTensor<uint8_t> tmpBuffer;
    bool res = PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
                                                                               ;

    PadImpl<T>(dstTensor, srcTensor, padParams, tmpBuffer, tiling);
}
# 83 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UnPad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, UnPadParams &unPadParams,
     LocalTensor<uint8_t> &sharedTmpBuffer, UnPadTiling &tiling)
{
    UnPadImpl<T>(dstTensor, srcTensor, unPadParams, sharedTmpBuffer, tiling);
}
# 101 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UnPad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, UnPadParams &unPadParams,
    UnPadTiling &tiling)
{
    LocalTensor<uint8_t> tmpBuffer;
    bool res = PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
                                                                               ;

    UnPadImpl<T>(dstTensor, srcTensor, unPadParams, tmpBuffer, tiling);
}
#pragma end_pipe
}
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl_common.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl_common.h"
namespace AscendC {
constexpr uint32_t HCCL_GROUP_ID_0 = 0;
using HcclHandle = int8_t;

enum class HcclCMDType {
    HCCL_CMD_INVALID = 0,
    HCCL_CMD_BROADCAST = 1,
    HCCL_CMD_ALLREDUCE,
    HCCL_CMD_REDUCE,
    HCCL_CMD_SEND,
    HCCL_CMD_RECEIVE,
    HCCL_CMD_ALLGATHER,
    HCCL_CMD_REDUCE_SCATTER,
    HCCL_CMD_ALLTOALLV,
    HCCL_CMD_ALLTOALLVC,
    HCCL_CMD_ALLTOALL,
    HCCL_CMD_GATHER,
    HCCL_CMD_SCATTER,
    HCCL_CMD_BATCH_SEND_RECV,
    HCCL_CMD_BATCH_PUT,
    HCCL_CMD_BATCH_GET,
    HCCL_CMD_ALLGATHER_V,
    HCCL_CMD_REDUCE_SCATTER_V,
    HCCL_CMD_BATCH_WRITE,
    HCCL_CMD_ALL,

    HCCL_CMD_FINALIZE = 100,
    HCCL_CMD_INTER_GROUP_SYNC,
    HCCL_CMD_INIT,
    HCCL_CMD_MAX
};

enum HcclReduceOp {
    HCCL_REDUCE_SUM = 0,
    HCCL_REDUCE_PROD = 1,
    HCCL_REDUCE_MAX = 2,
    HCCL_REDUCE_MIN = 3,
    HCCL_REDUCE_RESERVED
};

enum class MC2_BUFFER_LOCATION {
    MC2_BUFFER_TYPE_DEFAULT = 0,
    MC2_BUFFER_TYPE_OUTPUT,
    MC2_BUFFER_TYPE_WINDOW_IN,
    MC2_BUFFER_TYPE_WINDOW_OUT,
    MC2_BUFFER_TYPE_WORKSPACE,
    MC2_BUFFER_TYPE_INPUT,
    MC2_BUFFER_TYPE_COMMOUT,
    MC2_BUFFER_TYPE_END
};

enum HcclServerType {
    HCCL_SERVER_TYPE_AICPU = 0,
    HCCL_SERVER_TYPE_END
};

enum class CoreType: uint8_t {
    DEFAULT,
    ON_AIV,
    ON_AIC
};

struct HcclServerConfig {
    CoreType type;
    int64_t blockId;
};




enum HcclDataType {
    HCCL_DATA_TYPE_INT8 = 0,
    HCCL_DATA_TYPE_INT16 = 1,
    HCCL_DATA_TYPE_INT32 = 2,
    HCCL_DATA_TYPE_FP16 = 3,
    HCCL_DATA_TYPE_FP32 = 4,
    HCCL_DATA_TYPE_INT64 = 5,
    HCCL_DATA_TYPE_UINT64 = 6,
    HCCL_DATA_TYPE_UINT8 = 7,
    HCCL_DATA_TYPE_UINT16 = 8,
    HCCL_DATA_TYPE_UINT32 = 9,
    HCCL_DATA_TYPE_FP64 = 10,
    HCCL_DATA_TYPE_BFP16 = 11,
    HCCL_DATA_TYPE_INT128 = 12,
    HCCL_DATA_TYPE_RESERVED
};
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_def.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_def.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_msg.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_msg.h"
namespace AscendC {
constexpr int32_t HCCL_FAILED = -1;
constexpr int32_t HCCL_SUCCESS = 0;
constexpr int32_t HCCL_MAX_HANDLE_ID = 63;
constexpr int8_t INVALID_HANDLE_ID = -1;
constexpr int8_t INVALID_MSG_POSITION = -1;

constexpr uint32_t HCCL_MAX_RANK_NUM = 32U;
constexpr uint32_t HCCL_MAX_RANK_NUM_V2 = 256;
constexpr uint32_t HCCL_MSG_CNT = 64;
constexpr uint32_t HCCL_MSG_VALID_MASK = 0x5CDF123A;

constexpr uint32_t HCCL_CCTILING_SIZE = 280;
constexpr uint32_t HCCL_CMD_TYPE_OFFSET = HCCL_CCTILING_SIZE - 8;
constexpr uint32_t HCCL_ALG_NAME_OFFSET = HCCL_CMD_TYPE_OFFSET - 128U;
constexpr uint32_t HCCL_STEP_SIZE_OFFSET = 2U;
constexpr uint32_t HCCL_DEBUG_MODE_OFFSET = 40U;

constexpr uint32_t U64_CNT_PER_CACHELINE = 8U;
constexpr uint8_t HCCL_MSG_EXT_RESERVED_CNT = 6U;
constexpr uint32_t HCCL_VALID_POS = 12U;
constexpr uint32_t HCCL_MSG_DATA_CNT = 16U;
constexpr uint32_t HCCL_DATA_TYPE_MAP = 18U;
constexpr uint8_t HCCL_ONLY_COMPUTE = 1U;

constexpr uint32_t MAX_DCCI_CNT = 64;

struct DataBlock {
    uint32_t data[HCCL_MSG_DATA_CNT];
};



struct V0MsgAdditionInfo {
    HcclDataType hcclDataType;
    uint32_t p2pSrcDestRankId;
    uint32_t valid;
    uint8_t repeatCnt;
    uint8_t everyTurnRsp;
    uint8_t everyTurnWait;
    int8_t commDepGroupID;

    HcclHandle commDepHandleID;

    HcclHandle selfHandleID;
    uint8_t seqNum;
    uint8_t version;
    uint32_t xorCheck;
};

struct V1MsgAdditionInfo {
    uint64_t ccOpTilingData;
    uint32_t valid;
    HcclDataType hcclDataType;
    uint8_t repeatCnt;
    HcclHandle selfHandleID;
    uint8_t seqNum;
    uint8_t version;
    uint32_t xorCheck;
};

struct HcclMsg {
    HcclCMDType commType;
    HcclReduceOp opType;
    uint64_t sendBuffer;
    uint64_t recvBuffer;
    uint64_t dataCnt;
    uint64_t strideCount;



    union {
        V0MsgAdditionInfo v0Msg;
        V1MsgAdditionInfo v1Msg;
    } addMsg;
};




struct HcclMsgExt {

    uint64_t sendCounts[HCCL_MAX_RANK_NUM_V2];

    uint64_t sendOffset[HCCL_MAX_RANK_NUM_V2];

    uint64_t recvCounts[HCCL_MAX_RANK_NUM_V2];

    uint64_t recvOffset[HCCL_MAX_RANK_NUM_V2];
    uint64_t reserved[HCCL_MSG_EXT_RESERVED_CNT];
    uint64_t valid;
    uint64_t xorCheck;
};

struct AlltoAllVParamExt {
    uint64_t *sendCounts;
    uint64_t *sdispls;
    uint64_t *recvCounts;
    uint64_t *rdispls;
    [aicore] __inline__ __attribute__((always_inline)) void AssembleHcclMsgExt(uint32_t rankDim, __attribute__((cce_global)) HcclMsgExt *dst) const {
        uint64_t xorCheck = 0U;
        for (uint32_t i = 0U; i < rankDim; ++i) {
            xorCheck ^= dst->sendCounts[i] = sendCounts[i];
            xorCheck ^= dst->sendOffset[i] = sdispls[i];
            xorCheck ^= dst->recvCounts[i] = recvCounts[i];
            xorCheck ^= dst->recvOffset[i] = rdispls[i];
        }
        dst->xorCheck = (xorCheck ^ HCCL_MSG_VALID_MASK);
        dst->valid = HCCL_MSG_VALID_MASK;
    }
};

constexpr uint64_t COMMIT_VALID_MASK = 987654321U;
constexpr uint64_t FINALIZE_FINISH_CNT = 1234567899999999999UL;


struct TurnCnt {
    uint64_t valid;
    uint64_t cnt;
    uint64_t reserved[6];
};

struct ControlHcclMsg {
    uint8_t restart;
    uint8_t restarting;
    uint8_t restartCnt;
    uint8_t resetSeq;
    uint8_t reserved[60];
};

constexpr uint32_t BYTE_PER_KB = 1024U;
constexpr uint32_t BYTE_PER_MB = BYTE_PER_KB * BYTE_PER_KB;




struct HcclMsgArea {
    HcclMsg sendMsgs[HCCL_MSG_CNT];
    HcclMsg recvMsgs[HCCL_MSG_CNT];
    uint8_t reserved0[8 * BYTE_PER_KB];





    TurnCnt commitTurnCnt[HCCL_MSG_CNT];
    TurnCnt finishedTurnCnt[HCCL_MSG_CNT];
    uint8_t reserved1[BYTE_PER_MB];
    HcclMsgExt paramExtMsgList[HCCL_MSG_CNT];
    ControlHcclMsg controlMsg;
};

struct CommonPrepareParam {
    HcclCMDType commType;
    __attribute__((cce_global)) uint8_t* sendBuf;
    __attribute__((cce_global)) uint8_t* recvBuf;
    uint64_t count;
    HcclDataType dataType;
    HcclReduceOp op;
    uint64_t strideCount;
    uint8_t repeat = 1U;
    AlltoAllVParamExt paramExt;
};

struct MemDetails {
    uint64_t size;
    uint64_t addr;
    uint32_t key;
};

struct IbVerbsData {
    MemDetails remoteInput;
    MemDetails remoteOutput;
    MemDetails localInput;
    MemDetails localOutput;
    uint8_t res[24];
};

struct HcclCombineOpParam {
    uint64_t workSpace;

    uint64_t workSpaceSize;
    uint32_t rankId;
    uint32_t rankNum;
    uint64_t winSize;
    uint64_t windowsIn[HCCL_MAX_RANK_NUM];


    uint64_t windowsOut[HCCL_MAX_RANK_NUM];







    uint8_t res[8328];

    uint8_t multiFlag;
    __attribute__((cce_global)) IbVerbsData *data;
};

constexpr uint16_t CCU_CKE_SIZE = 8;
constexpr uint16_t CCU_USED_XN_NUM = 8;
constexpr uint16_t CCU_MAX_MSG_NUM = 8;
constexpr uint16_t CCU_MSG_XN_NUM = 16;
constexpr uint64_t CCU_LOOP_COUNT = 64;
constexpr uint64_t ALIGN_64_BYTE = 64;
constexpr uint64_t CCU_MEMSLICE_SIZE = 4096;
struct CCUConfig {

    __attribute__((cce_global)) uint8_t* XnAddr;
    __attribute__((cce_global)) uint8_t* CKEAddr;

};

struct CCUMsg {
    __attribute__((cce_global)) uint8_t* XnData;
    __attribute__((cce_global)) uint8_t* XnAddr;
    __attribute__((cce_global)) uint8_t* commitCKEAddr;
    __attribute__((cce_global)) uint8_t* waitCKEAddr;
};

struct ReduceDataTypeAbility {
    HcclReduceOp op;
    HcclDataType dstDataType;
    HcclDataType srcDataType;
};

template <typename T, int Size>
class CircularFifo {
public:
    [aicore] CircularFifo() : m_head(0), m_tail(0), m_size(0)
    {}

    [aicore] __inline__ __attribute__((always_inline)) bool push(const T &value)
    {
        if (m_size == Size) {
            return false;
        }

        m_buffer[m_tail] = value;
        m_tail = (m_tail + 1) % Size;
        ++m_size;

        return true;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool pop(T &value)
    {
        if (m_size == 0) {
            return false;
        }

        value = m_buffer[m_head];
        m_head = (m_head + 1) % Size;
        --m_size;

        return true;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool isFull() const
    {
        return m_size == Size;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool isEmpty() const
    {
        return m_size == 0;
    }

    [aicore] __inline__ __attribute__((always_inline)) T Head() const
    {
        return m_buffer[m_head];
    }

    [aicore] __inline__ __attribute__((always_inline)) T Tail() const
    {
        return m_buffer[m_tail];
    }

public:
    int m_head;
    int m_tail;

private:
    T m_buffer[Size];
    int m_size;
};


struct CCUCommOp {
    int8_t resourceId;
    int8_t isFinish;
    uint8_t finishedCnt;
    uint8_t repeatCnt;
    uint8_t commitCnt;
    uint8_t waitCnt;
    uint64_t xnData[CCU_MSG_XN_NUM];
};

}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_def.h" 2

namespace AscendC {
template <HcclServerType serverType, const auto &config>
class HcclImpl {
public:
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AllReduce(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count,
                                           HcclDataType dataType, HcclReduceOp op, uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
                                           HcclDataType dataType, uint64_t strideCount, uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount,
                                               HcclDataType dataType, HcclReduceOp op, uint64_t strideCount,
                                               uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
                                          HcclDataType dataType, uint64_t strideCount = 0, uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls, HcclDataType sendType,
                                           __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType,
                                           uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle BatchWrite(__attribute__((cce_global)) uint8_t* batchWriteInfo, uint32_t itemNum);

public:
    [aicore] __inline__ __attribute__((always_inline)) void Init(__attribute__((cce_global)) uint8_t* context, __attribute__((cce_global)) void *initTiling = nullptr);

    [aicore] __inline__ __attribute__((always_inline)) int32_t SetCcTiling(__attribute__((cce_global)) void *ccOpTilingData);

    [aicore] __inline__ __attribute__((always_inline)) void Commit(HcclHandle handleId);

    [aicore] __inline__ __attribute__((always_inline)) int32_t Wait(HcclHandle handleId);

    [aicore] __inline__ __attribute__((always_inline)) int32_t Query(HcclHandle handleId);

    [aicore] __inline__ __attribute__((always_inline)) void InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID);

    template <bool sync = true>
    [aicore] __inline__ __attribute__((always_inline)) int32_t Iterate(HcclHandle handleId, uint16_t *seqSlices, uint16_t seqSliceLen);

    [aicore] __inline__ __attribute__((always_inline)) void Finalize();

public:
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetWindowsInAddr(uint32_t rankId);

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetWindowsOutAddr(uint32_t rankId);

    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetRankId() { return hcclContext_->rankId; }

    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetRankDim() { return hcclContext_->rankNum; }

private:


    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle CommonPrepareImpl(const CommonPrepareParam &param);

    [aicore] __inline__ __attribute__((always_inline)) bool CheckCommonPrepareParamValid(const CommonPrepareParam &param);


    [aicore] __inline__ __attribute__((always_inline)) void ResetFinishedTurnCnt();

    [aicore] __inline__ __attribute__((always_inline)) void SendMsgToServer(const CommonPrepareParam &para,
                                           int8_t srcGroupID = -1, HcclHandle srcHandleID = INVALID_HANDLE_ID);

    [aicore] __inline__ __attribute__((always_inline)) void SendMsgToServer(const AlltoAllVParamExt &para);

    [aicore] __inline__ __attribute__((always_inline)) uint16_t GetStepSizeByHandle(HcclHandle handle);

    [aicore] __inline__ __attribute__((always_inline)) uint16_t GetStepCntsPerRepeatByHandle(HcclHandle handle);

    [aicore] __inline__ __attribute__((always_inline)) void SetCommitTurnCntToGm(uint8_t msgPos, uint64_t turnCnt);

    [aicore] __inline__ __attribute__((always_inline)) uint64_t WaitFinishCntFromGm(uint8_t msgPos, uint64_t expectedCnt);

private:
    uint64_t ccOpTilingDataTable_[static_cast<uint32_t>(HcclCMDType::HCCL_CMD_ALL)] = {0UL};
    __attribute__((cce_global)) HcclCombineOpParam *hcclContext_;
    __attribute__((cce_global)) HcclMsgArea *hcclMsgArea_;
    uint16_t handleId2CurrSliceId_[HCCL_MAX_HANDLE_ID] = {0U};
    uint16_t handleIdCommitTurnCnt_[HCCL_MAX_HANDLE_ID] = {0U};
    uint16_t handleIdWaitCallNum_[HCCL_MAX_HANDLE_ID] = {0U};
    uint8_t handleId2CmdType_[HCCL_MAX_HANDLE_ID] = {0U};
    int8_t handleIdMsgPosition_[HCCL_MAX_HANDLE_ID];
    uint8_t handleIdRepeat_[HCCL_MAX_HANDLE_ID] = {0U};
    HcclHandle curHandleId_ = INVALID_HANDLE_ID;



    uint8_t curMsgPosition_ = 0U;
    int8_t curVersion_ = -1;
    uint8_t workingFlag_ = false;
    uint8_t debugMode_ = 0U;
# 154 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_def.h"
};
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h" 2

namespace AscendC {
static constexpr HcclServerConfig DEFAULT_CFG = {CoreType::DEFAULT, 0};
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
template <HcclServerType serverType = HcclServerType::HCCL_SERVER_TYPE_AICPU, const auto &config = DEFAULT_CFG>
class Hccl {
public:
# 65 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AllReduce(
        __attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count, HcclDataType dataType, HcclReduceOp op, uint8_t repeat = 1);
# 91 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
                                           HcclDataType dataType, uint64_t strideCount, uint8_t repeat = 1);
# 119 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount,
        HcclDataType dataType, HcclReduceOp op, uint64_t strideCount, uint8_t repeat = 1);
# 150 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
                                          HcclDataType dataType, uint64_t strideCount = 0, uint8_t repeat = 1);
# 187 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls, HcclDataType sendType,
                                           __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType,
                                           uint8_t repeat = 1);
# 214 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle BatchWrite(__attribute__((cce_global)) uint8_t* batchWriteInfo, uint32_t itemNum);

public:







    [aicore] __inline__ __attribute__((always_inline)) int32_t SetCcTiling(__attribute__((cce_global)) void *ccOpTilingData);







    [aicore] __inline__ __attribute__((always_inline)) void Init(__attribute__((cce_global)) uint8_t* context, __attribute__((cce_global)) void *initTiling = nullptr);
# 244 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) void Commit(HcclHandle handleId);
# 256 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) int32_t Wait(HcclHandle handleId);
# 266 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) int32_t Query(HcclHandle handleId);
# 277 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) void InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID);
# 288 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool sync = true>
    [aicore] __inline__ __attribute__((always_inline)) int32_t Iterate(HcclHandle handleId, uint16_t *seqSlices, uint16_t seqSliceLen);
# 299 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) void Finalize();

public:
# 311 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetWindowsInAddr(uint32_t rankId);
# 322 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetWindowsOutAddr(uint32_t rankId);
# 331 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetRankId();
# 340 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetRankDim();
# 350 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) bool SetReduceDataTypeAbility(HcclReduceOp op,
        HcclDataType dstDataType, HcclDataType srcDataType);

private:
    HcclImpl<serverType, config> impl_;
};
}

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_v220_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_common.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_common.h"
namespace AscendC {
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_common.h"
[aicore] __inline__ __attribute__((always_inline)) void FlushDataCache(GlobalTensor<int64_t> &globalHcclMsgArea, __attribute__((cce_global)) void *gmAddr)
{
    AscendC::Barrier();
    globalHcclMsgArea.SetGlobalBuffer((__attribute__((cce_global)) int64_t *)gmAddr);
    __asm__("NOP");
    DataCacheCleanAndInvalid<int64_t, CacheLine::SINGLE_CACHE_LINE, DcciDst::CACHELINE_OUT>(globalHcclMsgArea);
    DataSyncBarrier<MemDsbT::ALL>();
}

[aicore] __inline__ __attribute__((always_inline)) void FlushDataCache(__attribute__((cce_global)) void *gmAddr)
{
    GlobalTensor<int64_t> globalHcclMsgArea;
    FlushDataCache(globalHcclMsgArea, gmAddr);
}

[aicore] __inline__ __attribute__((always_inline)) void CopyHcclMsg(const uint8_t *src, __attribute__((cce_global)) HcclMsg *dst)
{
    __attribute__((cce_global)) DataBlock *tmpDst = reinterpret_cast<__attribute__((cce_global)) DataBlock *>(dst);
    volatile uint32_t xorCheck = 0U;
    for (uint32_t i = 0; i < HCCL_MSG_DATA_CNT - 1U; ++i) {
        if (i == HCCL_VALID_POS) {
            xorCheck ^= HCCL_MSG_VALID_MASK;
        } else {
            xorCheck ^= tmpDst->data[i] = *(reinterpret_cast<const uint32_t *>(src));
        }
        src += sizeof(tmpDst->data[i]);
    }
    tmpDst->data[HCCL_MSG_DATA_CNT - 1U] = xorCheck;
    tmpDst->data[HCCL_VALID_POS] = HCCL_MSG_VALID_MASK;
}

[aicore] __inline__ __attribute__((always_inline)) void AssembleHcclMsg(const CommonPrepareParam &para,
    int8_t ver, HcclHandle handle, uint64_t tiling, __attribute__((cce_global)) HcclMsg *dst, __attribute__((cce_global)) HcclMsgArea *hcclMsgArea_)
{
    HcclMsg tmp;
    static uint8_t primitiveId = 0U;
# 85 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_common.h"
    tmp.commType = para.commType;
    if (para.commType == HcclCMDType::HCCL_CMD_FINALIZE) {
        primitiveId = 0U;
        if (ver != 0) {
            tmp.addMsg.v1Msg.ccOpTilingData = 0UL;
        }
    } else {
        tmp.opType = para.op;
        tmp.sendBuffer = reinterpret_cast<uint64_t>(para.sendBuf);
        tmp.recvBuffer = reinterpret_cast<uint64_t>(para.recvBuf);
        tmp.dataCnt = para.count;
        tmp.strideCount = para.strideCount;
        if (ver == 0) {
            tmp.addMsg.v0Msg.hcclDataType = para.dataType;
            tmp.addMsg.v0Msg.repeatCnt = para.repeat;
            tmp.addMsg.v0Msg.selfHandleID = handle;
            tmp.addMsg.v0Msg.seqNum = primitiveId++;
            tmp.addMsg.v0Msg.version = ver;
        } else {
            tmp.addMsg.v1Msg.ccOpTilingData = tiling;
            tmp.addMsg.v1Msg.hcclDataType = para.dataType;
            tmp.addMsg.v1Msg.repeatCnt = para.repeat;
            tmp.addMsg.v1Msg.selfHandleID = handle;
            tmp.addMsg.v1Msg.seqNum = primitiveId++;
            tmp.addMsg.v1Msg.version = ver;
        }
    }
    if (ver == 0) {
        tmp.addMsg.v0Msg.valid = HCCL_MSG_VALID_MASK;
    } else {
        tmp.addMsg.v1Msg.valid = HCCL_MSG_VALID_MASK;
    }
    CopyHcclMsg(reinterpret_cast<const uint8_t *>(&tmp), dst);
}

[aicore] __inline__ __attribute__((always_inline)) void AssembleHcclMsg(const CommonPrepareParam &para, int8_t srcGroupID, HcclHandle srcHandleID, __attribute__((cce_global)) HcclMsg *dst)
{
    HcclMsg tmp;
    tmp.commType = para.commType;
    tmp.addMsg.v0Msg.commDepGroupID = srcGroupID;
    tmp.addMsg.v0Msg.commDepHandleID = srcHandleID;
    tmp.addMsg.v0Msg.valid = HCCL_MSG_VALID_MASK;
    CopyHcclMsg(reinterpret_cast<const uint8_t *>(&tmp), dst);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) bool HcclImpl<serverType, config>::CheckCommonPrepareParamValid(const CommonPrepareParam &param)
{
    const HcclCMDType commType = param.commType;
    if (curVersion_ > 0) {


                                                                ;
    } else {


                                                                ;
    }


                                                                                             ;



                                                                                                 ;
    if (commType == HcclCMDType::HCCL_CMD_ALLTOALLV) {



                                                                                                                      ;
    } else {


                                                               ;
    }
    return true;
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
HcclImpl<serverType, config>::AllReduce(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count, HcclDataType dataType,
                                        HcclReduceOp op, uint8_t repeat)
{

                                                                                                                  ;

    return CommonPrepareImpl<commit>({ HcclCMDType::HCCL_CMD_ALLREDUCE, sendBuf, recvBuf, count, dataType,
                                       op, 0, repeat });
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
HcclImpl<serverType, config>::AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount, HcclDataType dataType,
                                        uint64_t strideCount, uint8_t repeat)
{
    return CommonPrepareImpl<commit>({ HcclCMDType::HCCL_CMD_ALLGATHER, sendBuf, recvBuf, sendCount, dataType,
                                       HCCL_REDUCE_RESERVED, strideCount, repeat });
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
HcclImpl<serverType, config>::ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount, HcclDataType dataType,
                                            HcclReduceOp op, uint64_t strideCount, uint8_t repeat)
{

                                                                                                                      ;
    return CommonPrepareImpl<commit>({ HcclCMDType::HCCL_CMD_REDUCE_SCATTER, sendBuf, recvBuf, recvCount,
                                       dataType, op, strideCount, repeat });
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
HcclImpl<serverType, config>::AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount, HcclDataType dataType,
                                       uint64_t strideCount, uint8_t repeat)
{
    return CommonPrepareImpl<commit>({ HcclCMDType::HCCL_CMD_ALLTOALL, sendBuf, recvBuf, dataCount, dataType,
                                       HCCL_REDUCE_RESERVED, strideCount, repeat });
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
HcclImpl<serverType, config>::AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls, HcclDataType sendType,
                                        __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType,
                                        uint8_t repeat)
{


                                                                                           ;
    return CommonPrepareImpl<commit>({ HcclCMDType::HCCL_CMD_ALLTOALLV, sendBuf, recvBuf, 0U, sendType,
                                       HCCL_REDUCE_RESERVED, 0U, repeat,
                                       {static_cast<uint64_t *>(sendCounts), static_cast<uint64_t *>(sdispls),
                                        static_cast<uint64_t *>(recvCounts), static_cast<uint64_t *>(rdispls)} });
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle HcclImpl<serverType, config>::BatchWrite(__attribute__((cce_global)) uint8_t* batchWriteInfo, uint32_t itemNum)
{
    return CommonPrepareImpl<true>({HcclCMDType::HCCL_CMD_BATCH_WRITE, batchWriteInfo, batchWriteInfo, itemNum,
                                    HCCL_DATA_TYPE_INT8});
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) int32_t HcclImpl<serverType, config>::SetCcTiling(__attribute__((cce_global)) void *ccOpTilingData)
{

                                                                                                              ;

                                                                                            ;
    auto ccTilingPtr = reinterpret_cast<__attribute__((cce_global)) char *>(ccOpTilingData);
    auto cmdType = *(reinterpret_cast<__attribute__((cce_global)) uint32_t *>(ccTilingPtr + HCCL_CMD_TYPE_OFFSET));

                                                                                                        ;
                                                                                                                       ;
    ccOpTilingDataTable_[cmdType] = reinterpret_cast<uint64_t>(ccOpTilingData);
    return HCCL_SUCCESS;
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) int32_t HcclImpl<serverType, config>::Query(HcclHandle handleId)
{

                                                                                                      ;


                                                         ;
    int8_t curMsgPos = handleIdMsgPosition_[handleId];

                                                                                                          ;
    return WaitFinishCntFromGm(curMsgPos, 0UL);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType, config>::InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID)
{

                                                                                                                   ;
    CommonPrepareParam param = {HcclCMDType::HCCL_CMD_INTER_GROUP_SYNC};
    SendMsgToServer(param, srcGroupID, srcHandleID);
    ++curMsgPosition_;

                                                                                        ;
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* HcclImpl<serverType, config>::GetWindowsInAddr(uint32_t rankId)
{

                                                                                                                ;
    if (hcclContext_->multiFlag == 0U) {
        return (__attribute__((cce_global)) uint8_t*)hcclContext_->windowsIn[rankId];
    } else {
        if (rankId == hcclContext_->rankId) {
            return (__attribute__((cce_global)) uint8_t*)(hcclContext_->data[rankId].localInput.addr);
        } else {
            return (__attribute__((cce_global)) uint8_t*)(hcclContext_->data[rankId].remoteInput.addr);
        }
    }
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* HcclImpl<serverType, config>::GetWindowsOutAddr(uint32_t rankId)
{

                                                                                                                 ;
    if (hcclContext_->multiFlag == 0U) {
        return (__attribute__((cce_global)) uint8_t*)hcclContext_->windowsOut[rankId];
    } else {
        if (rankId == hcclContext_->rankId) {
            return (__attribute__((cce_global)) uint8_t*)(hcclContext_->data[rankId].localOutput.addr);
        } else {
            return (__attribute__((cce_global)) uint8_t*)(hcclContext_->data[rankId].remoteOutput.addr);
        }
    }
}

}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_v220_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_control.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_control.h"
namespace AscendC {

constexpr uint8_t SINGLE_COMM_NUM = 1;
constexpr uint8_t MULTI_COMM_NUM = 2;

[aicore] __inline__ __attribute__((always_inline)) void GetRestartFromContext(__attribute__((cce_global)) uint8_t* context, uint8_t &restart)
{
    if (context == nullptr) {
        return;
    }
    __attribute__((cce_global)) HcclCombineOpParam *hcclContext = (__attribute__((cce_global)) HcclCombineOpParam *)context;
    uint64_t msgAddr = hcclContext->workSpace;
    __attribute__((cce_global)) HcclMsgArea *hcclMsgArea = (__attribute__((cce_global)) HcclMsgArea *)msgAddr;
    __attribute__((cce_global)) ControlHcclMsg *controlMsgGM = &hcclMsgArea->controlMsg;
    FlushDataCache(controlMsgGM);
    restart += controlMsgGM->restart;
}

[aicore] __inline__ __attribute__((always_inline)) void ResetRestartFlag(__attribute__((cce_global)) uint8_t* context)
{
    if (context == nullptr) {
        return;
    }
    __attribute__((cce_global)) HcclCombineOpParam *hcclContext = (__attribute__((cce_global)) HcclCombineOpParam *)context;
    uint64_t msgAddr = hcclContext->workSpace;
    __attribute__((cce_global)) HcclMsgArea *hcclMsgArea = (__attribute__((cce_global)) HcclMsgArea *)msgAddr;
    __attribute__((cce_global)) ControlHcclMsg *controlMsgGM = &hcclMsgArea->controlMsg;
    controlMsgGM->restart = 0;
    controlMsgGM->restarting = 1;
    controlMsgGM->resetSeq = 1;
    FlushDataCache(controlMsgGM);
}

[aicore] __inline__ __attribute__((always_inline)) uint8_t GetRestart(uint8_t ctxNum)
{
    uint8_t restart = 0;

    if (ctxNum >= SINGLE_COMM_NUM) {
        GetRestartFromContext(AscendC::GetHcclContext<0>(), restart);
    }
    if (ctxNum >= MULTI_COMM_NUM) {
        GetRestartFromContext(AscendC::GetHcclContext<1>(), restart);
    }
    return restart;
}

[aicore] __inline__ __attribute__((always_inline)) void SetRestart(uint8_t ctxNum)
{
    if (GetBlockIdx() == 0) {

        if (ctxNum >= SINGLE_COMM_NUM) {
            ResetRestartFlag(AscendC::GetHcclContext<0>());
        }
        if (ctxNum >= MULTI_COMM_NUM) {
            ResetRestartFlag(AscendC::GetHcclContext<1>());
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) bool CheckIfRestart(__attribute__((cce_global)) HcclMsgArea *hcclMsgArea_)
{

    __attribute__((cce_global)) ControlHcclMsg *controlMsgGM = &hcclMsgArea_->controlMsg;
    FlushDataCache(controlMsgGM);
    if (controlMsgGM->restart > 0) {
        return true;
    }
    return false;
}

}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_v220_impl.h" 2

namespace AscendC {

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType, config>::SendMsgToServer(const CommonPrepareParam &para,
                                                                     int8_t srcGroupID, HcclHandle srcHandleID)
{
    if (!workingFlag_) {
        return;
    }
    __attribute__((cce_global)) HcclMsg *hcclSendMsg = &(hcclMsgArea_->sendMsgs[curMsgPosition_]);
    do {





        FlushDataCache(hcclSendMsg);
    } while ((debugMode_ != HCCL_ONLY_COMPUTE) &&
             ((curVersion_ == 0 && hcclSendMsg->addMsg.v0Msg.valid == HCCL_MSG_VALID_MASK) ||
              (curVersion_ != 0 && hcclSendMsg->addMsg.v1Msg.valid == HCCL_MSG_VALID_MASK)));
                                                                                   ;
    if (srcGroupID < 0) {
        AssembleHcclMsg(para, curVersion_, curHandleId_,
                             ccOpTilingDataTable_[static_cast<uint32_t>(para.commType)], hcclSendMsg, hcclMsgArea_);
    } else {
        AssembleHcclMsg(para, srcGroupID, srcHandleID, hcclSendMsg);
    }
    FlushDataCache(reinterpret_cast<__attribute__((cce_global)) void *>(hcclSendMsg));
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType, config>::SendMsgToServer(const AlltoAllVParamExt &para)
{
    if (!workingFlag_) {
        return;
    }
    __attribute__((cce_global)) HcclMsgExt *hcclSendMsg = &(hcclMsgArea_->paramExtMsgList[curMsgPosition_]);
    do {





        FlushDataCache(hcclSendMsg);
    } while ((debugMode_ != HCCL_ONLY_COMPUTE) && (hcclSendMsg->valid == HCCL_MSG_VALID_MASK));
                                                                                      ;
    para.AssembleHcclMsgExt(hcclContext_->rankNum, hcclSendMsg);
    GlobalTensor<int64_t> globalHcclMsgArea;
    for (uint32_t i = 0U; i < hcclContext_->rankNum; i += U64_CNT_PER_CACHELINE) {
        FlushDataCache(globalHcclMsgArea, (hcclSendMsg->sendCounts + i));
        FlushDataCache(globalHcclMsgArea, (hcclSendMsg->sendOffset + i));
        FlushDataCache(globalHcclMsgArea, (hcclSendMsg->recvCounts + i));
        FlushDataCache(globalHcclMsgArea, (hcclSendMsg->recvOffset + i));
    }
    FlushDataCache(globalHcclMsgArea, hcclSendMsg->reserved);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) uint16_t HcclImpl<serverType, config>::GetStepSizeByHandle(HcclHandle handle)
{
    const uint8_t commType = handleId2CmdType_[handle];
    if (commType != static_cast<uint8_t>(HcclCMDType::HCCL_CMD_ALLTOALLV)) {
        return 0U;
    }
    __attribute__((cce_global)) uint8_t *tilingPtr = reinterpret_cast<__attribute__((cce_global)) uint8_t *>(ccOpTilingDataTable_[commType]);
    if (tilingPtr == nullptr) {
        return 0U;
    }
    return *(tilingPtr + HCCL_STEP_SIZE_OFFSET);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) uint16_t HcclImpl<serverType, config>::GetStepCntsPerRepeatByHandle(HcclHandle handle)
{
    return (GetStepSizeByHandle(handle) == 0U ? 1U : GetRankDim());
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType, config>::SetCommitTurnCntToGm(uint8_t msgPos, uint64_t turnCnt)
{
    if (!workingFlag_) {
        return;
    }

    __attribute__((cce_global)) TurnCnt *commitGM = hcclMsgArea_->commitTurnCnt + msgPos;
    do {





        FlushDataCache(commitGM);
    } while ((debugMode_ != HCCL_ONLY_COMPUTE) && (commitGM->cnt >= turnCnt));
                                                                                                      ;
    commitGM->cnt = turnCnt;
    commitGM->valid = COMMIT_VALID_MASK;
    FlushDataCache(commitGM);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) uint64_t HcclImpl<serverType, config>::WaitFinishCntFromGm(uint8_t msgPos, uint64_t expectedCnt)
{
    __attribute__((cce_global)) TurnCnt *finishGM = hcclMsgArea_->finishedTurnCnt + msgPos;
    GlobalTensor<int64_t> globalHcclMsgArea;
    while (true) {





        FlushDataCache(globalHcclMsgArea, finishGM);
        if ((debugMode_ == HCCL_ONLY_COMPUTE) || (finishGM->cnt >= expectedCnt)) {
            break;
        }
    }
    return finishGM->cnt;
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle HcclImpl<serverType, config>::CommonPrepareImpl(const CommonPrepareParam &param)
{






    if (__builtin_expect(!!(param.repeat == 0U), 0)) {
        return INVALID_HANDLE_ID;
    }


                                                                 ;

    HcclHandle handleId = ++curHandleId_;


                                                                                                   ;
    if (param.commType == HcclCMDType::HCCL_CMD_ALLTOALLV) {
        SendMsgToServer(param.paramExt);
    }
    SendMsgToServer(param);
    handleIdMsgPosition_[handleId] = curMsgPosition_;
    handleIdRepeat_[handleId] = param.repeat;
    handleId2CmdType_[handleId] = static_cast<uint8_t>(param.commType);
    if constexpr (commit) {
        handleIdCommitTurnCnt_[handleId] = param.repeat * GetStepCntsPerRepeatByHandle(handleId);
        SetCommitTurnCntToGm(curMsgPosition_, handleIdCommitTurnCnt_[handleId]);
    }
    ++curMsgPosition_;

                                                                                     ;
    return handleId;
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType, config>::Init(__attribute__((cce_global)) uint8_t* context, __attribute__((cce_global)) void *initTiling)
{
                                                                                                          ;
    hcclContext_ = (__attribute__((cce_global)) HcclCombineOpParam *)context;

    uint64_t msgAddr = hcclContext_->workSpace;
    if (msgAddr & 0x1ff) {
        msgAddr = (msgAddr & (~((uint64_t)0x1ff))) + 0x200;
    }
    if (__builtin_expect(!!((msgAddr == 0UL) || (initTiling == nullptr && curVersion_ > 0) || (initTiling != nullptr && curVersion_ == 0)), 0)) {

                                                                                                  ;
        curVersion_ = -1;
        return;
    }
    if (initTiling != nullptr) {
        curVersion_ = 1;
        auto initTilingPtr = reinterpret_cast<__attribute__((cce_global)) char *>(initTiling);
        debugMode_ = *(reinterpret_cast<__attribute__((cce_global)) uint8_t *>(initTilingPtr + HCCL_DEBUG_MODE_OFFSET));
    } else {
        curVersion_ = 0;
    }
    hcclMsgArea_ = (__attribute__((cce_global)) HcclMsgArea *)msgAddr;
    for (uint32_t i = 0U; i < HCCL_MAX_HANDLE_ID; ++i) {
        handleIdMsgPosition_[i] = INVALID_MSG_POSITION;
    }
    using T = decltype(config);
    static_assert(std::is_same<T, const HcclServerConfig &>::value);
                                                                                                             ;
    if constexpr (config.type == CoreType::ON_AIV) {
        workingFlag_ = (g_coreType == AscendC::AIV && GetBlockIdx() == config.blockId);
    } else if constexpr (config.type == CoreType::ON_AIC) {
        workingFlag_ = (g_coreType == AscendC::AIC && GetBlockIdx() == config.blockId);
    } else {
        workingFlag_ = (GetBlockIdx() == config.blockId);
    }
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) int32_t HcclImpl<serverType, config>::Wait(HcclHandle handleId)
{







                                                                                                     ;
    if (__builtin_expect(!!(handleId <= INVALID_HANDLE_ID || handleId >= HCCL_MAX_HANDLE_ID), 0)) {

                                                ;
        return HCCL_FAILED;
    }
    uint16_t &waitCnt = handleIdWaitCallNum_[handleId];
    if (__builtin_expect(!!(waitCnt >= handleIdCommitTurnCnt_[handleId]), 0)) {

                                                                                                                 ;
        return HCCL_FAILED;
    }
    int8_t curMsgPos = handleIdMsgPosition_[handleId];

                                                                                                         ;
    const uint16_t stepSize = GetStepSizeByHandle(handleId);
    waitCnt += (stepSize == 0U ? 1U : stepSize);
    (void)WaitFinishCntFromGm(curMsgPos, waitCnt);
    return HCCL_SUCCESS;
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType, config>::Commit(HcclHandle handleId)
{







                                                                                                       ;
    if (__builtin_expect(!!(handleId <= INVALID_HANDLE_ID || handleId >= HCCL_MAX_HANDLE_ID), 0)) {

                                                ;
        return;
    }
    uint16_t &commitCnt = handleIdCommitTurnCnt_[handleId];
    if (__builtin_expect(!!(commitCnt >= handleIdRepeat_[handleId] * GetStepCntsPerRepeatByHandle(handleId)), 0)) {


                                                                                                    ;
        return;
    }
    const uint16_t stepSize = GetStepSizeByHandle(handleId);
    commitCnt += (stepSize == 0U ? 1U : stepSize);
    SetCommitTurnCntToGm(handleIdMsgPosition_[handleId], commitCnt);
}

template <HcclServerType serverType, const auto &config>
template <bool sync>
[aicore] __inline__ __attribute__((always_inline)) int32_t
HcclImpl<serverType, config>::Iterate(HcclHandle handleId, uint16_t *seqSlices, uint16_t seqSliceLen)
{
                                                                                                                    ;

                                                         ;
    const uint16_t stepSize = GetStepSizeByHandle(handleId);
    const uint16_t stepsPerRepeat = GetStepCntsPerRepeatByHandle(handleId);

                                                                              ;
    uint16_t &curSlice = handleId2CurrSliceId_[handleId];

                                                            ;


    if (curSlice >= stepsPerRepeat * handleIdRepeat_[handleId]) {
                                                                                                         ;
        return 0;
    }
    const uint16_t slicesPerRepeat = stepsPerRepeat;
    const uint32_t rankId = GetRankId();
    const uint32_t rankDim = GetRankDim();
                                                                                        ;
    for (uint16_t i = 0U; i < seqSliceLen; ++i) {
        if constexpr (sync) {
            if ((curSlice + 1) % stepSize == 0) {
                (void)Wait(handleId);
            }
            seqSlices[i] = (rankId + rankDim - curSlice % slicesPerRepeat) % rankDim;
        } else {
            seqSlices[i] = (rankId + curSlice % slicesPerRepeat) % rankDim;
        }
        ++curSlice;
    }
    return seqSliceLen;
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType, config>::Finalize()
{







                                                                                                         ;

    if (workingFlag_) {


        if (curHandleId_ > INVALID_HANDLE_ID) {
                                                                                                                 ;
            while ((debugMode_ != HCCL_ONLY_COMPUTE) && (Query(curHandleId_) < handleIdRepeat_[curHandleId_])) {





            }
        }


                                                  ;
        CommonPrepareParam param = {HcclCMDType::HCCL_CMD_FINALIZE};
        SendMsgToServer(param);

                                                                                                            ;

        __attribute__((cce_global)) TurnCnt *finishGM = hcclMsgArea_->finishedTurnCnt + curMsgPosition_;
                                                                                                        ;
        do {





            FlushDataCache(finishGM);
        } while ((debugMode_ != HCCL_ONLY_COMPUTE) && (finishGM->cnt != FINALIZE_FINISH_CNT));

                                                                                                ;
        ResetFinishedTurnCnt();
    }
    ++curMsgPosition_;

                                                                                      ;
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType, config>::ResetFinishedTurnCnt()
{
    __attribute__((cce_global)) TurnCnt *finishArea = hcclMsgArea_->finishedTurnCnt;
    GlobalTensor<int64_t> globalHcclMsgArea;
    for (uint32_t i = 0U; i <= curMsgPosition_; ++i) {
        __attribute__((cce_global)) TurnCnt *finishGM = finishArea + i;
        finishGM->cnt = 0;
        FlushDataCache(globalHcclMsgArea, finishGM);
    }
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl.h" 2




# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_dfx.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_dfx.h"
namespace AscendC {
enum class HcclApiOperType : uint32_t {
    MIN_TYPE = 0x1000,

    INIT = MIN_TYPE,
    COMMIT = 0x1010,
    WAIT = 0x1020,
    QUERY = 0x1030,
    FINALIZE = 0x1040,
    GROUP_SYNC = 0x1050,
    GET_WINDOW_IN = 0x1060,
    GET_WINDOW_OUT = 0x1062,
    GET_RANK_ID = 0x1064,
    GET_RANK_DIM = 0x1066,
    SET_CCTILING = 0x1068,
    ITERATE = 0x1070,

    ALL_REDUCE_PREPARE = 0x1100,
    ALL_GATHER_PREPARE = 0x1110,
    REDUCE_SCATTER_PREPARE = 0x1120,
    ALL_TO_ALL_PREPARE = 0x1130,
    ALL_TO_ALL_V_PREPARE = 0x1140,
    BATCH_WRITE_PREPARE = 0x1150,

    MAX_TYPE = 0x1FFF
};
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_dfx.h"
class DfxScopeGuard {
public:
    [aicore] __inline__ __attribute__((always_inline)) DfxScopeGuard(HcclApiOperType operType): operType_(operType) {
        PrintTimeStamp(static_cast<uint32_t>(operType_));



    }
# 147 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_dfx.h"
    [aicore] __inline__ __attribute__((always_inline)) ~DfxScopeGuard() {



        PrintTimeStamp(static_cast<uint32_t>(operType_) + 1);
    }

private:
    HcclApiOperType operType_;







};
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl.h" 2
namespace AscendC {
template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle Hccl<serverType, config>::AllReduce(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count,
                                                         HcclDataType dataType, HcclReduceOp op, uint8_t repeat)
{

                                            ;
    return impl_.template AllReduce<commit>(sendBuf, recvBuf, count, dataType, op, repeat);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle Hccl<serverType, config>::AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
                                                         HcclDataType dataType, uint64_t strideCount, uint8_t repeat)
{

                                                     ;
    return impl_.template AllGather<commit>(sendBuf, recvBuf, sendCount, dataType, strideCount, repeat);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle Hccl<serverType, config>::AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
                                                        HcclDataType dataType, uint64_t strideCount, uint8_t repeat)
{

                                                              ;
    return impl_.template AlltoAll<commit>(sendBuf, recvBuf, dataCount, dataType, strideCount, repeat);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
Hccl<serverType, config>::AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls, HcclDataType sendType,
                            __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType,
                            uint8_t repeat)
{

                                                                                                  ;
    return impl_.template AlltoAllV<commit>(sendBuf, sendCounts, sdispls, sendType,
                                            recvBuf, recvCounts, rdispls, recvType, repeat);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
Hccl<serverType, config>::ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount, HcclDataType dataType,
                                HcclReduceOp op, uint64_t strideCount, uint8_t repeat)
{

                                                     ;
    return impl_.template ReduceScatter<commit>(sendBuf, recvBuf, recvCount, dataType, op, strideCount,
                                                repeat);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle Hccl<serverType, config>::BatchWrite(__attribute__((cce_global)) uint8_t* batchWriteInfo, uint32_t itemNum)
{
                                                        ;
    return impl_.template BatchWrite<commit>(batchWriteInfo, itemNum);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void Hccl<serverType, config>::Init(__attribute__((cce_global)) uint8_t* context, __attribute__((cce_global)) void *initTiling)
{
                                         ;
    impl_.Init(context, initTiling);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) int32_t Hccl<serverType, config>::SetCcTiling(__attribute__((cce_global)) void *ccOpTilingData)
{
                                                 ;
    return impl_.SetCcTiling(ccOpTilingData);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void Hccl<serverType, config>::Commit(HcclHandle handleId)
{
                                           ;
    impl_.Commit(handleId);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) int32_t Hccl<serverType, config>::Wait(HcclHandle handleId)
{
                                         ;
    return impl_.Wait(handleId);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) int32_t Hccl<serverType, config>::Query(HcclHandle handleId)
{
                                          ;
    return impl_.Query(handleId);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void Hccl<serverType, config>::InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID)
{
                                               ;
    impl_.InterHcclGroupSync(srcGroupID, srcHandleID);
}

template <HcclServerType serverType, const auto &config>
template <bool sync>
[aicore] __inline__ __attribute__((always_inline)) int32_t Hccl<serverType, config>::Iterate(HcclHandle handleId, uint16_t *seqSlices,
                                                            uint16_t seqSliceLen)
{
                                            ;
    return impl_.template Iterate<sync>(handleId, seqSlices, seqSliceLen);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) void Hccl<serverType, config>::Finalize()
{
                                             ;
    impl_.Finalize();
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* Hccl<serverType, config>::GetWindowsInAddr(uint32_t rankId)
{
    return impl_.GetWindowsInAddr(rankId);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* Hccl<serverType, config>::GetWindowsOutAddr(uint32_t rankId)
{
    return impl_.GetWindowsOutAddr(rankId);
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) uint32_t Hccl<serverType, config>::GetRankId()
{
    return impl_.GetRankId();
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) uint32_t Hccl<serverType, config>::GetRankDim()
{
    return impl_.GetRankDim();
}

template <HcclServerType serverType, const auto &config>
[aicore] __inline__ __attribute__((always_inline)) bool Hccl<serverType, config>::SetReduceDataTypeAbility(HcclReduceOp op,
    HcclDataType dstDataType, HcclDataType srcDataType)
{
    return impl_.SetReduceDataTypeAbility(op, dstDataType, srcDataType);
}
}
# 359 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h" 2
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/frac/frac_common_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/frac/frac_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/frac/frac_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/frac/frac_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void TruncCastForFrac(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& tmpTensor)
{
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_TRUNC, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/frac/frac_common_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FracCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{

    TruncCastForFrac(dstTensor, srcTensor, dstTensor);

    Sub<T, false>(dstTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void FracCompute(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    LocalTensor<float> srcTmpTensor = tmpTensor;
    LocalTensor<float> dstTmpTensor = tmpTensor[splitSize];


    Cast<float, __cce_half, false>(srcTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();

    TruncCastForFrac(dstTmpTensor, srcTmpTensor, dstTmpTensor);

    Sub<float, false>(dstTmpTensor, srcTmpTensor, dstTmpTensor, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Cast<__cce_half, float, false>(dstTensor, dstTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FracImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Frac");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Frac");


                                                                                                                       ;

    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t tmpBufferSize = bufferSize / sizeof(float);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackSize = 0;
    uint32_t round = 1;
    uint32_t tail = 0;
    constexpr uint8_t FRAC_HALF_CALC_PROCEDURE = 2;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        stackSize = tmpBufferSize / FRAC_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        CheckTmpBufferSize(stackSize, 0, bufferSize);
        round = calCount / stackSize;
        tail = calCount % stackSize;
        SetMaskCount();
        SetVectorMask<__cce_half, MaskMode::COUNTER>(0, stackSize);
    } else {
        SetMaskCount();
        SetVectorMask<__cce_half, MaskMode::COUNTER>(0, calCount);
    }

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        FracCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize);
        offset = offset + stackSize;
    }
    if (tail != 0) {
        SetVectorMask<__cce_half, MaskMode::COUNTER>(0, tail);
        FracCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FracImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    FracImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    FracImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Frac<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 79 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Frac<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 95 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    FracImpl(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_common_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_int_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_int_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_v220_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_common_utils.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_common_utils.h"
namespace AscendC {

struct AscPowerFParams {
    [aicore] AscPowerFParams() {};
    LocalTensor<float> tmpTensor1;
    LocalTensor<float> tmpTensor2;
    LocalTensor<float> tmpTensor3;
    LocalTensor<float> tmpTensor4;

    LocalTensor<uint8_t> tmpMask1;
    LocalTensor<uint8_t> tmpMask2;
    LocalTensor<uint8_t> tmpMask3;
    LocalTensor<uint8_t> finiteIntegerYMask;
};


struct AscPowerIParams {
    [aicore] AscPowerIParams() {};
    float expIterateSum;

    LocalTensor<int32_t> expUBIterate;
    LocalTensor<int32_t> oriAbsExp;
    LocalTensor<int32_t> recordExpNode;
    LocalTensor<int32_t> tmpTensor1;
    LocalTensor<int32_t> tmpTensor2;
    LocalTensor<int32_t> tmpTensor3;

    LocalTensor<uint8_t> negMask;
    LocalTensor<uint8_t> mask;
    LocalTensor<uint8_t> tmpScalar;
};


[aicore] __inline__ __attribute__((always_inline)) void VselPowerTensorScalar(const LocalTensor<float>& dst, const LocalTensor<uint8_t>& sel,
    const LocalTensor<float>& src0, const LocalTensor<float>& tmpScalar,
    SELMODE selMode, int32_t repeat, const BinaryRepeatParams &binaryParam, const uint32_t calCount)
{
    SetCmpMask<float>(tmpScalar);
    PipeBarrier<PIPE_V>();

    Select<float, uint8_t>(dst, sel, src0, repeat, binaryParam);
}


[aicore] __inline__ __attribute__((always_inline)) void VselPowerTensorTensor(const LocalTensor<float>& dst, const LocalTensor<uint8_t>& sel,
    const LocalTensor<float>& src0, const LocalTensor<float>& src1, const LocalTensor<float>& tmpScalar,
    SELMODE selMode, int32_t repeat, const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{




    uint32_t selAddr = static_cast<uint32_t>(
        reinterpret_cast<int64_t>(reinterpret_cast<__attribute__((cce_unif_buff)) int64_t*>(sel.GetPhyAddr())));
    SetVectorMask<uint32_t>(0, 1);
    Duplicate<uint32_t, false>(tmpScalar.ReinterpretCast<uint32_t>(), selAddr, MASK_PLACEHOLDER, 1,
        DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float>(0, calCount);
    SetCmpMask<int64_t>(tmpScalar.ReinterpretCast<int64_t>());
    PipeBarrier<PIPE_V>();
    Select<float, SELMODE::VSEL_TENSOR_TENSOR_MODE>(dst, src0, src1, repeat, binaryParam);
}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_v220_impl.h" 2

namespace AscendC {

constexpr uint32_t TENSOR_TENSOR_FLOAT = 4;
constexpr uint32_t TENSOR_TENSOR_INT = 6;
constexpr uint32_t TENSOR_TENSOR_HALF = 7;
constexpr uint32_t TENSOR_SCALAR_FLOAT = 5;
constexpr uint32_t TENSOR_SCALAR_INT = 7;
constexpr uint32_t TENSOR_SCALAR_HALF = 7;


[aicore] __inline__ __attribute__((always_inline)) void PowerIParamsCalc(const LocalTensor<uint8_t>& tmpTensor, AscPowerIParams& param,
    uint32_t splitSize)
{
    param.expUBIterate = tmpTensor.ReinterpretCast<int32_t>();
    param.oriAbsExp = param.expUBIterate[splitSize];
    param.recordExpNode = param.oriAbsExp[splitSize];
    param.tmpTensor1 = param.recordExpNode[splitSize];
    param.tmpTensor2 = param.tmpTensor1[splitSize];
    param.negMask = param.tmpTensor2[splitSize].ReinterpretCast<uint8_t>();
    param.mask = param.negMask[splitSize];
    param.tmpScalar = param.mask[splitSize];
    param.expUBIterate.SetSize(splitSize);
    param.oriAbsExp.SetSize(splitSize);
    param.recordExpNode.SetSize(splitSize);
    param.tmpTensor1.SetSize(splitSize);
    param.tmpTensor2.SetSize(splitSize);
    param.negMask.SetSize(splitSize);
    param.mask.SetSize(splitSize);
    param.tmpScalar.SetSize(ONE_BLK_SIZE);
}


[aicore] __inline__ __attribute__((always_inline)) void PowerFParamsCalc(const LocalTensor<float>& tmpTensor,
    AscPowerFParams& param, uint32_t splitSize)
{
    param.tmpTensor1 = tmpTensor;
    param.tmpTensor2 = tmpTensor[splitSize];
    param.tmpTensor3 = param.tmpTensor2[splitSize];
    param.tmpMask1 = param.tmpTensor3[splitSize].ReinterpretCast<uint8_t>();
    param.tmpMask2 = param.tmpMask1[splitSize];
    param.tmpMask3 = param.tmpMask2[splitSize];
    param.finiteIntegerYMask = param.tmpMask3[splitSize];
    param.tmpTensor1.SetSize(splitSize);
    param.tmpTensor2.SetSize(splitSize);
    param.tmpTensor3.SetSize(splitSize);
    param.tmpMask1.SetSize(splitSize);
    param.tmpMask2.SetSize(splitSize);
    param.tmpMask3.SetSize(splitSize);
    param.finiteIntegerYMask.SetSize(splitSize);
}


[aicore] __inline__ __attribute__((always_inline)) void CompareIntZero(const LocalTensor<uint8_t>& mask, const LocalTensor<int32_t>& intInput,
    const LocalTensor<int32_t>& tmpTensor, const UnaryRepeatParams& unaryParam, const uint8_t repeat)
{
    (void)(tmpTensor);
    CompareScalar<int32_t, uint8_t, false>(mask, intInput, static_cast<int32_t>(0), CMPMODE::EQ, MASK_PLACEHOLDER,
        repeat, unaryParam);
}


[aicore] __inline__ __attribute__((always_inline)) void CastFloat2Float(const LocalTensor<float>& dst, const LocalTensor<float>& src, RoundMode mode,
    const UnaryRepeatParams& unaryParam)
{
    Cast<float, float, false>(dst, src, mode, MASK_PLACEHOLDER, 1, unaryParam);
}


[aicore] __inline__ __attribute__((always_inline)) void GrepSignBit(const LocalTensor<uint8_t>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor1, const LocalTensor<float>& tmpTensor2,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    constexpr uint32_t signBit = 31;
    const uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    ShiftRight<uint32_t, false>(tmpTensor1.ReinterpretCast<uint32_t>(), src.ReinterpretCast<uint32_t>(),
        signBit, MASK_PLACEHOLDER, 1, unaryParam, false);
    PipeBarrier<PIPE_V>();
    CompareScalar<int32_t, uint8_t, false>(dst, tmpTensor1.ReinterpretCast<int32_t>(),
        static_cast<int32_t>(1), CMPMODE::EQ, MASK_PLACEHOLDER, repeat, unaryParam);
}


[aicore] __inline__ __attribute__((always_inline)) void ShiftRightOneBit(const LocalTensor<int32_t>& srcDst, const LocalTensor<int32_t>& tmpTensor,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    (void)(binaryParam);
    (void)(calCount);
    ShiftRight<int32_t, false>(srcDst, srcDst, 1, MASK_PLACEHOLDER, 1, unaryParam, false);
}


[aicore] __inline__ __attribute__((always_inline)) void CompareZeroPositive(const LocalTensor<uint8_t>& dst, const LocalTensor<int32_t>& src,
    const UnaryRepeatParams& unaryParam, const uint8_t repeat)
{
    CompareScalar<int32_t, uint8_t, false>(dst, src, static_cast<int32_t>(0), CMPMODE::EQ,
        MASK_PLACEHOLDER, repeat, unaryParam);
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumCount(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const uint32_t calCount)
{
    ReduceSum<float, false>(dst, src, tmpTensor, calCount);
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_int_impl.h" 2




namespace AscendC {




[aicore] __inline__ __attribute__((always_inline)) void InitFinePowerI(AscPowerIParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Sub<int32_t, false>(param.recordExpNode, param.oriAbsExp, param.recordExpNode, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    CompareZeroPositive(param.mask, param.recordExpNode, unaryParam, repeat);
}

[aicore] __inline__ __attribute__((always_inline)) void FineProcessPowerI(const LocalTensor<int32_t>& dst, const LocalTensor<int32_t>& src0,
    AscPowerIParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Cast<float, int32_t, false>(param.tmpTensor1.ReinterpretCast<float>(), param.recordExpNode,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();

    ReduceSumCount(param.tmpScalar.ReinterpretCast<float>(), param.tmpTensor1.ReinterpretCast<float>(),
        param.tmpTensor3.ReinterpretCast<float>(), calCount);
    param.expIterateSum = param.tmpScalar.ReinterpretCast<float>().GetValue(0);
    PipeBarrier<PIPE_V>();
    if (param.expIterateSum != 0) {
        Mul<int32_t, false>(param.tmpTensor1, dst, src0, MASK_PLACEHOLDER, 1, binaryParam);
        Adds<int32_t, false>(param.tmpTensor2, param.recordExpNode, -1, MASK_PLACEHOLDER, 1, unaryParam);
        PipeBarrier<PIPE_V>();
        VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, dst.ReinterpretCast<float>(),
            param.tmpTensor1.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(),
            SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        VselPowerTensorTensor(param.recordExpNode.ReinterpretCast<float>(), param.mask,
            param.recordExpNode.ReinterpretCast<float>(), param.tmpTensor2.ReinterpretCast<float>(),
            param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        PipeBarrier<PIPE_V>();
        CompareZeroPositive(param.mask, param.recordExpNode, unaryParam, repeat);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BulkProcessPowerI(const LocalTensor<int32_t>& dst, AscPowerIParams& param,
    const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Cast<float, int32_t, false>(param.tmpTensor1.ReinterpretCast<float>(), param.expUBIterate,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();

    ReduceSumCount(param.tmpScalar.ReinterpretCast<float>(), param.tmpTensor1.ReinterpretCast<float>(),
        param.tmpTensor3.ReinterpretCast<float>(), calCount);
    param.expIterateSum = param.tmpScalar.ReinterpretCast<float>().GetValue(0);
    PipeBarrier<PIPE_V>();
    if (param.expIterateSum != 0) {
        Mul<int32_t, false>(param.tmpTensor1, dst, dst, MASK_PLACEHOLDER, 1, binaryParam);
        int32_t scalarValue = 2;
        Muls<int32_t, false>(param.tmpTensor2, param.recordExpNode, scalarValue, MASK_PLACEHOLDER, 1, unaryParam);
        PipeBarrier<PIPE_V>();
        VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, dst.ReinterpretCast<float>(),
            param.tmpTensor1.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(),
            SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        VselPowerTensorTensor(param.recordExpNode.ReinterpretCast<float>(), param.mask,
            param.recordExpNode.ReinterpretCast<float>(), param.tmpTensor2.ReinterpretCast<float>(),
            param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        ShiftRightOneBit(param.expUBIterate, param.tmpTensor3, unaryParam, binaryParam, calCount);
        PipeBarrier<PIPE_V>();
        CompareIntZero(param.mask, param.expUBIterate, param.tmpTensor3, unaryParam, repeat);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void InitBulkPowerI(AscPowerIParams& param, const LocalTensor<int32_t>& src0,
    const LocalTensor<int32_t>& src1, const LocalTensor<int32_t>& dst, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Cast<float, int32_t, false>(
        param.tmpTensor1.ReinterpretCast<float>(), src1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.negMask, param.tmpTensor1.ReinterpretCast<float>(),
        static_cast<float>(0), CMPMODE::LT, MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    Muls<int32_t, false>(param.tmpTensor1, src1, -1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(param.expUBIterate.ReinterpretCast<float>(), param.negMask,
        param.tmpTensor1.ReinterpretCast<float>(), src1.ReinterpretCast<float>(),
        param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    Muls<int32_t, false>(param.oriAbsExp, param.expUBIterate, 1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareZeroPositive(param.mask, param.oriAbsExp, unaryParam, repeat);
    Duplicate<int32_t, false>(param.recordExpNode, 0, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Duplicate<int32_t, false>(param.tmpTensor2, 1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(param.recordExpNode.ReinterpretCast<float>(), param.mask,
        param.recordExpNode.ReinterpretCast<float>(), param.tmpTensor2.ReinterpretCast<float>(),
        param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
    VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, param.tmpTensor2.ReinterpretCast<float>(),
        src0.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE,
        1, binaryParam, calCount);
    ShiftRightOneBit(param.expUBIterate, param.tmpTensor3, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    CompareZeroPositive(param.mask, param.expUBIterate, unaryParam, repeat);
}

[aicore] __inline__ __attribute__((always_inline)) void HandleNegativeExpPowerI(const LocalTensor<int32_t>& dst, const LocalTensor<int32_t>& src0,
    AscPowerIParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    CompareIntZero(param.mask, dst, param.tmpTensor3, unaryParam, repeat);
    LocalTensor<float> resF32 = param.oriAbsExp.ReinterpretCast<float>();
    LocalTensor<float> oneTensor = param.expUBIterate.ReinterpretCast<float>();
    Cast<float, int32_t, false>(resF32, dst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    Duplicate<float, false>(oneTensor, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(resF32, oneTensor, resF32, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Cast<int32_t, float, false>(param.tmpTensor1, resF32, RoundMode::CAST_TRUNC, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.negMask, param.tmpTensor1.ReinterpretCast<float>(),
        dst.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE,
        1, binaryParam, calCount);
    Duplicate<int32_t, false>(param.tmpTensor2, 0, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, param.tmpTensor2.ReinterpretCast<float>(),
        dst.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE,
        1, binaryParam, calCount);
}

[aicore] __inline__ __attribute__((always_inline)) void CommonPowerI(const LocalTensor<int32_t>& dstTensor, const LocalTensor<int32_t>& srcTensor0,
    const LocalTensor<int32_t>& srcTensor1, AscPowerIParams& param, const uint32_t calCount)
{
    const UnaryRepeatParams unaryParam;
    const BinaryRepeatParams binaryParam;
    const uint8_t repeat = DivCeil(calCount * sizeof(int32_t), ONE_REPEAT_BYTE_SIZE);

    PipeBarrier<PIPE_V>();
    InitBulkPowerI(param, srcTensor0, srcTensor1, dstTensor, unaryParam, binaryParam, repeat, calCount);
    PipeBarrier<PIPE_V>();
    param.expIterateSum = 1;
    do {
        BulkProcessPowerI(dstTensor, param, unaryParam, binaryParam, repeat, calCount);
        PipeBarrier<PIPE_V>();
    } while (param.expIterateSum != 0);
    InitFinePowerI(param, unaryParam, binaryParam, repeat, calCount);
    PipeBarrier<PIPE_V>();
    do {
        FineProcessPowerI(dstTensor, srcTensor0, param, unaryParam, binaryParam, repeat, calCount);
        PipeBarrier<PIPE_V>();
    } while (param.expIterateSum != 0);
    HandleNegativeExpPowerI(dstTensor, srcTensor0, param, unaryParam, binaryParam, repeat, calCount);
    PipeBarrier<PIPE_V>();
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_common_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_float_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_float_impl.h"
namespace AscendC {





[aicore] __inline__ __attribute__((always_inline)) void InitTmpScalar(const LocalTensor<float>& tmpScalar)
{
    NotNumUnion notNum;
    notNum.i = F32_NAN;
    SetVectorMask<float>(0, ONE_BLK_SIZE / sizeof(float));
    Duplicate<float, false>(tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    Duplicate<float, false>(
        tmpScalar[ONE_BLK_SIZE / sizeof(float)], notNum.f, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
}


[aicore] __inline__ __attribute__((always_inline)) void InitDst(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam)
{
    Abs<float, false>(dst, src0, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dst, src1, dst, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParam);
}


[aicore] __inline__ __attribute__((always_inline)) void DetermineSign(const LocalTensor<float>& src1, const AscPowerFParams& param,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam)
{
    Abs<float, false>(param.tmpTensor1, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CastFloat2Float(param.tmpTensor1, param.tmpTensor1, RoundMode::CAST_RINT, unaryParam);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(param.tmpTensor2, param.tmpTensor1, 0.5f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CastFloat2Float(param.tmpTensor2, param.tmpTensor2, RoundMode::CAST_FLOOR, unaryParam);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(param.tmpTensor2, param.tmpTensor2, 2.0f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(param.tmpTensor1, param.tmpTensor1, param.tmpTensor2, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(param.tmpTensor1, param.tmpTensor1, -2.0f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(param.tmpTensor1, param.tmpTensor1, 1.0f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CastFloat2Float(param.tmpTensor1, param.tmpTensor1, RoundMode::CAST_RINT, unaryParam);
}


[aicore] __inline__ __attribute__((always_inline)) void GenMaskForOne(const LocalTensor<float>& src0, const LocalTensor<float>& src1,
    const AscPowerFParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    NotNumUnion notNum;
    uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);

    Abs<float, false>(param.tmpTensor1, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask1, param.tmpTensor1, static_cast<float>(0), CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask2, src0, static_cast<float>(1), CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(), param.tmpMask2.ReinterpretCast<uint16_t>(),
        param.tmpMask1.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask2, src0, static_cast<float>(-1), CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    notNum.i = F32_INF;
    CompareScalar<float, uint8_t, false>(param.tmpMask3, param.tmpTensor1, notNum.f, CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Or<uint16_t, false>(param.tmpMask2.ReinterpretCast<uint16_t>(), param.tmpMask3.ReinterpretCast<uint16_t>(),
        param.tmpMask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(), param.tmpMask2.ReinterpretCast<uint16_t>(),
        param.tmpMask1.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
}

[aicore] __inline__ __attribute__((always_inline)) void GenMaskForNan(const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const AscPowerFParams& param,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    NotNumUnion notNum;
    notNum.i = F32_INF;
    uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);

    Abs<float, false>(param.tmpTensor1, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask1, param.tmpTensor1, notNum.f, CMPMODE::EQ,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Or<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(),
        param.finiteIntegerYMask.ReinterpretCast<uint16_t>(), param.tmpMask1.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
    CompareScalar<float, uint8_t, false>(param.tmpMask2, src0, static_cast<float>(0), CMPMODE::GE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    notNum.i = F32_NEG_INF;
    CompareScalar<float, uint8_t, false>(param.tmpMask3, src0, notNum.f, CMPMODE::EQ,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Or<uint16_t, false>(param.tmpMask2.ReinterpretCast<uint16_t>(), param.tmpMask3.ReinterpretCast<uint16_t>(),
        param.tmpMask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Or<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(), param.tmpMask2.ReinterpretCast<uint16_t>(),
        param.tmpMask1.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
}


[aicore] __inline__ __attribute__((always_inline)) void GenMaskForSign(const LocalTensor<float>& src0, const LocalTensor<float>& src1,
    const AscPowerFParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    constexpr float intThreshold = 0.00000001f;
    const uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);

    GrepSignBit(param.tmpMask1, src0, param.tmpTensor2, param.tmpTensor4, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(param.tmpTensor2, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CastFloat2Float(param.tmpTensor3, param.tmpTensor2, RoundMode::CAST_RINT, unaryParam);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(param.tmpTensor3, param.tmpTensor2, param.tmpTensor3, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(param.tmpTensor3, param.tmpTensor3, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.finiteIntegerYMask, param.tmpTensor3, static_cast<float>(intThreshold),
        CMPMODE::LT, MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(),
        param.finiteIntegerYMask.ReinterpretCast<uint16_t>(), param.tmpMask1.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
}

[aicore] __inline__ __attribute__((always_inline)) void CommonPowerF(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const LocalTensor<float>& srcTensor1, const LocalTensor<float>& tmpScalar,
    const AscPowerFParams& powerParam, const uint32_t calCount)
{
    const UnaryRepeatParams unaryParam;
    const BinaryRepeatParams binaryParam;

    PipeBarrier<PIPE_V>();
    InitDst(dstTensor, srcTensor0, srcTensor1, unaryParam, binaryParam);
    PipeBarrier<PIPE_V>();
    DetermineSign(srcTensor1, powerParam, unaryParam, binaryParam);
    PipeBarrier<PIPE_V>();
    GenMaskForSign(srcTensor0, srcTensor1, powerParam, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    VselPowerTensorScalar(powerParam.tmpTensor2, powerParam.tmpMask1, powerParam.tmpTensor1, tmpScalar,
        SELMODE::VSEL_TENSOR_SCALAR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, powerParam.tmpTensor2, dstTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    GenMaskForOne(srcTensor0, srcTensor1, powerParam, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    VselPowerTensorScalar(dstTensor, powerParam.tmpMask1, dstTensor, tmpScalar,
        SELMODE::VSEL_TENSOR_SCALAR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    GenMaskForNan(srcTensor0, srcTensor1, powerParam, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    VselPowerTensorScalar(dstTensor, powerParam.tmpMask1, dstTensor, tmpScalar[ONE_BLK_SIZE / sizeof(float)],
        SELMODE::VSEL_TENSOR_SCALAR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
}

}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_common_impl.h" 2


namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor0,
    const LocalTensor<__cce_half>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    constexpr uint32_t tripleFactor = 3;
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);

    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_TENSOR_HALF / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    LocalTensor<float> stackSrc1 = stackSrc0[splitSize];
    LocalTensor<float> stackDst = stackSrc1[splitSize];
    stackSrc1.SetSize(splitSize);
    stackSrc0.SetSize(splitSize);
    stackDst.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(
        stackTensor[ONE_REPEAT_BYTE_SIZE + tripleFactor * splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<__cce_half>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    struct UnaryRepeatParams fp162fp32Param(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    struct UnaryRepeatParams fp322fp16Param(1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, __cce_half, false>(
            stackSrc0, srcTensor0[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        Cast<float, __cce_half, false>(
            stackSrc1, srcTensor1[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, splitSize);
        Cast<__cce_half, float, false>(
            dstTensor[i * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<__cce_half>(0, calcTail);
        Cast<float, __cce_half, false>(
            stackSrc0, srcTensor0[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        Cast<float, __cce_half, false>(
            stackSrc1, srcTensor1[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, calcTail);
        Cast<__cce_half, float, false>(
            dstTensor[loopCount * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const LocalTensor<float>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);

    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_TENSOR_FLOAT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    AscPowerFParams powerParam;
    PowerFParamsCalc(stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>(), powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<float>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        PipeBarrier<PIPE_V>();
        CommonPowerF(dstTensor[i * splitSize], srcTensor0[i * splitSize],
            srcTensor1[i * splitSize], tmpScalar, powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<float>(0, calcTail);
        CommonPowerF(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize],
            srcTensor1[loopCount * splitSize], tmpScalar, powerParam, calcTail);
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<int32_t>& dstTensor, const LocalTensor<int32_t>& srcTensor0,
    const LocalTensor<int32_t>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    uint32_t splitSize = tmpBufferSize / sizeof(int32_t) /
        TENSOR_TENSOR_INT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    AscPowerIParams powerParam;
    PowerIParamsCalc(stackTensor, powerParam, splitSize);
    SetVectorMask<int32_t>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        PipeBarrier<PIPE_V>();
        CommonPowerI(dstTensor[i * splitSize], srcTensor0[i * splitSize], srcTensor1[i * splitSize],
            powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<int32_t>(0, calcTail);
        PipeBarrier<PIPE_V>();
        CommonPowerI(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize],
            srcTensor1[loopCount * splitSize], powerParam, calcTail);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor0,
    const __cce_half& scalarValue, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    constexpr uint32_t tripleFactor = 3;
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);

    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) / TENSOR_SCALAR_HALF /
        ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    LocalTensor<float> stackSrc1 = stackSrc0[splitSize];
    LocalTensor<float> stackDst = stackSrc1[splitSize];
    stackDst.SetSize(splitSize);
    stackSrc0.SetSize(splitSize);
    stackSrc1.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(
        stackTensor[ONE_REPEAT_BYTE_SIZE + tripleFactor * splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<__cce_half>(0, splitSize);
    Duplicate<float, false>(stackSrc1, static_cast<float>(scalarValue), MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    struct UnaryRepeatParams fp162fp32Param(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    struct UnaryRepeatParams fp322fp16Param(1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, __cce_half, false>(
            stackSrc0, srcTensor0[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, splitSize);
        Cast<__cce_half, float, false>(
            dstTensor[i * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<__cce_half>(0, calcTail);
        Cast<float, __cce_half, false>(
            stackSrc0, srcTensor0[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, calcTail);
        Cast<__cce_half, float, false>(
            dstTensor[loopCount * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const float& scalarValue, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);
    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_SCALAR_FLOAT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc1 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    stackSrc1.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(stackTensor[ONE_REPEAT_BYTE_SIZE + splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<float>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(stackSrc1, scalarValue, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerF(dstTensor[i * splitSize], srcTensor0[i * splitSize],
            stackSrc1, tmpScalar, powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<float>(0, calcTail);
        CommonPowerF(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize],
            stackSrc1, tmpScalar, powerParam, calcTail);
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<int32_t>& dstTensor, const LocalTensor<int32_t>& srcTensor0,
    const int32_t& scalarValue, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    uint32_t splitSize = tmpBufferSize / sizeof(int32_t) /
        TENSOR_SCALAR_INT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<int32_t> stackSrc1 = stackTensor.ReinterpretCast<int32_t>();
    stackSrc1.SetSize(splitSize);
    AscPowerIParams powerParam;
    PowerIParamsCalc(stackTensor[splitSize * sizeof(int32_t)], powerParam, splitSize);
    SetVectorMask<int32_t>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    PipeBarrier<PIPE_V>();
    Duplicate<int32_t, false>(stackSrc1, scalarValue, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerI(dstTensor[i * splitSize], srcTensor0[i * splitSize], stackSrc1,
            powerParam, splitSize);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<int32_t>(0, calcTail);
        CommonPowerI(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize], stackSrc1,
            powerParam, calcTail);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<__cce_half>& dstTensor, const __cce_half& scalarValue,
    const LocalTensor<__cce_half>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    constexpr uint32_t tripleFactor = 3;
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);

    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_SCALAR_HALF / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    LocalTensor<float> stackSrc1 = stackSrc0[splitSize];
    LocalTensor<float> stackDst = stackSrc1[splitSize];
    stackSrc0.SetSize(splitSize);
    stackSrc1.SetSize(splitSize);
    stackDst.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(
        stackTensor[ONE_REPEAT_BYTE_SIZE + tripleFactor * splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<__cce_half>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(stackSrc0, static_cast<float>(scalarValue), MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    struct UnaryRepeatParams fp162fp32Param(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    struct UnaryRepeatParams fp322fp16Param(1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, __cce_half, false>(
            stackSrc1, srcTensor1[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, splitSize);
        Cast<__cce_half, float, false>(
            dstTensor[i * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<__cce_half>(0, calcTail);
        Cast<float, __cce_half, false>(
            stackSrc1, srcTensor1[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, calcTail);
        Cast<__cce_half, float, false>(
            dstTensor[loopCount * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<float>& dstTensor, const float& scalarValue,
    const LocalTensor<float>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);

    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_SCALAR_FLOAT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    stackSrc0.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(stackTensor[ONE_REPEAT_BYTE_SIZE + splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<float>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(stackSrc0, scalarValue, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerF(dstTensor[i * splitSize], stackSrc0, srcTensor1[i * splitSize],
            tmpScalar, powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<float>(0, calcTail);
        CommonPowerF(dstTensor[loopCount * splitSize], stackSrc0, srcTensor1[loopCount * splitSize],
            tmpScalar, powerParam, calcTail);
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<int32_t>& dstTensor, const int32_t& scalarValue,
    const LocalTensor<int32_t>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    uint32_t splitSize = tmpBufferSize / sizeof(int32_t) /
        TENSOR_SCALAR_INT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<int32_t> stackSrc0 = stackTensor.ReinterpretCast<int32_t>();
    stackSrc0.SetSize(splitSize);
    AscPowerIParams powerParam;
    PowerIParamsCalc(stackTensor[splitSize * sizeof(int32_t)], powerParam, splitSize);
    SetVectorMask<int32_t>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<int32_t, false>(stackSrc0, scalarValue, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerI(dstTensor[i * splitSize], stackSrc0, srcTensor1[i * splitSize],
            powerParam, splitSize);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<int32_t>(0, calcTail);
        CommonPowerI(dstTensor[loopCount * splitSize], stackSrc0, srcTensor1[loopCount * splitSize],
            powerParam, calcTail);
        PipeBarrier<PIPE_V>();
    }
}





template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const T& scalarValue,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor1, "srcTensor1", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor1, "srcTensor1", "Power");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Power");



                                                                                                         ;

    SetMaskCount();
    PowerImpl(dstTensor, scalarValue, srcTensor1, sharedTmpBuffer, calCount);
    SetMaskNorm();
    ResetMask();
}





template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const T& scalarValue, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor0, "srcTensor0", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor0, "srcTensor0", "Power");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Power");



                                                                                                         ;

    SetMaskCount();
    PowerImpl(dstTensor, srcTensor0, scalarValue, sharedTmpBuffer, calCount);
    SetMaskNorm();
    ResetMask();
}





template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor0, "srcTensor0", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor1, "srcTensor1", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor0, "srcTensor0", "Power");
    CheckCalCount(calCount, "calCount", srcTensor1, "srcTensor1", "Power");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Power");



                                                                                                         ;

    SetMaskCount();
    PowerImpl(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, stackTensor, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const T& src0Scalar,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, stackTensor, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, stackTensor, calCount);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, calCount);
}
# 83 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize());
}
# 100 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, src0Tensor.GetSize());
}
# 123 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor, const T& src1Scalar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, sharedTmpBuffer, calCount);
}
# 141 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, calCount);
}
# 163 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, sharedTmpBuffer, src0Tensor.GetSize());
}
# 180 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, src0Tensor.GetSize());
}
# 203 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar, const LocalTensor<T>& src1Tensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, sharedTmpBuffer, calCount);
}
# 221 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, calCount);
}
# 243 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar, const LocalTensor<T>& src1Tensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, sharedTmpBuffer, src1Tensor.GetSize());
}
# 260 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar, const LocalTensor<T>& src1Tensor)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, src1Tensor.GetSize());
}
#pragma end_pipe
}
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/log/log_common_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/log/log_common_impl.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Log2Compute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{

    const T Ln2Reciprocal = 1.4426950408889634;
    const UnaryRepeatParams unaryParams;
    Ln<float, false>(dstTensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(dstTensor, dstTensor, Ln2Reciprocal, MASK_PLACEHOLDER, 1, unaryParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Log2Compute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{

    const float Ln2Reciprocal = 1.4426950408889634;
    const UnaryRepeatParams unaryParams;


    Cast<float, T, false>(tmpTensor.ReinterpretCast<float>(), srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmpTensor.ReinterpretCast<float>(),
        tmpTensor.ReinterpretCast<float>(),
        MASK_PLACEHOLDER,
        1,
        unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpTensor.ReinterpretCast<float>(),
        tmpTensor.ReinterpretCast<float>(),
        static_cast<float>(Ln2Reciprocal),
        MASK_PLACEHOLDER,
        1,
        unaryParams);
    PipeBarrier<PIPE_V>();


    Cast<T, float, false>(dstTensor, tmpTensor.ReinterpretCast<float>(),
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LogImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{

    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Log");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Log");


                                                                                                                       ;

    const UnaryRepeatParams unaryParams;
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
    Ln<T, false>(dstTensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    SetVectorMask<__cce_half, MaskMode::NORMAL>(FULL_MASK, FULL_MASK);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log2Impl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Log");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Log");


                                                                                                                       ;

    SetMaskCount();
    if constexpr (sizeof(T) == sizeof(float)) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
        Log2Compute(dstTensor, srcTensor);
    } else {
        CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

        uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
        uint32_t splitSize = tmpBufferSize / sizeof(float) / ONE_BLK_SIZE * ONE_BLK_SIZE;
        CheckTmpBufferSize(splitSize, 0, tmpBufferSize);
        uint32_t loopCount = calCount / splitSize;
        uint32_t calcTail = calCount % splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
        for (uint32_t i = 0; i < loopCount; ++i) {
            Log2Compute(dstTensor[i * splitSize], srcTensor[i * splitSize], sharedTmpBuffer);
        }
        if (calcTail > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
            Log2Compute(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize], sharedTmpBuffer);
        }
    }
    SetMaskNorm();
    SetVectorMask<__cce_half, MaskMode::NORMAL>(FULL_MASK, FULL_MASK);
}


template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log10Impl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Log");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Log");


                                                                                                                       ;
    const T Ln10Reciprocal = 0.43429448190325176;
    const UnaryRepeatParams unaryParams;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
    Ln<T, false>(dstTensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dstTensor, dstTensor, Ln10Reciprocal, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    SetVectorMask<__cce_half, MaskMode::NORMAL>(FULL_MASK, FULL_MASK);
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h" 2



namespace AscendC {

#pragma begin_pipe(V)







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LogImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Log<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 65 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    Log2Impl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 85 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Log2<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 99 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;

    if constexpr (std::is_same<T, __cce_half>::value) {
        bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                       ;
    }

    Log2<T, isReuseSource>(dstTensor, srcTensor, stackTensor, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Log2<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 133 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log10(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    Log10Impl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log10(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Log10<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}

#pragma end_pipe
}
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h" 1
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_common_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SinCast(
    const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor, RoundMode castType)
{
    Cast<float, float, false>(dstTensor, srcTensor, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_common_impl.h" 2




namespace AscendC {
const uint8_t SIN_HALF_CALC_PROCEDURE = 4;
const uint8_t SIN_FLOAT_NOREUSE_CALC_PROCEDURE = 3;
const uint8_t SIN_FLOAT_REUSE_CALC_PROCEDURE = 2;


constexpr float SIN_PI_FOR_X_TODIV = 0.3183098733425140380859375;

constexpr float SIN_PI_V2 = 3.140625;
constexpr float SIN_KPI_FIRS_PI_MULS = 0.0009670257568359375;
constexpr float SIN_KPI_TWI_PI_MULS = 6.2771141529083251953125e-7;
constexpr float SIN_KPI_THIR_PI_MULS = 1.21644916362129151821136474609375e-10;

constexpr float SIN_RES_MULIT_SCA = 2.604926501e-6;
constexpr float SIN_RES_ADDICT_UP = -0.0001980894471;
constexpr float SIN_2ADDS = 0.008333049340;
constexpr float SIN_3ADDS = -0.1666665792;
constexpr float SIN_POINT_FIVE = 0.5;
constexpr float SIN_M4_SCA = 4.0;
constexpr float SIN_K2_SCA = -2.0;

[aicore] __inline__ __attribute__((always_inline)) void SinSignCompute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Mul<float, false>(kpi, inputX, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, roundTensor, SIN_POINT_FIVE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    SinCast(dstTensor, dstTensor, RoundMode::CAST_FLOOR);

    Muls<float, false>(dstTensor, dstTensor, SIN_M4_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(roundTensor, roundTensor, SIN_K2_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(dstTensor, dstTensor, 1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void SinPolynomialApproximation(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{





    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SinSignCompute(dstTensor, inputX, roundTensor, kpi);


    Muls<float, false>(roundTensor, kpi, SIN_RES_MULIT_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, SIN_RES_ADDICT_UP, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, SIN_2ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, SIN_3ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, 1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, roundTensor, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void SinKpi(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Muls<float, false>(kpi, roundTensor, SIN_PI_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, srcTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, SIN_KPI_FIRS_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, SIN_KPI_TWI_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, SIN_KPI_THIR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void SinRound(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
                                const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
# 158 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_common_impl.h"
    const UnaryRepeatParams unaryParams;
    Muls<float, false>(roundTensor, srcTensor, SIN_PI_FOR_X_TODIV, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    SinCast(roundTensor, roundTensor, RoundMode::CAST_ROUND);
    SinKpi(inputX, srcTensor, roundTensor, kpi);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SinCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize, bool isReuseSource)
{
    const BinaryRepeatParams binParams;
    LocalTensor<T> roundTensor = tmpTensor;
    LocalTensor<T> kpi = tmpTensor[splitSize];
    LocalTensor<T> inputX = srcTensor;
    if (!isReuseSource) {
        inputX = tmpTensor[splitSize * 2];
    }
    SinRound(inputX, srcTensor, roundTensor, kpi);
    SinPolynomialApproximation(dstTensor, inputX, roundTensor, kpi);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SinCompute(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize, bool isReuseSource)
{
    (void)isReuseSource;
    const BinaryRepeatParams binParams;
    const LocalTensor<float>& tmpBuffer = tmpTensor;
    const LocalTensor<float>& roundTensor = tmpBuffer[splitSize];
    const LocalTensor<float>& kpi = roundTensor[splitSize];
    const LocalTensor<float>& inputX = kpi[splitSize];

    Cast<float, __cce_half, false>(tmpBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    SinRound(inputX, tmpBuffer, roundTensor, kpi);
    SinPolynomialApproximation(tmpBuffer, inputX, roundTensor, kpi);

    Cast<__cce_half, float, false>(dstTensor, tmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}


template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Sin");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Sin");


                                                                                                                       ;

    const uint32_t bufferSize = sharedTmpBuffer.GetSize();
    const uint32_t tmpBufferSize = bufferSize / sizeof(float);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackSize = 0;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        stackSize = tmpBufferSize / SIN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        if constexpr (isReuseSource) {
            stackSize = tmpBufferSize / SIN_FLOAT_REUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        } else {
            stackSize = tmpBufferSize / SIN_FLOAT_NOREUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        }
    }
    CheckTmpBufferSize(stackSize, 0, bufferSize);

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, stackSize);
    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        SinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize, isReuseSource);
        offset = offset + stackSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        SinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize, isReuseSource);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    SinImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SinImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sin<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 100 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    SinImpl(dstTensor, srcTensor, calCount);
}
# 117 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Sin<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}

#pragma end_pipe
}
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h" 1
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_common_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void CosCast(
    const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor, RoundMode castType)
{
    Cast<float, float, false>(dstTensor, srcTensor, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_common_impl.h" 2




namespace AscendC {
const uint8_t COS_HALF_CALC_PROCEDURE = 4;
const uint8_t COS_FLOAT_NOREUSE_CALC_PROCEDURE = 3;
const uint8_t COS_FLOAT_REUSE_CALC_PROCEDURE = 2;


constexpr float COS_PI_FOR_X_TODIV = 0.3183098733425140380859375;

constexpr float PI_0 = 3.140625;
constexpr float COS_KPI_FIRS_PI_MULS = 0.0009670257568359375;
constexpr float COS_KPI_TWI_PI_MULS = 6.2771141529083251953125e-7;
constexpr float COS_KPI_THIR_PI_MULS = 1.21644916362129151821136474609375e-10;
constexpr float COS_KPI_FOR_PI_MULS = -1.0290623200529979163359041220560e-13;

constexpr float COS_PI_DOWN = 1.57079637050628662109375;

constexpr float COS_PI_RESDOWN_ADDS_NEG = -0.00000004371139000189375;

constexpr float COS_RES_MULIT_SCA = 2.604926501e-6;
constexpr float COS_RES_ADDICT_UP = -0.0001980894471;
constexpr float COS_2ADDS = 0.008333049340;
constexpr float COS_3ADDS = -0.1666665792;
constexpr float COS_POINT_FIVE = 0.5;
constexpr float COS_M4_SCA = 4.0;
constexpr float COS_K2_SCA = -2.0;

[aicore] __inline__ __attribute__((always_inline)) void KPI(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Muls<float, false>(kpi, roundTensor, PI_0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, srcTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_FIRS_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(inputX, inputX, COS_PI_DOWN, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_TWI_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_THIR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_FOR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(inputX, inputX, COS_PI_RESDOWN_ADDS_NEG, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void CosRound(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
# 113 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_common_impl.h"
    const UnaryRepeatParams unaryParams;
    Muls<float, false>(roundTensor, srcTensor, COS_PI_FOR_X_TODIV, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_POINT_FIVE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    CosCast(roundTensor, roundTensor, RoundMode::CAST_RINT);
    KPI(inputX, srcTensor, roundTensor, kpi);
}

[aicore] __inline__ __attribute__((always_inline)) void SignCompute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Mul<float, false>(kpi, inputX, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, roundTensor, COS_POINT_FIVE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    CosCast(dstTensor, dstTensor, RoundMode::CAST_FLOOR);


    Muls<float, false>(dstTensor, dstTensor, COS_M4_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(roundTensor, roundTensor, COS_K2_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(dstTensor, dstTensor, 1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void CosPolynomialApproximation(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{





    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SignCompute(dstTensor, inputX, roundTensor, kpi);


    Muls<float, false>(roundTensor, kpi, COS_RES_MULIT_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_RES_ADDICT_UP, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_2ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_3ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, 1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, roundTensor, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mins<float, false>(dstTensor, dstTensor, 1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(dstTensor, dstTensor, -1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CosCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpBuffer, const uint32_t calSize, bool isReuseSource)
{
    const LocalTensor<T>& roundTensor = tmpBuffer;
    const LocalTensor<T>& kpi = roundTensor[calSize];
    LocalTensor<T> inputX = srcTensor;
    if (!isReuseSource) {
        inputX = roundTensor[calSize * 2];
    }

    CosRound(inputX, srcTensor, roundTensor, kpi);
    CosPolynomialApproximation(dstTensor, inputX, roundTensor, kpi);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void CosCompute(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<float>& tmpBuffer, const uint32_t calSize, bool isReuseSource)
{
    (void)isReuseSource;
    const LocalTensor<float>& tempTensorConv = tmpBuffer;
    const LocalTensor<float>& roundTensor = tempTensorConv[calSize];
    const LocalTensor<float>& kpi = roundTensor[calSize];
    const LocalTensor<float>& inputX = kpi[calSize];

    Cast<float, __cce_half, false>(tempTensorConv, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    CosRound(inputX, tempTensorConv, roundTensor, kpi);
    CosPolynomialApproximation(tempTensorConv, inputX, roundTensor, kpi);

    Cast<__cce_half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Cos");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Cos");


                                                                                                                       ;

    const uint32_t bufferSize = sharedTmpBuffer.GetSize();
    const uint32_t tmpBufferSize = bufferSize / sizeof(float);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {

                                                                                                            ;
        calSize = tmpBufferSize / COS_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        if constexpr (isReuseSource) {
            calSize = tmpBufferSize / COS_FLOAT_REUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        } else {
            calSize = tmpBufferSize / COS_FLOAT_NOREUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        }
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        CosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize, isReuseSource);
        offset = offset + calSize;
    }

    if (tail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        CosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize, isReuseSource);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CosImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Cos<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 87 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    CosImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 104 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Cos<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 121 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    CosImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h" 1
# 13 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/../math_constant_util.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/../math_constant_util.h"
namespace AscendC {

constexpr float NUM_ONE = 1.0;
constexpr float NEG_ONE = -1.0;
constexpr float HALF_PI = 1.5707963267948966192313216916398;
constexpr float BOUNDARY = 0.70710678118654752440084436210485;

}
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h" 2


namespace AscendC {
constexpr uint8_t ASIN_HALF_CALC_PROCEDURE = 6;
constexpr uint8_t ASIN_FLOAT_CALC_PROCEDURE = 4;
constexpr uint32_t ASIN_TAYLOR_EXPAND_COUNT = 7;

constexpr float kCOEF[] = {
    1.0,
    0.16666666666666666666666666666667,
    0.075,
    0.04464285714285714285714285714286,
    0.03038194444444444444444444444444,
    0.02237215909090909090909090909091,
    0.01735276442307692307692307692308,
    0.01396484375,
};





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetSign(const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<T>& denominator)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    constexpr float FP16_MAX = 32768;
    constexpr float FP16_MIN = 3.0517578125e-05;
    constexpr float FP32_MAX = 4611686018427387904;
    constexpr float FP32_MIN = 2.168404344971009e-19;
    constexpr float kFpMax = sizeof(T) == sizeof(float) ? FP32_MAX : FP16_MAX;
    constexpr float kFpMin = sizeof(T) == sizeof(float) ? FP32_MIN : FP16_MIN;
    Muls<T, false>(dst, src, static_cast<T>(kFpMax), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Abs<T, false>(denominator, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(denominator, denominator, static_cast<T>(kFpMin), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, dst, denominator, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinTaylorCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& localTemp)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    Mul<T, false>(dst, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    Mul<T, false>(localTemp, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, dst, static_cast<T>(kCOEF[ASIN_TAYLOR_EXPAND_COUNT]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = ASIN_TAYLOR_EXPAND_COUNT - 1; i > 0; i--) {

        Adds<T, false>(dst, dst, static_cast<T>(kCOEF[i]), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Mul<T, false>(dst, dst, localTemp, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Adds<T, false>(dst, dst, static_cast<T>(kCOEF[0]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<T, false>(dst, dst, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinTaylorComputeBySquareValue(const LocalTensor<T>& dst, const LocalTensor<T>& src)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    Muls<T, false>(dst, src, static_cast<T>(NUM_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, dst, static_cast<T>(kCOEF[ASIN_TAYLOR_EXPAND_COUNT]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = ASIN_TAYLOR_EXPAND_COUNT - 1; i > 0; i--) {

        Adds<T, false>(dst, dst, static_cast<T>(kCOEF[i]), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Mul<T, false>(dst, dst, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Adds<T, false>(dst, dst, static_cast<T>(kCOEF[0]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sqrt<T, false>(src, src, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<T, false>(dst, dst, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] __inline__ __attribute__((always_inline)) void AsinFp16Compute(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& src,
    const LocalTensor<__cce_half>& stackBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;


    const LocalTensor<float>& tmpFloatBuffer1 = stackBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpFloatBuffer2 = tmpFloatBuffer1[calSize];
    const uint32_t tmpHalfBuffer1Offset = calSize * 4;
    const uint32_t tmpHalfBuffer2Offset = calSize * 5;
    const LocalTensor<__cce_half>& tmpHalfBuffer1 = stackBuffer[tmpHalfBuffer1Offset];
    const LocalTensor<__cce_half>& tmpHalfBuffer2 = stackBuffer[tmpHalfBuffer2Offset];



    Cast<float, __cce_half, false>(tmpFloatBuffer2, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, NUM_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorComputeBySquareValue(tmpFloatBuffer1, tmpFloatBuffer2);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, HALF_PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Abs<__cce_half, false>(tmpHalfBuffer2, src, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorCompute(dst, tmpHalfBuffer2, tmpHalfBuffer1);
    PipeBarrier<PIPE_V>();
# 163 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h"
    Mins<__cce_half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<__cce_half>(BOUNDARY), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<__cce_half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<__cce_half>(-BOUNDARY), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    const LocalTensor<int8_t>& tmpS8Buffer = tmpHalfBuffer1.ReinterpretCast<int8_t>();
    Cast<int8_t, __cce_half, false>(tmpS8Buffer, tmpHalfBuffer2, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<__cce_half, int8_t, false>(tmpHalfBuffer2, tmpS8Buffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Muls<__cce_half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<__cce_half>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<__cce_half, false>(dst, dst, tmpHalfBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<__cce_half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<__cce_half>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<__cce_half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<__cce_half>(NUM_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, __cce_half, false>(tmpFloatBuffer2, tmpHalfBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, __cce_half, false>(tmpFloatBuffer2, dst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    GetSign(tmpHalfBuffer1, src, tmpHalfBuffer2);
    PipeBarrier<PIPE_V>();
    Cast<float, __cce_half, false>(tmpFloatBuffer2, tmpHalfBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Cast<__cce_half, float, false>(dst, tmpFloatBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}






template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<T>& tmpBuffer2 = tmpBuffer[calSize];
    const LocalTensor<T>& dupBuffer = tmpBuffer[calSize * 2];

    Mul<T, false>(tmpBuffer2, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(tmpBuffer2, tmpBuffer2, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(tmpBuffer2, tmpBuffer2, NUM_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sqrt<T, false>(dst, tmpBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorCompute(tmpBuffer2, dst, tmpBuffer);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(tmpBuffer2, tmpBuffer2, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tmpBuffer2, tmpBuffer2, HALF_PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(tmpBuffer, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorComputeBySquareValue(dst, tmpBuffer);
    PipeBarrier<PIPE_V>();
# 263 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h"
    Mins<T, false>(tmpBuffer, tmpBuffer, BOUNDARY, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(tmpBuffer, tmpBuffer, -BOUNDARY, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LocalTensor<int32_t> tmpS32Buffer = tmpBuffer.template ReinterpretCast<int32_t>();
    Cast<int32_t, T, false>(tmpS32Buffer, tmpBuffer, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<T, int32_t, false>(tmpBuffer, tmpS32Buffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    PipeBarrier<PIPE_V>();
    Muls<T, false>(tmpBuffer, tmpBuffer, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(dst, dst, tmpBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tmpBuffer, tmpBuffer, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(tmpBuffer, tmpBuffer, NUM_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tmpBuffer2, tmpBuffer2, tmpBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(dst, dst, tmpBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();



    GetSign(tmpBuffer2, src, tmpBuffer);
    PipeBarrier<PIPE_V>();
    Mul<T, false>(dst, dst, tmpBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void AsinCompute<__cce_half>(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& src,
    const LocalTensor<__cce_half>& tmpBuffer, uint32_t calSize)
{


    AsinFp16Compute(dst, src, tmpBuffer, calSize);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Asin");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Asin");


                                                                                                                       ;
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t tmpBufferSize = bufferSize / sizeof(T);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();



    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(float)) {
        calSize = tmpBufferSize / ASIN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ASIN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AsinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AsinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AsinImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AsinImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Asin<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 79 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    AsinImpl(dstTensor, srcTensor, calCount);
}
# 101 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Asin<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

#pragma end_pipe
}
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acos/acos_common_impl.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acos/acos_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acos/../math_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acos/../math_common_impl.h"
namespace AscendC {
namespace Internal {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CommonCheckInputsValidness(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                        const uint32_t calCount) {
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acos/../math_common_impl.h"
}
}
}
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acos/acos_common_impl.h" 2



namespace AscendC {


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AcosCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    AsinCompute(dst, src, tmpBuffer, calSize);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(dst, dst, static_cast<T>(-HALF_PI), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, dst, static_cast<T>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void AcosCompute<__cce_half>(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& src,
    const LocalTensor<__cce_half>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;

    AsinFp16Compute(dst, src, tmpBuffer, calSize);
    PipeBarrier<PIPE_V>();



    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<float>();
    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, -HALF_PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<__cce_half, float, false>(dst, tmpFloatBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AcosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Acos");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Acos");


                                                                                                                       ;

    const uint32_t bufferSize = sharedTmpBuffer.GetSize();
    const uint32_t tmpBufferSize = bufferSize / sizeof(T);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();



    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        calSize = tmpBufferSize / ASIN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ASIN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AcosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AcosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AcosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AcosImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AcosImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    AcosImpl(dstTensor, srcTensor, calCount);
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Acos<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 102 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Acos<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

#pragma end_pipe
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asinh/asinh_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asinh/asinh_common_impl.h"
namespace AscendC {
constexpr uint32_t ASINH_HALF_CALC_PROC = 3;
constexpr uint32_t ASINH_FLOAT_CALC_PROC = 3;
constexpr float ASINH_ONE = 1;
constexpr float ASINH_FP16_MAX = 32768;
constexpr float ASINH_FP16_MIN = 3.0517578125e-05;
constexpr float ASINH_FP32_MAX = 4611686018427387904;
constexpr float ASINH_FP32_MIN = 2.168404344971009e-19;
constexpr uint32_t ASINH_STRIDE_DIGITS = 2;

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AsinhImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AsinhImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AsinhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Asinh");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Asinh");


                                                                                                                       ;
    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(float);
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));

    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        splitCount = splitCount / ASINH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;;
    } else {
        splitCount = splitCount / ASINH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;
    SetMaskCount();
    SetVectorMask<T>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AsinhCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], tmpBuffer, splitCount);
    }
    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T>(0, calcTail);
        AsinhCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], tmpBuffer, tailCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void AsinhGetSign(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<T> &denominator)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    constexpr float kFpMax = sizeof(T) == sizeof(float) ? ASINH_FP32_MAX : ASINH_FP16_MAX;
    constexpr float kFpMin = sizeof(T) == sizeof(float) ? ASINH_FP32_MIN : ASINH_FP16_MIN;
    Muls<T, false>(dst, src, static_cast<T>(kFpMax), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Abs<T, false>(denominator, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(denominator, denominator, static_cast<T>(kFpMin), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, dst, denominator, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinhCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;





    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer3 = tmpFloatBuffer1[calCount];
    LocalTensor<float> tmpFloatBuffer2 = tmpFloatBuffer1[calCount * 2];


    Abs<T, false>(tmpFloatBuffer1, src, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<T>(ASINH_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sqrt<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    AsinhGetSign(tmpFloatBuffer1, src, tmpFloatBuffer3);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(dst, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void AsinhCompute(const LocalTensor<__cce_half> &dst, const LocalTensor<__cce_half> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer3 = tmpFloatBuffer1[calCount];
    LocalTensor<float> tmpFloatBuffer2 = tmpFloatBuffer1[calCount * 2];





    Cast<float, __cce_half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Abs<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<__cce_half>(ASINH_ONE), MASK_PLACEHOLDER,
        1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sqrt<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, __cce_half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    AsinhGetSign(tmpFloatBuffer1, tmpFloatBuffer1, tmpFloatBuffer3);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<__cce_half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / ASINH_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Asinh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asinh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asinh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor);
}
#pragma end_pipe
}
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acosh.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acosh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acosh/acosh_common_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acosh/acosh_common_impl.h"
namespace AscendC {
constexpr uint32_t ACOSH_HALF_CALC_PROC = 2;
constexpr uint32_t ACOSH_FLOAT_CALC_PROC = 1;
constexpr float ACOSH_NEG_ONE = -1;
constexpr uint32_t ACOSH_STRIDE_DIGITS = 2;

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AcoshCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;





    Mul<T, false>(tmpFloatBuffer1, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<T>(ACOSH_NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sqrt<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(tmpFloatBuffer1, src, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Div<T, false>(dst, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<T, false>(dst, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void AcoshCompute(const LocalTensor<__cce_half> &dst, const LocalTensor<__cce_half> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer2 = tmpFloatBuffer1[calCount];






    Cast<float, __cce_half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE,
        MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / ACOSH_STRIDE_DIGITS });
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<__cce_half>(ACOSH_NEG_ONE),
        MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sqrt<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<__cce_half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE,
        MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE / ACOSH_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AcoshImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Acosh");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Acosh");


                                                                                                                       ;

    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(float);

    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        splitCount = splitCount / ACOSH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ACOSH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));

    SetMaskCount();
    SetVectorMask<T>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AcoshCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], tmpBuffer, splitCount);
    }
    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T>(0, calcTail);
        AcoshCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], tmpBuffer, tailCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AcoshImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AcoshImpl(const LocalTensor<T> &dstTensor,
 const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AcoshImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acosh.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acosh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acosh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}






template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acosh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor);
}
# 85 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
#pragma end_pipe
}
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/atan/atan_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/atan/atan_common_impl.h"
namespace AscendC {
constexpr uint8_t ATAN_HALF_CALC_PROCEDURE = 6;
constexpr uint8_t ATAN_FLOAT_CALC_PROCEDURE = 5;
constexpr float ATAN_FP16_MAX = 32768;
constexpr float ATAN_FP16_MIN = 3.0517578125e-05;
constexpr float ATAN_FP32_MAX = 4611686018427387904;
constexpr float ATAN_FP32_MIN = 2.168404344971009e-19;
constexpr uint8_t TAYLOR_COUNT_FOUR = 4;
constexpr uint8_t TAYLOR_COUNT_SIX = 6;
constexpr float MIN_INPUT_VALUE = -10000;
constexpr float MAX_INPUT_VALUE = 10000;





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sign(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& denominator)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    constexpr float kFpMax = sizeof(T) == sizeof(float) ? ATAN_FP32_MAX : ATAN_FP16_MAX;
    constexpr float kFpMin = sizeof(T) == sizeof(float) ? ATAN_FP32_MIN : ATAN_FP16_MIN;
    Muls<T, false>(dst, src, static_cast<T>(kFpMax), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Abs<T, false>(denominator, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(denominator, denominator, static_cast<T>(kFpMin), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, dst, denominator, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void TaylorExpand(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& squareTensor, int32_t expandLevel)
{


    constexpr float factorList[7] = {1, -0.3333333333333333, 0.2, -0.14285714285714285,
        0.1111111111111111, - 0.09090909090909091, 0.07692307692307693};
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Mul<float, false>(squareTensor, srcTensor, srcTensor, MASK_PLACEHOLDER, 1, binaryParams);
    Mul<float, false>(dstTensor, srcTensor, srcTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(dstTensor, dstTensor, factorList[expandLevel], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = expandLevel - 1; i > 0; --i) {

        Adds<float, false>(dstTensor, dstTensor, factorList[i], MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Mul<float, false>(dstTensor, dstTensor, squareTensor, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Adds<float, false>(dstTensor, dstTensor, factorList[0], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, dstTensor, srcTensor, MASK_PLACEHOLDER, 1, binaryParams);
}

[aicore] __inline__ __attribute__((always_inline)) void AtanTransform(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& tmpTensor, const float transFactor)
{

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;
    const float transFactorNeg = 0 - transFactor;


    Muls<float, false>(dstTensor, srcTensor, transFactor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);

    Adds<float, false>(tmpTensor, srcTensor, transFactorNeg, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstTensor, tmpTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(dstTensor, dstTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}





[aicore] __inline__ __attribute__((always_inline)) void AtanFormulaImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;
    const float piByFour = 0.78539816339744830961566084581988;
    const float piByEight = 0.39269908169872415480783042290994;
    const float tanPiByEight = 0.4142135623730950;
    LocalTensor<float> clipTensor = tmpTensor[splitSize];
    LocalTensor<float> absTensor = clipTensor[splitSize];
    LocalTensor<float> tmpTensor2 = absTensor[splitSize];
    LocalTensor<float> squareTensor = tmpTensor2[splitSize];




    Mins<float, false>(clipTensor, srcTensor, MAX_INPUT_VALUE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(clipTensor, clipTensor, MIN_INPUT_VALUE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Abs<float, false>(absTensor, clipTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    TaylorExpand(dstTensor, absTensor, squareTensor, TAYLOR_COUNT_FOUR);



    AtanTransform(tmpTensor, absTensor, tmpTensor2, tanPiByEight);
    TaylorExpand(tmpTensor2, tmpTensor, squareTensor, TAYLOR_COUNT_FOUR);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpTensor2, tmpTensor2, piByEight, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Min<float, false>(dstTensor, dstTensor, tmpTensor2, MASK_PLACEHOLDER, 1, binParams);




    Adds<float, false>(tmpTensor2, absTensor, static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    Adds<float, false>(tmpTensor, absTensor, -static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<float, false>(tmpTensor, tmpTensor, tmpTensor2, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(tmpTensor, tmpTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    TaylorExpand(tmpTensor2, tmpTensor, squareTensor, TAYLOR_COUNT_FOUR);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpTensor2, tmpTensor2, piByFour, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Min<float, false>(dstTensor, dstTensor, tmpTensor2, MASK_PLACEHOLDER, 1, binParams);


    AtanTransform(tmpTensor2, tmpTensor, squareTensor, tanPiByEight);
    TaylorExpand(tmpTensor, tmpTensor2, squareTensor, TAYLOR_COUNT_SIX);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpTensor, tmpTensor, piByEight, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpTensor, tmpTensor, piByFour, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Min<float, false>(dstTensor, dstTensor, tmpTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sign(tmpTensor, clipTensor, tmpTensor2);


    Mul<float, false>(dstTensor, dstTensor, tmpTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AtanCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    AtanFormulaImpl(dstTensor, srcTensor, tmpTensor, splitSize);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void AtanCompute(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    const LocalTensor<float>& tempTensorConv = tmpTensor[splitSize * 5];
    Cast<float, __cce_half, false>(tempTensorConv, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    AtanFormulaImpl(tempTensorConv, tempTensorConv, tmpTensor, splitSize);
    Cast<__cce_half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Atan");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Atan");


                                                                                                                       ;
    const uint32_t bufferSize = sharedTmpBuffer.GetSize();
    const uint32_t tmpBufferSize = bufferSize / sizeof(float);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        calSize = tmpBufferSize / ATAN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ATAN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AtanCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AtanCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AtanImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Atan<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 68 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AtanImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 85 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Atan<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 102 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    AtanImpl(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void CoshCast(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& src)
{
    Cast<__cce_half, float, false>(dst, src, RoundMode::CAST_ROUND, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_common_impl.h" 2




namespace AscendC {
constexpr float SCALAR_LN2 = -0.69314718055994530941723212145818;
constexpr float SCALAR_BROAD_CAST = 0.25;
const uint8_t COSH_HALF_CALC_PROCEDURE = 6;
const uint8_t COSH_FLOAT_CALC_PROCEDURE = 2;



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CoshCompute(
    const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<T> tmpBuffer2 = tmpBuffer[calSize];

    Adds<T, false>(tmpBuffer2, src, static_cast<T>(SCALAR_LN2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<T, false>(tmpBuffer, tmpBuffer2, MASK_PLACEHOLDER, 1, unaryParams);


    PipeBarrier<PIPE_V>();
    Duplicate<T, false>(dst, static_cast<T>(SCALAR_BROAD_CAST), MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<T, false>(tmpBuffer2, dst, tmpBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Add<T, false>(dst, tmpBuffer, tmpBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void CoshCompute<__cce_half>(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& src,
    const LocalTensor<__cce_half>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<float>& tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpFloatBuffer2 = tmpFloatBuffer1[calSize];
    const LocalTensor<float>& tmpFloatBuffer3 = tmpFloatBuffer2[calSize];

    Cast<float, __cce_half, false>(tmpFloatBuffer3, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer3, SCALAR_LN2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);


    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(tmpFloatBuffer3, SCALAR_BROAD_CAST, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(tmpFloatBuffer2, tmpFloatBuffer3, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer3, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    CoshCast(dst, tmpFloatBuffer3);
}


template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CoshImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Cosh");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Cosh");


                                                                                                                       ;

    const uint32_t bufferSize = sharedTmpBuffer.GetSize();
    const uint32_t tmpBufferSize = bufferSize / sizeof(T);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        calSize = tmpBufferSize / COSH_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / COSH_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);


    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        CoshCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        CoshCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CoshImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CoshImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Cosh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    CoshImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Cosh<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 98 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    CoshImpl(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/erf/erf_common_impl.h" 1
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/erf/erf_common_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) constexpr RoundMode GetErfCastType()
{

    return RoundMode::CAST_ROUND;



}


[aicore] __inline__ __attribute__((always_inline)) void ErfClip(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer)
{
    constexpr float ERF_BOUNDARY_MAX = 3.92;
    UnaryRepeatParams unaryParams;

    Mins<float, false>(tmpBuffer, src, static_cast<float>(ERF_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(dst, tmpBuffer, static_cast<float>(-ERF_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] __inline__ __attribute__((always_inline)) void ErfComputeP(const LocalTensor<float>& tmpBuffer, const LocalTensor<float>& src,
    const uint32_t calSize)
{
    constexpr float SCALAR_P0 = 0.29639384698e5;
    constexpr float SCALAR_P1 = 0.50637915060e4;
    constexpr float SCALAR_P2 = 0.13938061484e4;
    constexpr float SCALAR_P3 = 0.10162808918e3;
    constexpr float SCALAR_P4 = 0.75517016694e1;
    constexpr float SCALAR_P5 = 0.053443748819;

    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpBuffer1 = tmpBuffer;
    LocalTensor<float> tmpBuffer2 = tmpBuffer1[calSize];
    LocalTensor<float> tmpBuffer3 = tmpBuffer2[calSize];

    Mul<float, false>(tmpBuffer1, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpBuffer2, tmpBuffer1, static_cast<float>(SCALAR_P5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, src, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void ErfComputeQ(const LocalTensor<float>& tmpBuffer, const LocalTensor<float>& src,
    const uint32_t calSize)
{
    constexpr float SCALAR_Q0 = 0.26267224157e5;
    constexpr float SCALAR_Q1 = 0.13243365831e5;
    constexpr float SCALAR_Q2 = 0.30231248150e4;
    constexpr float SCALAR_Q3 = 0.39856963806e3;
    constexpr float SCALAR_Q4 = 0.31212858877e2;

    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpBuffer1 = tmpBuffer;
    LocalTensor<float> tmpBuffer3 = tmpBuffer1[calSize * 2];

    Adds<float, false>(tmpBuffer3, tmpBuffer1, static_cast<float>(SCALAR_Q4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ErfCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<T>& tmpBuffer,
    const uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<T> tmpBuffer1 = tmpBuffer;
    LocalTensor<T> tmpBuffer2 = tmpBuffer1[calSize];
    LocalTensor<T> tmpBuffer3 = tmpBuffer2[calSize];


    ErfClip(dst, src, tmpBuffer1);

    ErfComputeP(tmpBuffer1, dst, calSize);
    ErfComputeQ(tmpBuffer1, dst, calSize);

    Div<T, false>(dst, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void ErfCompute<__cce_half>(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& src,
    const LocalTensor<__cce_half>& tmpBuffer, const uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpBuffer1 = tmpBuffer.ReinterpretCast<float>();
    LocalTensor<float> tmpBuffer2 = tmpBuffer1[calSize];
    LocalTensor<float> tmpBuffer3 = tmpBuffer2[calSize];
    LocalTensor<float> tmpBuffer4 = tmpBuffer3[calSize];


    Cast<float, __cce_half, false>(tmpBuffer4, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();


    ErfClip(tmpBuffer4, tmpBuffer4, tmpBuffer1);

    ErfComputeP(tmpBuffer1, tmpBuffer4, calSize);
    ErfComputeQ(tmpBuffer1, tmpBuffer4, calSize);

    Div<float, false>(tmpBuffer1, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    constexpr RoundMode castType = GetErfCastType();

    Cast<__cce_half, float, false>(dst, tmpBuffer1, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ErfImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Erf");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Erf");


                                                                                                                       ;
    constexpr uint8_t ERF_HALF_CALC_PROCEDURE = 8;
    constexpr uint8_t ERF_FLOAT_CALC_PROCEDURE = 3;

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t tmpBufferSize = bufferSize / sizeof(T);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        calSize = tmpBufferSize / ERF_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ERF_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        ErfCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<__cce_half, MaskMode::COUNTER>(0, tail);
        ErfCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ErfImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ErfImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erf(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    ErfImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erf(
    const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    Erf<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 84 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erf(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{
    ErfImpl(dstTensor, srcTensor, calCount);
}
# 99 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erf(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{
    Erf<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}

#pragma end_pipe
}
# 54 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h" 1
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/erfc/erfc_common_impl.h" 1
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/erfc/erfc_common_impl.h"
namespace AscendC {
constexpr float ERFC_BOUNDARY_MAX = 10;
constexpr uint8_t TMPBUF_IDX_3 = 2;
constexpr uint8_t TMPBUF_IDX_5 = 4;
constexpr uint8_t TMPBUF_IDX_6 = 5;

[aicore] __inline__ __attribute__((always_inline)) constexpr RoundMode GetErfcCastType()
{

    return RoundMode::CAST_ROUND;



}


[aicore] __inline__ __attribute__((always_inline)) void ErfcPreCompute(const LocalTensor<float>& dstBuf1, const LocalTensor<float>& srcBuf1,
    const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    Abs<float, false>(tmpCompBuf1, srcBuf1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    constexpr float SCALAR_ERFC_FP32_MIN = 2.168404344971009e-19;
    Adds<float, false>(tmpCompBuf1, tmpCompBuf1, static_cast<float>(SCALAR_ERFC_FP32_MIN), MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstBuf1, srcBuf1, tmpCompBuf1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] __inline__ __attribute__((always_inline)) void ErfcComputeR(const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf3 = tmpCompBuf1[TMPBUF_IDX_3 * calSize];
    LocalTensor<float> tmpCompBuf4 = tmpCompBuf3[calSize];


    constexpr float R0 = 0.1735313680e-7;
    constexpr float R1 = -0.9856738394e-6;
    constexpr float R2 = 0.2517003236e-4;
    constexpr float R3 = -0.3848015171e-3;
    constexpr float R4 = 0.5681528564e0;
    constexpr float R5 = 0.5245623129e1;
    constexpr float R6 = 0.2107740710e2;
    constexpr float R7 = 0.4212761755e2;
    constexpr float R8 = 0.4380524149e2;


    Muls<float, false>(tmpCompBuf3, tmpCompBuf1, static_cast<float>(R0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R6), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R7), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R8), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] __inline__ __attribute__((always_inline)) void ErfcComputeS(const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf3 = tmpCompBuf1[TMPBUF_IDX_3 * calSize];
    LocalTensor<float> tmpCompBuf5 = tmpCompBuf1[TMPBUF_IDX_5 * calSize];


    constexpr float S1 = 0.9349684299e1;
    constexpr float S2 = 0.3756930664e2;
    constexpr float S3 = 0.8058268949e2;
    constexpr float S4 = 0.9155653738e2;
    constexpr float S5 = 0.4380524152e2;


    Adds<float, false>(tmpCompBuf3, tmpCompBuf1, static_cast<float>(S1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void ErfcPostCompute(const LocalTensor<float>& dstBuf1, const LocalTensor<float>& srcBuf1,
    const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf2 = tmpCompBuf1[calSize];
    LocalTensor<float> tmpCompBuf3 = tmpCompBuf2[calSize];

    Muls<float, false>(tmpCompBuf2, srcBuf1, static_cast<float>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf2, tmpCompBuf2, static_cast<float>(NUM_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf3, srcBuf1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstBuf1, tmpCompBuf3, tmpCompBuf2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void ErfcPublicSteps(const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf2 = tmpCompBuf1[calSize];
    LocalTensor<float> tmpCompBuf3 = tmpCompBuf2[calSize];
    LocalTensor<float> tmpCompBuf4 = tmpCompBuf3[calSize];
    LocalTensor<float> tmpCompBuf5 = tmpCompBuf4[calSize];

    Mins<float, false>(tmpCompBuf2, tmpCompBuf1, static_cast<float>(ERFC_BOUNDARY_MAX), MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();
    ErfcComputeR(tmpCompBuf1, calSize);
    ErfcComputeS(tmpCompBuf1, calSize);


    Div<float, false>(tmpCompBuf3, tmpCompBuf4, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf1, tmpCompBuf1, tmpCompBuf1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpCompBuf1, tmpCompBuf1, static_cast<float>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(tmpCompBuf1, tmpCompBuf1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void ErfcClip(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer)
{
    UnaryRepeatParams unaryParams;

    Mins<float, false>(dst, src, static_cast<float>(ERFC_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(tmpBuffer, dst, static_cast<float>(-ERFC_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ErfcCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{

    LocalTensor<T> tmpCompBuf1 = tmpBuffer;


    ErfcClip(dst, src, dst);
    ErfcPreCompute(dst, dst, tmpCompBuf1, calSize);


    ErfcPublicSteps(tmpCompBuf1, calSize);

    ErfcPostCompute(dst, dst, tmpCompBuf1, calSize);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void ErfcCompute<__cce_half>(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& src,
    const LocalTensor<__cce_half>& tmpBuffer, uint32_t calSize)
{

    LocalTensor<float> tmpCompBuf1 = tmpBuffer.ReinterpretCast<float>();
    LocalTensor<float> tmpCompBuf6 = tmpCompBuf1[TMPBUF_IDX_6 * calSize];


    Cast<float, __cce_half, false>(tmpCompBuf6, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();


    ErfcClip(tmpCompBuf1, tmpCompBuf6, tmpCompBuf6);

    ErfcPreCompute(tmpCompBuf6, tmpCompBuf6, tmpCompBuf1, calSize);

    ErfcPublicSteps(tmpCompBuf1, calSize);

    ErfcPostCompute(tmpCompBuf1, tmpCompBuf6, tmpCompBuf1, calSize);

    constexpr RoundMode castType = GetErfcCastType();

    Cast<__cce_half, float, false>(dst, tmpCompBuf1, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ErfcImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Erfc");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Erfc");


                                                                                                                       ;

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t tmpBufferSize = bufferSize / sizeof(T);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    constexpr uint8_t ERFC_HALF_CALC_PROCEDURE = 12;
    constexpr uint8_t ERFC_FLOAT_CALC_PROCEDURE = 5;
    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        calSize = tmpBufferSize / ERFC_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ERFC_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;

    for (uint32_t i = 0; i < round; i++) {
        ErfcCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<__cce_half, MaskMode::COUNTER>(0, tail);
        ErfcCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ErfcImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ErfcImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    ErfcImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Erfc<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    ErfcImpl(dstTensor, srcTensor, calCount);
}
# 112 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Erfc<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

#pragma end_pipe
}
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/clamp.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/clamp.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/clamp/clamp_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/clamp/clamp_common_impl.h"
namespace AscendC {
constexpr uint32_t CLAMP_FLOAT_MASK = 64;
constexpr uint32_t CLAMP_HALF_MASK = 128;
constexpr uint32_t CLAMP_BYTE_PER_REPEAT = 512;

struct ClampParams {
    [aicore] ClampParams(){};
    uint32_t vcmpvsRepeat = 0;
    uint64_t ClampMask = 0;
    uint64_t selectTailElement = 0;
    uint32_t selectTailOffset = 0;
    uint32_t clampSplitCount = 0;
    uint32_t selectTailRepeatLoop = 0;
    uint32_t selectTailRepeatTail = 0;
    uint32_t selectTailPreRepeatOffset = 0;
    uint32_t selectTailMainRepeatOffset = 0;
    uint32_t selectTailTailRepeatOffset = 0;
    uint32_t loopCount = 0;
    uint32_t calcTail = 0;
    uint32_t vcmpvsRepeatLoop = 0;
    uint32_t vcmpvsRepeatTail = 0;
    uint32_t vcmpvsPreRepeatOffset = 0;
    uint32_t vcmpvsMainRepeatOffset = 0;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ClampComputeCount(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t repeat, const uint64_t mask,
    CLAMPMODE selMode, const BinaryRepeatParams& repeatParams)
{
    if (selMode == CLAMPMODE::CLAMP_MAX) {
        CompareScalar(sharedTmpBuffer, srcTensor, static_cast<T>(scalar), CMPMODE::LT, mask, (uint8_t)repeat,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    } else if (selMode == CLAMPMODE::CLAMP_MIN) {
        CompareScalar(sharedTmpBuffer, srcTensor, static_cast<T>(scalar), CMPMODE::GT, mask, (uint8_t)repeat,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    } else {
                                                                                        ;
    }
    PipeBarrier<PIPE_V>();

    Select(dstTensor, sharedTmpBuffer, srcTensor, static_cast<T>(scalar), SELMODE::VSEL_TENSOR_SCALAR_MODE, mask,
        (uint8_t)repeat, repeatParams);
    PipeBarrier<PIPE_V>();
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void GetMainParams(const uint32_t calCount, ClampParams& params,
    const uint32_t sharedTmpBufferSize)
{
    if constexpr (sizeof(T) == sizeof(uint16_t)) {
        params.clampSplitCount = params.clampSplitCount / CLAMP_HALF_MASK * CLAMP_HALF_MASK;
        params.vcmpvsRepeat = params.clampSplitCount / CLAMP_HALF_MASK;
        params.ClampMask = CLAMP_HALF_MASK;
    } else {
        params.clampSplitCount = params.clampSplitCount / CLAMP_FLOAT_MASK * CLAMP_FLOAT_MASK;
        params.vcmpvsRepeat = params.clampSplitCount / CLAMP_FLOAT_MASK;
        params.ClampMask = CLAMP_FLOAT_MASK;
    }
    CheckTmpBufferSize(params.clampSplitCount, 0, sharedTmpBufferSize);
    params.loopCount = calCount / params.clampSplitCount;
    params.calcTail = calCount % params.clampSplitCount;
    params.vcmpvsRepeatLoop = params.vcmpvsRepeat / MAX_REPEAT_TIMES;
    params.vcmpvsRepeatTail = params.vcmpvsRepeat % MAX_REPEAT_TIMES;
    params.vcmpvsPreRepeatOffset = MAX_REPEAT_TIMES * params.ClampMask;
    params.vcmpvsMainRepeatOffset = params.vcmpvsRepeatLoop * MAX_REPEAT_TIMES * params.ClampMask;
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void GetTailParams(const uint32_t calcTail, ClampParams& params)
{
    if constexpr (sizeof(T) == sizeof(uint16_t)) {
        params.vcmpvsRepeat = calcTail / CLAMP_HALF_MASK;
        params.ClampMask = (calcTail < CLAMP_HALF_MASK) ? calcTail : CLAMP_HALF_MASK;
        params.selectTailElement = calcTail % CLAMP_HALF_MASK;
        params.selectTailOffset = params.vcmpvsRepeat * CLAMP_HALF_MASK;
    } else {
        params.vcmpvsRepeat = calcTail / CLAMP_FLOAT_MASK;
        params.ClampMask = (calcTail < CLAMP_FLOAT_MASK) ? calcTail : CLAMP_FLOAT_MASK;
        params.selectTailElement = calcTail % CLAMP_FLOAT_MASK;
        params.selectTailOffset = params.vcmpvsRepeat * CLAMP_FLOAT_MASK;
    }
    params.selectTailRepeatLoop = params.vcmpvsRepeat / MAX_REPEAT_TIMES;
    params.selectTailRepeatTail = params.vcmpvsRepeat % MAX_REPEAT_TIMES;
    params.selectTailPreRepeatOffset = MAX_REPEAT_TIMES * params.ClampMask;
    params.selectTailMainRepeatOffset = params.selectTailRepeatLoop * MAX_REPEAT_TIMES * params.ClampMask;
    params.selectTailTailRepeatOffset = params.selectTailRepeatLoop * MAX_REPEAT_TIMES * params.ClampMask +
        params.selectTailRepeatTail * params.ClampMask;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ClampCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount, CLAMPMODE selMode,
    ClampParams& params)
{
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Clamp");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Clamp");


                                                                                                                       ;
    uint32_t sharedTmpBufferSize = sharedTmpBuffer.GetSize();
    params.clampSplitCount = sharedTmpBufferSize * sizeof(uint8_t) / sizeof(uint8_t);

    GetMainParams<T>(calCount, params, sharedTmpBufferSize);
    BinaryRepeatParams vselRepeatParams;

    for (uint32_t i = 0; i < params.loopCount; i++) {
        for (uint32_t j = 0; j < params.vcmpvsRepeatLoop; j++) {
            ClampComputeCount<T>(dstTensor[i * params.clampSplitCount + j * params.vcmpvsPreRepeatOffset],
                srcTensor[i * params.clampSplitCount + j * params.vcmpvsPreRepeatOffset], sharedTmpBuffer, scalar,
                MAX_REPEAT_TIMES, params.ClampMask, selMode, vselRepeatParams);
        }
        if (params.vcmpvsRepeatTail) {
            ClampComputeCount<T>(dstTensor[i * params.clampSplitCount + params.vcmpvsMainRepeatOffset],
                srcTensor[i * params.clampSplitCount + params.vcmpvsMainRepeatOffset], sharedTmpBuffer, scalar,
                params.vcmpvsRepeatTail, params.ClampMask, selMode, vselRepeatParams);
        }
    }





    uint32_t mainCount = params.loopCount * params.clampSplitCount;
    if (params.calcTail > 0) {
        GetTailParams<T>(params.calcTail, params);
        for (uint32_t j = 0; j < params.selectTailRepeatLoop; j++) {
            ClampComputeCount<T>(dstTensor[mainCount + j * params.selectTailPreRepeatOffset],
                srcTensor[mainCount + j * params.selectTailPreRepeatOffset], sharedTmpBuffer, scalar, MAX_REPEAT_TIMES,
                params.ClampMask, selMode, vselRepeatParams);
        }
        if (params.selectTailRepeatTail) {
            ClampComputeCount<T>(dstTensor[mainCount + params.selectTailMainRepeatOffset],
                srcTensor[mainCount + params.selectTailMainRepeatOffset], sharedTmpBuffer, scalar,
                params.selectTailRepeatTail, params.ClampMask, selMode, vselRepeatParams);
        }
        if (params.selectTailElement) {
            ClampComputeCount<T>(dstTensor[mainCount + params.selectTailTailRepeatOffset],
                srcTensor[mainCount + params.selectTailTailRepeatOffset], sharedTmpBuffer, scalar, 1,
                params.selectTailElement, selMode, vselRepeatParams);
        }
    }
}



#pragma begin_pipe(V)
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMaxImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{
    ClampParams params;
    ClampCompute<T>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount, CLAMPMODE::CLAMP_MAX, params);
}





template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{
    ClampParams params;
    ClampCompute<T>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount, CLAMPMODE::CLAMP_MIN, params);
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/clamp.h" 2



namespace AscendC {
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/clamp.h"
#pragma begin_pipe(V)
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    ClampMaxImpl<T, false>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const T scalar,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ClampMaxImpl<T, false>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/clamp.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    ClampMinImpl<T, false>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const T scalar,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ClampMinImpl<T, false>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}
#pragma end_pipe
}
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_common_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_common_impl.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_v220_impl.h" 2

namespace AscendC {
namespace RmsNormAPI {


[aicore] __inline__ __attribute__((always_inline)) void RmsNormBasicBlockBrc(const LocalTensor<float>& dst, const LocalTensor<float>& inputAddr,
    const LocalTensor<float>& reduceAddr, const uint32_t hLength, const uint32_t bsLength)
{
    constexpr uint32_t BASIC_BLK_HLENGTH = 64;
    constexpr uint32_t BASIC_BLK_BSLENGTH = 8;
    constexpr uint32_t FLOAT_PER_BLOCK = 8;
    const uint16_t dstBlkStride = hLength / FLOAT_PER_BLOCK;
    const uint16_t dstRepStride = dstBlkStride * BASIC_BLK_BSLENGTH;
    const uint32_t repTime = bsLength / BASIC_BLK_BSLENGTH;

    SetMaskNorm();
    ResetMask();
    BrcbRepeatParams brcParams(dstBlkStride, dstRepStride);
    Brcb(dst, reduceAddr, repTime, brcParams);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float>(0, bsLength * BASIC_BLK_HLENGTH);
    const uint8_t repStride = hLength / FLOAT_PER_BLOCK;
    const int32_t loop = hLength / BASIC_BLK_HLENGTH;
    UnaryRepeatParams unaryParams(1, 0, repStride, repStride);
    for (int32_t i = 0; i < loop; i++) {
        const uint32_t offset = i * BASIC_BLK_HLENGTH;
        Adds<float, false>(dst[offset], dst, 0, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}
}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_common_impl.h" 2




namespace AscendC {
namespace RmsNormAPI {
constexpr uint32_t BASIC_BLK_HLENGTH = 64;
constexpr uint32_t BASIC_BLK_BSLENGTH = 8;
constexpr uint32_t FLOAT_PER_BLOCK = 8;
constexpr float RSQRT_EXPONENT = -0.5;

struct RmsNormParams {
    [aicore] RmsNormParams() {};
    uint32_t curBsLength = 0;
    uint32_t curBshLength = 0;
    LocalTensor<float> tmpAddr;
    LocalTensor<float> reducedAddr;
    LocalTensor<float> srcFp32Addr;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetRmsNormInfo(
    const LocalTensor<float>& tmpLocal, const RmsNormTiling& tiling, RmsNormParams& params)
{
    params.reducedAddr = tmpLocal;
    params.tmpAddr = tmpLocal[tiling.mainBsLengthAlign];
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        params.srcFp32Addr = tmpLocal[tiling.mainBshLength + tiling.mainBsLengthAlign];
    }
    params.curBsLength = tiling.mainBsLength;
    params.curBshLength = tiling.mainBshLength;
}


[aicore] __inline__ __attribute__((always_inline)) void RmsNormGenericReduceSum(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength, const uint32_t originalHLength)
{
    for (uint32_t i = 0; i < bsLength; i++) {
        uint32_t totalNum = originalHLength;
        LocalTensor<float> srcTmp = src[i * hLength];
        LocalTensor<float> dstTmp = srcTmp;

        while (totalNum > 1) {
            if (totalNum <= ONE_REPEAT_FLOAT_SIZE) {
                dstTmp = dst[i];
            }
            SetVectorMask<float>(0, totalNum);
            RepeatReduceSum<float, false>(dstTmp, srcTmp, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            PipeBarrier<PIPE_V>();
            totalNum = DivCeil(totalNum, ONE_REPEAT_FLOAT_SIZE);
        }
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNormReduceSum(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength, const uint32_t originalHLength)
{
    if constexpr (isBasicBlock) {







        SetVectorMask<float>(0, bsLength * BASIC_BLK_HLENGTH);
        const uint32_t basicBlockNum = hLength / BASIC_BLK_HLENGTH;
        const uint8_t repStride = hLength / FLOAT_PER_BLOCK;
        BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, repStride);
        for (uint32_t i = 1; i < basicBlockNum; i++) {
            const uint32_t offset = i * BASIC_BLK_HLENGTH;
            Add<float, false>(src, src, src[offset], MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        }


        RepeatReduceSum<float, false>(dst, src, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, repStride);
        PipeBarrier<PIPE_V>();
    } else {
        RmsNormGenericReduceSum(dst, src, bsLength, hLength, originalHLength);
    }
}



[aicore] __inline__ __attribute__((always_inline)) void RmsNormGeneralFirstAxisBrcMul(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bshLength, const uint32_t bsLength, const uint32_t hLength)
{
    SetVectorMask<float>(0, hLength);
    UnaryRepeatParams unaryParams;
    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    for (uint32_t i = 0; i < bsLength; i++) {
        const uint32_t offset = i * hLength;
        Muls<float, false>(dst[offset], src0[offset], src1.GetValue(i), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, bshLength);
}


template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNormFirstAxisBrcMul(const LocalTensor<float>& dst, const LocalTensor<float>& inputAddr,
    const LocalTensor<float>& reduceAddr, const uint32_t bshLength, const uint32_t bsLength, const uint32_t hLength)
{
    if constexpr (isBasicBlock) {
        if (bsLength > BASIC_BLK_BSLENGTH && bsLength > hLength / BASIC_BLK_HLENGTH) {
            RmsNormBasicBlockBrc(dst, inputAddr, reduceAddr, hLength, bsLength);
            SetVectorMask<float>(0, bshLength);
            BinaryRepeatParams binaryParams;
            Mul<float, false>(dst, dst, inputAddr, MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        } else {
            RmsNormGeneralFirstAxisBrcMul(dst, inputAddr, reduceAddr, bshLength, bsLength, hLength);
        }
    } else {
        RmsNormGeneralFirstAxisBrcMul(dst, inputAddr, reduceAddr, bshLength, bsLength, hLength);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void RmsNormLastAxisBrcMulImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    const uint32_t loop = hLength / BASIC_BLK_HLENGTH;
    if (loop >= bsLength) {

        BinaryRepeatParams binaryParams;
        SetVectorMask<float>(0, hLength);
        for (uint32_t i = 0; i < bsLength; ++i) {
            uint32_t offset = i * hLength;
            Mul<float, false>(dst[offset], src0[offset], src1, MASK_PLACEHOLDER, 1, binaryParams);
        }
    } else {

        SetVectorMask<float>(0, bsLength * BASIC_BLK_HLENGTH);
        const uint16_t repStride = hLength / FLOAT_PER_BLOCK;
        BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, 0);
        for (uint32_t i = 0; i < loop; ++i) {
            uint32_t offset = i * BASIC_BLK_HLENGTH;
            Mul<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, 1, binaryParams);
        }
        if (hLength % BASIC_BLK_HLENGTH != 0) {
            uint32_t offset = loop * BASIC_BLK_HLENGTH;
            uint32_t tail = hLength - offset;
            SetMaskNorm();
            SetVectorMask<float>(0, (1ull << tail) - 1);

            Mul<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, bsLength, binaryParams);
            SetMaskCount();
        }
    }
}


template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNormLastAxisBrcMul(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    if constexpr (isBasicBlock) {
        RmsNormLastAxisBrcMulImpl(dst, src0, src1, bsLength, hLength);
    } else {
        if (hLength == BASIC_BLK_HLENGTH) {
            BinaryRepeatParams binaryParams;
            binaryParams.src1RepStride = 0;
            SetVectorMask<float>(0, bsLength * hLength);
            Mul<float, false>(dst, src0, src1, MASK_PLACEHOLDER, 1, binaryParams);
        } else if (hLength < BASIC_BLK_HLENGTH) {
            SetMaskNorm();
            SetVectorMask<float>(0, (1ull << hLength) - 1);
            uint32_t repStride = hLength / FLOAT_PER_BLOCK;
            BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, 0);
            Mul<float, false>(dst, src0, src1, MASK_PLACEHOLDER, bsLength, binaryParams);
            SetMaskCount();
        } else {
            RmsNormLastAxisBrcMulImpl(dst, src0, src1, bsLength, hLength);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNormCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& gamma, const T epsilon, const RmsNormTiling& tiling, RmsNormParams& params)
{
    UnaryRepeatParams unaryParams;

    SetVectorMask<T>(0, params.curBshLength);
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);
        Cast<float, __cce_half, false>(params.srcFp32Addr, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    } else {
        params.srcFp32Addr = src;
    }

    BinaryRepeatParams binaryParams;
    Mul<float, false>(params.tmpAddr, params.srcFp32Addr, params.srcFp32Addr, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    RmsNormReduceSum<isBasicBlock>(
        params.reducedAddr, params.tmpAddr, params.curBsLength, tiling.hLength, tiling.originalHLength);

    SetVectorMask<T>(0, params.curBsLength);
    Muls<float, false>(
        params.reducedAddr, params.reducedAddr, tiling.reciprocalOfHLength, MASK_PLACEHOLDER, 1, unaryParams);

    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.reducedAddr, params.reducedAddr, epsilon, MASK_PLACEHOLDER, 1, unaryParams);


    PipeBarrier<PIPE_V>();
    Ln<float, false>(params.reducedAddr, params.reducedAddr, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.reducedAddr, params.reducedAddr, RSQRT_EXPONENT, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(params.reducedAddr, params.reducedAddr, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    RmsNormFirstAxisBrcMul<isBasicBlock>(params.tmpAddr, params.srcFp32Addr, params.reducedAddr, params.curBshLength,
        params.curBsLength, tiling.hLength);
    PipeBarrier<PIPE_V>();
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);
        SetVectorMask<T>(0, tiling.hLength);
        Cast<float, __cce_half, false>(
            params.srcFp32Addr, gamma, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);

        PipeBarrier<PIPE_V>();
        RmsNormLastAxisBrcMul<isBasicBlock>(
            params.tmpAddr, params.tmpAddr, params.srcFp32Addr, params.curBsLength, tiling.hLength);
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
        unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);
        SetVectorMask<T>(0, params.curBshLength);
        Cast<__cce_half, float, false>(dst, params.tmpAddr, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    } else {

        RmsNormLastAxisBrcMul<isBasicBlock>(dst, params.tmpAddr, gamma, params.curBsLength, tiling.hLength);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNormImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon,
    const RmsNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                    ;

    LocalTensor<float> tmpLocal = sharedTmpBuffer.ReinterpretCast<float>();
    RmsNormParams params;
    GetRmsNormInfo<T>(tmpLocal, tiling, params);
    SetMaskCount();
    for (uint32_t i = 0; i < tiling.loopRound; ++i) {
        uint32_t offset = i * tiling.mainBshLength;
        RmsNormCompute<T, isBasicBlock>(
            dstLocal[offset], srcLocal[offset], gammaLocal, epsilon, tiling, params);
    }
    if (tiling.tailBsLength != 0) {
        params.curBshLength = tiling.tailBshLength;
        params.curBsLength = tiling.tailBsLength;
        RmsNormCompute<T, isBasicBlock>(
            dstLocal[tiling.inputTailPos], srcLocal[tiling.inputTailPos], gammaLocal, epsilon, tiling, params);
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h"
template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon,
    const RmsNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                    ;
    RmsNormAPI::RmsNormImpl<T, isBasicBlock>(dstLocal, srcLocal, gammaLocal, sharedTmpBuffer, epsilon, tiling);
}
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h"
template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& gammaLocal, const T epsilon, const RmsNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> stackBufer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(stackBufer);
                                                                                                   ;
    RmsNorm<T, isBasicBlock>(dstLocal, srcLocal, gammaLocal, stackBufer, epsilon, tiling);
}
#pragma end_pipe
}
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/batchnorm.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/batchnorm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/batchnorm.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_impl.h" 2






# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_v220_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_v220_impl.h"
namespace AscendC {
constexpr uint32_t FLOAT_BLOCK_NUM_V220 = 8;
constexpr uint32_t BRC_ADDS_LOOP = 7;
constexpr uint32_t BASIC_BLOCK_LEN_V220 = 64;

[aicore] __inline__ __attribute__((always_inline)) void BrcFirstBlockByAdds(const LocalTensor<float>& dst, const uint32_t repeat,
    const uint32_t firstOffset, UnaryRepeatParams& addsUnaryParams, const BatchNormParams<float>& params)
{
    for (uint32_t m = 0; m < repeat; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Adds<float, false>(dst[firstOffset + m * firstOffset], dst, 0, MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                addsUnaryParams);
        }
        if (params.oriBTail) {
            Adds<float, false>(dst[firstOffset + m * firstOffset], dst, 0, MASK_PLACEHOLDER, (uint8_t)params.oriBTail,
                addsUnaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
    ResetMask();
    addsUnaryParams.srcBlkStride = DEFAULT_BLK_STRIDE;
    for (uint32_t m = 0; m < (params.basicLoop - 1); m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Adds<float, false>(dst[BASIC_BLOCK_LEN_V220 + m * BASIC_BLOCK_LEN_V220 + i * params.oriBTmpLoopOffset],
                dst[i * params.oriBTmpLoopOffset], 0, MASK_PLACEHOLDER, MAX_REPEAT_TIMES, addsUnaryParams);
        }
        if (params.oriBTail) {
            Adds<float, false>(dst[BASIC_BLOCK_LEN_V220 + m * BASIC_BLOCK_LEN_V220 + params.oriBTmpTailOffset],
                dst[params.oriBTmpTailOffset], 0, MASK_PLACEHOLDER, (uint8_t)params.oriBTail, addsUnaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BrcFirstDimByBrcb(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    BrcbRepeatParams repeatParams;
    repeatParams.dstBlkStride = (uint16_t)tiling.shCurLengthBlockNum;
    repeatParams.dstRepStride = tiling.shCurLength * FLOAT_BLOCK_NUM_V220 / FLOAT_BLOCK_NUM_V220;


    Brcb(dst, src, (uint8_t)params.brcRepeatTimes, repeatParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::NORMAL>(FLOAT_BLOCK_NUM_V220);
    UnaryRepeatParams addsUnaryParams;
    addsUnaryParams.dstBlkStride = DEFAULT_BLK_STRIDE;
    addsUnaryParams.srcBlkStride = 0;
    addsUnaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
    addsUnaryParams.srcRepStride = (uint8_t)tiling.shCurLengthBlockNum;
    BrcFirstBlockByAdds(dst, BRC_ADDS_LOOP, FLOAT_BLOCK_NUM_V220, addsUnaryParams, params);
}
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_pre_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_pre_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_pre_impl.h" 2
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_pre_impl.h"
namespace AscendC {
constexpr uint32_t FLOAT_BLOCK_NUMBER = 8;
constexpr uint32_t BASIC_BLOCK_LEN = 64;


[aicore] __inline__ __attribute__((always_inline)) void StackBufferChecker(const LocalTensor<float>& stackBuffer, const BatchNormTiling& tiling)
{



      ;
}

template <bool needCast = false> [aicore] __inline__ __attribute__((always_inline)) void GetSrcOffset(uint32_t& srcOffset, const BatchNormTiling& tiling)
{
    if constexpr (!needCast) {
        srcOffset = tiling.meanVarSize;
    } else {
        srcOffset = tiling.shCurLength;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GetUpdataParams(const BatchNormTiling& tiling, BatchNormParams<float>& params)
{
    params.srcRepeatStride = params.srcOffset / FLOAT_BLOCK_NUMBER;
    params.brcRepeatTimes = tiling.originalBLength / FLOAT_BLOCK_NUMBER;
    params.oriBloop = tiling.originalBLength / MAX_REPEAT_TIMES;
    params.oriBTail = tiling.originalBLength % MAX_REPEAT_TIMES;
    params.oriBTmpLoopOffset = tiling.shCurLength * MAX_REPEAT_TIMES;
    params.oriBTmpTailOffset = params.oriBloop * params.oriBTmpLoopOffset;
    params.oriBOutLoopOffset = tiling.meanVarSize * MAX_REPEAT_TIMES;
    params.oriBOutTailOffset = params.oriBloop * params.oriBOutLoopOffset;
    params.reduceAddLoop = (tiling.originalBLength - 1) / MAX_REPEAT_TIMES;
    params.reduceAddTail = (tiling.originalBLength - 1) % MAX_REPEAT_TIMES;
    params.reduceAddTailOffset = BASIC_BLOCK_LEN + params.reduceAddLoop * params.oriBTmpLoopOffset;
    params.basicLoop = tiling.shCurLength / BASIC_BLOCK_LEN;
}

template <typename T, bool needCast = false>
[aicore] __inline__ __attribute__((always_inline)) void GetMainTailOffset(uint64_t& inputMainOffset, uint64_t& inputTailOffset,
    const BatchNormParams<float>& params)
{
    inputMainOffset = params.oriBTmpLoopOffset;
    inputTailOffset = params.oriBTmpTailOffset;
    if constexpr (!needCast) {
        inputMainOffset = params.oriBOutLoopOffset;
        inputTailOffset = params.oriBOutTailOffset;
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void CastGammBeta(const LocalTensor<float>& dst, const LocalTensor<__cce_half>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams castUnaryParams;
    castUnaryParams.srcRepStride = (uint8_t)tiling.castHalfRepStride;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.originalBLength);
    Cast<float, __cce_half, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, castUnaryParams);

    if constexpr (!isBasicBlock) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
    } else {
        PipeBarrier<PIPE_V>();
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void CastOutput(const LocalTensor<__cce_half>& output, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfOutRepStride;
        unaryParams.srcRepStride = (uint8_t)tiling.shCurLengthBlockNum;
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.oriBloop; i++) {
                Cast<__cce_half, float, false>(output[i * params.oriBOutLoopOffset + m * BASIC_BLOCK_LEN],
                    src[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    MAX_REPEAT_TIMES, unaryParams);
            }
            if (params.oriBTail) {
                Cast<__cce_half, float, false>(output[params.oriBOutTailOffset + m * BASIC_BLOCK_LEN],
                    src[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    (uint8_t)params.oriBTail, unaryParams);
            }
        }
    } else {
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfRepStride;
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Cast<__cce_half, float, false>(output[i * tiling.meanVarSize], src[i * tiling.shCurLength], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, 1, unaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void CastInput(const LocalTensor<float>& dst, const LocalTensor<__cce_half>& input,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        unaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
        unaryParams.srcRepStride = (uint8_t)tiling.castHalfOutRepStride;
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.oriBloop; i++) {
                Cast<float, __cce_half, false>(dst[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                    input[i * params.oriBOutLoopOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    MAX_REPEAT_TIMES, unaryParams);
            }
            if (params.oriBTail) {
                Cast<float, __cce_half, false>(dst[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                    input[params.oriBOutTailOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    (uint8_t)params.oriBTail, unaryParams);
            }
        }
    } else {
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        unaryParams.srcRepStride = (uint8_t)tiling.castHalfRepStride;
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Cast<float, __cce_half, false>(dst[i * tiling.shCurLength], input[i * tiling.meanVarSize], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, 1, unaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceAddResult(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    SetMaskNorm();
    ResetMask();
    DataCopyParams datacopyParams;
    datacopyParams.blockCount = 1;
    datacopyParams.blockLen = (uint16_t)tiling.shCurLengthBlockNum;
    DataCopy<float>(dst, src, datacopyParams);
    PipeBarrier<PIPE_V>();
    BinaryRepeatParams binaryParams;
    if constexpr (isBasicBlock) {
        binaryParams.dstRepStride = 0;
        binaryParams.src0RepStride = (uint8_t)tiling.shCurLengthBlockNum;
        binaryParams.src1RepStride = 0;
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.reduceAddLoop; i++) {
                Add<float, false>(dst[m * BASIC_BLOCK_LEN],
                    src[tiling.shCurLength + i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                    dst[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, MAX_REPEAT_TIMES, binaryParams);
            }
            if (params.reduceAddTail) {
                Add<float, false>(dst[m * BASIC_BLOCK_LEN], src[tiling.shCurLength + m * BASIC_BLOCK_LEN],
                    dst[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, (uint8_t)params.reduceAddTail, binaryParams);
            }
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        for (uint32_t i = 1; i < tiling.originalBLength; i++) {
            Add<float, false>(dst, dst, src[i * tiling.shCurLength], MASK_PLACEHOLDER, (uint8_t)1, binaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void BrcFirstDimByDup(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t shLength, const uint32_t bLength)
{
    for (uint32_t i = 0; i < bLength; i++) {
        Duplicate<float, false>(dst[i * shLength], float(src.GetValue(i)), MASK_PLACEHOLDER, (uint8_t)1, (uint16_t)1,
            (uint8_t)DEFAULT_REPEAT_STRIDE);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BrcFirstDim(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    SetMaskNorm();
    ResetMask();

    BrcFirstDimByBrcb<isBasicBlock>(dst, src, tiling, params);



}

template <bool isBasicBlock = false, bool needCast = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputMean(const LocalTensor<float>& outputMean, const LocalTensor<float>& inputX,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        SetMaskNorm();
        ResetMask();
        unaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
        unaryParams.srcRepStride = params.srcRepeatStride;
        uint64_t inputMainOffset = 0;
        uint64_t inputTailOffset = 0;
        GetMainTailOffset<float, needCast>(inputMainOffset, inputTailOffset, params);
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.oriBloop; i++) {
                Muls<float, false>(params.tempTensorC[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                    inputX[i * inputMainOffset + m * BASIC_BLOCK_LEN], params.firstDimValueBack, MASK_PLACEHOLDER,
                    MAX_REPEAT_TIMES, unaryParams);
            }
            if (params.oriBTail) {
                Muls<float, false>(params.tempTensorC[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                    inputX[inputTailOffset + m * BASIC_BLOCK_LEN], params.firstDimValueBack, MASK_PLACEHOLDER,
                    (uint8_t)params.oriBTail, unaryParams);
            }
        }
        PipeBarrier<PIPE_V>();
        GetReduceAddResult<isBasicBlock>(outputMean, params.tempTensorC, tiling, params);
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Muls<float, false>(params.tempTensorC[i * tiling.shCurLength], inputX[i * params.srcOffset],
                params.firstDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
        }
        PipeBarrier<PIPE_V>();

        GetReduceAddResult<isBasicBlock>(outputMean, params.tempTensorC, tiling, params);
    }
}

template <bool needCast = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputVarianceBasicBlock(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    BinaryRepeatParams subBinaryParams;
    const BinaryRepeatParams mulBinaryParams;
    const UnaryRepeatParams mulsUnaryParams;
    SetMaskNorm();
    ResetMask();
    subBinaryParams.src0RepStride = params.srcRepeatStride;
    subBinaryParams.src1RepStride = 0;
    subBinaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;

    uint64_t inputMainOffset = 0;
    uint64_t inputTailOffset = 0;
    GetMainTailOffset<float, needCast>(inputMainOffset, inputTailOffset, params);
    for (uint32_t m = 0; m < params.basicLoop; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Sub<float, false>(params.tempTensorC[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                inputX[i * inputMainOffset + m * BASIC_BLOCK_LEN], outputMean[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER,
                MAX_REPEAT_TIMES, subBinaryParams);
            PipeBarrier<PIPE_V>();
        }
        if (params.oriBTail) {
            Sub<float, false>(params.tempTensorC[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                inputX[inputTailOffset + m * BASIC_BLOCK_LEN], outputMean[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER,
                (uint8_t)params.oriBTail, subBinaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorB, params.tempTensorC, params.tempTensorC, MASK_PLACEHOLDER, 1, mulBinaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tempTensorA, params.tempTensorB, params.firstDimValueBack, MASK_PLACEHOLDER, 1,
        mulsUnaryParams);
    PipeBarrier<PIPE_V>();
    GetReduceAddResult<true>(outputVariance, params.tempTensorA, tiling, params);
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputVarianceNorm(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    const BinaryRepeatParams binaryParams;
    const UnaryRepeatParams mulsUnaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);

    for (uint32_t i = 0; i < tiling.originalBLength; i++) {
        Sub<float, false>(params.tempTensorC[i * tiling.shCurLength], inputX[i * params.srcOffset], outputMean,
            MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorB, params.tempTensorC, params.tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(params.tempTensorA, params.tempTensorB, params.firstDimValueBack, MASK_PLACEHOLDER, 1,
        mulsUnaryParams);
    PipeBarrier<PIPE_V>();

    GetReduceAddResult<false>(outputVariance, params.tempTensorA, tiling, params);
}

template <bool isBasicBlock = false, bool needCast = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputVariance(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        GetBatchNormOutputVarianceBasicBlock<needCast>(outputVariance, inputX, outputMean, tiling, params);
    } else {
        GetBatchNormOutputVarianceNorm(outputVariance, inputX, outputMean, tiling, params);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputPreProcess(const LocalTensor<float>& addSrc,
    const LocalTensor<float>& addDst, const LocalTensor<float>& tmpDst, const float epsilon,
    const BatchNormTiling& tiling)
{
    const UnaryRepeatParams unaryParams;
    constexpr float exponent = -0.5;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
    Adds<float, false>(addDst, addSrc, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmpDst, addDst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(addDst, tmpDst, exponent, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(tmpDst, addDst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputPreBasicBlock(const LocalTensor<float>& addSrc,
    const LocalTensor<float>& addDst, const LocalTensor<float>& tmpDst, const LocalTensor<float>& dst,
    const float epsilon, const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    GetBatchNormOutputPreProcess(addSrc, addDst, tmpDst, epsilon, tiling);
    SetMaskNorm();
    ResetMask();
    BinaryRepeatParams binaryParams;
    binaryParams.src0RepStride = 0;
    binaryParams.src1RepStride = (uint8_t)tiling.shCurLengthBlockNum;
    binaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
    for (uint32_t m = 0; m < params.basicLoop; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Mul<float, false>(dst[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], tmpDst[m * BASIC_BLOCK_LEN],
                dst[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                binaryParams);
        }
        if (params.oriBTail) {
            Mul<float, false>(dst[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], tmpDst[m * BASIC_BLOCK_LEN],
                dst[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, (uint8_t)params.oriBTail,
                binaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputPreNorm(const LocalTensor<float>& addSrc, const LocalTensor<float>& addDst,
    const LocalTensor<float>& tmpDst, const LocalTensor<float>& dst, const float epsilon, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    GetBatchNormOutputPreProcess(addSrc, addDst, tmpDst, epsilon, tiling);
    const BinaryRepeatParams binaryParams;
    for (uint32_t i = 0; i < tiling.originalBLength; i++) {
        Mul<float, false>(dst[i * tiling.shCurLength], dst[i * tiling.shCurLength], tmpDst, MASK_PLACEHOLDER, 1,
            binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputPre(const LocalTensor<float>& src, const LocalTensor<float>& dst,
    const float epsilon, const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        GetBatchNormOutputPreBasicBlock(src, params.tempTensorA, params.tempTensorB, dst, epsilon, tiling, params);
    } else {
        GetBatchNormOutputPreNorm(src, params.tempTensorA, params.tempTensorB, dst, epsilon, tiling, params);
    }
}
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_impl.h" 2

namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputBasicBlock(const LocalTensor<float>& src, const LocalTensor<float>& output,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    const BinaryRepeatParams binaryParams;
    BrcFirstDim(params.tempTensorB, gamm, tiling, params);
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorC, params.tempTensorB, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    BrcFirstDim(params.tempTensorB, beta, tiling, params);
    BinaryRepeatParams addBinaryParams;
    addBinaryParams.dstRepStride = tiling.meanVarSize / FLOAT_BLOCK_NUMBER;
    addBinaryParams.src0RepStride = (uint8_t)tiling.shCurLengthBlockNum;
    addBinaryParams.src1RepStride = (uint8_t)tiling.shCurLengthBlockNum;
    for (uint32_t m = 0; m < params.basicLoop; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Add<float, false>(output[i * params.oriBOutLoopOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorB[params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorC[params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                addBinaryParams);
        }
        if (params.oriBTail) {
            Add<float, false>(output[params.oriBOutTailOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorB[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorC[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER,
                (uint8_t)params.oriBTail, addBinaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputNorm(const LocalTensor<float>& src, const LocalTensor<float>& output,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    const BinaryRepeatParams binaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
    BrcFirstDimByDup(params.tempTensorB, gamm, tiling.shCurLength, tiling.originalBLength);
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorC, params.tempTensorB, params.tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
    BrcFirstDimByDup(params.tempTensorB, beta, tiling.shCurLength, tiling.originalBLength);
    for (uint32_t i = 0; i < tiling.originalBLength; i++) {
        Add<float, false>(output[i * tiling.meanVarSize], params.tempTensorB[i * tiling.shCurLength],
            params.tempTensorC[i * tiling.shCurLength], MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutput(const LocalTensor<float>& src, const LocalTensor<float>& output,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        GetBatchNormOutputBasicBlock(src, output, gamm, beta, tiling, params);
    } else {
        GetBatchNormOutputNorm(src, output, gamm, beta, tiling, params);
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutput(const LocalTensor<float>& src, const LocalTensor<__cce_half>& output,
    const LocalTensor<__cce_half>& gamm, const LocalTensor<__cce_half>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    BinaryRepeatParams binaryParams;
    if constexpr (isBasicBlock) {
        CastGammBeta<isBasicBlock>(params.tempTensorA, gamm, tiling, params);
        BrcFirstDim(params.tempTensorB, params.tempTensorA, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
        Mul<float, false>(params.tempTensorC, params.tempTensorB, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
        CastGammBeta<isBasicBlock>(params.tempTensorA, beta, tiling, params);
        BrcFirstDim(params.tempTensorB, params.tempTensorA, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
        Add<float, false>(params.tempTensorB, params.tempTensorB, params.tempTensorC, MASK_PLACEHOLDER, 1,
            binaryParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
    } else {
        CastGammBeta<isBasicBlock>(params.tempTensorA, gamm, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        BrcFirstDimByDup(params.tempTensorB, params.tempTensorA, tiling.shCurLength, tiling.originalBLength);
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
        Mul<float, false>(params.tempTensorC, params.tempTensorB, params.tempTensorC, MASK_PLACEHOLDER, 1,
            binaryParams);
        PipeBarrier<PIPE_V>();
        CastGammBeta<isBasicBlock>(params.tempTensorA, beta, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        BrcFirstDimByDup(params.tempTensorB, params.tempTensorA, tiling.shCurLength, tiling.originalBLength);
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Add<float, false>(params.tempTensorB[i * tiling.shCurLength], params.tempTensorB[i * tiling.shCurLength],
                params.tempTensorC[i * tiling.shCurLength], MASK_PLACEHOLDER, 1, binaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetOutputMeanVariance(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        UnaryRepeatParams unaryParams;
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfRepStride;
        Cast<__cce_half, float, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        UnaryRepeatParams unaryParams;
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfRepStride;
        Cast<__cce_half, float, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormInfo(const LocalTensor<__cce_half>& inputX, const LocalTensor<__cce_half>& outputMean,
    const LocalTensor<__cce_half>& outputVariance, const LocalTensor<float>& stackBuffer, const BatchNormTiling& tiling,
    BatchNormParams<float>& params)
{
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];



      ;
    StackBufferChecker(stackBuffer, tiling);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormInfo(const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const LocalTensor<float>& stackBuffer, const BatchNormTiling& tiling,
    BatchNormParams<float>& params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];



      ;
    StackBufferChecker(stackBuffer, tiling);
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormExeImpl(const LocalTensor<float>& inputX, const LocalTensor<float>& gamm,
    const LocalTensor<float>& beta, const LocalTensor<float>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const LocalTensor<float>& tmpOutputMean,
    const LocalTensor<float>& tmpOutputVariance, const float epsilon, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    constexpr bool needCast = false;
    if constexpr (isBasicBlock) {
        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, inputX, tiling, params);
        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, inputX, tmpOutputMean, tiling, params);
        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);
        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);
    } else {

        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, inputX, tiling, params);

        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, inputX, tmpOutputMean, tiling, params);

        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);

        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormExeImpl(const LocalTensor<__cce_half>& inputX, const LocalTensor<__cce_half>& gamm,
    const LocalTensor<__cce_half>& beta, const LocalTensor<__cce_half>& output, const LocalTensor<__cce_half>& outputMean,
    const LocalTensor<__cce_half>& outputVariance, const LocalTensor<float>& tmpOutputMean,
    const LocalTensor<float>& tmpOutputVariance, const __cce_half epsilon, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    constexpr bool needCast = true;
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        SetMaskNorm();
        ResetMask();
        CastInput<isBasicBlock>(params.tempTensorA, inputX, tiling, params);

        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, params.tempTensorA, tiling, params);

        GetOutputMeanVariance<isBasicBlock>(outputMean, tmpOutputMean, tiling, params);

        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, params.tempTensorA, tmpOutputMean, tiling,
            params);

        GetOutputMeanVariance<isBasicBlock>(outputVariance, tmpOutputVariance, tiling, params);

        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);

        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);

        CastOutput<isBasicBlock>(output, params.tempTensorB, tiling, params);
    } else {
        CastInput<isBasicBlock>(params.tempTensorA, inputX, tiling, params);
        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, params.tempTensorA, tiling, params);
        GetOutputMeanVariance(outputMean, tmpOutputMean, tiling, params);
        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, params.tempTensorA, tmpOutputMean, tiling,
            params);
        GetOutputMeanVariance(outputVariance, tmpOutputVariance, tiling, params);
        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);
        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);
        CastOutput<isBasicBlock>(output, params.tempTensorB, tiling, params);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormCompute(const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const T epsilon, BatchNormTiling& tiling,
    BatchNormParams<float>& params)
{
    constexpr bool needCast = IsSameType<T, __cce_half>::value;
    uint32_t mvOffset = 0;

    GetSrcOffset<needCast>(params.srcOffset, tiling);
    GetUpdataParams(tiling, params);

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        BatchNormExeImpl<isBasicBlock>(inputX[mvOffset], gamm, beta, output[mvOffset], outputMean[mvOffset],
            outputVariance[mvOffset], params.meanTmpTensor[mvOffset], params.varianceTmpTensor[mvOffset], epsilon,
            tiling, params);

        mvOffset += tiling.shCurLength;
    }
    if (tiling.inputTailSize > 0) {

        tiling.bshCurLength = tiling.inputTailSize;
        tiling.shCurLength = tiling.meanVarTailSize;
        tiling.shCurLengthBlockNum = tiling.shCurLength / FLOAT_BLOCK_NUMBER;
        GetSrcOffset<needCast>(params.srcOffset, tiling);
        GetUpdataParams(tiling, params);

        BatchNormExeImpl<isBasicBlock>(inputX[tiling.inputTailPos], gamm, beta, output[tiling.inputTailPos],
            outputMean[tiling.meanVarTailPos], outputVariance[tiling.meanVarTailPos],
            params.meanTmpTensor[tiling.meanVarTailPos], params.varianceTmpTensor[tiling.meanVarTailPos], epsilon,
            tiling, params);
    }
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, BatchNormTiling& tiling)
{

                                                                                                 ;

                                                                                                                    ;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    BatchNormParams<float> params;
    GetBatchNormInfo<isReuseSource>(inputX, outputMean, outputVariance, stackBuffer, tiling, params);
    params.firstDimValueBack = tiling.firstDimValueBack;

    SetMaskCount();
    BatchNormCompute<T, isBasicBlock>(inputX, gamm, beta, output, outputMean, outputVariance, epsilon, tiling, params);

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const T epsilon, BatchNormTiling& tiling)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                           ;
    BatchNormImpl<T, isReuseSource, isBasicBlock>(output, outputMean, outputVariance, inputX, gamm, beta,
        sharedTmpBuffer, epsilon, tiling);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/batchnorm.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/batchnorm.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, BatchNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    BatchNormImpl<T, isReuseSource, isBasicBlock>(output, outputMean, outputVariance, inputX, gamm, beta,
        sharedTmpBuffer, epsilon, tiling);
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const T epsilon, BatchNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    BatchNormImpl<T, isReuseSource, isBasicBlock>(output, outputMean, outputVariance, inputX, gamm, beta, epsilon,
        tiling);
}
#pragma end_pipe
}
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tanh/tanh_common_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tanh/tanh_common_impl.h"
namespace AscendC {
constexpr float FP32_MIN_V2 = -8.8;
constexpr float FP32_MAX_V2 = 8.8;
constexpr float DOUBLE_X = 2;
const uint8_t TANH_HALF_CALC_PROCEDURE = 2;
const uint8_t TANH_FLOAT_CALC_PROCEDURE = 1;



[aicore] __inline__ __attribute__((always_inline)) void TanhFormulaImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const TanhParams<float>& params)
{
    const LocalTensor<float>& tmpClip = params.tmpClip;
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Mins<float, false>(tmpClip, srcTensor, FP32_MAX_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(tmpClip, tmpClip, FP32_MIN_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpClip, tmpClip, DOUBLE_X, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Exp<float, false>(tmpClip, tmpClip, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(dstTensor, tmpClip, -1.0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpClip, tmpClip, 1.0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstTensor, dstTensor, tmpClip, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TanhCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const TanhParams<float>& params)
{
    TanhFormulaImpl(dstTensor, srcTensor, params);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TanhCompute(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const TanhParams<float>& params)
{
    const LocalTensor<float>& tempTensorConv = params.tempTensorConv;
    Cast<float, __cce_half, false>(tempTensorConv, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    TanhFormulaImpl(tempTensorConv, tempTensorConv, params);

    Cast<__cce_half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TanhFormulasTmpCalc(TanhParams<float>& params, uint32_t tmpBufferSize)
{
    uint32_t tmpUbIndex = 0;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        params.stackSize = params.tmpBufferSize / TANH_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        params.stackSize = params.tmpBufferSize / TANH_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(params.stackSize, 0, tmpBufferSize);
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        params.tempTensorConv = params.sharedTmpBuffer[params.stackSize * (tmpUbIndex++)];
    }
    params.tmpClip = params.sharedTmpBuffer[params.stackSize * (tmpUbIndex++)];
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TanhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Tanh");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Tanh");


                                                                                                                       ;

    TanhParams<float> params;
    params.calCount = calCount;
    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    params.tmpBufferSize = tmpBufferSize / sizeof(float);
    CheckTmpBufferSize(params.tmpBufferSize, 0, tmpBufferSize);


    params.sharedTmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    TanhFormulasTmpCalc<T>(params, tmpBufferSize);

    const uint32_t round = params.calCount / params.stackSize;
    const uint32_t tail = params.calCount % params.stackSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, params.stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        TanhCompute(dstTensor[offset], srcTensor[offset], params);
        offset = offset + params.stackSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        TanhCompute(dstTensor[offset], srcTensor[offset], params);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TanhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    TanhImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Tanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    TanhImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Tanh<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 98 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    TanhImpl(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/atanh/atanh_common_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/atanh/atanh_common_impl.h"
namespace AscendC {
constexpr uint32_t ATANH_FLOAT_CALC_PROC = 1;
constexpr uint32_t ATANH_HALF_CALC_PROC = 4;




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AtanhCompute(const LocalTensor<T>& dstTensor,
    const LocalTensor<T>& srcTensor,
    const LocalTensor<float> &tmpBuffer,
    uint32_t calSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Adds<float, false>(tmpBuffer, srcTensor, static_cast<T>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(dstTensor, srcTensor, static_cast<T>(-1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(dstTensor, dstTensor, static_cast<T>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(dstTensor, tmpBuffer, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(dstTensor, dstTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, dstTensor, static_cast<T>(0.5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



template <>
[aicore] __inline__ __attribute__((always_inline)) void AtanhCompute(const LocalTensor<__cce_half>& dstTensor,
    const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<float> &tmpBuffer,
    uint32_t calSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer2 = tmpBuffer[calSize];


    Cast<float, __cce_half, false>(tmpFloatBuffer1, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, static_cast<float>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<float>(-1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<float>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<float>(0.5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Cast<__cce_half, float, false>(dstTensor, tmpFloatBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanhImpl(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Atanh");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Atanh");


                                                                                                                       ;
    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(T);

    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        splitCount = splitCount / ATANH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ATANH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T>(0, splitCount);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    for (uint32_t i = 0; i < loopCount; i++) {
        AtanhCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], tmpBuffer, splitCount);
    }

    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T>(0, calcTail);
        AtanhCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], tmpBuffer, tailCount);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    AtanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanhImpl(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    AtanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanhImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    AtanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atanh(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AtanhImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    Atanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 74 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atanh(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    Atanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 89 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atanh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    Atanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}

#pragma end_pipe
}
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/deepnorm.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/deepnorm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/deepnorm/deepnorm_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/deepnorm/deepnorm_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/deepnorm/deepnorm_v220_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/deepnorm/deepnorm_v220_impl.h"
namespace AscendC {
namespace DeepNormAPI {
constexpr uint32_t BASIC_BLOCK_HLENGTH = 64;
constexpr uint32_t BASIC_BLOCK_BSLENGTH = 8;
constexpr uint32_t FLOAT_PER_BLOCK = 8;
constexpr uint8_t HALF_REPEAT_STRIDE = 4;
constexpr float SQRT_EXPONENT = -0.5;



[aicore] __inline__ __attribute__((always_inline)) void DeepNormBasicBlockVbrcb(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength)
{
    constexpr uint16_t brcbDstBlkStride = 8;
    constexpr uint16_t brcbDstRepStride = 64;
    constexpr uint16_t addSrcBlkStride = 0;
    const uint8_t repeatTimes = bsLength / 8;
    SetMaskNorm();
    ResetMask();

    BrcbRepeatParams brcbParams(brcbDstBlkStride, brcbDstRepStride);

    Brcb<float>(dst, src, repeatTimes, brcbParams);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength * BASIC_BLOCK_HLENGTH);


    Adds<float, false>(dst, dst, 0, MASK_PLACEHOLDER, 1,
        {DEFAULT_BLK_STRIDE, addSrcBlkStride, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormVarianceBasicBlockByBrcb(const LocalTensor<float>& inputX,
    const LocalTensor<float>& inputMean, const DeepNormTiling& tiling, const DeepNormParams<float>& params)
{
    const uint8_t num = tiling.hLength / BASIC_BLOCK_HLENGTH;



    DeepNormBasicBlockVbrcb(params.tempTensorC, inputMean, tiling.bsCurLength);

    BinaryRepeatParams binaryParams;
    binaryParams.dstRepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src0RepStride = num * DEFAULT_REPEAT_STRIDE;

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength / num);
    for (uint32_t i = 0; i < num; i++) {

        Sub<float, false>(params.tempTensorB[i * BASIC_BLOCK_HLENGTH], inputX[i * BASIC_BLOCK_HLENGTH],
            params.tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormOutputBasicBlockByBrcb(const LocalTensor<float>& xSubMean, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    const uint8_t num = tiling.hLength / BASIC_BLOCK_HLENGTH;
    const UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    binaryParams.dstRepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src1RepStride = num * DEFAULT_REPEAT_STRIDE;



    DeepNormBasicBlockVbrcb(params.tempTensorC, params.tempTensorA, tiling.bsCurLength);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength / num);
    for (uint32_t i = 0; i < num; i++) {

        Mul<float, false>(params.tempTensorA[i * BASIC_BLOCK_HLENGTH], params.tempTensorC,
            xSubMean[i * BASIC_BLOCK_HLENGTH], MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();


    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Adds<float, false>(params.tempTensorC, params.tempTensorA, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/deepnorm/deepnorm_common_impl.h" 2




namespace AscendC {
namespace DeepNormAPI {
template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) bool IsDeepNormParamValid(DeepNormTiling& tiling)
{

                                                                                   ;

                                                                                                           ;

    const bool hDivBy64 = (tiling.hLength % BASIC_BLOCK_HLENGTH == 0) &&
        (tiling.originalHLength % BASIC_BLOCK_HLENGTH == 0);
    const bool bsDivBy8 = ((tiling.bLength * tiling.sLength) % BASIC_BLOCK_BSLENGTH == 0);
    if constexpr (isBasicBlock) {


                                                                                ;
    }

    return true;
}

[aicore] __inline__ __attribute__((always_inline)) void IsStackBufferValid(const LocalTensor<float>& stackBuffer, const DeepNormTiling& tiling)
{



      ;
}


[aicore] __inline__ __attribute__((always_inline)) bool IsBasicBlockTmp8HBetter(const DeepNormTiling& tiling)
{
    bool bs8Check = (tiling.oneTmpSize % (tiling.hLength * BASIC_BLOCK_BSLENGTH)) == 0;

    bool bsWorse = tiling.bsCurLength > (tiling.hLength / BASIC_BLOCK_HLENGTH);
    return bs8Check && bsWorse;
}



template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormTensorInfo(const LocalTensor<__cce_half>& inputX, const LocalTensor<__cce_half>& outputMean,
    const LocalTensor<__cce_half>& outputVariance, const LocalTensor<float>& stackBuffer, const DeepNormTiling& tiling,
    DeepNormParams<float>& params)
{
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];




      ;

    IsStackBufferValid(stackBuffer, tiling);
}


template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormTensorInfo(const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const LocalTensor<float>& stackBuffer, const DeepNormTiling& tiling,
    DeepNormParams<float>& params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    if constexpr (isReuseSource) {
        params.tempTensorA = inputX;
        params.tempTensorB = stackBuffer[tiling.firstTmpStartPos];
        params.tempTensorC = stackBuffer[tiling.secondTmpStartPos];



          ;
    } else {
        params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
        params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
        params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];



          ;
    }

    IsStackBufferValid(stackBuffer, tiling);
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormExec(const LocalTensor<__cce_half>& inputX, const LocalTensor<__cce_half>& inputGx,
    const LocalTensor<__cce_half>& output, const __cce_half alpha, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);


    unaryParams.srcRepStride = HALF_REPEAT_STRIDE;
    Cast<float, __cce_half, false>(params.tempTensorA, inputX, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<float, __cce_half, false>(params.tempTensorC, inputGx, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;

    Muls<float, false>(params.tempTensorA, params.tempTensorA, static_cast<float>(alpha), MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(params.tempTensorA, params.tempTensorC, params.tempTensorA, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormExec(const LocalTensor<float>& inputX, const LocalTensor<float>& inputGx,
    const LocalTensor<float>& output, const float alpha, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);


    Muls<float, false>(params.tempTensorB, inputX, alpha, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(params.tempTensorA, inputGx, params.tempTensorB, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormBasicBlockReduceSum(const LocalTensor<float>& output, const LocalTensor<float>& tmp,
    const LocalTensor<float>& input, const UnaryRepeatParams& unaryParams, const uint32_t bsLength,
    const uint32_t hLength)
{
    const uint8_t num = hLength / BASIC_BLOCK_HLENGTH;

    BinaryRepeatParams binaryParams;
    binaryParams.dstRepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src0RepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src1RepStride = num * DEFAULT_REPEAT_STRIDE;





    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength * BASIC_BLOCK_HLENGTH);
    for (uint32_t i = 1; i < num; i++) {
        Add<float, false>(input, input[i * BASIC_BLOCK_HLENGTH], input, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    RepeatReduceSum<float, false>(output, input, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
        num * DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    RepeatReduceSum<float, false>(tmp, input, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
        num * DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength);
}



[aicore] __inline__ __attribute__((always_inline)) void DeepNormReduceSumImpl(const LocalTensor<float>& dstMVTmp, const LocalTensor<float>& dst,
    const LocalTensor<float>& src, const uint32_t bsLength, const uint32_t hLength, const uint32_t originalHLength)
{
    for (uint32_t i = 0; i < bsLength; i++) {
        uint32_t totalNum = originalHLength;
        LocalTensor<float> srcTmp = src[i * hLength];
        LocalTensor<float> dstTmp = dst[i * hLength];

        while (totalNum > 1) {
            SetVectorMask<float, MaskMode::COUNTER>(0, totalNum);


            if (totalNum <= ONE_REPEAT_FLOAT_SIZE) {
                RepeatReduceSum<float, false>(dstMVTmp[i], srcTmp, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                    DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
                dstTmp = dst[i];
            }

            RepeatReduceSum<float, false>(dstTmp, srcTmp, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            PipeBarrier<PIPE_V>();

            totalNum = DivCeil(totalNum, ONE_REPEAT_FLOAT_SIZE);
            srcTmp = dstTmp;
        }
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength);
}



template <bool isBasicBlock = false, uint8_t mode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormBshHCalc(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    if constexpr(isBasicBlock) {
        const uint32_t loop = hLength / BASIC_BLOCK_HLENGTH;
        const uint16_t repStride = hLength / FLOAT_PER_BLOCK;
        BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, 0);

        SetVectorMask<float, MaskMode::COUNTER>(0, bsLength * BASIC_BLOCK_HLENGTH);
        for (uint32_t i = 0; i < loop; i++) {
            uint32_t offset = i * BASIC_BLOCK_HLENGTH;
            if constexpr(mode) {
                Mul<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, 1, binaryParams);
            } else {
                Add<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, 1, binaryParams);
            }
        }
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, hLength);
    } else {
        BinaryRepeatParams binaryParams;
        for (uint32_t i = 0; i < bsLength; i++) {
            uint32_t offset = i * hLength;
            if constexpr(mode) {
                Mul<float, false>(dst[offset], src0[offset], src1, MASK_PLACEHOLDER, 1, binaryParams);
            } else {
                Add<float, false>(dst[offset], src0[offset], src1, MASK_PLACEHOLDER, 1, binaryParams);
            }
        }
        PipeBarrier<PIPE_V>();
    }
}




template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutputMean(const LocalTensor<float>& tmpMean, const LocalTensor<float>& inputX,
    const DeepNormTiling& tiling, const DeepNormParams<float>& params, const LocalTensor<float>& outputMean)
{
    const UnaryRepeatParams unaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);

    Muls<float, false>(params.tempTensorC, inputX, params.lastDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    if constexpr(isBasicBlock) {
        DeepNormBasicBlockReduceSum(outputMean, tmpMean, params.tempTensorC, unaryParams, tiling.bsCurLength,
            tiling.hLength);
    } else {
        DeepNormReduceSumImpl(outputMean, tmpMean, params.tempTensorC, tiling.bsCurLength, tiling.hLength,
            tiling.originalHLength);
    }
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormVarianceByForLoop(const LocalTensor<float>& inputX, const LocalTensor<float>& inputMean,
    const DeepNormTiling& tiling, const DeepNormParams<float>& params, const UnaryRepeatParams& unaryParams)
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);
    for (uint32_t i = 0; i < tiling.bsCurLength; i++) {
        Adds<float, false>(params.tempTensorB[i * tiling.hLength], inputX[i * tiling.hLength],
            (float)((inputMean.GetValue(i))*(-1)), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormOutputByForLoop(const LocalTensor<float>& xSubMean, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params, const UnaryRepeatParams& unaryParams)
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);
    for (uint32_t i = 0; i < tiling.bsCurLength; i++) {
        Muls<float, false>(params.tempTensorC[i * tiling.hLength], xSubMean[i * tiling.hLength],
            (float)params.tempTensorA.GetValue(i), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
}



template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutputVariance(const LocalTensor<float>& tmpVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& inputMean, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params, const LocalTensor<float>& outputVariance)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    if constexpr(isBasicBlock) {
        if ((IsBasicBlockTmp8HBetter(tiling))) {
            DeepNormVarianceBasicBlockByBrcb(inputX, inputMean, tiling, params);
        } else {
            DeepNormVarianceByForLoop(inputX, inputMean, tiling, params, unaryParams);
        }
    } else {
        DeepNormVarianceByForLoop(inputX, inputMean, tiling, params, unaryParams);
    }


    Mul<float, false>(params.tempTensorC, params.tempTensorB, params.tempTensorB, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tempTensorA, params.tempTensorC, params.lastDimValueBack, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();


    if constexpr(isBasicBlock) {
        DeepNormBasicBlockReduceSum(outputVariance, tmpVariance, params.tempTensorA, unaryParams, tiling.bsCurLength,
            tiling.hLength);
    }
    else {
        DeepNormReduceSumImpl(outputVariance, tmpVariance, params.tempTensorA, tiling.bsCurLength, tiling.hLength,
            tiling.originalHLength);
    }
}


template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutputPre(const LocalTensor<float>& xSubMean,
    const LocalTensor<float>& inputVariance, const float epsilon, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bsCurLength);

    Adds<float, false>(params.tempTensorA, inputVariance, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tempTensorC, float(1.0), 1, 1, 1, 8);
    PipeBarrier<PIPE_V>();


    Sqrt<float, false>(params.tempTensorA, params.tempTensorA, 1, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(params.tempTensorA, params.tempTensorC, params.tempTensorA, 1, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    if constexpr(isBasicBlock) {

        if ((IsBasicBlockTmp8HBetter(tiling))) {
            DeepNormOutputBasicBlockByBrcb(xSubMean, tiling, params);
            return;
        }
    }
    DeepNormOutputByForLoop(xSubMean, tiling, params, unaryParams);
}


template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutput(const LocalTensor<__cce_half>& output, const LocalTensor<float>& inputY,
    const LocalTensor<__cce_half>& gamm, const LocalTensor<__cce_half>& beta, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);

    unaryParams.srcRepStride = HALF_REPEAT_STRIDE;
    Cast<float, __cce_half, false>(params.tempTensorA, gamm, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    DeepNormBshHCalc<isBasicBlock, 1>(params.tempTensorB, inputY, params.tempTensorA, tiling.bsCurLength,
        tiling.hLength);


    Cast<float, __cce_half, false>(params.tempTensorC, beta, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    DeepNormBshHCalc<isBasicBlock, 0>(params.tempTensorA, params.tempTensorB, params.tempTensorC, tiling.bsCurLength,
        tiling.hLength);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    unaryParams.dstRepStride = HALF_REPEAT_STRIDE;


    Cast<__cce_half, float, false>(output, params.tempTensorA, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutput(const LocalTensor<float>& output, const LocalTensor<float>& inputY,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);
    DeepNormBshHCalc<isBasicBlock, 1>(params.tempTensorA, inputY, gamm, tiling.bsCurLength, tiling.hLength);
    DeepNormBshHCalc<isBasicBlock, 0>(output, params.tempTensorA, beta, tiling.bsCurLength, tiling.hLength);
}


[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutputMeanVariance(const LocalTensor<__cce_half>& outputMean,
    const LocalTensor<__cce_half>& outputVariance, const DeepNormTiling& tiling, const DeepNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = HALF_REPEAT_STRIDE;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.meanVarSize);

    Cast<__cce_half, float, false>(outputMean, params.meanTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<__cce_half, float, false>(outputVariance, params.varianceTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();
}



template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormLayerNormExec(const LocalTensor<float>& inputX, const LocalTensor<__cce_half>& gamm,
    const LocalTensor<__cce_half>& beta, const LocalTensor<__cce_half>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const __cce_half epsilon, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    GetDeepNormOutputMean<isBasicBlock>(params.tempTensorC, params.tempTensorA, tiling, params, outputMean);
    GetDeepNormOutputVariance<isBasicBlock>(params.tempTensorC, params.tempTensorA, outputMean, tiling, params,
        outputVariance);
    GetDeepNormOutputPre<isBasicBlock>(params.tempTensorB, params.tempTensorC, static_cast<float>(epsilon), tiling,
        params);
    GetDeepNormOutput<isBasicBlock>(output, params.tempTensorC, gamm, beta, tiling, params);
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormLayerNormExec(const LocalTensor<float>& inputX, const LocalTensor<float>& gamm,
    const LocalTensor<float>& beta, const LocalTensor<float>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const float epsilon, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    GetDeepNormOutputMean<isBasicBlock>(params.tempTensorC, inputX, tiling, params, outputMean);
    GetDeepNormOutputVariance<isBasicBlock>(params.tempTensorC, inputX, outputMean, tiling, params, outputVariance);
    GetDeepNormOutputPre<isBasicBlock>(params.tempTensorB, params.tempTensorC, epsilon, tiling, params);
    GetDeepNormOutput<isBasicBlock>(output, params.tempTensorC, gamm, beta, tiling, params);
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormND(const LocalTensor<T>& inputX, const LocalTensor<T>& inputGx,
    const LocalTensor<T>& gamm, const LocalTensor<T>& beta, const LocalTensor<T>& output,
    const LocalTensor<T>& outputMean, const LocalTensor<T>& outputVariance, const T alpha, const T epsilon,
    DeepNormTiling& tiling, const DeepNormParams<float>& params)
{
    uint32_t BSHOffset = 0;
    uint32_t BSOffset = 0;

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        DeepNormExec(inputX[BSHOffset], inputGx[BSHOffset], output, alpha, tiling, params);
        DeepNormLayerNormExec<isBasicBlock>(params.tempTensorA, gamm, beta, output[BSHOffset],
            params.meanTmpTensor[BSOffset], params.varianceTmpTensor[BSOffset], epsilon, tiling, params);
        BSHOffset += tiling.inputRoundSize;
        BSOffset += tiling.meanVarRoundSize;
    }

    if (tiling.inputTailSize > 0) {
        tiling.bshCurLength = tiling.inputTailSize;
        tiling.bsCurLength = tiling.meanVarTailSize;

        BSHOffset = tiling.inputTailPos;
        BSOffset = tiling.meanVarTailPos;
        DeepNormExec(inputX[BSHOffset], inputGx[BSHOffset], output, alpha, tiling, params);
        DeepNormLayerNormExec<isBasicBlock>(params.tempTensorA, gamm, beta, output[BSHOffset],
            params.meanTmpTensor[BSOffset], params.varianceTmpTensor[BSOffset], epsilon, tiling, params);
    }


    if constexpr(IsSameType<T, __cce_half>::value) {
        GetDeepNormOutputMeanVariance(outputMean, outputVariance, tiling, params);
    }
}

template <typename T, bool isReuseSrc, bool isBasicBlock>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const T alpha, const T epsilon, DeepNormTiling& tiling)
{
    if (!DeepNormAPI::IsDeepNormParamValid<T, isBasicBlock>(tiling)) {
        return;
    }
                                                                                                                    ;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    DeepNormParams<float> deepnormParams;
    DeepNormAPI::GetDeepNormTensorInfo<isReuseSrc>(srcLocal, meanLocal, rstdLocal, stackBuffer, tiling, deepnormParams);
    deepnormParams.lastDimValueBack = tiling.lastDimValueBack;

    SetMaskCount();
    DeepNormAPI::DeepNormND<T, isBasicBlock>(srcLocal, gxLocal, gammaLocal, betaLocal, dstLocal, meanLocal, rstdLocal,
        alpha, epsilon, tiling, deepnormParams);
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSrc, bool isBasicBlock>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const T alpha, const T epsilon,
    DeepNormTiling& tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    DeepNormImpl<T, isReuseSrc, isBasicBlock>(dstLocal, meanLocal, rstdLocal, srcLocal, gxLocal, betaLocal,
        gammaLocal, sharedTmpBuffer, alpha, epsilon, tiling);
}

}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/deepnorm.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/deepnorm.h"
template <typename T, bool isReuseSrc = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void DeepNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const T alpha, const T epsilon, DeepNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DeepNormAPI::DeepNormImpl<T, isReuseSrc, isBasicBlock>(dstLocal, meanLocal, rstdLocal, srcLocal, gxLocal, betaLocal,
        gammaLocal, sharedTmpBuffer, alpha, epsilon, tiling);
}
# 88 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/deepnorm.h"
template <typename T, bool isReuseSrc = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void DeepNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const T alpha, const T epsilon,
    DeepNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DeepNormAPI::DeepNormImpl<T, isReuseSrc, isBasicBlock>(dstLocal, meanLocal, rstdLocal, srcLocal, gxLocal, betaLocal,
        gammaLocal, alpha, epsilon, tiling);
}
#pragma end_pipe
}
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/exp.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/exp.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/exp/exp_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/exp/exp_common_impl.h"
namespace AscendC {
namespace ExpAPI {
constexpr uint8_t HALF_REPEAT_STRIDE = 4;
constexpr uint32_t EXP_TWO = 2;
constexpr uint32_t EXP_THREE = 3;
constexpr uint32_t EXP_FOUR = 4;


template <typename T, bool isReuseSource = false, uint8_t expandLevel = 10>
[aicore] __inline__ __attribute__((always_inline)) void UpdataExpParams(const LocalTensor<T>& src, const uint32_t calCount,
    const LocalTensor<float>& stackBuffer, ExpParams<float>& params)
{
    uint32_t alignNum = ONE_BLK_SIZE / sizeof(T);



    bool isFloat = IsSameType<T, float>::value;
    uint32_t numberOfTmpBuf = EXP_FOUR;
    if (isFloat) {
        numberOfTmpBuf = isReuseSource ? EXP_TWO : EXP_THREE;
    }

    uint32_t inputSize = calCount;
    uint32_t stackBufferSize = stackBuffer.GetSize();
    uint32_t oneTmpSize = stackBufferSize / numberOfTmpBuf;
    oneTmpSize = oneTmpSize / alignNum * alignNum;
    uint32_t secondOffset = (isFloat && isReuseSource)? 0 : oneTmpSize;
    uint32_t fourthOffset = isFloat ? 0 : oneTmpSize;

    CheckTmpBufferSize(oneTmpSize, 0, stackBufferSize);

    params.inputSize = inputSize;
    params.oneTmpSize = oneTmpSize;
    params.firstTmpStartPos = 0;
    params.secondTmpStartPos = secondOffset;
    params.thirdTmpStartPos = params.secondTmpStartPos + oneTmpSize;
    params.fourthTmpStartPos = params.thirdTmpStartPos + fourthOffset;
    params.loopNum = inputSize / oneTmpSize;
    params.tailSize = inputSize % oneTmpSize;
    params.tailPos = inputSize - params.tailSize;
    params.curDataLength = oneTmpSize;
    params.expandLevel = expandLevel;
}

template <bool isReuseSource = false, uint8_t expandLevel = 10>
[aicore] __inline__ __attribute__((always_inline)) void GetExpTensorInfo(const LocalTensor<__cce_half>& src, const LocalTensor<__cce_half>& dst,
    const uint32_t calCount, const LocalTensor<float>& stackBuffer, ExpParams<float>& params)
{
    UpdataExpParams<__cce_half, isReuseSource, expandLevel>(src, calCount, stackBuffer, params);
    params.tempTensorFloorX = stackBuffer[params.firstTmpStartPos];
    params.tempTensorFloorXPow = stackBuffer[params.secondTmpStartPos];
    params.tempTensorRes = stackBuffer[params.thirdTmpStartPos];
    params.tempTensorIntPart = stackBuffer[params.fourthTmpStartPos];
}

template <bool isReuseSource = false, uint8_t expandLevel = 10>
[aicore] __inline__ __attribute__((always_inline)) void GetExpTensorInfo(const LocalTensor<float>& src, const LocalTensor<float>& dst,
    const uint32_t calCount, const LocalTensor<float>& stackBuffer, ExpParams<float>& params)
{
    UpdataExpParams<float, isReuseSource, expandLevel>(src, calCount, stackBuffer, params);
    if constexpr(isReuseSource) {
        params.tempTensorFloorX = src;
    } else {
        params.tempTensorFloorX = stackBuffer[params.firstTmpStartPos];
    }
    params.tempTensorFloorXPow = stackBuffer[params.secondTmpStartPos];
    params.tempTensorRes = dst;
    params.tempTensorIntPart = stackBuffer[params.fourthTmpStartPos];
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetExpInputInTmp(const LocalTensor<T>& src, const ExpParams<float>& params, uint32_t maskLength)
{
    UnaryRepeatParams unaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);
    if constexpr (IsSameType<T, __cce_half>::value) {
        unaryParams.srcRepStride = HALF_REPEAT_STRIDE;
        Cast<float, __cce_half, false>(params.tempTensorFloorX, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    } else {
        Adds<float, false>(params.tempTensorFloorX, src, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}




[aicore] __inline__ __attribute__((always_inline)) void GetExpFloorInput(const ExpParams<float>& params, uint32_t maskLength)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);



    Cast<float, float, false>(params.tempTensorIntPart, params.tempTensorFloorX, RoundMode::CAST_FLOOR,
        MASK_PLACEHOLDER, 1, unaryParams);







    PipeBarrier<PIPE_V>();


    Sub<float, false>(params.tempTensorFloorX, params.tempTensorFloorX, params.tempTensorIntPart, MASK_PLACEHOLDER, 1,
        binaryParams);
    PipeBarrier<PIPE_V>();


    Exp<float, false>(params.tempTensorIntPart, params.tempTensorIntPart, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}




[aicore] __inline__ __attribute__((always_inline)) void ExpHighPrecisionExec(const ExpParams<float>& params, uint32_t maskLength, uint32_t offset)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);


    Adds<float, false>(params.tempTensorFloorXPow, params.tempTensorFloorX, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(params.tempTensorRes[offset], params.tempTensorFloorX, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(params.tempTensorRes[offset], params.tempTensorRes[offset], 1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    for (int32_t i = 2; i < params.expandLevel + 1; i++) {

        Mul<float, false>(params.tempTensorFloorXPow, params.tempTensorFloorX, params.tempTensorFloorXPow,
            MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();


        Muls<float, false>(params.tempTensorFloorXPow, params.tempTensorFloorXPow, float(1.0) / float(i),
            MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();


        Add<float, false>(params.tempTensorRes[offset], params.tempTensorRes[offset], params.tempTensorFloorXPow,
            MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }


    Mul<float, false>(params.tempTensorRes[offset], params.tempTensorRes[offset], params.tempTensorIntPart,
        MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void GetExpCastedResult(const LocalTensor<__cce_half>& dst, const ExpParams<float>& params,
    uint32_t maskLength)
{
    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = HALF_REPEAT_STRIDE;
    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);
    Cast<__cce_half, float, false>(dst, params.tempTensorRes, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ExpHighPrecisionND(const LocalTensor<T>& src, const LocalTensor<T>& dst,
    const ExpParams<float>& params, uint32_t offset, uint32_t maskLength)
{
    GetExpInputInTmp(src[offset], params, maskLength);
    GetExpFloorInput(params, maskLength);


    if constexpr(IsSameType<T, __cce_half>::value) {
        ExpHighPrecisionExec(params, maskLength, 0);
        GetExpCastedResult(dst[offset], params, maskLength);
    } else {
        ExpHighPrecisionExec(params, maskLength, offset);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ExpND(const LocalTensor<T>& src, const LocalTensor<T>& dst, const ExpParams<float>& params)
{
    SetMaskCount();

    uint32_t offset = 0;
    for (uint32_t index = 0; index < params.loopNum; index++) {
        ExpHighPrecisionND(src, dst, params, offset, params.curDataLength);
        offset += params.oneTmpSize;
    }

    if (params.tailSize > 0) {
        ExpHighPrecisionND(src, dst, params, offset, params.tailSize);
    }
}

template <typename T, uint8_t taylorExpandLevel, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void ExpImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if (taylorExpandLevel == 0) {
        Exp<T>(dstLocal, srcLocal, calCount);
        return;
    }

    CheckTensorPosition(dstLocal, "dstLocal", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcLocal, "srcLocal", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcLocal, "srcLocal", "Exp");
    CheckCalCount(calCount, "calCount", dstLocal, "dstLocal", "Exp");

    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    CheckTmpBufferSize(bufferSize, 0, bufferSize);

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    ExpParams<float> expParams;

    ExpAPI::GetExpTensorInfo<isReuseSource, taylorExpandLevel>(srcLocal, dstLocal, calCount, stackBuffer, expParams);
    ExpAPI::ExpND<T>(srcLocal, dstLocal, expParams);

    SetMaskNorm();
    ResetMask();
}

template <typename T, uint8_t taylorExpandLevel, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void ExpImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ExpImpl<T, taylorExpandLevel, isReuseSource>(dstLocal, srcLocal, sharedTmpBuffer, calCount);
}

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/exp.h" 2

namespace AscendC {

#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/exp.h"
template <typename T, uint8_t taylorExpandLevel, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    static_assert((std::is_same<T, float>::value || std::is_same<T, __cce_half>::value),
        "Failed to check the data types, current api support data types are half/float.");
    ExpAPI::ExpImpl<T, taylorExpandLevel, isReuseSource>(dstLocal, srcLocal, calCount);
}
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/exp.h"
template <typename T, uint8_t taylorExpandLevel, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    static_assert((std::is_same<T, float>::value || std::is_same<T, __cce_half>::value),
        "Failed to check the data types, current api support data types are half/float.");
    ExpAPI::ExpImpl<T, taylorExpandLevel, isReuseSource>(dstLocal, srcLocal, sharedTmpBuffer, calCount);
}

#pragma end_pipe
}
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm_utils.h"
namespace AscendC {

struct LayerNormConfig {
    bool isNoBeta = false;
    bool isNoGamma = false;
    bool isOnlyOutput = false;
};

[aicore] constexpr LayerNormConfig GetLayerNormNormalConfig()
{
    return {.isNoBeta = false, .isNoGamma = false, .isOnlyOutput = false};
}

constexpr LayerNormConfig LNCFG_NORM = GetLayerNormNormalConfig();

struct WelfordUpdateConfig {
    [aicore] constexpr WelfordUpdateConfig(const bool isInplaceIn): isInplace(isInplaceIn) {}
    bool isInplace = false;
};

constexpr WelfordUpdateConfig WFUPDATE_DEFAULT_CFG = {false};

struct LayerNormPara {
    uint32_t aLength;
    uint32_t rLength;
    uint32_t rLengthWithPadding;
};

struct WelfordUpdateParam {
    uint32_t rnLength;
    uint32_t abLength;
    uint32_t abComputeLength;
    float nRec;
};

};
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/layernorm_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/layernorm_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/../../../lib/normalization/normalize.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/../../../lib/normalization/normalize.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/normalize_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/normalize_utils.h"
namespace AscendC {



enum class ReducePattern : uint32_t {
    AR = 0,
    RA = 1,
    R,
    ARA,
    ARAR,
    ARARA,
    ARARAR,
    ARARARA,
    ARARARAR,
    ARARARARA,
    RAR,
    RARA,
    RARAR,
    RARARA,
    RARARAR,
    RARARARA,
};


struct NormalizeConfig {
    ReducePattern reducePattern = ReducePattern::AR;
    int32_t aLength = -1;
    bool isNoBeta = false;
    bool isNoGamma = false;
    bool isOnlyOutput = false;
};

[aicore] constexpr NormalizeConfig GetNormalizeConfig(bool isNoBeta, bool isNoGamma)
{
    return {.reducePattern = ReducePattern::AR,
        .aLength = -1,
        .isNoBeta = isNoBeta,
        .isNoGamma = isNoGamma,
        .isOnlyOutput = false};
}

constexpr NormalizeConfig NLCFG_NORM = GetNormalizeConfig(false, false);

constexpr NormalizeConfig NLCFG_NOBETA = GetNormalizeConfig(true, false);

constexpr NormalizeConfig NLCFG_NOGAMMA = GetNormalizeConfig(false, true);

constexpr NormalizeConfig NLCFG_NOOPT = GetNormalizeConfig(true, true);

struct NormalizePara {
    uint32_t aLength;
    uint32_t rLength;
    uint32_t rLengthWithPadding;
};

template <typename T>
struct NormalizeTmpTensor {
    [aicore] NormalizeTmpTensor(){};
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> gammaTmpTensor;
    LocalTensor<T> betaTmpTensor;
};

};
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/../../../lib/normalization/normalize.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/normalize/normalize_common_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/normalize/normalize_common_impl.h"
namespace AscendC {
const float DEFAULT_EPSILON = 1e-5;

template <typename U, typename T>
[aicore] __inline__ __attribute__((always_inline)) constexpr bool IsDtypeValid()
{

    constexpr bool isValid1 = (IsSameType<T, float>::value) && (IsSameType<U, float>::value);
    constexpr bool isValid2 = (IsSameType<T, __cce_half>::value) && (IsSameType<U, __cce_half>::value);
    constexpr bool isValid3 = (IsSameType<T, __cce_half>::value) && (IsSameType<U, float>::value);
    return isValid1 || isValid2 || isValid3;
}

template <const NormalizeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) bool CheckParams(const NormalizePara& para)
{
    static_assert(config.isOnlyOutput == false, "isOnlyOutput must be set false for now.");
    static_assert(config.aLength != 1, "aLength in config must not be 1.");
    if constexpr(config.aLength != -1) {

                                                            ;
    }
    return true;
}

template <typename U, typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetNormalizeTensorInfo(const LocalTensor<float>& stackBuffer, const NormalizePara& para,
    NormalizeTmpTensor<float>& tempTensor, uint32_t& N)
{



                                                       ;
    uint32_t R2 = para.rLengthWithPadding * 2;
    if constexpr(IsSameType<U, float>::value) {
        N = stackBuffer.GetSize() / R2;
        N = (N >= para.aLength) ? para.aLength : N;
                                                                                              ;
        tempTensor.tempTensorA = stackBuffer[0];
        tempTensor.tempTensorB = stackBuffer[N * para.rLengthWithPadding];
    } else {
        N = (stackBuffer.GetSize() - R2) / R2;
        N = (N >= para.aLength) ? para.aLength : N;
                                                                                              ;
        tempTensor.tempTensorA = stackBuffer[0];
        tempTensor.tempTensorB = stackBuffer[N * para.rLengthWithPadding];
        tempTensor.gammaTmpTensor = stackBuffer[R2 * N];
        tempTensor.betaTmpTensor = stackBuffer[R2 * N + para.rLengthWithPadding];
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GetNormalizeOutputRstd(const LocalTensor<float>& dstRstd, const LocalTensor<float>& srcVar,
    const NormalizeTmpTensor<float>& tmpTensor, const UnaryRepeatParams& unaryParams,
    const BinaryRepeatParams& binaryParams, const NormalizePara& para, const float epsilon)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, para.aLength);


    Adds<float, false>(dstRstd, srcVar, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(tmpTensor.tempTensorA, float(1), 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Sqrt<float, false>(dstRstd, dstRstd, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<float, false>(dstRstd, tmpTensor.tempTensorA, dstRstd, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void CastTensor(const LocalTensor<U>& src, const LocalTensor<float>& castRes)
{

    Cast<float, U, false>(castRes, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE,
        HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}

template <typename U, const NormalizeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void CastGammaBeta(const LocalTensor<U>& gamma, const LocalTensor<U>& beta,
    const NormalizeTmpTensor<float>& tmpTensor, const NormalizePara& para)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, para.rLength);
    if constexpr(!config.isNoGamma) {
        CastTensor<U>(gamma, tmpTensor.gammaTmpTensor);
    }
    if constexpr(!config.isNoBeta) {
        CastTensor<U>(beta, tmpTensor.betaTmpTensor);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CastSrc(const LocalTensor<T>& srcX, const NormalizeTmpTensor<float>& tmpTensor,
    const NormalizePara& para, const uint32_t N)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, N * para.rLengthWithPadding);
    if constexpr(IsSameType<T, float>::value) {
        Adds<float, false>(tmpTensor.tempTensorA, srcX, (float)0, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE,
            DEFAULT_REPEAT_STRIDE});
    } else {
        Cast<float, T, false>(tmpTensor.tempTensorA, srcX, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, {1, 1,
            DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    }
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void GetNormalizeOutputPre(const LocalTensor<float>& srcX, const LocalTensor<float>& srcMean,
    const LocalTensor<float>& srcRstd, const LocalTensor<float>& brcbTmp, const LocalTensor<float>& dstVmuls,
    const NormalizePara& para, const BinaryRepeatParams& binaryParams, const uint32_t N, const uint32_t NBase)
{

    SetVectorMask<float, MaskMode::COUNTER>(0, para.rLength);
    auto eventId = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventId);
    WaitFlag<HardEvent::V_S>(eventId);
    for (uint32_t i = 0; i < N; i++) {
        float value = srcMean.GetValue(i + NBase);
        eventId = GetTPipePtr()->FetchEventID(HardEvent::S_V);
        SetFlag<HardEvent::S_V>(eventId);
        WaitFlag<HardEvent::S_V>(eventId);
        Duplicate<float, false>(brcbTmp[i * para.rLengthWithPadding], value, 1, 1, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, N * para.rLengthWithPadding);
    Sub<float, false>(srcX, srcX, brcbTmp, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    SetVectorMask<float, MaskMode::COUNTER>(0, para.rLength);
    for (uint32_t i = 0; i < N; i++) {
        float value = srcRstd.GetValue(i + NBase);
        eventId = GetTPipePtr()->FetchEventID(HardEvent::S_V);
        SetFlag<HardEvent::S_V>(eventId);
        WaitFlag<HardEvent::S_V>(eventId);
        Duplicate<float, false>(brcbTmp[i * para.rLengthWithPadding], value, 1, 1, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, N * para.rLengthWithPadding);
    Mul<float, false>(dstVmuls, srcX, brcbTmp, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, const NormalizeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void GetNormalizeOutput(const LocalTensor<float>& srcxFP32, const LocalTensor<float>& gammaFP32,
    const LocalTensor<float>& betaFP32, const LocalTensor<T>& output, const NormalizePara& para,
    const UnaryRepeatParams& unaryParams, const BinaryRepeatParams& binaryParams, const uint32_t N)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, para.rLength);
    for (uint32_t i = 0; i < N; i++) {
        if constexpr(!config.isNoGamma) {
            Mul<float, false>(srcxFP32[i * para.rLengthWithPadding], srcxFP32[i * para.rLengthWithPadding], gammaFP32,
                MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        if constexpr(!config.isNoBeta) {
            Add<float, false>(srcxFP32[i * para.rLengthWithPadding], srcxFP32[i * para.rLengthWithPadding], betaFP32,
                MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, N * para.rLengthWithPadding);
    if constexpr(IsSameType<T, float>::value) {
        Adds<float, false>(output, srcxFP32, (float)0, MASK_PLACEHOLDER, 1, unaryParams);
    } else {
        Cast<T, float, false>(output, srcxFP32, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, {1, 1,
            HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    }
    PipeBarrier<PIPE_V>();
}

template <typename U, typename T, bool isReuseSource, const NormalizeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void NormalizeImpl(const LocalTensor<T>& output, const LocalTensor<float>& outputRstd,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<U>& gamma, const LocalTensor<U>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const float epsilon, const NormalizePara& para)
{
    static_assert(IsDtypeValid<U, T>(), "Failed to check dtype in Normalize, current api support dtype combination is "
        "T: float, U: float; T: half, U: half / float.");
    bool res = CheckParams<config>(para);

    const UnaryRepeatParams unaryParam;
    const BinaryRepeatParams binaryParam;
    NormalizeTmpTensor<float> tmpTensor;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t N = 0;
    GetNormalizeTensorInfo<U, T>(stackBuffer, para, tmpTensor, N);
    uint32_t mainRepeatTimes = para.aLength / N;
    uint32_t tailN = para.aLength % N;

    GetNormalizeOutputRstd(outputRstd, inputVariance, tmpTensor, unaryParam, binaryParam, para, epsilon);
    if constexpr(IsSameType<U, __cce_half>::value) {
        CastGammaBeta<U, config>(gamma, beta, tmpTensor, para);
    }

    for (uint32_t i = 0; i < mainRepeatTimes; i++) {
        uint32_t index = para.rLengthWithPadding * N * i;
        CastSrc<T>(inputX[index], tmpTensor, para, N);
        GetNormalizeOutputPre(tmpTensor.tempTensorA, inputMean, outputRstd, tmpTensor.tempTensorB,
            tmpTensor.tempTensorA, para, binaryParam, N, N * i);
        if constexpr(IsSameType<U, float>::value) {
            GetNormalizeOutput<T, config>(tmpTensor.tempTensorA, gamma, beta,
                output[index], para, unaryParam, binaryParam, N);
        } else {
            GetNormalizeOutput<T, config>(tmpTensor.tempTensorA, tmpTensor.gammaTmpTensor, tmpTensor.betaTmpTensor,
                output[index], para, unaryParam, binaryParam, N);
        }
    }

    if (tailN > 0) {
        uint32_t index = para.rLengthWithPadding * N * mainRepeatTimes;
        CastSrc<T>(inputX[index], tmpTensor, para, tailN);
        GetNormalizeOutputPre(tmpTensor.tempTensorA, inputMean, outputRstd, tmpTensor.tempTensorB,
            tmpTensor.tempTensorA, para, binaryParam, tailN, N * mainRepeatTimes);
        if constexpr(IsSameType<U, float>::value) {
            GetNormalizeOutput<T, config>(tmpTensor.tempTensorA, gamma, beta,
                output[index], para, unaryParam, binaryParam, tailN);
        } else {
            GetNormalizeOutput<T, config>(tmpTensor.tempTensorA, tmpTensor.gammaTmpTensor, tmpTensor.betaTmpTensor,
                output[index], para, unaryParam, binaryParam, tailN);
        }
    }

    SetMaskNorm();
    ResetMask();
}

template <typename U, typename T, bool isReuseSource, const NormalizeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void NormalizeImpl(const LocalTensor<T>& output, const LocalTensor<float>& outputRstd,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<U>& gamma, const LocalTensor<U>& beta, const float epsilon, const NormalizePara& para)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    NormalizeImpl<U, T, isReuseSource, config>(output, outputRstd, inputMean, inputVariance, inputX, gamma, beta,
        sharedTmpBuffer, epsilon, para);
}

}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/../../../lib/normalization/normalize.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/../../../lib/normalization/normalize.h"
template <typename U, typename T, bool isReuseSource = false, const NormalizeConfig& config = NLCFG_NORM>
[aicore] __inline__ __attribute__((always_inline)) void Normalize(const LocalTensor<T>& output, const LocalTensor<float>& outputRstd,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<U>& gamma, const LocalTensor<U>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const float epsilon, const NormalizePara& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    NormalizeImpl<U, T, isReuseSource, config>(output, outputRstd, inputMean, inputVariance, inputX, gamma, beta,
        sharedTmpBuffer, epsilon, para);
}
# 70 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/../../../lib/normalization/normalize.h"
template <typename U, typename T, bool isReuseSource = false, const NormalizeConfig& config = NLCFG_NORM>
[aicore] __inline__ __attribute__((always_inline)) void Normalize(const LocalTensor<T>& output, const LocalTensor<float>& outputRstd,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<U>& gamma, const LocalTensor<U>& beta, const float epsilon, const NormalizePara& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    NormalizeImpl<U, T, isReuseSource, config>(output, outputRstd, inputMean, inputVariance, inputX, gamma, beta,
        epsilon, para);
}
#pragma end_pipe
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/layernorm_common_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/layernorm_common_basic_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/layernorm_common_basic_impl.h"
namespace AscendC {
constexpr uint32_t MASK_LOW_6BITS = 0x3f;
constexpr uint32_t MASK_HIGH_26BITS = 0xFFFFFFC0;
template <typename T>
struct LayerNormRstdTmpTensorParams {
    [aicore] LayerNormRstdTmpTensorParams(){};
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> varianceTmpTensor;
};

template <bool isRelocate = true, bool isTransposeDst = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormReduceSumImpl(const LocalTensor<float>& dstMVTmp, const LocalTensor<float>& dst,
    const LocalTensor<float>& src, const uint32_t bsLength, const uint32_t hLength)
{
    ResetMask();
    SetMaskNorm();

    constexpr uint32_t rightShiftSix = 6;
    if (hLength > ONE_REPEAT_FLOAT_SIZE) {
        uint32_t addRepeatTime = (hLength >> rightShiftSix) - 1;
        uint32_t addTailNumber = (hLength & MASK_LOW_6BITS);
        if ((hLength & MASK_LOW_6BITS) == 0) {
            for (uint32_t i = 0; i < bsLength * hLength; i += hLength) {
                LocalTensor<float> dstTmp = src[i];
                LocalTensor<float> srcTmp = src[i + ONE_REPEAT_FLOAT_SIZE];
                Add(dstTmp, srcTmp, dstTmp, ONE_REPEAT_FLOAT_SIZE, addRepeatTime,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
            }
        } else if (addRepeatTime > 0) {
            for (uint32_t i = 0; i < bsLength * hLength; i += hLength) {
                LocalTensor<float> dstTmp = src[i];
                LocalTensor<float> srcTmp = src[i + ONE_REPEAT_FLOAT_SIZE];
                LocalTensor<float> srcTailTmp = src[i + (hLength & MASK_HIGH_26BITS)];
                Add(dstTmp, srcTmp, dstTmp, ONE_REPEAT_FLOAT_SIZE, addRepeatTime,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
                Add(dstTmp, srcTailTmp, dstTmp, addTailNumber, 1,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
            }
        } else {
            for (uint32_t i = 0; i < bsLength * hLength; i += hLength) {
                LocalTensor<float> dstTmp = src[i];
                LocalTensor<float> srcTailTmp = src[i + (hLength & MASK_HIGH_26BITS)];
                Add(dstTmp, srcTailTmp, dstTmp, addTailNumber, 1,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
            }
        }
    }

    uint32_t repeatTime = bsLength;
    uint32_t cursorSrc = 0;
    uint32_t wholeReduceSumHLength = (hLength > ONE_REPEAT_FLOAT_SIZE) ? ONE_REPEAT_FLOAT_SIZE : hLength;
    constexpr uint32_t rightShiftThree = 3;
    const uint32_t reduceSumSrcRepeatStride = hLength >> rightShiftThree;

    while (repeatTime >= MAX_REPEAT_TIMES) {
        LocalTensor<float> srcTmp = src[cursorSrc * MAX_REPEAT_TIMES * hLength];
        LocalTensor<float> dstTmp = dst[cursorSrc * MAX_REPEAT_TIMES * hLength];
        if constexpr (isRelocate) {
            WholeReduceSum<float>(dstMVTmp[cursorSrc * MAX_REPEAT_TIMES], srcTmp, wholeReduceSumHLength,
                MAX_REPEAT_TIMES, 1, DEFAULT_BLK_STRIDE, reduceSumSrcRepeatStride);
        }
        WholeReduceSum<float>(dstTmp, srcTmp, wholeReduceSumHLength, MAX_REPEAT_TIMES, hLength, DEFAULT_BLK_STRIDE,
            reduceSumSrcRepeatStride);
        PipeBarrier<PIPE_V>();
        repeatTime -= MAX_REPEAT_TIMES;
        ++cursorSrc;
    }

    uint32_t reduceSumSrcRepeatTimeTail = bsLength - cursorSrc * MAX_REPEAT_TIMES;
    if (reduceSumSrcRepeatTimeTail > 0) {
        LocalTensor<float> srcTmp = src[cursorSrc * MAX_REPEAT_TIMES * hLength];
        LocalTensor<float> dstTmp = dst[cursorSrc * MAX_REPEAT_TIMES * hLength];
        if constexpr (isRelocate) {
            WholeReduceSum<float>(dstMVTmp[cursorSrc * MAX_REPEAT_TIMES], srcTmp, wholeReduceSumHLength,
                reduceSumSrcRepeatTimeTail, 1, DEFAULT_BLK_STRIDE, reduceSumSrcRepeatStride);
        }
        WholeReduceSum<float>(dstTmp, srcTmp, wholeReduceSumHLength, reduceSumSrcRepeatTimeTail, hLength,
            DEFAULT_BLK_STRIDE, reduceSumSrcRepeatStride);
        PipeBarrier<PIPE_V>();
    }

    SetMaskCount();
}

[aicore] __inline__ __attribute__((always_inline)) void BroadcastLastDim(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength)
{
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, hLength);

    SetCmpMask<float>(src);
    PipeBarrier<PIPE_V>();

    LocalTensor<int16_t> maskLocal = src.ReinterpretCast<int16_t>();

    const UnaryRepeatParams unaryParams;
    Muls<int16_t, false>(maskLocal, maskLocal, 0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Select<float, int16_t>(dst, maskLocal, dst, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    for (uint32_t i = 1; i < bsLength; i++) {
        SetCmpMask<float>(src[i * hLength]);
        PipeBarrier<PIPE_V>();

        Select<float, int16_t>(dst[i * hLength], maskLocal, dst, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void DuplicateMulImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    const BinaryRepeatParams binaryParams;
    for (uint32_t i = 0; i < bsLength; i++) {
        Mul<float, false>(dst[i * hLength], src0[i * hLength], src1, MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DuplicateAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    const BinaryRepeatParams binaryParams;
    for (uint32_t i = 0; i < bsLength; i++) {
        Add<float, false>(dst[i * hLength], src0[i * hLength], src1, MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormNDTensorInfo(const LocalTensor<T>& inputX, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<float>& stackBuffer, const LayerNormTiling& tiling,
    LayerNormParams<float>& params)
{
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];



      ;



      ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormNDTensorInfo<float, false>(const LocalTensor<float> &inputX,
    const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
    const LocalTensor<float> &stackBuffer, const LayerNormTiling &tiling, LayerNormParams<float> &params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];




      ;




      ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormNDTensorInfo<float, true>(const LocalTensor<float> &inputX,
    const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
    const LocalTensor<float> &stackBuffer, const LayerNormTiling &tiling, LayerNormParams<float> &params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    params.tempTensorA = inputX;
    params.tempTensorB = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.secondTmpStartPos];




      ;




      ;
}

[aicore] __inline__ __attribute__((always_inline)) void GetOutputMeanVariance(const LocalTensor<__cce_half>& outputMean,
    const LocalTensor<__cce_half>& outputVariance, const LayerNormTiling& tiling, const LayerNormParams<float>& params)
{
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.meanVarSize);

    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);

    Cast<__cce_half, float, false>(outputMean, params.meanTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<__cce_half, float, false>(outputVariance, params.varianceTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormRstdTensorInfo(const LocalTensor<float>& stackBuffer,
    const LayerNormSeparateTiling& tiling, LayerNormRstdTmpTensorParams<float>& params)
{
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];



      ;



      ;
}

template <typename U, typename T, const LayerNormConfig& config = LNCFG_NORM>
[aicore] __inline__ __attribute__((always_inline)) void CheckLayerNormRstd(const LocalTensor<float> stackBuffer, const LayerNormPara& para) {
    static_assert(SupportType<T, __cce_half, float>(), "current data type is not supported on current device!");
    if constexpr (IsSameType<T, __cce_half>::value) {
        static_assert(SupportType<U, __cce_half, float>(), "current data type is not supported on current device!");
    } else if constexpr (IsSameType<T, float>::value) {
        static_assert(SupportType<U, float>(), "current data type is not supported on current device!");
    }
    static_assert(config.isOnlyOutput == false, "current data type is not supported on current device!");

                                                                                          ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormPreProc(const LocalTensor<T>& inputX, const LocalTensor<float>& stackBuffer,
    const LayerNormPara& para)
{
    const LocalTensor<T> tempTensor = stackBuffer.ReinterpretCast<T>();
    Duplicate(tempTensor, (T)0, para.rLengthWithPadding);
    PipeBarrier<PIPE_V>();
    Adds(tempTensor, tempTensor, (T)1, para.rLength);
    PipeBarrier<PIPE_V>();
    for (int i = 0; i < para.aLength; i++) {
        Mul(inputX[i * para.rLengthWithPadding], inputX[i * para.rLengthWithPadding], tempTensor,
            para.rLengthWithPadding);
    }
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateComputeMean(const LocalTensor<float>& tmpVreg, const LocalTensor<float>& src,
    const LocalTensor<float>& inMean, const LocalTensor<float>& outVreg, const LocalTensor<float>& outMean,
    const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams, const WelfordUpdateParam &para)
{
    PipeBarrier<PIPE_V>();
    Sub<float, false>(tmpVreg, src, inMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(outVreg, tmpVreg, static_cast<float>(para.nRec), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(outMean, outVreg, inMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateComputeVar(const LocalTensor<float>& tmpVreg, const LocalTensor<float>& inVar,
    const LocalTensor<float>& outVar, const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams,
    const WelfordUpdateParam &para)
{
    PipeBarrier<PIPE_V>();
    Add<float, false>(outVar, tmpVreg, inVar, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, typename U, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) constexpr uint32_t WelfordUpdateGetTmpSize()
{
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        return 0x3;
    }

    if constexpr (isReuseSource) {
        return 1;
    }
    return 0x2;
}

[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutputMean(const LocalTensor<float>& outputMean, const LocalTensor<float>& inputX,
    const LayerNormTiling& tiling, const LayerNormParams<float>& params, const LocalTensor<float>& tmpMean)
{
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    const UnaryRepeatParams unaryParams;
    Muls<float, false>(params.tempTensorC, inputX, tiling.lastDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl(tmpMean, outputMean, params.tempTensorC, tiling.bsCurLength, tiling.hLength);
}

[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutputVariance(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& inputMean, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params, const LocalTensor<float>& tmpVariance)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    BroadcastLastDim(tempTensorC, inputMean, tiling.bsCurLength, tiling.hLength);

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    const BinaryRepeatParams binaryParams;
    Sub<float, false>(tempTensorB, inputX, tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempTensorC, tempTensorB, tempTensorB, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    const UnaryRepeatParams unaryParams;
    Muls<float, false>(tempTensorA, tempTensorC, tiling.lastDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl(tmpVariance, outputVariance, tempTensorA, tiling.bsCurLength, tiling.hLength);
    PipeBarrier<PIPE_V>();
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateInplaceCompute(const LocalTensor<U>& outMean, const LocalTensor<U>& outVar,
    const LocalTensor<U>& inMean, const LocalTensor<U>& inVar, const WelfordUpdateParam &para, uint32_t alignNum)
{
    uint32_t inPlaceLength = AlignUp(para.abLength - para.abComputeLength, alignNum);
    uint32_t dstOffset = para.abLength - inPlaceLength;

    DataCopy(outMean[dstOffset], inMean[dstOffset], inPlaceLength);
    DataCopy(outVar[dstOffset], inVar[dstOffset], inPlaceLength);
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateInplace(const LocalTensor<float>& outMean, const LocalTensor<float>& outVar,
    const LocalTensor<float>& inMean, const LocalTensor<float>& inVar, const WelfordUpdateParam &para)
{
    WelfordUpdateInplaceCompute(outMean, outVar, inMean, inVar, para, B32_DATA_NUM_PER_BLOCK);
}

[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateInplace(const LocalTensor<__cce_half>& outMean, const LocalTensor<__cce_half>& outVar,
    const LocalTensor<__cce_half>& inMean, const LocalTensor<__cce_half>& inVar, const WelfordUpdateParam &para)
{
    WelfordUpdateInplaceCompute(outMean, outVar, inMean, inVar, para, B16_DATA_NUM_PER_BLOCK);
}

[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutputPre(const LocalTensor<float>& xSubMean,
    const LocalTensor<float>& inputVariance, const float epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    const float exponent = -0.5;
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    BroadcastLastDim(tempTensorA, inputVariance, tiling.bsCurLength, tiling.hLength);

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    const UnaryRepeatParams unaryParams;
    Adds<float, false>(tempTensorC, tempTensorA, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sqrt<float, false>(tempTensorA, tempTensorC, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, B32_DATA_NUM_PER_BLOCK);
    Duplicate<float, false>(tempTensorC, 1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Div<float, false>(tempTensorA, tempTensorC, tempTensorA, MASK_PLACEHOLDER, 1,
        { 1, 0, 1, DEFAULT_REPEAT_STRIDE, 0, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Mul<float, false>(tempTensorC, tempTensorA, xSubMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/layernorm_common_impl.h" 2

namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutput(const LocalTensor<T>& output, const LocalTensor<float>& inputY,
    const LocalTensor<T>& gamma, const LocalTensor<T>& beta, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{}

template <>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutput<__cce_half>(const LocalTensor<__cce_half>& output, const LocalTensor<float>& inputY,
    const LocalTensor<__cce_half>& gamma, const LocalTensor<__cce_half>& beta, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.hLength);

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);
    Cast<float, __cce_half, false>(tempTensorA, gamma, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    DuplicateMulImpl(tempTensorB, inputY, tempTensorA, tiling.bsCurLength, tiling.hLength);

    Cast<float, __cce_half, false>(tempTensorC, beta, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    DuplicateAddImpl(tempTensorA, tempTensorB, tempTensorC, tiling.bsCurLength, tiling.hLength);

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);

    Cast<__cce_half, float, false>(output, tempTensorA, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutput<float>(const LocalTensor<float>& output, const LocalTensor<float>& inputY,
    const LocalTensor<float>& gamma, const LocalTensor<float>& beta, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.hLength);

    DuplicateMulImpl(tempTensorA, inputY, gamma, tiling.bsCurLength, tiling.hLength);

    DuplicateAddImpl(output, tempTensorA, beta, tiling.bsCurLength, tiling.hLength);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormExe(const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const T epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormExe<__cce_half>(const LocalTensor<__cce_half>& inputX, const LocalTensor<__cce_half>& gamma,
    const LocalTensor<__cce_half>& beta, const LocalTensor<__cce_half>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const __cce_half epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);
    Cast<float, __cce_half, false>(tempTensorA, inputX, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    GetLayerNormOutputMean(tempTensorB, tempTensorA, tiling, params, outputMean);

    GetLayerNormOutputVariance(tempTensorC, tempTensorA, tempTensorB, tiling, params, outputVariance);

    GetLayerNormOutputPre(tempTensorB, tempTensorC, static_cast<float>(epsilon), tiling, params);

    GetLayerNormOutput(output, tempTensorC, gamma, beta, tiling, params);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormExe<float>(const LocalTensor<float>& inputX, const LocalTensor<float>& gamma,
    const LocalTensor<float>& beta, const LocalTensor<float>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const float epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    GetLayerNormOutputMean(tempTensorB, inputX, tiling, params, outputMean);

    GetLayerNormOutputVariance(tempTensorC, inputX, tempTensorB, tiling, params, outputVariance);

    GetLayerNormOutputPre(tempTensorB, tempTensorC, epsilon, tiling, params);

    GetLayerNormOutput(output, tempTensorC, gamma, beta, tiling, params);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormND(const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const T epsilon, LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    uint32_t inputOffset = 0;
    uint32_t mvOffset = 0;

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        LayerNormExe<T>(inputX[inputOffset], gamma, beta, output[inputOffset], params.meanTmpTensor[mvOffset],
            params.varianceTmpTensor[mvOffset], epsilon, tiling, params);

        inputOffset += tiling.inputRoundSize;
        mvOffset += tiling.meanVarRoundSize;
    }

    if (tiling.inputTailSize > 0) {
        tiling.bshCurLength = tiling.inputTailSize;
        tiling.bsCurLength = tiling.meanVarTailSize;

        inputOffset = tiling.inputTailPos;
        mvOffset = tiling.meanVarTailPos;

        LayerNormExe<T>(inputX[inputOffset], gamma, beta, output[inputOffset], params.meanTmpTensor[mvOffset],
            params.varianceTmpTensor[mvOffset], epsilon, tiling, params);
    }

    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        GetOutputMeanVariance(outputMean, outputVariance, tiling, params);
    }
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, LayerNormTiling& tiling)
{
                                   ;
                                                                                                         ;

    if constexpr(g_coreType == AscendC::AIC) {
                                      ;
        return;
    }

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
                                                                                                                ;

    LayerNormParams<float> params;
    GetLayerNormNDTensorInfo<T, isReuseSource>(inputX, outputMean, outputVariance, stackBuffer, tiling, params);

    SetMaskCount();
    LayerNormND<T>(inputX, gamma, beta, output, outputMean, outputVariance, epsilon, tiling, params);

    SetMaskNorm();
    ResetMask();
                                  ;
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const T epsilon, LayerNormTiling& tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    LayerNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, sharedTmpBuffer, epsilon,
        tiling);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ComputeMeanVariance(const LocalTensor<float>& outputMean, const LocalTensor<float>& variance,
    const LocalTensor<T>& src, const LayerNormRstdTmpTensorParams<float>& params, const LayerNormPara& para,
    const LayerNormSeparateTiling& tiling)
{
    if constexpr (IsSameType<T, __cce_half>::value) {
        Cast<float, T>(params.tempTensorA, src, RoundMode::CAST_NONE, tiling.arCurLength);
        PipeBarrier<PIPE_V>();
    } else {
        DataCopy(params.tempTensorA, src, tiling.arCurLength);
        PipeBarrier<PIPE_V>();
    }

    Muls<float>(params.tempTensorA, params.tempTensorA, tiling.rValueBack, tiling.arCurLength);
    PipeBarrier<PIPE_V>();


    LayerNormReduceSumImpl(outputMean, params.tempTensorB, params.tempTensorA, tiling.aCurLength,
        para.rLengthWithPadding);
    auto eventId = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventId);
    WaitFlag<HardEvent::V_S>(eventId);

    if constexpr (IsSameType<T, __cce_half>::value) {
        PipeBarrier<PIPE_V>();
        Cast<float, T>(params.tempTensorA, src, RoundMode::CAST_NONE, tiling.arCurLength);
        PipeBarrier<PIPE_V>();
    } else {
        DataCopy(params.tempTensorA, src, tiling.arCurLength);
        PipeBarrier<PIPE_V>();
    }

    eventId = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    for (uint32_t j = 0; j < tiling.aCurLength; j++) {
        float scalar = float(-1) * outputMean.GetValue(j);
        SetFlag<HardEvent::S_V>(eventId);
        WaitFlag<HardEvent::S_V>(eventId);
        Adds<float>(params.tempTensorA[j * para.rLengthWithPadding], params.tempTensorA[j * para.rLengthWithPadding],
            scalar, tiling.rLength);
    }
    PipeBarrier<PIPE_V>();

    Mul<float>(params.tempTensorA, params.tempTensorA, params.tempTensorA, tiling.arCurLength);
    PipeBarrier<PIPE_V>();

    Muls<float>(params.tempTensorA, params.tempTensorA, tiling.rValueBack, tiling.arCurLength);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl(variance, params.tempTensorB, params.tempTensorA, tiling.aCurLength,
        para.rLengthWithPadding);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormCalMeanVarImpl(const LocalTensor<float>& OutputMean, const LocalTensor<float>& variance,
    const LocalTensor<T>& inputX, const LocalTensor<float>& stackBuffer, const LayerNormPara& para,
    LayerNormSeparateTiling& tiling)
{
    if (para.rLength != para.rLengthWithPadding) {
        LayerNormPreProc<T>(inputX, stackBuffer, para);
    }

    LayerNormRstdTmpTensorParams<float> params;
    GetLayerNormRstdTensorInfo<isReuseSource>(stackBuffer, tiling, params);

    uint32_t inputOffset = 0;
    uint32_t mvOffset = 0;

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        ComputeMeanVariance<T>(OutputMean[mvOffset], variance[mvOffset], inputX[inputOffset], params, para, tiling);

        inputOffset += tiling.inputRoundSize;
        mvOffset += tiling.meanVarRoundSize;
    }
    if (tiling.inputTailSize > 0) {
        tiling.arCurLength = tiling.inputTailSize;
        tiling.aCurLength = tiling.meanVarTailSize;

        inputOffset = tiling.inputTailPos;
        mvOffset = tiling.meanVarTailPos;

        ComputeMeanVariance<T>(OutputMean[mvOffset], variance[mvOffset], inputX[inputOffset], params, para, tiling);
    }
}

template <typename U, typename T, bool isReuseSource = false, const LayerNormConfig& config = LNCFG_NORM>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormImpl(const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputRstd, const LocalTensor<T>& inputX, const LocalTensor<U>& gamma,
    const LocalTensor<U>& beta, const float epsilon, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LayerNormPara& para, const LayerNormSeparateTiling& tiling) {
        const LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
        CheckLayerNormRstd<U, T, config>(stackBuffer, para);

        const LocalTensor<float> variance = stackBuffer[tiling.varianceTmpTensorPos];
        LayerNormSeparateTiling& Tiling = const_cast<LayerNormSeparateTiling&>(tiling);
        LayerNormCalMeanVarImpl<T, isReuseSource>(outputMean, variance, inputX, stackBuffer, para, Tiling);
        const LocalTensor<uint8_t> shareTmpBuffer = stackBuffer[tiling.firstTmpStartPos].ReinterpretCast<uint8_t>();

        NormalizePara normallizepara = {para.aLength, para.rLength, para.rLengthWithPadding};
        if (config.isNoBeta == false && config.isNoGamma == false) {
            Normalize<U, T, false, NLCFG_NORM>(output, outputRstd, outputMean, variance, inputX, gamma, beta,
                shareTmpBuffer, epsilon, normallizepara);
        } else if (config.isNoBeta == true && config.isNoGamma == false) {
            Normalize<U, T, false, NLCFG_NOBETA>(output, outputRstd, outputMean, variance, inputX, gamma, beta,
                shareTmpBuffer, epsilon, normallizepara);
        } else if (config.isNoBeta == false && config.isNoGamma == true) {
            Normalize<U, T, false, NLCFG_NOGAMMA>(output, outputRstd, outputMean, variance, inputX, gamma, beta,
                shareTmpBuffer, epsilon, normallizepara);
        } else {
            Normalize<U, T, false, NLCFG_NOOPT>(output, outputRstd, outputMean, variance, inputX, gamma, beta,
                shareTmpBuffer, epsilon, normallizepara);
        }
    }

template <typename U, typename T, bool isReuseSource = false, const LayerNormConfig& config = LNCFG_NORM>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormImpl(const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputRstd, const LocalTensor<T>& inputX, const LocalTensor<U>& gamma,
    const LocalTensor<U>& beta, const float epsilon, const LayerNormPara& para, const LayerNormSeparateTiling& tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    LayerNormImpl<U, T, isReuseSource, config>(output, outputMean, outputRstd, inputX, gamma, beta, epsilon,
        sharedTmpBuffer, para, tiling);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateCompute(const LocalTensor<float>& outMean, const LocalTensor<float>& outVar,
    const LocalTensor<__cce_half>& src, const LocalTensor<float>& inMean, const LocalTensor<float>& inVar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam &para, const uint32_t tmpNum,
    const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams)
{
    LocalTensor<float> srcVreg = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t tmpIndex = B32_DATA_NUM_PER_REPEAT * tmpNum;
    LocalTensor<float> tmpVreg = srcVreg[tmpIndex];
    LocalTensor<float> outVreg = srcVreg[tmpIndex + tmpIndex];

    PipeBarrier<PIPE_V>();
    Cast<float, __cce_half, false>(srcVreg, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});

    WelfordUpdateComputeMean(tmpVreg, srcVreg, inMean, outVreg, outMean, unaryParams, binaryParams, para);

    Sub<float, false>(outVreg, srcVreg, outMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpVreg, tmpVreg, outVreg, MASK_PLACEHOLDER, 1, binaryParams);

    WelfordUpdateComputeVar(tmpVreg, inVar, outVar, unaryParams, binaryParams, para);
}

[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateComputeTo32Res(const LocalTensor<float>& outMean, const LocalTensor<float>& outVar,
    const LocalTensor<float>& src, const LocalTensor<float>& inMean, const LocalTensor<float>& inVar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam &para, const uint32_t tmpNum,
    const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams)
{
    LocalTensor<float> tmpVreg = sharedTmpBuffer.ReinterpretCast<float>();

    WelfordUpdateComputeMean(tmpVreg, src, inMean, tmpVreg, outMean, unaryParams, binaryParams, para);

    Sub<float, false>(tmpVreg, src, outMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(src, src, inMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpVreg, tmpVreg, src, MASK_PLACEHOLDER, 1, binaryParams);

    WelfordUpdateComputeVar(tmpVreg, inVar, outVar, unaryParams, binaryParams, para);
}

[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateComputeTo32(const LocalTensor<float>& outMean, const LocalTensor<float>& outVar,
    const LocalTensor<float>& src, const LocalTensor<float>& inMean, const LocalTensor<float>& inVar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam &para, const uint32_t tmpNum,
    const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams)
{
    LocalTensor<float> tmpVreg = sharedTmpBuffer.ReinterpretCast<float>();
    LocalTensor<float> outVreg = tmpVreg[B32_DATA_NUM_PER_REPEAT * tmpNum];

    WelfordUpdateComputeMean(tmpVreg, src, inMean, outVreg, outMean, unaryParams, binaryParams, para);

    Sub<float, false>(outVreg, src, outMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpVreg, tmpVreg, outVreg, MASK_PLACEHOLDER, 1, binaryParams);

    WelfordUpdateComputeVar(tmpVreg, inVar, outVar, unaryParams, binaryParams, para);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateCompute(const LocalTensor<float>& outMean, const LocalTensor<float>& outVar,
    const LocalTensor<float>& src, const LocalTensor<float>& inMean, const LocalTensor<float>& inVar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam &para, const uint32_t tmpNum,
    const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams)
{
    if (isReuseSource) {
        WelfordUpdateComputeTo32Res(outMean, outVar, src, inMean, inVar, sharedTmpBuffer, para, tmpNum, unaryParams,
            binaryParams);
    } else {
        WelfordUpdateComputeTo32(outMean, outVar, src, inMean, inVar, sharedTmpBuffer, para, tmpNum, unaryParams,
            binaryParams);
    }
}

template <typename T, typename U, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateComputeImpl(const LocalTensor<U>& outMean, const LocalTensor<U>& outVar,
    const LocalTensor<T>& src, const LocalTensor<U>& inMean, const LocalTensor<U>& inVar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam &para)
{
    constexpr uint32_t tmpBufNum = WelfordUpdateGetTmpSize<T, U, isReuseSource>();

    uint32_t tmpNum = sharedTmpBuffer.GetSize() / (ONE_REPEAT_BYTE_SIZE * tmpBufNum);
# 419 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/layernorm_common_impl.h"
    const uint32_t round = para.abComputeLength / (B32_DATA_NUM_PER_REPEAT * tmpNum);
    const uint32_t tail = para.abComputeLength % (B32_DATA_NUM_PER_REPEAT * tmpNum);

    SetVectorMask<float, MaskMode::COUNTER>(0, B32_DATA_NUM_PER_REPEAT * tmpNum);
    uint32_t offset = 0;

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    for (uint32_t i = 0; i < round; ++i) {
        WelfordUpdateCompute<isReuseSource>(outMean[offset], outVar[offset], src[offset], inMean[offset],
            inVar[offset], sharedTmpBuffer, para, tmpNum, unaryParams, binaryParams);
        offset = offset + B32_DATA_NUM_PER_REPEAT * tmpNum;
    }

    if (tail != 0) {
        SetVectorMask<float, MaskMode::COUNTER>(0, tail);
        WelfordUpdateCompute<isReuseSource>(outMean[offset], outVar[offset], src[offset], inMean[offset],
            inVar[offset], sharedTmpBuffer, para, tmpNum, unaryParams, binaryParams);
    }
}

template <typename T, typename U, bool isReuseSource = false, const WelfordUpdateConfig &config = WFUPDATE_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateImpl(const LocalTensor<U>& outputMean, const LocalTensor<U>& outputVariance,
    const LocalTensor<U>& inputMean, const LocalTensor<U>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam& para)
{
    static_assert((std::is_same<T, float>::value || std::is_same<T, __cce_half>::value),
        "Failed to check dtype of inputX, inputX support dtype is: half/float.");
    static_assert((std::is_same<U, float>::value),
        "Failed to check dtype of mean/var, mean/var support dtype is: float.");
# 481 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/layernorm_common_impl.h"
    SetMaskCount();
    if (config.isInplace && (para.abComputeLength < para.abLength)) {
        WelfordUpdateInplace(outputMean, outputVariance, inputMean, inputVariance, para);
    }
    WelfordUpdateComputeImpl<T, U, isReuseSource>(outputMean, outputVariance, inputX, inputMean, inputVariance,
        sharedTmpBuffer, para);
    SetMaskNorm();
    ResetMask();
}

template <typename T, typename U, bool isReuseSource = false, const WelfordUpdateConfig &config = WFUPDATE_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdateImpl(const LocalTensor<U>& outMean, const LocalTensor<U>& outVar,
    const LocalTensor<U>& inMean, const LocalTensor<U>& inVar, const LocalTensor<T>& srcUb,
    const WelfordUpdateParam& para)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);
                                                                                 ;

    WelfordUpdateImpl<T, U, isReuseSource, config>(outMean, outVar, inMean, inVar, srcUb, stackTensor, para);
}

}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, LayerNormTiling& tiling)
{
    LayerNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, sharedTmpBuffer, epsilon,
        tiling);
}
# 65 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const T epsilon, LayerNormTiling& tiling)
{
    LayerNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, epsilon, tiling);
}
# 89 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h"
template <typename U, typename T, bool isReuseSource = false, const LayerNormConfig& config = LNCFG_NORM>
[aicore] __inline__ __attribute__((always_inline)) void LayerNorm(const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputRstd, const LocalTensor<T>& inputX, const LocalTensor<U>& gamma,
    const LocalTensor<U>& beta, const float epsilon, const LayerNormPara& para, const LayerNormSeparateTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LayerNormImpl<U, T, isReuseSource, config>(output, outputMean, outputRstd, inputX, gamma, beta, epsilon, para,
        tiling);
}
# 117 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h"
template <typename U, typename T, bool isReuseSource = false, const LayerNormConfig& config = LNCFG_NORM>
[aicore] __inline__ __attribute__((always_inline)) void LayerNorm(const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputRstd, const LocalTensor<T>& inputX, const LocalTensor<U>& gamma,
    const LocalTensor<U>& beta, const float epsilon, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LayerNormPara& para, const LayerNormSeparateTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LayerNormImpl<U, T, isReuseSource, config>(output, outputMean, outputRstd, inputX, gamma, beta, epsilon,
        sharedTmpBuffer, para, tiling);
}
# 142 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h"
template <typename T, typename U, bool isReuseSource = false, const WelfordUpdateConfig& config = WFUPDATE_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdate(const LocalTensor<U>& outputMean, const LocalTensor<U>& outputVariance,
    const LocalTensor<U>& inputMean, const LocalTensor<U>& inputVariance, const LocalTensor<T>& inputX,
    const WelfordUpdateParam& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    WelfordUpdateImpl<T, U, isReuseSource, config>(outputMean, outputVariance, inputMean, inputVariance, inputX, para);
}
# 166 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h"
template <typename T, typename U, bool isReuseSource = false, const WelfordUpdateConfig& config = WFUPDATE_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void WelfordUpdate(const LocalTensor<U>& outputMean, const LocalTensor<U>& outputVariance,
    const LocalTensor<U>& inputMean, const LocalTensor<U>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    WelfordUpdateImpl<T, U, isReuseSource, config>(outputMean, outputVariance, inputMean, inputVariance, inputX,
        sharedTmpBuffer, para);
}
#pragma end_pipe
}
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/welfordfinalize.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/welfordfinalize.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/welfordfinalize_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/welfordfinalize_utils.h"
namespace AscendC {
struct WelfordFinalizeConfig {
    [aicore] constexpr WelfordFinalizeConfig(const bool isCorrectionIn)
    {
        isCorrection = isCorrectionIn;
    }
    bool isCorrection = false;
};

constexpr WelfordFinalizeConfig WFFINALIZE_DEFAULT_CFG = { false };


struct WelfordFinalizePara
{
    uint32_t rnLength;
    uint32_t abLength;
    uint32_t headCount;
    uint32_t headCountLength;
    uint32_t tailCount;
    uint32_t tailCountLength;
    float abRec;
    float rRec;
};


};
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/welfordfinalize.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/welfordfinalize/welfordfinalize_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/welfordfinalize/welfordfinalize_common_impl.h"
namespace AscendC
{
constexpr uint32_t OUTPUT_SIZE = 8;
constexpr uint32_t B32_LEN = 256 / sizeof(float);
constexpr uint32_t OUTPUT_MASK_B32 = 254;

struct WelfordFinalizeTilingData
{
    uint32_t computeLength;
    uint32_t round;
    uint32_t tail;
};

template <typename T>
struct WelfordFinalizeTmpTensors
{
    [aicore] WelfordFinalizeTmpTensors() {}
    LocalTensor<T> tempOutputMean;
    LocalTensor<T> tempOutputVariance;
    LocalTensor<T> tempMean;
    LocalTensor<T> tempVariance;
};

[aicore] __inline__ __attribute__((always_inline)) void welfordFinalizeOutputPre(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
    const LocalTensor<float> &tempOutputMean, const LocalTensor<float> &tempOutputVariance)
{
    Adds(outputMean, tempOutputMean, static_cast<float>(0), 1);
    Adds(outputVariance, tempOutputVariance, static_cast<float>(0), 1);
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    SetVectorMask<float>(0, OUTPUT_MASK_B32);

    Duplicate<float, false>(outputMean, 0, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(outputVariance, 0, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordFinalizeExeVariance(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &outputVariance, const LocalTensor<int32_t> &counts, const WelfordFinalizeTmpTensors<float> &tempTensors,
    const WelfordFinalizePara &para, const uint32_t computeLength)
{
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempVariance = tempTensors.tempVariance;

    Adds(tempVariance, tempMean, static_cast<float>(0), 1);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    BroadcastLastDim(tempMean, tempVariance, 1, computeLength);

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);

    Sub<float, false>(tempVariance, inputMean, tempMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempMean, tempVariance, tempVariance, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, int32_t, false>(tempVariance, counts, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempMean, tempMean, tempVariance, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(tempVariance, tempMean, inputVariance, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempVariance, tempVariance, para.rRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl<true, false>(tempMean, outputVariance, tempVariance, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordFinalizeExeMean(const LocalTensor<float> &inputMean, const LocalTensor<float> &outputMean, const LocalTensor<int32_t> &counts,
    const WelfordFinalizeTmpTensors<float> &tempTensors, const WelfordFinalizePara &para, const uint32_t computeLength)
{
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempVariance = tempTensors.tempVariance;

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);
    Cast<float, int32_t, false>(tempMean, counts, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempMean, inputMean, tempMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempMean, tempMean, para.rRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl<true, false>(tempVariance, outputMean, tempMean, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetWelfordFinalizeOutputMeanWithTail(const LocalTensor<float> &inputMean, const LocalTensor<float> &tempTensorCal,
    const LocalTensor<float> &tempoutputMean, const WelfordFinalizePara &para, const LocalTensor<float> &outputMean, const uint32_t computeLength, const uint32_t offset)
{
    const UnaryRepeatParams unaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);
    int32_t headCount = para.headCount;
    int32_t tailCount = para.tailCount;

    Muls<float, false>(tempTensorCal, inputMean, (float)tailCount, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempoutputMean, tempTensorCal, para.rRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    int32_t headComputeLength = para.headCountLength - offset;
    if (headComputeLength > static_cast<int32_t>(computeLength)) {
        headComputeLength = computeLength;
    } else if (headComputeLength < 0) {
        headComputeLength = 0;
    }

    if (headComputeLength > 0) {
        SetVectorMask<float, MaskMode::COUNTER>(0, headComputeLength);
        Muls<float, false>(tempoutputMean, tempoutputMean, (float)headCount / (float)tailCount, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);

    LayerNormReduceSumImpl<true, false>(tempTensorCal, outputMean, tempoutputMean, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetWelfordFinalizeOutputMeanNoTail(const LocalTensor<float> &inputMean, const LocalTensor<float> &tempoutputMean,
    const LocalTensor<float> &tempTensorCal, const WelfordFinalizePara &para, const LocalTensor<float> &outputMean, const uint32_t computeLength)
{
    SetMaskCount();
    const UnaryRepeatParams unaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);

    Muls<float, false>(tempoutputMean, inputMean, para.abRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl<true, false>(tempTensorCal, outputMean, tempoutputMean, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetWelfordFinalizeOutputVarianceWithTail(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &tempoutputMean, const LocalTensor<float> &tempTensorCal, const WelfordFinalizePara &para, const LocalTensor<float> &outputVariance,
    const uint32_t computeLength, const uint32_t offset)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);
    int32_t headCount = para.headCount;
    int32_t tailCount = para.tailCount;

    Sub<float, false>(tempTensorCal, inputMean, tempoutputMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempoutputMean, tempTensorCal, tempTensorCal, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempTensorCal, tempoutputMean, (float)tailCount, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    int32_t headComputeLength = para.headCountLength - offset;
    if (headComputeLength > static_cast<int32_t>(computeLength)) {
        headComputeLength = computeLength;
    } else if (headComputeLength < 0) {
        headComputeLength = 0;
    }

    if (headComputeLength > 0) {
        SetVectorMask<float, MaskMode::COUNTER>(0, headComputeLength);
        Muls<float, false>(tempTensorCal, tempTensorCal, (float)headCount / (float)tailCount, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);
    Add<float, false>(tempoutputMean, inputVariance, tempTensorCal, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempTensorCal, tempoutputMean, para.rRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl<true, false>(tempoutputMean, outputVariance, tempTensorCal, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetWelfordFinalizeOutputVarianceNoTail(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &tempoutputMean, const LocalTensor<float> &tempTensorCal, const WelfordFinalizePara &para, const LocalTensor<float> &outputVariance, const uint32_t computeLength)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);
    int32_t rnLength = para.rnLength;

    Sub<float, false>(tempTensorCal, inputMean, tempoutputMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempoutputMean, tempTensorCal, tempTensorCal, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempTensorCal, tempoutputMean, (float)rnLength, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(tempoutputMean, inputVariance, tempTensorCal, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempTensorCal, tempoutputMean, para.rRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl<true, false>(tempoutputMean, outputVariance, tempTensorCal, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordFinalizeExeMean(const LocalTensor<float> &inputMean, const LocalTensor<float> &outputMean,
    const WelfordFinalizeTmpTensors<float> &tempTensors, const WelfordFinalizePara &para, const uint32_t computeLength, const uint32_t offset)
{
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempVariance = tempTensors.tempVariance;

    if (para.tailCountLength == 0 || para.tailCount == 0) {
        GetWelfordFinalizeOutputMeanNoTail(inputMean, tempMean, tempVariance, para, outputMean, computeLength);
    } else {



              ;
        GetWelfordFinalizeOutputMeanWithTail(inputMean, tempVariance, tempMean, para, outputMean, computeLength, offset);
    }
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordFinalizeExeVariance(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &outputVariance, WelfordFinalizeTmpTensors<float> &tempTensors, const WelfordFinalizePara &para, const uint32_t computeLength, const uint32_t offset)
{
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempVariance = tempTensors.tempVariance;
    Adds(tempVariance, tempMean, static_cast<float>(0), 1);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    BroadcastLastDim(tempMean, tempVariance, 1, computeLength);
    if (para.tailCountLength == 0 || para.tailCount == 0) {
        GetWelfordFinalizeOutputVarianceNoTail(inputMean, inputVariance, tempMean, tempVariance, para, outputVariance, computeLength);
    } else {



              ;
        GetWelfordFinalizeOutputVarianceWithTail(inputMean, inputVariance, tempMean, tempVariance, para, outputVariance, computeLength, offset);
    }
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordFinalizeComputeImpl(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance, WelfordFinalizeTmpTensors<float> &tempTensors,
    const WelfordFinalizePara &para, WelfordFinalizeTilingData &tiling)
{
    uint32_t offset = 0;
    uint32_t outOffset = 0;
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempOutputMean = tempTensors.tempOutputMean;
    LocalTensor<float> tempOutputVariance = tempTensors.tempOutputVariance;

    for (uint32_t i = 0; i < tiling.round; i++)
    {
        WelfordFinalizeExeMean<isReuseSource>(inputMean[offset], tempOutputMean[outOffset],
            tempTensors, para, tiling.computeLength, offset);
        offset += tiling.computeLength;
        outOffset++;
        if (outOffset == B32_LEN) {
            SetMaskNorm();
            WholeReduceSum(tempOutputMean, tempOutputMean, B32_LEN, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
            PipeBarrier<PIPE_V>();
            outOffset = 1;
        }
    }

    if (tiling.tail > 0) {
        WelfordFinalizeExeMean<isReuseSource>(inputMean[offset], tempOutputMean[outOffset],
            tempTensors, para, tiling.tail, offset);
        outOffset++;
    }
    SetMaskNorm();
    WholeReduceSum(tempOutputMean, tempOutputMean, outOffset, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
    PipeBarrier<PIPE_V>();

    offset = 0;
    outOffset = 0;
    for (uint32_t i = 0; i < tiling.round; i++)
    {
        Adds(tempMean, tempOutputMean, static_cast<float>(0), 1);
        PipeBarrier<PIPE_V>();
        WelfordFinalizeExeVariance<isReuseSource>(inputMean[offset], inputVariance[offset], tempOutputVariance[outOffset],
            tempTensors, para, tiling.computeLength, offset);
        offset += tiling.computeLength;
        outOffset++;
        if (outOffset == B32_LEN) {
            SetMaskNorm();
            WholeReduceSum(tempOutputVariance, tempOutputVariance, B32_LEN, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
            PipeBarrier<PIPE_V>();
            outOffset = 1;
        }
    }

    if (tiling.tail > 0) {
        Adds(tempMean, tempOutputMean, static_cast<float>(0), 1);
        PipeBarrier<PIPE_V>();
        WelfordFinalizeExeVariance<isReuseSource>(inputMean[offset], inputVariance[offset], tempOutputVariance[outOffset],
            tempTensors, para, tiling.tail, offset);
        outOffset++;
    }
    SetMaskNorm();
    WholeReduceSum(tempOutputVariance, tempOutputVariance, outOffset, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
    PipeBarrier<PIPE_V>();
    welfordFinalizeOutputPre(outputMean, outputVariance, tempOutputMean, tempOutputVariance);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordFinalizeComputeImpl(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance, const LocalTensor<int32_t> &counts,
    const WelfordFinalizeTmpTensors<float> &tempTensors, const WelfordFinalizePara &para, WelfordFinalizeTilingData &tiling)
{
    uint32_t offset = 0;
    uint32_t outOffset = 0;
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempOutputMean = tempTensors.tempOutputMean;
    LocalTensor<float> tempOutputVariance = tempTensors.tempOutputVariance;

    for (uint32_t i = 0; i < tiling.round; i++)
    {
        WelfordFinalizeExeMean<isReuseSource>(inputMean[offset], tempOutputMean[outOffset], counts[offset],
            tempTensors, para, tiling.computeLength);
        offset += tiling.computeLength;
        outOffset++;
        if (outOffset == B32_LEN) {
            SetMaskNorm();
            WholeReduceSum(tempOutputMean, tempOutputMean, B32_LEN, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
            PipeBarrier<PIPE_V>();
            outOffset = 1;
        }
    }

    if (tiling.tail > 0) {
        WelfordFinalizeExeMean<isReuseSource>(inputMean[offset], tempOutputMean[outOffset], counts[offset],
            tempTensors, para, tiling.tail);
        outOffset++;
    }
    SetMaskNorm();
    WholeReduceSum(tempOutputMean, tempOutputMean, outOffset, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
    PipeBarrier<PIPE_V>();

    offset = 0;
    outOffset = 0;
    for (uint32_t i = 0; i < tiling.round; i++)
    {
        Adds(tempMean, tempOutputMean, static_cast<float>(0), 1);
        PipeBarrier<PIPE_V>();
        WelfordFinalizeExeVariance<isReuseSource>(inputMean[offset], inputVariance[offset], tempOutputVariance[outOffset], counts[offset],
            tempTensors, para, tiling.computeLength);
        offset += tiling.computeLength;
        outOffset++;
        if (outOffset == B32_LEN) {
            SetMaskNorm();
            WholeReduceSum(tempOutputVariance, tempOutputVariance, B32_LEN, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
            PipeBarrier<PIPE_V>();
            outOffset = 1;
        }
    }

    if (tiling.tail > 0) {
        Adds(tempMean, tempOutputMean, static_cast<float>(0), 1);
        PipeBarrier<PIPE_V>();
        WelfordFinalizeExeVariance<isReuseSource>(inputMean[offset], inputVariance[offset], tempOutputVariance[outOffset], counts[offset],
            tempTensors, para, tiling.tail);
        outOffset++;
    }
    SetMaskNorm();
    WholeReduceSum(tempOutputVariance, tempOutputVariance, outOffset, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
    PipeBarrier<PIPE_V>();
    welfordFinalizeOutputPre(outputMean, outputVariance, tempOutputMean, tempOutputVariance);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetWelfordFinalizeTensorInfo(const LocalTensor<float> &stackBuffer, const WelfordFinalizePara &para,
                                                    WelfordFinalizeTmpTensors<float> &tempTensors, WelfordFinalizeTilingData &tiling)
{

    uint32_t minTmpSize = B32_LEN * 0x2;

    const uint32_t minTmpOutSize = B32_LEN * 0x2;

                                                                                                                                      ;

    const uint32_t expFactor = (stackBuffer.GetSize() - minTmpOutSize) / minTmpSize;
    tiling.computeLength = expFactor * B32_LEN;
    tiling.round = para.abLength / tiling.computeLength;
    tiling.tail = para.abLength % tiling.computeLength;

    tempTensors.tempOutputMean = stackBuffer;
    tempTensors.tempOutputVariance = stackBuffer[B32_LEN];
    tempTensors.tempMean = stackBuffer[minTmpOutSize];
    tempTensors.tempVariance = stackBuffer[minTmpOutSize + tiling.computeLength];
}

[aicore] __inline__ __attribute__((always_inline)) void welfordFinalizeCommonCheck(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
                                                const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance, const WelfordFinalizePara &para)
{



      ;



      ;



      ;



      ;



      ;



      ;
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordFinalizeImpl(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
                                        const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance, const LocalTensor<uint8_t> &sharedTmpBuffer, const WelfordFinalizePara &para)
{
    welfordFinalizeCommonCheck(inputMean, inputVariance, outputMean, outputVariance, para);

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    WelfordFinalizeTmpTensors<float> tempTensors;
    WelfordFinalizeTilingData tiling;
    GetWelfordFinalizeTensorInfo<isReuseSource>(stackBuffer, para, tempTensors, tiling);

    SetMaskCount();
    WelfordFinalizeComputeImpl<isReuseSource>(inputMean, inputVariance, outputMean, outputVariance, tempTensors, para, tiling);

    SetMaskNorm();
    ResetMask();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordFinalizeImpl(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
                                        const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance, const LocalTensor<int32_t> &counts, const LocalTensor<uint8_t> &sharedTmpBuffer, const WelfordFinalizePara &para)
{



      ;
    welfordFinalizeCommonCheck(inputMean, inputVariance, outputMean, outputVariance, para);

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    WelfordFinalizeTmpTensors<float> tempTensors;
    WelfordFinalizeTilingData tiling;
    GetWelfordFinalizeTensorInfo<isReuseSource>(stackBuffer, para, tempTensors, tiling);

    SetMaskCount();
    WelfordFinalizeComputeImpl<isReuseSource>(inputMean, inputVariance, outputMean, outputVariance, counts, tempTensors, para, tiling);

    SetMaskNorm();
    ResetMask();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordFinalizeImpl(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
                                        const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance, const WelfordFinalizePara &para)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, sharedTmpBuffer, para);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void WelfordFinalizeImpl(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance, const LocalTensor<float> &inputMean,
                                        const LocalTensor<float> &inputVariance, const LocalTensor<int32_t> &counts, const WelfordFinalizePara &para)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, counts, sharedTmpBuffer, para);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/welfordfinalize.h" 2


namespace AscendC
{
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/welfordfinalize.h"
    template <bool isReuseSource = false>
    [aicore] __inline__ __attribute__((always_inline)) void WelfordFinalize(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
                                           const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance, const LocalTensor<uint8_t> &sharedTmpBuffer, WelfordFinalizePara &para)
    {
        if constexpr(g_coreType == AscendC::AIC) {
            return;
        }
        WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, sharedTmpBuffer, para);
    }
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/welfordfinalize.h"
    template <bool isReuseSource = false>
    [aicore] __inline__ __attribute__((always_inline)) void WelfordFinalize(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
            const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance, const LocalTensor<int32_t> &counts, const LocalTensor<uint8_t> &sharedTmpBuffer, WelfordFinalizePara &para)
    {
        if constexpr(g_coreType == AscendC::AIC) {
            return;
        }
        WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, counts, sharedTmpBuffer, para);
    }
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/welfordfinalize.h"
    template <bool isReuseSource = false>
    [aicore] __inline__ __attribute__((always_inline)) void WelfordFinalize(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
                                           const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance, WelfordFinalizePara &para)
    {
        if constexpr(g_coreType == AscendC::AIC) {
            return;
        }
        WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, para);
    }
# 104 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/welfordfinalize.h"
    template <bool isReuseSource = false>
    [aicore] __inline__ __attribute__((always_inline)) void WelfordFinalize(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
                                           const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance, const LocalTensor<int32_t> &counts, WelfordFinalizePara &para)
    {
        if constexpr(g_coreType == AscendC::AIC) {
            return;
        }
        WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, counts, para);
    }
#pragma end_pipe
}
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum_utils.h"
namespace AscendC {
struct SumParams {
    uint32_t outter = 1;
    uint32_t inner;
    uint32_t n;
};

};
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/sum/sum_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/sum/sum_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/sum/sum_common_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/sum/../../api_check/kernel_api_check.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/sum/sum_common_impl.h" 2
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SumForOneRepeatTime(
    const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const SumParams &sumParams)
{
    SetVectorMask<T>(0, sumParams.n);
    for (uint32_t row = 0; row < sumParams.outter; ++row) {
        RepeatReduceSum<T, false>(dstTensor[row], srcTensor[row * sumParams.inner], 1, MASK_PLACEHOLDER,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, int32_t reduceDim = -1, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SumCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SumParams &sumParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                                                  ;

    uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t firstRepeatTimes = (sumParams.n + elementNumPerRep - 1) / elementNumPerRep;
    SetMaskCount();
    if (firstRepeatTimes == 1) {
        return SumForOneRepeatTime(dstTensor, srcTensor, sumParams);
    }
    uint32_t totalCnt = 1;
    uint32_t dataSize = firstRepeatTimes;
    while (dataSize > 1) {
        ++totalCnt;
        dataSize = (dataSize + elementNumPerRep - 1) / elementNumPerRep;
    }
    LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
    for (uint32_t row = 0; row < sumParams.outter; ++row) {
        uint32_t cnt = totalCnt;
        uint64_t lowMask = sumParams.n;
        SetVectorMask<T>(0, lowMask);
        RepeatReduceSum<T, false>(tmpTensor, srcTensor[row * sumParams.inner], 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);

        PipeBarrier<PIPE_V>();
        lowMask = (lowMask + elementNumPerRep - 1) / elementNumPerRep;
        --cnt;
        while (cnt != 0) {
            SetVectorMask<T>(0, lowMask);
            if (cnt == 1) {
                RepeatReduceSum<T, false>(dstTensor[row], tmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                    DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            } else {
                RepeatReduceSum<T, false>(tmpTensor, tmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                    DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();
            lowMask = (lowMask + elementNumPerRep - 1) / elementNumPerRep;
            --cnt;
        }
    }
    SetMaskNorm();

}

}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h" 2




namespace AscendC {
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h"
#pragma begin_pipe(V)
template <typename T, int32_t reduceDim = -1, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void Sum(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SumParams &sumParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SumCompute<T, reduceDim, isReuseSource, isBasicBlock>(dstTensor, srcTensor, sharedTmpBuffer, sumParams);
}
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h"
template <typename T, int32_t reduceDim = -1, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void Sum(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const SumParams &sumParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    Sum<T, reduceDim, isReuseSource, isBasicBlock>(dstTensor, srcTensor, sharedTmpBuffer, sumParams);

}
#pragma end_pipe
}
# 66 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/silu.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/silu.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/silu/silu_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/silu/silu_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/silu/silu_common_impl.h" 2




namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SiluCalcSimplified(const LocalTensor<T> &dstAddr, const LocalTensor<T> &srcAddr,
    uint32_t repeatTimes)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Muls<T, false>(dstAddr, srcAddr, T(-1), MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(dstAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(dstAddr, dstAddr, 1.0, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<T, false>(dstAddr, srcAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void SiluCompute(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    uint32_t dataSize)
{
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/silu/silu_common_impl.h"
    SetMaskCount();
    SetVectorMask<T>(0, dataSize);
    SiluCalcSimplified<T>(dstLocal, srcLocal, 1);
    SetMaskNorm();
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/silu/silu_common_impl.h"
    ResetMask();
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/silu.h" 2

namespace AscendC {
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/silu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Silu(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    uint32_t dataSize)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SiluCompute<T, false>(dstLocal, srcLocal, dataSize);
}

}
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/gelu/gelu_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/gelu/gelu_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/gelu/gelu_impl.h" 2

namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GeluCalcTanhParams(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& tempTensorB,
    const LocalTensor<T>& srcLocal, const GeluParams<T>& params)
{
    const T coefficientsA = 0.044715;
    const T coefficientsB = 1.5957691216057308;

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Mul<T, false>(tempTensorA, srcLocal, srcLocal, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tempTensorB, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorB, coefficientsA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(tempTensorB, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorB, coefficientsB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GeluCalcYGreaterThanZero(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& tempTensorB,
    const GeluParams<T>& params)
{
    const UnaryRepeatParams unaryParams;


    Mins<T, false>(tempTensorB, tempTensorA, 0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(tempTensorB, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void GeluCalcYLessThanZero(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& tempTensorB,
    const LocalTensor<T>& srcLocal, const GeluParams<T>& params)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Abs<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorA, -1, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, 1, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    if constexpr (highPerformance) {
        Reciprocal<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
        PipeBarrier<PIPE_V>();

        Mul<T, false>(tempTensorA, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        Div<T, false>(tempTensorA, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void GeluCalcSimplifiedAvoid(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const BinaryRepeatParams binaryParams;
    const LocalTensor<T>& tempTensorA = params.tempTensorA;
    const LocalTensor<T>& tempTensorB = params.tempTensorB;



    GeluCalcTanhParams(tempTensorA, tempTensorB, srcLocal, params);


    GeluCalcYGreaterThanZero(tempTensorA, tempTensorB, params);


    GeluCalcYLessThanZero<T, highPerformance>(tempTensorA, tempTensorB, srcLocal, params);


    Mul<T, false>(dstLocal, tempTensorA, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FastGeluCalcSimplified(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const LocalTensor<T>& stackBuffer = params.tempTensorA;


    const T coefficients = -1.702;


    const UnaryRepeatParams unaryParams;
    Muls<T, false>(stackBuffer, srcLocal, coefficients, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(stackBuffer, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(stackBuffer, stackBuffer, 1.0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    const BinaryRepeatParams binaryParams;
    if constexpr (highPerformance) {
        Reciprocal<T, false>(stackBuffer, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
        PipeBarrier<PIPE_V>();

        Mul<T, false>(dstLocal, srcLocal, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        Div<T, false>(dstLocal, srcLocal, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FastGeluV2ClipParams(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const T coefficientsA = -0.1444;
    const T coefficientsB = -1.769;
    const T coefficientsBInv = 1.769;
    const T coefficientsC = 0.7071;
    const T coefficientsD = 0.5;

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Muls<T, false>(tempTensorA, srcLocal, coefficientsC, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Abs<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Mins<T, false>(tempTensorA, tempTensorA, coefficientsBInv, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, coefficientsB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tempTensorA, tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorA, coefficientsA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, coefficientsD, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FastGeluV2CalcSimplified(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const T coefficients = 0.000000000001;
    const T coefficientsHalf = 0.5;
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    const LocalTensor<T>& tempTensorA = params.tempTensorA;
    const LocalTensor<T>& tempTensorB = params.tempTensorB;
    const LocalTensor<T>& tempTensorC = params.tempTensorC;


    FastGeluV2ClipParams(tempTensorA, srcLocal, params);


    Adds<T, false>(tempTensorB, srcLocal, coefficients, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Abs<T, false>(tempTensorC, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    if constexpr (highPerformance) {
        Reciprocal<T, false>(tempTensorC, tempTensorC, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
        PipeBarrier<PIPE_V>();

        Mul<T, false>(tempTensorB, tempTensorB, tempTensorC, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        Div<T, false>(tempTensorB, tempTensorB, tempTensorC, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    }


    Mul<T, false>(tempTensorA, tempTensorA, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, coefficientsHalf, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(dstLocal, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPrecision = false, uint32_t bufferNumber = 1>
[aicore] __inline__ __attribute__((always_inline)) void GeluFormulasTmpCalc(GeluParams<T>& params)
{
    uint32_t needConvBuffer = bufferNumber;
    if constexpr (highPrecision) {
        needConvBuffer += 1;
    }

    params.tempTensorA = params.sharedTmpBuffer;
    params.stackSize = params.tmpBufferSize / needConvBuffer / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                                       ;

    uint32_t nextTmpPos = params.stackSize;
    if constexpr (bufferNumber == TWO_OF_STACK_BUFFER) {
        params.tempTensorB = params.sharedTmpBuffer[nextTmpPos];
        nextTmpPos += params.stackSize;
    }

    if constexpr (bufferNumber >= THREE_OF_STACK_BUFFER) {
        params.tempTensorB = params.sharedTmpBuffer[nextTmpPos];
        nextTmpPos += params.stackSize;
        params.tempTensorC = params.sharedTmpBuffer[nextTmpPos];
        nextTmpPos += params.stackSize;
    }

    if constexpr (highPrecision) {
        params.tempTensorConv = params.sharedTmpBuffer[nextTmpPos];
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GeluCastIntrinsicsImpl(const LocalTensor<float>& dstLocal, const LocalTensor<__cce_half>& srcLocal)
{
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);
    Cast<float, __cce_half, false>(dstLocal, srcLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GeluCastIntrinsicsImpl(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<float>& srcLocal)
{
    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(__cce_half);
    Cast<__cce_half, float, false>(dstLocal, srcLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <uint32_t bufferNumber = 1>
[aicore] __inline__ __attribute__((always_inline)) void GeluFormulasHighPrecision(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<__cce_half>& srcLocal,
    GeluParams<float>& params,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, const GeluParams<float>&))
{
    GeluFormulasTmpCalc<float, true, bufferNumber>(params);

    const LocalTensor<float>& stackBufferConv = params.tempTensorConv;

    const uint32_t round = params.dataSize / params.stackSize;
    const uint32_t tail = params.dataSize % params.stackSize;

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, params.stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        GeluCastIntrinsicsImpl(stackBufferConv, srcLocal[offset]);

        func(stackBufferConv, stackBufferConv, params);

        GeluCastIntrinsicsImpl(dstLocal[offset], stackBufferConv);
        offset = offset + params.stackSize;
    }

    if (tail != 0) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tail);

        GeluCastIntrinsicsImpl(stackBufferConv, srcLocal[offset]);

        func(stackBufferConv, stackBufferConv, params);

        GeluCastIntrinsicsImpl(dstLocal[offset], stackBufferConv);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, uint32_t bufferNumber = 1>
[aicore] __inline__ __attribute__((always_inline)) void GeluFormulas(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    GeluParams<T>& params, void (*func)(const LocalTensor<T>&, const LocalTensor<T>&, const GeluParams<T>&))
{
    GeluFormulasTmpCalc<T, false, bufferNumber>(params);

    const uint32_t round = params.dataSize / params.stackSize;
    const uint32_t tail = params.dataSize % params.stackSize;

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, params.stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        func(dstLocal[offset], srcLocal[offset], params);
        offset = offset + params.stackSize;
    }

    if (tail != 0) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tail);
        func(dstLocal[offset], srcLocal[offset], params);
    }

    SetMaskNorm();
    ResetMask();
}

template <uint32_t bufferNumber = 1>
[aicore] __inline__ __attribute__((always_inline)) void GeluClass(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<__cce_half>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, const GeluParams<float>&))
{
    GeluParams<float> params;
    params.dataSize = dataSize;
    params.sharedTmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    params.tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);

                                                                                                               ;
    GeluFormulasHighPrecision<bufferNumber>(dstLocal, srcLocal, params, func);
}

template <typename T, uint32_t bufferNumber = 1>
[aicore] __inline__ __attribute__((always_inline)) void GeluClass(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize,
    void (*func)(const LocalTensor<T>&, const LocalTensor<T>&, const GeluParams<T>&))
{
    GeluParams<T> params;
    params.dataSize = dataSize;
    params.sharedTmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    params.tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(T);

                                                                                                               ;
    GeluFormulas<T, bufferNumber>(dstLocal, srcLocal, params, func);
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void GeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr (highPrecision && (sizeof(T) == sizeof(__cce_half))) {
        GeluClass<TWO_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            GeluCalcSimplifiedAvoid<float, highPerformance>);
    } else {
        GeluClass<T, TWO_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            GeluCalcSimplifiedAvoid<T, highPerformance>);
    }
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void GeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t dataSize)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    GeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr (highPrecision && (sizeof(T) == sizeof(__cce_half))) {
        GeluClass(dstLocal, srcLocal, sharedTmpBuffer, dataSize, FastGeluCalcSimplified<float, highPerformance>);
    } else {
        GeluClass(dstLocal, srcLocal, sharedTmpBuffer, dataSize, FastGeluCalcSimplified<T, highPerformance>);
    }
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    FasterGeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluV2Impl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr (highPrecision && (IsSameType<T, __cce_half>::value)) {
        GeluClass<THREE_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            FastGeluV2CalcSimplified<float, highPerformance>);
    } else {
        GeluClass<T, THREE_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            FastGeluV2CalcSimplified<T, highPerformance>);
    }
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluV2Impl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    FasterGeluV2Impl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
#pragma end_pipe
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void Gelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    GeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void Gelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    GeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, dataSize);
}
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    FasterGeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
# 86 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    FasterGeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, dataSize);
}
# 106 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluV2(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                         ;
    FasterGeluV2Impl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
# 127 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluV2(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                         ;
    FasterGeluV2Impl<T, highPrecision, highPerformance>(dstLocal, srcLocal, dataSize);
}
#pragma end_pipe
}
# 68 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant_utils.h"
namespace AscendC {
struct AscendQuantConfig {
    [aicore] constexpr AscendQuantConfig(const uint32_t calcCount, const uint32_t offsetCount,
        const uint32_t scaleCount, const uint32_t workLocalSize): calcCount(calcCount), offsetCount(offsetCount),
        scaleCount(scaleCount), workLocalSize(workLocalSize) {}
    uint32_t calcCount = 0;
    uint32_t offsetCount = 0;
    uint32_t scaleCount = 0;
    uint32_t workLocalSize = 0;
};

constexpr AscendQuantConfig ASCEND_QUANT_DEFAULT_CFG = {0, 0, 0, 0};
};
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_v220_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_pre_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_pre_impl.h"
namespace AscendC {

template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void IsQuantValid(const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{



      ;



      ;



      ;
}

template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void IsQuantConfigValid(const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor)
{



      ;



      ;




      ;




      ;



      ;
}

template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void IsQuantConfigValid(const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor)
{
    IsQuantConfigValid<T, config>(srcTensor, sharedTmpBuffer, scaleTensor);




      ;




      ;
}


[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<__cce_half>& stackTensor, const LocalTensor<__cce_half>& scaleTensor,
    const __cce_half offset)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Cast<__cce_half, float, false>(stackTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
    Mul<__cce_half, false>(stackTensor, stackTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Adds<__cce_half, false>(stackTensor, stackTensor, offset, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, __cce_half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<__cce_half>& srcTensor, const LocalTensor<__cce_half>& stackTensor, const LocalTensor<__cce_half>& scaleTensor,
    const __cce_half offset)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Mul<__cce_half, false>(stackTensor, srcTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Adds<__cce_half, false>(stackTensor, stackTensor, static_cast<__cce_half>(offset), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, __cce_half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<__cce_half>& stackTensor, const LocalTensor<__cce_half>& scaleTensor,
    const LocalTensor<__cce_half>& offsetTensor)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Cast<__cce_half, float, false>(stackTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
    Mul<__cce_half, false>(stackTensor, stackTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Add<__cce_half, false>(stackTensor, stackTensor, offsetTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, __cce_half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<__cce_half>& srcTensor, const LocalTensor<__cce_half>& stackTensor, const LocalTensor<__cce_half>& scaleTensor,
    const LocalTensor<__cce_half>& offsetTensor)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Mul<__cce_half, false>(stackTensor, srcTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Add<__cce_half, false>(stackTensor, stackTensor, offsetTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, __cce_half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}


template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LocalTensor<__cce_half>& scaleTensor, const LocalTensor<__cce_half>& offsetTensor, const uint32_t calCount)
{
    LocalTensor<__cce_half> tmpBuffer = sharedTmpBuffer.ReinterpretCast<__cce_half>();
    if constexpr(config.workLocalSize != 0 && config.scaleCount != 0) {
        constexpr uint32_t splitSize = config.workLocalSize / sizeof(__cce_half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


          ;
        constexpr uint32_t loopCount = config.scaleCount / splitSize;
        constexpr uint32_t calcTail = config.scaleCount % splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
        for (uint32_t i = 0; i < loopCount; ++i) {
            AscendQuantPerChannelIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize],
                tmpBuffer, scaleTensor[i * splitSize], offsetTensor[i * splitSize]);
        }
        if constexpr(calcTail > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
            AscendQuantPerChannelIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize],
                tmpBuffer, scaleTensor[loopCount * splitSize], offsetTensor[loopCount * splitSize]);
        }
        return;
    }

    uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(__cce_half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize],
            sharedTmpBuffer.ReinterpretCast<__cce_half>(), scaleTensor[i * splitSize],
            offsetTensor[i * splitSize]);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize],
            sharedTmpBuffer.ReinterpretCast<__cce_half>(), scaleTensor[loopCount * splitSize],
            offsetTensor[loopCount * splitSize]);
    }
}
template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LocalTensor<__cce_half>& scaleTensor, const __cce_half offset, const uint32_t calCount)
{
    LocalTensor<__cce_half> tmpBuffer = sharedTmpBuffer.ReinterpretCast<__cce_half>();
    if constexpr(config.workLocalSize != 0 && config.scaleCount != 0) {
        constexpr uint32_t splitSize = config.workLocalSize / sizeof(__cce_half) / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                                       ;
        constexpr uint32_t calcTail = config.scaleCount % splitSize;
        constexpr uint32_t loopCount = config.scaleCount / splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);

        for (uint32_t i = 0; i < loopCount; ++i) {
            AscendQuantPerChannelIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize], tmpBuffer,
                scaleTensor[i * splitSize], offset);
        }
        if constexpr(calcTail > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
            AscendQuantPerChannelIntrinsicsImpl(dstTensor[loopCount * splitSize],
                srcTensor[loopCount * splitSize], tmpBuffer, scaleTensor[loopCount * splitSize], offset);
        }
        return;
    }

    uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(__cce_half) / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                                   ;
    SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);

    uint32_t calcTail = calCount % splitSize;
    uint32_t loopCount = calCount / splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize],
            sharedTmpBuffer.ReinterpretCast<__cce_half>(), scaleTensor[i * splitSize], offset);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize],
            sharedTmpBuffer.ReinterpretCast<__cce_half>(), scaleTensor[loopCount * splitSize], offset);
    }
}

template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImplStatic(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T offset)
{
    if constexpr(config.scaleCount != 0 && config.calcCount != 0) {
        constexpr uint32_t N = config.calcCount / config.scaleCount;
        if constexpr (IsSameType<T, float>::value) {
            LocalTensor<__cce_half> halfScaleTensor = scaleTensor.template ReinterpretCast<__cce_half>();
            UnaryRepeatParams f162s8Param;
            f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
            SetVectorMask<__cce_half, MaskMode::COUNTER>(0, config.scaleCount);
            Cast<__cce_half, float, false>(halfScaleTensor, scaleTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                f162s8Param);
            PipeBarrier<PIPE_V>();
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * config.scaleCount], srcTensor[i * config.scaleCount],
                    sharedTmpBuffer, halfScaleTensor, static_cast<__cce_half>(offset), config.scaleCount);
            }
        } else {
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * config.scaleCount], srcTensor[i * config.scaleCount],
                    sharedTmpBuffer, scaleTensor, static_cast<__cce_half>(offset), config.scaleCount);
            }
        }
    }
}

template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImplStatic(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor)
{
    if constexpr(config.scaleCount != 0 && config.calcCount != 0) {
        constexpr uint32_t N = config.calcCount / config.scaleCount;
        if constexpr (IsSameType<T, float>::value) {
            SetVectorMask<__cce_half, MaskMode::COUNTER>(0, config.scaleCount);
            LocalTensor<__cce_half> halfScaleTensor = scaleTensor.template ReinterpretCast<__cce_half>();
            UnaryRepeatParams f162s8Param;
            f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
            Cast<__cce_half, float, false>(halfScaleTensor, scaleTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                f162s8Param);
            LocalTensor<__cce_half> halfOffsetTensor = offsetTensor.template ReinterpretCast<__cce_half>();
            Cast<__cce_half, float, false>(halfOffsetTensor, offsetTensor, RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, 1, f162s8Param);

            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * config.scaleCount], srcTensor[i * config.scaleCount],
                    sharedTmpBuffer, halfScaleTensor, halfOffsetTensor, config.scaleCount);
            }
        } else {
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * config.scaleCount], srcTensor[i * config.scaleCount],
                    sharedTmpBuffer, scaleTensor, offsetTensor, config.scaleCount);
            }
        }
    }
}



[aicore] __inline__ __attribute__((always_inline)) void AscendQuantIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<__cce_half>& srcTensor, const LocalTensor<__cce_half>& stackBuffer, __cce_half scale, __cce_half offset)
{
    UnaryRepeatParams unaryParams;
    UnaryRepeatParams f162s8Params;
    f162s8Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Muls<__cce_half, false>(stackBuffer, srcTensor, scale, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<__cce_half, false>(stackBuffer, stackBuffer, offset, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, __cce_half, false>(dstTensor, stackBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Params);
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<__cce_half>& stackBuffer, __cce_half scale, __cce_half offset)
{
    UnaryRepeatParams unaryParams;
    UnaryRepeatParams f162s8Params;
    f162s8Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Cast<__cce_half, float, false>(stackBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Params);
    PipeBarrier<PIPE_V>();
    Muls<__cce_half, false>(stackBuffer, stackBuffer, scale, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<__cce_half, false>(stackBuffer, stackBuffer, offset, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, __cce_half, false>(dstTensor, stackBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Params);
    PipeBarrier<PIPE_V>();
}


template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantCalc(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset, const uint32_t calCount)
{
    IsQuantValid<T, config>(srcTensor, sharedTmpBuffer, calCount);

    SetMaskCount();
    LocalTensor<__cce_half> tmpBuffer = sharedTmpBuffer.ReinterpretCast<__cce_half>();
    if constexpr(config.workLocalSize != 0 && config.calcCount != 0) {
        constexpr uint32_t splitSize = config.workLocalSize / sizeof(__cce_half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


          ;
        constexpr uint32_t loopCount = config.calcCount / splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
        for (uint32_t i = 0; i < loopCount; ++i) {
            AscendQuantIntrinsicsImpl(dstTensor[splitSize * i], srcTensor[splitSize * i],
                tmpBuffer, static_cast<__cce_half>(scale), static_cast<__cce_half>(offset));
        }
        if constexpr(config.calcCount % splitSize > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, config.calcCount % splitSize);
            AscendQuantIntrinsicsImpl(dstTensor[splitSize * loopCount], srcTensor[splitSize * loopCount],
                tmpBuffer, static_cast<__cce_half>(scale), static_cast<__cce_half>(offset));
        }
    } else {
        uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(__cce_half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


          ;
        uint32_t loopCount = calCount / splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
        for (uint32_t i = 0; i < loopCount; ++i) {
            AscendQuantIntrinsicsImpl(dstTensor[splitSize * i], srcTensor[splitSize * i],
                sharedTmpBuffer.ReinterpretCast<__cce_half>(), static_cast<__cce_half>(scale), static_cast<__cce_half>(offset));
        }
        if (calCount % splitSize > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, calCount % splitSize);
            AscendQuantIntrinsicsImpl(dstTensor[splitSize * loopCount], srcTensor[splitSize * loopCount],
                sharedTmpBuffer.ReinterpretCast<__cce_half>(),
                static_cast<__cce_half>(scale), static_cast<__cce_half>(offset));
        }
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantCalc(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    IsQuantParamValid(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offset, scaleCount, calCount);
    IsQuantConfigValid<T, config>(srcTensor, sharedTmpBuffer, scaleTensor);
    SetMaskCount();

    if constexpr(config.scaleCount != 0 && config.calcCount != 0) {
        AscendQuantImplStatic<T, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offset);
    } else {
        uint32_t N = calCount / scaleCount;
        if constexpr (IsSameType<T, float>::value) {

            LocalTensor<__cce_half> halfScaleTensor = scaleTensor.template ReinterpretCast<__cce_half>();
            UnaryRepeatParams f162s8Param;
            f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
            SetVectorMask<__cce_half, MaskMode::COUNTER>(0, scaleCount);
            Cast<__cce_half, float, false>(halfScaleTensor, scaleTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                f162s8Param);
            PipeBarrier<PIPE_V>();
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * scaleCount], srcTensor[i * scaleCount],
                    sharedTmpBuffer, halfScaleTensor, static_cast<__cce_half>(offset), scaleCount);
            }
        } else {
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * scaleCount], srcTensor[i * scaleCount],
                    sharedTmpBuffer, scaleTensor, static_cast<__cce_half>(offset), scaleCount);
            }
        }
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantCalc(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor, const uint32_t scaleCount, const uint32_t offsetCount,
    const uint32_t calCount)
{
    IsQuantParamValid(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
    IsQuantConfigValid<T, config>(srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor);
    SetMaskCount();

    if constexpr(config.scaleCount != 0 && config.calcCount != 0) {
        AscendQuantImplStatic<T, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor);
    } else {
        uint32_t N = calCount / scaleCount;
        if constexpr (IsSameType<T, float>::value) {
            SetVectorMask<__cce_half, MaskMode::COUNTER>(0, scaleCount);
            UnaryRepeatParams f162s8Param;
            f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;

            LocalTensor<__cce_half> halfScaleTensor = scaleTensor.template ReinterpretCast<__cce_half>();
            Cast<__cce_half, float, false>(halfScaleTensor, scaleTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                f162s8Param);

            LocalTensor<__cce_half> halfOffsetTensor = offsetTensor.template ReinterpretCast<__cce_half>();
            Cast<__cce_half, float, false>(halfOffsetTensor, offsetTensor, RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, 1, f162s8Param);

            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * scaleCount], srcTensor[i * scaleCount],
                    sharedTmpBuffer, halfScaleTensor, halfOffsetTensor, scaleCount);
            }
        } else {
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * scaleCount], srcTensor[i * scaleCount],
                    sharedTmpBuffer, scaleTensor, offsetTensor, scaleCount);
            }
        }
    }

    SetMaskNorm();
    ResetMask();
}

}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_v220_impl.h" 2

namespace AscendC {

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendQuantCalc<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scale, offset, calCount);
}

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendQuantCalc<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor,
        offset, scaleCount, calCount);
}

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor, const uint32_t scaleCount, const uint32_t offsetCount,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendQuantCalc<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor,
        offsetTensor, scaleCount, offsetCount, calCount);
}

}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_common_impl.h" 2






namespace AscendC {
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const float scale, const float offset, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, stackTensor, scale, offset, calCount);
}

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, stackTensor, scaleTensor, offset, scaleCount,
        calCount);
}

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const LocalTensor<T>& offsetTensor, const uint32_t scaleCount,
    const uint32_t offsetCount, const uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, stackTensor, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void IsQuantParamValid(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor, const uint32_t scaleCount, const uint32_t offsetCount,
    const uint32_t calCount)
{



      ;



      ;



      ;



      ;



      ;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void IsQuantParamValid(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T& offset, const uint32_t scaleCount, const uint32_t calCount)
{



      ;



      ;



      ;



      ;
}
}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scale, offset, calCount);
}
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const float scale, const float offset, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, scale, offset, calCount);
}
# 90 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scale, offset, srcTensor.GetSize());
}
# 108 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const float scale, const float offset)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, scale, offset, srcTensor.GetSize());
}
# 133 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offset,
        scaleCount, calCount);
}
# 155 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, scaleTensor, offset, scaleCount, calCount);
}
# 178 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor, const T offset)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offset,
        scaleTensor.GetSize(), srcTensor.GetSize());
}
# 197 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<T>& scaleTensor, const T offset)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, scaleTensor, offset,
        scaleTensor.GetSize(), srcTensor.GetSize());
}
# 224 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor, const uint32_t scaleCount, const uint32_t offsetCount,
    const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
}
# 248 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const LocalTensor<T>& offsetTensor, const uint32_t scaleCount,
    const uint32_t offsetCount, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
}
# 273 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor,
        scaleTensor.GetSize(), offsetTensor.GetSize(), srcTensor.GetSize());
}
# 293 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<T>& scaleTensor, const LocalTensor<T>& offsetTensor)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, scaleTensor, offsetTensor,
        scaleTensor.GetSize(), offsetTensor.GetSize(), srcTensor.GetSize());
}
#pragma end_pipe
}
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant_utils.h"
namespace AscendC {
struct DequantParams {
    uint32_t m;
    uint32_t n;
    uint32_t calCount;
};

};
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/dequant/ascend_dequant_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/dequant/ascend_dequant_common_impl.h"
namespace AscendC {
constexpr uint32_t FLOAT_PER_BLOCK = 8;
constexpr uint32_t FLOAT_PER_REPEAT = 64;

[aicore] __inline__ __attribute__((always_inline)) bool IsCalCountValid(const LocalTensor<int32_t>& srcTensor, uint32_t calCount)
{



      ;
    return true;
}


template <typename scaleT>
[aicore] __inline__ __attribute__((always_inline)) bool IsWithoutDequantParamsValid(const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale)
{



                                                                            ;

    if constexpr(IsSameType<scaleT, uint64_t>::value) {


                                                       ;
    }
    return true;
}


template <typename dstT>
[aicore] __inline__ __attribute__((always_inline)) bool IsDequantParamsValid(const LocalTensor<int32_t>& srcTensor, const LocalTensor<dstT>& dstTensor,
    DequantParams& params)
{


                                                           ;


                                                                                                          ;


                                                                                                          ;

    uint32_t oneBlockNum = ONE_BLK_SIZE / sizeof(dstT);
    uint32_t alignInner = (params.n + oneBlockNum - 1) / oneBlockNum * oneBlockNum;

                                                                                      ;

    return true;
}


template <typename scaleT>
[aicore] __inline__ __attribute__((always_inline)) bool IsDeqscaleTensorValid(const LocalTensor<scaleT>& deqScale, DequantParams& params)
{


                                                                                      ;
    return true;
}


template <typename dstT, typename scaleT, bool isTensor>
[aicore] __inline__ __attribute__((always_inline)) constexpr bool IsTemplateValid()
{
    if constexpr(isTensor) {


        constexpr bool isValid1 = (IsSameType<scaleT, uint64_t>::value) && (IsSameType<dstT, __cce_half>::value);
        constexpr bool isValid2 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, float>::value);



        constexpr bool isValid3 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, float>::value);
        constexpr bool isValid4 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, bfloat16_t>::value);
        constexpr bool isValid5 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, bfloat16_t>::value);
        return isValid1 || isValid2 || isValid3 || isValid4 || isValid5;

    } else {






        constexpr bool isValid1 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, bfloat16_t>::value);
        constexpr bool isValid2 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, float>::value);
        constexpr bool isValid3 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, float>::value);
        constexpr bool isValid4 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, bfloat16_t>::value);
        return isValid1 || isValid2 || isValid3 || isValid4;

    }
}


template <typename scaleT>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantTmpCalc(const LocalTensor<float>& stackBuffer, DequantParams& dqParams,
    AscendDequantParams<float>& params, uint32_t srcSize, uint32_t deqScaleSize)
{
    uint32_t base = dqParams.n;

    deqScaleSize = (deqScaleSize + FLOAT_PER_BLOCK - 1) / FLOAT_PER_BLOCK * FLOAT_PER_BLOCK;

    uint32_t tmpSrcSize = (stackBuffer.GetSize() - deqScaleSize) / base * base;
                                                                                                           ;
    tmpSrcSize = (tmpSrcSize > srcSize) ? srcSize : tmpSrcSize;
    params.tmpSize = tmpSrcSize;
    params.tmpAddrA = stackBuffer;
    params.tmpAddrB = stackBuffer[deqScaleSize];
}


template <typename scaleT>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantTmpCalc(const LocalTensor<int32_t>& srcTensor, const scaleT deqScale,
    const LocalTensor<float>& stackBuffer, DequantParams& dqParams, AscendDequantParams<float>& params)
{
    uint32_t srcSize = dqParams.m * dqParams.n;
    uint32_t deqScaleSize = (dqParams.calCount + FLOAT_PER_BLOCK - 1) / FLOAT_PER_BLOCK * FLOAT_PER_BLOCK;

    AscendDequantTmpCalc<scaleT>(stackBuffer, dqParams, params, srcSize, deqScaleSize);

    if constexpr(IsSameType<scaleT, float>::value) {
        Duplicate<float>(params.tmpAddrA, deqScale, static_cast<int32_t>(dqParams.calCount));
    } else {
        Duplicate<float>(params.tmpAddrA, ToFloat(deqScale), static_cast<int32_t>(dqParams.calCount));
    }
    PipeBarrier<PIPE_V>();
}



template <typename dstT>
[aicore] __inline__ __attribute__((always_inline)) RoundMode GetFP32CastMode()
{



    constexpr RoundMode castMode = IsSameType<dstT, bfloat16_t>::value ? RoundMode::CAST_RINT: RoundMode::CAST_NONE;
    return castMode;

}


template <typename dstT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void UpdateDequantParams(DequantParams& params)
{
    if constexpr(mode == DeQuantMode::DEQUANT_WITH_SINGLE_ROW) {
        constexpr uint32_t ONE_BLK_SIZE = 32;
        uint32_t oneBlockNum = ONE_BLK_SIZE / sizeof(dstT);
        bool isCalCountAlign = (params.calCount % oneBlockNum == 0);
        bool isNDivisible = (params.n % params.calCount == 0);



        if (params.m == 1 && isCalCountAlign && isNDivisible) {
            params.m = params.n / params.calCount;
            params.n = params.calCount;
        }
    }
}



template <typename scaleT>
[aicore] __inline__ __attribute__((always_inline)) void CastDeqscale(const LocalTensor<scaleT>& deqScale, AscendDequantParams<float>& params,
    uint32_t scaleSize)
{
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = IsSameType<scaleT, float>::value ? DEFAULT_REPEAT_STRIDE: HALF_DEFAULT_REPEAT_STRIDE;

    if constexpr(IsSameType<scaleT, float>::value) {
        SetVectorMask<float, MaskMode::COUNTER>(0, scaleSize);
        Adds<float, false>(params.tmpAddrA, deqScale, 0, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    } else if constexpr(IsSameType<scaleT, uint64_t>::value) {

        LocalTensor<float> deqScaleFP32 = deqScale.template ReinterpretCast<float>();


        GatherMaskParams reducev2Params;
        reducev2Params.repeatTimes = 1;
        uint64_t rsvdCnt = 0;
        GatherMask<float>(params.tmpAddrA, deqScaleFP32, 1, true, scaleSize * 2, reducev2Params, rsvdCnt);
        PipeBarrier<PIPE_V>();
        SetMaskCount();
    } else {
        SetVectorMask<float, MaskMode::COUNTER>(0, scaleSize);
        Cast<float, scaleT, false>(params.tmpAddrA, deqScale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void CastSrc(const LocalTensor<int32_t>& srcTensor, const LocalTensor<float>& dstTensor,
    UnaryRepeatParams& unaryParams, uint64_t counter)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, counter);
    Cast<float, int32_t, false>(dstTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] __inline__ __attribute__((always_inline)) void DequantMul(const LocalTensor<float>& srcTensor, const LocalTensor<float>& deqScaleTensor,
    const LocalTensor<float>& dstTensor, BinaryRepeatParams& binaryParams, DequantParams& dqParams, uint32_t k,
    uint32_t loopCount, uint32_t tail)
{
    if (k == 0) {
        return;
    }


    if (dqParams.n > MAX_REPEAT_TIMES * FLOAT_PER_BLOCK) {
        BinaryRepeatParams binaryParamsDefault;
        SetVectorMask<float, MaskMode::COUNTER>(0, dqParams.calCount);
        for (uint32_t i = 0; i < k; i++) {
            Mul<float, false>(dstTensor[i * dqParams.n], srcTensor[i * dqParams.n], deqScaleTensor, MASK_PLACEHOLDER, 1,
                binaryParamsDefault);
        }
        PipeBarrier<PIPE_V>();
        return;
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, FLOAT_PER_REPEAT * k);
    for (uint32_t i = 0; i < loopCount; i++) {
        Mul<float, false>(dstTensor[i * FLOAT_PER_REPEAT], srcTensor[i * FLOAT_PER_REPEAT],
            deqScaleTensor[i * FLOAT_PER_REPEAT], MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();

    if (tail != 0) {
        SetMaskNorm();

        uint32_t kTimes = k / MAX_REPEAT_TIMES;
        uint32_t kRemains = k % MAX_REPEAT_TIMES;
        SetVectorMask<float, MaskMode::NORMAL>(0, ((uint64_t)1 << tail) - 1);

        uint32_t baseIndex = loopCount * FLOAT_PER_REPEAT;
        for (uint32_t i = 0; i < kTimes; i++) {
            uint32_t index = baseIndex + MAX_REPEAT_TIMES * i * dqParams.n;
            Mul<float, false>(dstTensor[index], srcTensor[index], deqScaleTensor[baseIndex], MASK_PLACEHOLDER,
                MAX_REPEAT_TIMES, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        if (kRemains > 0) {
            uint32_t index = baseIndex + MAX_REPEAT_TIMES * kTimes * dqParams.n;
            Mul<float, false>(dstTensor[index], srcTensor[index], deqScaleTensor[baseIndex], MASK_PLACEHOLDER, kRemains,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
}


template <typename dstT>
[aicore] __inline__ __attribute__((always_inline)) void CastDst(const LocalTensor<dstT>& dstTensor, const LocalTensor<float>& srcFP32,
    UnaryRepeatParams& unaryParams, uint32_t srcInner, uint32_t dstInner, uint32_t dataNum)
{
    if constexpr(IsSameType<dstT, float>::value) {
        SetVectorMask<float, MaskMode::COUNTER>(0, dataNum);
        Adds<float, false>(dstTensor, srcFP32, 0, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        return;
    }

    RoundMode castMode = GetFP32CastMode<dstT>();
    if (srcInner == dstInner) {
        SetVectorMask<float, MaskMode::COUNTER>(0, dataNum);
        Cast<dstT, float, false>(dstTensor, srcFP32, castMode, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        uint32_t loopNum = dataNum / srcInner;
        uint32_t tailPart = dataNum % srcInner;
        SetVectorMask<float, MaskMode::COUNTER>(0, srcInner);
        for (uint32_t i = 0; i < loopNum; i++) {
            Cast<dstT, float, false>(dstTensor[i * dstInner], srcFP32[i * srcInner], castMode, MASK_PLACEHOLDER, 1,
                unaryParams);
        }
        PipeBarrier<PIPE_V>();

        if (tailPart > 0) {
            SetVectorMask<float, MaskMode::COUNTER>(0, tailPart);
            Cast<dstT, float, false>(dstTensor[loopNum * dstInner], srcFP32[loopNum * srcInner], castMode,
                MASK_PLACEHOLDER, 1, unaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
}


template <typename dstT, typename scaleT, bool isPureDqParams = false>
[aicore] __inline__ __attribute__((always_inline)) void CalculateByInner(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, DequantParams& dqParams, AscendDequantParams<float>& ascendDqParams,
    uint32_t calCount)
{
    LocalTensor<float> deqScaleFP32 = ascendDqParams.tmpAddrA;
    LocalTensor<float> srcFP32 = ascendDqParams.tmpAddrB;

    uint32_t oneBlockNum = ONE_BLK_SIZE / sizeof(dstT);
    uint32_t dstInner = (dqParams.n + oneBlockNum - 1) / oneBlockNum * oneBlockNum;
    uint32_t tmpSize = ascendDqParams.tmpSize;
    uint32_t loopCount = calCount / tmpSize;
    uint32_t tailSize = calCount % tmpSize;
    uint32_t k = tmpSize / dqParams.n;

    uint32_t mainBlockLoopCount = dqParams.calCount / FLOAT_PER_REPEAT;
    uint32_t mainBlockTail = dqParams.calCount % FLOAT_PER_REPEAT;
    BinaryRepeatParams binaryParams;
    BinaryRepeatParams binaryParamsMul(1, 1, 1, dqParams.n / FLOAT_PER_BLOCK, dqParams.n / FLOAT_PER_BLOCK, 0);
    UnaryRepeatParams unaryParams;
    UnaryRepeatParams unaryParamsDst;
    if constexpr(!IsSameType<dstT, float>::value) {
        unaryParamsDst.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    }

    CastDeqscale(deqScale, ascendDqParams, dqParams.calCount);


    uint32_t castDstIndex = (dqParams.n == dstInner) ? tmpSize : k * dstInner;
    for (uint32_t i = 0; i < loopCount; i++) {
        SetMaskCount();
        CastSrc(srcTensor[i * tmpSize], srcFP32, unaryParams, tmpSize);

        DequantMul(srcFP32, deqScaleFP32, srcFP32, binaryParamsMul, dqParams, k, mainBlockLoopCount, mainBlockTail);

        SetMaskCount();
        CastDst<dstT>(dstTensor[i * castDstIndex], srcFP32, unaryParamsDst, dqParams.n, dstInner, tmpSize);
    }


    if (tailSize > 0) {
        CastSrc(srcTensor[calCount - tailSize], srcFP32, unaryParams, tailSize);

        k = tailSize / dqParams.n;
        DequantMul(srcFP32, deqScaleFP32, srcFP32, binaryParamsMul, dqParams, k, mainBlockLoopCount, mainBlockTail);

        if constexpr(!isPureDqParams) {
            uint32_t tailK = tailSize % dqParams.n;
            if (tailK != 0) {
                SetMaskCount();
                SetVectorMask<float, MaskMode::COUNTER>(0, tailK);
                uint32_t idxMul = tailSize - tailK;
                Mul<float, false>(srcFP32[idxMul], srcFP32[idxMul], deqScaleFP32, MASK_PLACEHOLDER, 1, binaryParams);
                PipeBarrier<PIPE_V>();
            }
        }

        SetMaskCount();

        uint32_t index = (dqParams.n == dstInner) ? calCount - tailSize :
            (calCount - tailSize) / dqParams.n * dstInner;
        CastDst<dstT>(dstTensor[index], srcFP32, unaryParamsDst, dqParams.n, dstInner, tailSize);
    }
}



template <typename dstT, typename scaleT, bool isPureDqParams, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantImpl(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, DequantParams& params,
    uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    static_assert(IsTemplateValid<dstT, scaleT, true>(),
        "current combination of deqScale dtype and dstTensor dtype is not supported, please check the document");

    if (!IsDequantParamsValid<dstT>(srcTensor, dstTensor, params) || !IsDeqscaleTensorValid(deqScale, params)) {
        return;
    }
    UpdateDequantParams<dstT, mode>(params);
                                                                                                                    ;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    AscendDequantParams<float> ascendDqParams;
    AscendDequantTmpCalc<scaleT>(stackBuffer, params, ascendDqParams, params.m * params.n, params.calCount);

    SetMaskCount();
    CalculateByInner<dstT, scaleT, isPureDqParams>(dstTensor, srcTensor, deqScale, params, ascendDqParams, calCount);

    SetMaskNorm();
    ResetMask();
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantImpl(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, DequantParams params)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                ;
    AscendDequantImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params,
        params.m * params.n);
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if (!IsCalCountValid(srcTensor, calCount) || !IsWithoutDequantParamsValid<scaleT>(srcTensor, deqScale)) {
        return;
    }
    DequantParams params = {srcTensor.GetSize() / deqScale.GetSize(), deqScale.GetSize(), deqScale.GetSize()};
    AscendDequantImpl<dstT, scaleT, false, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params, calCount);
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                ;
    AscendDequantCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, calCount);
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantNoCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    if (!IsWithoutDequantParamsValid<scaleT>(srcTensor, deqScale)) {
        return;
    }
    DequantParams params = {srcTensor.GetSize() / deqScale.GetSize(), deqScale.GetSize(), deqScale.GetSize()};
    AscendDequantImpl<dstT, scaleT, false, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params,
        srcTensor.GetSize());
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantNoCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AscendDequantNoCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer);
}


template <typename dstT, typename scaleT, bool isPureDqParams, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantScalarImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const scaleT deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer,
    DequantParams& params)
{
    static_assert(IsTemplateValid<dstT, scaleT, false>(),
        "current combination of deqScale dtype and dstTensor dtype is not supported, please check the document");

    if (!IsDequantParamsValid<dstT>(srcTensor, dstTensor, params)) {
        return;
    }
    UpdateDequantParams<dstT, mode>(params);
                                                                                                                    ;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    SetMaskCount();
    AscendDequantParams<float> ascendDqParams;
    AscendDequantTmpCalc<scaleT>(srcTensor, deqScale, stackBuffer, params, ascendDqParams);
    LocalTensor<float> deqScaleFP32 = ascendDqParams.tmpAddrA;

    SetMaskCount();
    CalculateByInner<dstT, float, true>(dstTensor, srcTensor, deqScaleFP32, params, ascendDqParams,
        params.m * params.n);

    SetMaskNorm();
    ResetMask();
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantScalarImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const scaleT deqScale, DequantParams& params)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AscendDequantScalarImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, DequantParams params)
{
    AscendDequantImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params,
        params.m * params.n);
}
# 70 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, DequantParams params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, params);
}
# 101 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AscendDequantCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, calCount);
}
# 123 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, calCount);
}
# 153 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    AscendDequantNoCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer);
}
# 175 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantNoCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale);
}
# 206 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const scaleT deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, DequantParams params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantScalarImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params);
}
# 232 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const scaleT deqScale, DequantParams params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantScalarImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, params);
}
#pragma end_pipe
}
# 70 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant_utils.h"
namespace AscendC {
struct AntiQuantShapeInfo {
    uint32_t offsetHeight{0};
    uint32_t offsetWidth{0};
    uint32_t scaleHeight{0};
    uint32_t scaleWidth{0};
};

};
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_common.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_common.h"
namespace AscendC {
constexpr uint32_t ANTIQUANT_TWO = 2;
constexpr uint32_t ANTIQUANT_FOUR = 4;
constexpr uint32_t ANTIQUANT_BRCB_BASE = 8;
constexpr uint32_t ANTIQUANT_MIN_METHOD2 = 80;
constexpr uint32_t ANTIQUANT_SINGLE_N_SIZE = 64;
constexpr uint32_t ANTIQUANT_SINGLE_N_SIZE_BF16 = 64;
constexpr uint32_t ANTIQUANT_SINGLE_N_SIZE_FP16 = 128;
constexpr uint32_t ANTIQUANT_MAX_K = 255;
constexpr uint32_t MAX_K_FOR_FP16_BRCB = 4096;
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_c220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_c220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_c220_impl.h" 2



namespace AscendC {

template <typename SrcType, typename OutType>
[aicore] __inline__ __attribute__((always_inline)) void CheckApiDtypeValid()
{
    constexpr bool inputValid = (IsSameType<SrcType, int8_t>::value) || (IsSameType<SrcType, int4b_t>::value);
    constexpr bool outputValid = (IsSameType<OutType, __cce_half>::value) || (IsSameType<OutType, bfloat16_t>::value);

                                                                                                     ;
}

template <typename SrcType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantInnerLoop(const LocalTensor<bfloat16_t> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const UnaryRepeatParams &unaryParamsCastSrc,
    const UnaryRepeatParams &unaryParamsToFP32, const UnaryRepeatParams &unaryParamsFP32ToDst,
    const BinaryRepeatParams &binaryParams, const uint32_t calCount)
{
    uint32_t srcFp16Pos = calCount * sizeof(bfloat16_t);
    uint32_t offsetFp32Pos = calCount * sizeof(float);
    auto fp16TmpBuffer = sharedTmpBuffer[srcFp16Pos].ReinterpretCast<__cce_half>();
    auto offsetBuffer = sharedTmpBuffer[offsetFp32Pos].ReinterpretCast<float>();
    auto resultBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<__cce_half, SrcType, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsCastSrc);
    PipeBarrier<PIPE_V>();
    Cast<float, __cce_half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsToFP32);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Cast<float, bfloat16_t, false>(offsetBuffer, offset, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            unaryParamsToFP32);
        PipeBarrier<PIPE_V>();
        Add<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Cast<float, bfloat16_t, false>(offsetBuffer, scale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsToFP32);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Cast<bfloat16_t, float, false>(dst, resultBuffer, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1, unaryParamsFP32ToDst);
    PipeBarrier<PIPE_V>();
}

template <typename SrcType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantInnerLoop(const LocalTensor<bfloat16_t> &dst, const LocalTensor<SrcType> &src,
    const bfloat16_t offset, const bfloat16_t scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const UnaryRepeatParams &unaryParamsCastSrc, const UnaryRepeatParams &unaryParamsToFP32,
    const UnaryRepeatParams &unaryParamsFP32ToDst, const UnaryRepeatParams &unaryParamsScalar, const uint32_t calCount)
{
    uint32_t srcFp16Pos = calCount * sizeof(bfloat16_t);
    auto fp16TmpBuffer = sharedTmpBuffer[srcFp16Pos].ReinterpretCast<__cce_half>();
    auto resultBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<__cce_half, SrcType, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsCastSrc);
    PipeBarrier<PIPE_V>();
    Cast<float, __cce_half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsToFP32);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Adds<float, false>(resultBuffer, resultBuffer, ToFloat(offset), MASK_PLACEHOLDER, 1, unaryParamsScalar);
        PipeBarrier<PIPE_V>();
    }
    Muls<float, false>(resultBuffer, resultBuffer, ToFloat(scale), MASK_PLACEHOLDER, 1, unaryParamsScalar);
    PipeBarrier<PIPE_V>();
    Cast<bfloat16_t, float, false>(dst, resultBuffer, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1, unaryParamsFP32ToDst);
    PipeBarrier<PIPE_V>();
}

template <typename SrcType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTransposePerformance(const LocalTensor<bfloat16_t> &dst,
    const LocalTensor<SrcType> &src, const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N)
{
    uint32_t posOffsetScale = N * sizeof(float) * ANTIQUANT_TWO;
    uint32_t posCast = posOffsetScale + ANTIQUANT_SINGLE_N_SIZE_BF16 * K * sizeof(__cce_half);
    auto fp16TmpBuffer = sharedTmpBuffer[posCast].ReinterpretCast<__cce_half>();
    auto resultBuffer = sharedTmpBuffer[posOffsetScale].ReinterpretCast<float>();

    UnaryRepeatParams s42f16unaryParams;
    s42f16unaryParams.srcRepStride = N / ANTIQUANT_TWO / ONE_BLK_SIZE;
    s42f16unaryParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams s82f16unaryParams;
    s82f16unaryParams.srcRepStride = N * sizeof(int8_t) / ONE_BLK_SIZE;
    s82f16unaryParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f162f32unaryParams;
    f162f32unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    BinaryRepeatParams binaryParams;
    binaryParams.src1RepStride = 0;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = N * sizeof(__cce_half) / ONE_BLK_SIZE;

    for (uint32_t i = 0; i < N / ANTIQUANT_SINGLE_N_SIZE_BF16; i++) {
        SetMaskNorm();
        SetVectorMask<__cce_half, MaskMode::NORMAL>(ANTIQUANT_SINGLE_N_SIZE_BF16);
        if constexpr (IsSameType<SrcType, int4b_t>::value) {

            Cast<__cce_half, int4b_t, false>(fp16TmpBuffer, src[ANTIQUANT_SINGLE_N_SIZE_BF16 * i], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, K, s42f16unaryParams);
        } else {

            Cast<__cce_half, int8_t, false>(fp16TmpBuffer, src[ANTIQUANT_SINGLE_N_SIZE_BF16 * i], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, K, s82f16unaryParams);
        }
        PipeBarrier<PIPE_V>();

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE_BF16 * K);
        Cast<float, __cce_half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, K, f162f32unaryParams);
        PipeBarrier<PIPE_V>();

        auto offsetBuffer = sharedTmpBuffer[ANTIQUANT_SINGLE_N_SIZE_BF16 * i * sizeof(float)].ReinterpretCast<float>();
        Add<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();

        auto scaleBuffer = sharedTmpBuffer[N * sizeof(float) +
            ANTIQUANT_SINGLE_N_SIZE_BF16 * i * sizeof(float)].ReinterpretCast<float>();
        Mul<float, false>(resultBuffer, resultBuffer, scaleBuffer, MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();

        Cast<bfloat16_t, float, false>(dst[ANTIQUANT_SINGLE_N_SIZE_BF16 * i], resultBuffer,
            RoundMode::CAST_RINT, MASK_PLACEHOLDER, K, f322f16Params);
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
}

template <typename SrcType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTransposePerformanceTail(const LocalTensor<bfloat16_t> &dst,
    const LocalTensor<SrcType> &src, const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N, const uint32_t mask)
{
    uint32_t index = N / ANTIQUANT_SINGLE_N_SIZE_BF16 * ANTIQUANT_SINGLE_N_SIZE_BF16;
    uint32_t posOffset = N * sizeof(float);
    uint32_t posOffsetScale = posOffset * ANTIQUANT_TWO;
    uint32_t posCast = posOffsetScale + ANTIQUANT_SINGLE_N_SIZE_BF16 * K * sizeof(__cce_half);
    auto fp16TmpBuffer = sharedTmpBuffer[posCast].ReinterpretCast<__cce_half>();
    auto resultBuffer = sharedTmpBuffer[posOffsetScale].ReinterpretCast<float>();
    auto offsetBuffer = sharedTmpBuffer[index * sizeof(float)].ReinterpretCast<float>();
    auto scaleBuffer = sharedTmpBuffer[posOffset + index * sizeof(float)].ReinterpretCast<float>();

    UnaryRepeatParams s42f16unaryParams;
    s42f16unaryParams.srcRepStride = N / ANTIQUANT_TWO / ONE_BLK_SIZE;
    UnaryRepeatParams s82f16unaryParams;
    s82f16unaryParams.srcRepStride = N * sizeof(int8_t) / ONE_BLK_SIZE;
    s82f16unaryParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f162f32unaryParams;
    f162f32unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    BinaryRepeatParams binaryParams;
    binaryParams.src1RepStride = 0;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = N * sizeof(bfloat16_t) / ONE_BLK_SIZE;


    SetMaskNorm();
    SetVectorMask<__cce_half, MaskMode::NORMAL>(mask);
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        Cast<__cce_half, int4b_t, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, K, s42f16unaryParams);
    } else {
        Cast<__cce_half, int8_t, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, K, s82f16unaryParams);
    }
    PipeBarrier<PIPE_V>();


    Cast<float, __cce_half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, K,
        f162f32unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(resultBuffer, resultBuffer, scaleBuffer, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<bfloat16_t, float, false>(dst, resultBuffer, RoundMode::CAST_RINT, MASK_PLACEHOLDER, K, f322f16Params);
    PipeBarrier<PIPE_V>();
    ResetMask();
}

template <typename SrcType>
[aicore] __inline__ __attribute__((always_inline)) void PreCast(const LocalTensor<bfloat16_t> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K)
{
    uint32_t posOffset = offset.GetSize() * sizeof(float);
    uint32_t repeatEle = ONE_REPEAT_BYTE_SIZE / sizeof(bfloat16_t);
    uint32_t repeatTimes =
        offset.GetSize() % repeatEle == 0 ? offset.GetSize() / repeatEle : offset.GetSize() / repeatEle + 1;
    auto offsetBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    auto scaleBuffer = sharedTmpBuffer[posOffset].ReinterpretCast<float>();

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;

    SetMaskCount();
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, offset.GetSize());
    Cast<float, bfloat16_t, false>(offsetBuffer, offset, RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<float, bfloat16_t, false>(scaleBuffer, scale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename DstType>
[aicore] __inline__ __attribute__((always_inline)) bool AntiQuantCheckPerformanceMode(const LocalTensor<DstType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K)
{
    if constexpr (IsSameType<DstType, bfloat16_t>::value) {
        uint32_t maxTmpBufferSize =
            scale.GetSize() * ANTIQUANT_TWO * sizeof(float) + ANTIQUANT_SINGLE_N_SIZE_BF16 * K * sizeof(float);
        return sharedTmpBuffer.GetSize() >= maxTmpBufferSize;
    }
    return true;
}



template <typename SrcType, typename DstType, bool isOffset>
[aicore] __inline__ __attribute__((always_inline)) void CalculationMax(const LocalTensor<SrcType> &src, const LocalTensor<DstType> &dst,
    AntiquantParams<float> &params, const uint32_t calCount, const uint32_t N, const uint32_t K, const uint32_t NOffset)
{

    uint32_t srcFp16Pos = calCount / ANTIQUANT_TWO;
    auto fp16TmpBuffer = params.tempTensorInput[srcFp16Pos].ReinterpretCast<__cce_half>();

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    uint32_t count = K / ANTIQUANT_SINGLE_N_SIZE;



    BinaryRepeatParams binaryParams(1, 1, 0, count * DEFAULT_REPEAT_STRIDE, count * DEFAULT_REPEAT_STRIDE, 1);

    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, calCount);

    Cast<__cce_half, int8_t, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, __cce_half, false>(params.tempTensorInput, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE * N);
    for (uint32_t i = 0; i < count; i++) {

        uint32_t curOffset = i * ANTIQUANT_SINGLE_N_SIZE;

        if constexpr (isOffset) {
            Add<float, false>(params.tempTensorInput[curOffset], params.tempTensorInput[curOffset],
                params.tempTensorOffset[NOffset], MASK_PLACEHOLDER, N, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<float, false>(params.tempTensorInput[curOffset], params.tempTensorInput[curOffset],
            params.tempTensorScale[NOffset], MASK_PLACEHOLDER, N, binaryParams);
        PipeBarrier<PIPE_V>();
    }


    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<bfloat16_t, float, false>(dst, params.tempTensorInput, RoundMode::CAST_RINT,
        MASK_PLACEHOLDER, 1, f322f16Params);
    PipeBarrier<PIPE_V>();
}


template <typename DstType>
[aicore] __inline__ __attribute__((always_inline)) void GetAntiquantTensorInfo(const LocalTensor<DstType> &scale, const LocalTensor<float> &stackBuffer,
    AntiquantParams<float> &params)
{
    uint32_t N = scale.GetSize();
    params.tempTensorOffset = stackBuffer[0];
    params.tempTensorScale = stackBuffer[ANTIQUANT_BRCB_BASE * N];
    params.tempTensorInput = stackBuffer[ANTIQUANT_BRCB_BASE * ANTIQUANT_TWO * N];
}



template <typename DstType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void CastAndBrcb(const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale,
    AntiquantParams<float> &params, const UnaryRepeatParams &unaryParams, const uint32_t scaleEleNum)
{
    uint32_t offsetEleNum = offset.GetSize();
    const uint32_t alignBase = (ONE_BLK_SIZE / sizeof(float));
    uint32_t scaleAlign = (scaleEleNum + alignBase - 1) / alignBase * alignBase;
    uint32_t tensorIndex = ANTIQUANT_BRCB_BASE * offsetEleNum - offsetEleNum;


    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, scaleEleNum);
    if constexpr (withOffset) {
        Cast<float, DstType, false>(params.tempTensorOffset[tensorIndex], offset, RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }
    Cast<float, DstType, false>(params.tempTensorScale[tensorIndex], scale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();

    constexpr uint16_t brcbDstBlkStride = 1;
    constexpr uint16_t brcbDstRepStride = ANTIQUANT_BRCB_BASE;
    uint8_t repeatTimes = (scaleEleNum + ANTIQUANT_BRCB_BASE - 1 ) / ANTIQUANT_BRCB_BASE;
    BrcbRepeatParams brcbParams(brcbDstBlkStride, brcbDstRepStride);

    SetMaskNorm();
    ResetMask();

    if constexpr (withOffset) {
        Brcb(params.tempTensorOffset, params.tempTensorOffset[tensorIndex], repeatTimes, brcbParams);
        PipeBarrier<PIPE_V>();
    }
    Brcb(params.tempTensorScale, params.tempTensorScale[tensorIndex], repeatTimes, brcbParams);
    PipeBarrier<PIPE_V>();
}


template <typename SrcType>
[aicore] __inline__ __attribute__((always_inline)) void CastSrcProcess(const LocalTensor<SrcType> &src, const LocalTensor<float> &srcFP32,
    const UnaryRepeatParams &src2Fp16Params, const UnaryRepeatParams &fp162Fp32Params, const uint32_t calCount,
    const uint32_t scaleEleNum, const uint32_t n)
{

    uint32_t srcFp16Pos = calCount / ANTIQUANT_TWO;
    auto fp16TmpBuf = srcFP32[srcFp16Pos].ReinterpretCast<__cce_half>();
    SetMaskNorm();
    SetVectorMask<__cce_half, MaskMode::NORMAL>(0, FULL_MASK);
    Cast<__cce_half, SrcType, false>(fp16TmpBuf, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, scaleEleNum, src2Fp16Params);
    PipeBarrier<PIPE_V>();
    Cast<float, __cce_half, false>(srcFP32, fp16TmpBuf, RoundMode::CAST_NONE, MASK_PLACEHOLDER, scaleEleNum, fp162Fp32Params);
    PipeBarrier<PIPE_V>();
}


template <bool withOffset>
[aicore] __inline__ __attribute__((always_inline)) void AddMulProcess(const LocalTensor<float> &srcFP32, const LocalTensor<float> &offsetFP32,
    const LocalTensor<float> &scaleFP32, const BinaryRepeatParams &binaryParams, const uint32_t N)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE * N);

    if constexpr (withOffset) {
        Add<float, false>(srcFP32, srcFP32, offsetFP32, MASK_PLACEHOLDER, N, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Mul<float, false>(srcFP32, srcFP32, scaleFP32, MASK_PLACEHOLDER, N, binaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename dstT>
[aicore] __inline__ __attribute__((always_inline)) void CastDstProcess(const LocalTensor<float> &resFP32, const LocalTensor<dstT> &dst,
    const UnaryRepeatParams &unaryParamsFp322Dst, const uint32_t n, const uint32_t srcN)
{
    SetMaskNorm();
    SetVectorMask<float, MaskMode::NORMAL>(0, FULL_MASK);
    constexpr RoundMode castMode = IsSameType<dstT, bfloat16_t>::value ? RoundMode::CAST_RINT : RoundMode::CAST_NONE;
    Cast<dstT, float, false>(dst, resFP32, castMode, MASK_PLACEHOLDER, srcN, unaryParamsFp322Dst);
    PipeBarrier<PIPE_V>();
}


template <typename SrcType, typename DstType, bool withOffset>
[aicore] __inline__ __attribute__((always_inline)) void CalcN64ByBrcb(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<float> &stackBuffer,
    const uint32_t scaleEleNum, const uint32_t K, const uint32_t shapeN)
{
    AntiquantParams<float> params;
    GetAntiquantTensorInfo<DstType>(scale, stackBuffer, params);

    uint32_t n = K / ANTIQUANT_SINGLE_N_SIZE;
    uint32_t numPerLoop = ANTIQUANT_SINGLE_N_SIZE * scaleEleNum;

    UnaryRepeatParams unaryParamsSrc2Fp16;
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        unaryParamsSrc2Fp16.srcRepStride = n;
    } else {
        unaryParamsSrc2Fp16.srcRepStride = ANTIQUANT_TWO * n;
    }
    unaryParamsSrc2Fp16.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams unaryParamsB16Fp32;
    unaryParamsB16Fp32.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;

    BinaryRepeatParams binaryParams;
    binaryParams.src1BlkStride = 0;
    binaryParams.src1RepStride = 1;

    UnaryRepeatParams unaryParamsFp322Dst;
    unaryParamsFp322Dst.dstRepStride = ANTIQUANT_SINGLE_N_SIZE * n / (ONE_BLK_SIZE / sizeof(DstType));

    SetMaskCount();
    CastAndBrcb<DstType, withOffset>(offset, scale, params, unaryParamsB16Fp32, scaleEleNum);

    uint32_t curNKOffset = 0;
    for (uint32_t i = 0; i < n; i++) {
        curNKOffset = ANTIQUANT_SINGLE_N_SIZE * i;
        CastSrcProcess<SrcType>(src[curNKOffset], params.tempTensorInput, unaryParamsSrc2Fp16, unaryParamsB16Fp32,
            numPerLoop, scaleEleNum, n);
        AddMulProcess<withOffset>(params.tempTensorInput, params.tempTensorOffset, params.tempTensorScale, binaryParams,
            scaleEleNum);
        CastDstProcess<DstType>(params.tempTensorInput, dst[curNKOffset], unaryParamsFp322Dst, n, shapeN);
    }
}

template <bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantFp16Brcb(const LocalTensor<__cce_half> &scale, const LocalTensor<__cce_half> &offset,
    AntiquantParams<__cce_half> &params, const uint32_t scaleN)
{
    const uint8_t repeatTimes = scaleN / BRCB_BROADCAST_NUMBER;
    BrcbRepeatParams brcbParams(DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    SetMaskNorm();
    ResetMask();
    Brcb(params.tempTensorScale, scale, repeatTimes, brcbParams);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Brcb(params.tempTensorOffset, offset, repeatTimes, brcbParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename SrcType, typename DstType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantBF16Transpose(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    uint32_t srcEleNum = src.GetSize();
    if (K > ANTIQUANT_MAX_K * ANTIQUANT_BRCB_BASE || (K % ANTIQUANT_SINGLE_N_SIZE != 0)) {
        return AntiQuantImplScalar(dst, src, offset, scale, sharedTmpBuffer, srcEleNum, K, shapeInfo);
    }

    auto stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    uint32_t scaleEleNum = scale.GetSize();
    uint32_t shapeN = src.GetSize() / K;
    uint32_t stackBufferSize = scaleEleNum * (ANTIQUANT_SINGLE_N_SIZE + ANTIQUANT_BRCB_BASE * ANTIQUANT_TWO);
    stackBuffer.SetSize(stackBufferSize);
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        scaleEleNum = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
        shapeN = scaleEleNum;
    }
    CalcN64ByBrcb<SrcType, DstType, true>(dst, src, offset, scale, stackBuffer, scaleEleNum, K, shapeN);
}

template <typename SrcType, typename DstType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantBF16Transpose(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    uint32_t srcEleNum = src.GetSize();
    if (K > ANTIQUANT_MAX_K * ANTIQUANT_BRCB_BASE || (K % ANTIQUANT_SINGLE_N_SIZE != 0)) {
        return AntiQuantImplScalar(dst, src, scale, sharedTmpBuffer, srcEleNum, K, shapeInfo);
    }


    uint32_t scaleEleNum = scale.GetSize();
    uint32_t shapeN = src.GetSize() / K;
    auto stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackBufferSize = scaleEleNum * (ANTIQUANT_SINGLE_N_SIZE + ANTIQUANT_BRCB_BASE * ANTIQUANT_TWO);
    stackBuffer.SetSize(stackBufferSize);
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        scaleEleNum = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
        shapeN = scaleEleNum;
    }
    CalcN64ByBrcb<SrcType, DstType, false>(dst, src, scale, scale, stackBuffer, scaleEleNum, K, shapeN);
}

}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_impl.h" 2




namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantInnerLoopF16(const LocalTensor<__cce_half> &dst, const LocalTensor<__cce_half> &src,
    const LocalTensor<__cce_half> &offset, const LocalTensor<__cce_half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const BinaryRepeatParams &binaryParams, const uint32_t calCount)
{
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, calCount);
    Add<__cce_half, false>(dst, offset, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<__cce_half, false>(dst, scale, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename SrcType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantInnerLoop(const LocalTensor<__cce_half> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<__cce_half> &offset, const LocalTensor<__cce_half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const UnaryRepeatParams &unaryParamsCastSrc, const BinaryRepeatParams &binaryParams, const uint32_t calCount)
{
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, calCount);
    Cast<__cce_half, SrcType, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsCastSrc);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Add<__cce_half, false>(dst, offset, dst, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Mul<__cce_half, false>(dst, scale, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename SrcType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantInnerLoop(const LocalTensor<__cce_half> &dst, const LocalTensor<SrcType> &src,
    const __cce_half offset, const __cce_half scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const UnaryRepeatParams &unaryParamsCastSrc, const UnaryRepeatParams &unaryParamsScalar, const uint32_t calCount)
{
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, calCount);
    Cast<__cce_half, SrcType, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsCastSrc);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Adds<__cce_half, false>(dst, dst, offset, MASK_PLACEHOLDER, 1, unaryParamsScalar);
        PipeBarrier<PIPE_V>();
    }
    Muls<__cce_half, false>(dst, dst, scale, MASK_PLACEHOLDER, 1, unaryParamsScalar);
    PipeBarrier<PIPE_V>();
}

template <typename SrcType, typename DstType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantOuterLoop(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{
    UnaryRepeatParams unaryParamsCastSrc;
    if constexpr(IsSameType<SrcType, int8_t>::value) {
        unaryParamsCastSrc.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    } else {
        unaryParamsCastSrc.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    }
    BinaryRepeatParams binaryParams;
    if constexpr (IsSameType<DstType, __cce_half>::value) {
        AntiQuantInnerLoop<SrcType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, unaryParamsCastSrc,
            binaryParams, calCount);
    } else {
        uint32_t tmpSize = sharedTmpBuffer.GetSize() / sizeof(DstType) / ANTIQUANT_FOUR;
        uint32_t loopCount = calCount / tmpSize;
        uint32_t tailSize = calCount % tmpSize;

        UnaryRepeatParams unaryParamsFP32ToDst;
        unaryParamsFP32ToDst.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
        UnaryRepeatParams unaryParamsToFP32;
        unaryParamsToFP32.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;

        for (uint32_t i = 0; i < loopCount; i++) {
            AntiQuantInnerLoop<SrcType, withOffset>(dst[i * tmpSize], src[i * tmpSize], offset, scale, sharedTmpBuffer,
                unaryParamsCastSrc, unaryParamsToFP32, unaryParamsFP32ToDst, binaryParams, tmpSize);
        }
        if (tailSize > 0) {
            AntiQuantInnerLoop<SrcType, withOffset>(dst[loopCount * tmpSize], src[loopCount * tmpSize], offset, scale,
                sharedTmpBuffer, unaryParamsCastSrc, unaryParamsToFP32, unaryParamsFP32ToDst, binaryParams, tailSize);
        }
    }
}

template <typename SrcType, typename DstType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantOuterLoop(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    UnaryRepeatParams unaryParamsCastSrc;
    if constexpr(IsSameType<SrcType, int8_t>::value) {
        unaryParamsCastSrc.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    } else {
        unaryParamsCastSrc.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    }
    UnaryRepeatParams unaryParamsScalar;
    if constexpr (IsSameType<DstType, __cce_half>::value) {
        AntiQuantInnerLoop<SrcType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, unaryParamsCastSrc,
            unaryParamsScalar, calCount);
    } else {
        uint32_t tmpSize = sharedTmpBuffer.GetSize() / sizeof(DstType) / ANTIQUANT_FOUR;
        uint32_t loopCount = calCount / tmpSize;
        uint32_t tailSize = calCount % tmpSize;

        UnaryRepeatParams unaryParamsToFP32;
        unaryParamsToFP32.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
        UnaryRepeatParams unaryParamsFP32ToDst;
        unaryParamsFP32ToDst.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;

        for (uint32_t i = 0; i < loopCount; i++) {
            AntiQuantInnerLoop<SrcType, withOffset>(dst[i * tmpSize], src[i * tmpSize], offset, scale, sharedTmpBuffer,
                unaryParamsCastSrc, unaryParamsToFP32, unaryParamsFP32ToDst, unaryParamsScalar, tmpSize);
        }
        if (tailSize > 0) {
            AntiQuantInnerLoop<SrcType, withOffset>(dst[loopCount * tmpSize], src[loopCount * tmpSize], offset, scale,
                sharedTmpBuffer, unaryParamsCastSrc, unaryParamsToFP32, unaryParamsFP32ToDst, unaryParamsScalar,
                tailSize);
        }
    }
}

template <typename SrcType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTransposePerformance(const LocalTensor<__cce_half> &dst,
    const LocalTensor<SrcType> &src, const LocalTensor<__cce_half> &offset, const LocalTensor<__cce_half> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N)
{
    BinaryRepeatParams binaryParams;
    binaryParams.src0RepStride = N * sizeof(__cce_half) / ONE_BLK_SIZE;
    binaryParams.src1RepStride = 0;
    binaryParams.dstRepStride = N * sizeof(__cce_half) / ONE_BLK_SIZE;
    uint32_t repeatEle = ONE_REPEAT_BYTE_SIZE;
    uint32_t repeatTimes = src.GetSize() % repeatEle == 0 ? src.GetSize() / repeatEle : src.GetSize() / repeatEle + 1;

    SetMaskCount();
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE_FP16 * K);
    uint32_t loopN = N / ANTIQUANT_SINGLE_N_SIZE_FP16;
    for (uint32_t i = 0; i < loopN; i++) {
        uint32_t loopOffset = ANTIQUANT_SINGLE_N_SIZE_FP16 * i;

        Add<__cce_half, false>(dst[loopOffset], dst[loopOffset], offset[loopOffset], MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();

        Mul<__cce_half, false>(dst[loopOffset], dst[loopOffset], scale[loopOffset], MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
}

template <typename SrcType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTransposePerformanceTail(const LocalTensor<__cce_half> &dst,
    const LocalTensor<SrcType> &src, const LocalTensor<__cce_half> &offset, const LocalTensor<__cce_half> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N, const uint32_t mask)
{
    BinaryRepeatParams binaryParams;
    binaryParams.src0RepStride = N * sizeof(__cce_half) / ONE_BLK_SIZE;
    binaryParams.src1RepStride = 0;
    binaryParams.dstRepStride = N * sizeof(__cce_half) / ONE_BLK_SIZE;


    SetMaskNorm();
    SetVectorMask<__cce_half, MaskMode::NORMAL>(mask);

    Add<__cce_half, false>(dst, dst, offset, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<__cce_half, false>(dst, dst, scale, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();
    ResetMask();
}

template <typename SrcType>
[aicore] __inline__ __attribute__((always_inline)) void PreCast(const LocalTensor<__cce_half> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<__cce_half> &offset, const LocalTensor<__cce_half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K)
{
    UnaryRepeatParams s42f16unaryParams;
    s42f16unaryParams.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    uint32_t repeatEle = ONE_REPEAT_BYTE_SIZE;
    uint32_t repeatTimes = src.GetSize() % repeatEle == 0 ? src.GetSize() / repeatEle : src.GetSize() / repeatEle + 1;

    SetMaskCount();
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, src.GetSize());
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        Cast<__cce_half, int4b_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, s42f16unaryParams);
    } else {
        Cast<__cce_half, int8_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename SrcType, typename DstType>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantNoTransposeImplScalar(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount, const uint32_t K, const uint32_t N, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t groupCount = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
    uint32_t groupSize = K / groupCount;
    SetMaskCount();
    if constexpr (IsSameType<DstType, __cce_half>::value && IsSameType<SrcType, int8_t>::value) {
        SetVectorMask<__cce_half, MaskMode::COUNTER>(0, calCount);
        UnaryRepeatParams unaryParams;
        unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
        BinaryRepeatParams binaryParams;

        Cast<__cce_half, int8_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < groupSize; j++) {
            AntiQuantInnerLoopF16(dst[j * N], dst[j * N], offset, scale, sharedTmpBuffer, binaryParams, N);
        }
        return;
    }
    for (uint32_t j = 0; j < groupSize; j++) {
        AntiQuantOuterLoop<SrcType, DstType, true>(dst[j * N], src[j * N], offset, scale, sharedTmpBuffer, N);
    }
}

template <typename SrcType, typename DstType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTranspose(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount, const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t N = src.GetSize() / K;
    bool isPerformance = AntiQuantCheckPerformanceMode(scale, sharedTmpBuffer, K);
    if (isPerformance) {

        PreCast(dst, src, offset, scale, sharedTmpBuffer, K);
        uint32_t kTail = K % ANTIQUANT_MAX_K, loopK = K / ANTIQUANT_MAX_K, nTail;
        if constexpr (IsSameType<DstType, __cce_half>::value) {
            nTail = N % ANTIQUANT_SINGLE_N_SIZE_FP16;
        } else {
            nTail = N % ANTIQUANT_SINGLE_N_SIZE_BF16;
        }
        uint32_t NAlign = N - nTail;
        for (int i = 0; i < K / ANTIQUANT_MAX_K; i++) {
            uint32_t offsetSrc = i * ANTIQUANT_MAX_K * N;
            AscendAntiQuantNoTransposePerformance(dst[offsetSrc], src[offsetSrc], offset, scale, sharedTmpBuffer,
                ANTIQUANT_MAX_K, N);
            if (nTail > 0) {
                AscendAntiQuantNoTransposePerformanceTail(dst[offsetSrc + NAlign], src[offsetSrc + NAlign],
                    offset[NAlign], scale[NAlign], sharedTmpBuffer, ANTIQUANT_MAX_K, N, nTail);
            }
        }
        if (kTail > 0) {
            uint32_t offsetSrc = K / ANTIQUANT_MAX_K * ANTIQUANT_MAX_K * N;
            AscendAntiQuantNoTransposePerformance(dst[offsetSrc], src[offsetSrc], offset, scale, sharedTmpBuffer, kTail,
                N);
            if (nTail > 0) {
                AscendAntiQuantNoTransposePerformanceTail(dst[offsetSrc + NAlign], src[offsetSrc + NAlign],
                    offset[NAlign], scale[NAlign], sharedTmpBuffer, kTail, N, nTail);
            }
        }
        return;
    }
    AntiQuantNoTransposeImplScalar(dst, src, offset, scale, sharedTmpBuffer, calCount, K, N, shapeInfo);
}

template <typename SrcType, typename DstType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTranspose(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t groupCount = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
    uint32_t groupSize = K / groupCount;
    uint32_t N = (shapeInfo.scaleWidth == 0 ? scale.GetShapeInfo().shape[1] : shapeInfo.scaleWidth);

    SetMaskCount();
    for (uint32_t i = 0; i < groupCount; i++) {
        for (uint32_t j = 0; j < groupSize; j++) {

            AntiQuantOuterLoop<SrcType, DstType, false>(dst[(i * groupSize + j) * N], src[(i * groupSize + j) * N],
                scale, scale[i * N], sharedTmpBuffer, N);
        }
    }
    SetMaskNorm();
    ResetMask();
}

template <typename SrcType, typename DstType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTranspose(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    SetMaskCount();
    AntiQuantOuterLoop<SrcType, DstType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, calCount);
}

template <typename SrcType, typename DstType>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantImplScalar(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount, const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t N = src.GetSize() / K;
    uint32_t groupSize = K / (shapeInfo.offsetWidth == 0 ? offset.GetShapeInfo().shape[1] : shapeInfo.offsetWidth);
    uint32_t offsetLength = K / groupSize;
    SetMaskCount();
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < offsetLength; j++) {
            auto offsetValue = offset.GetValue(i * offsetLength + j);
            auto scaleValue = scale.GetValue(i * offsetLength + j);
            AntiQuantOuterLoop<SrcType, DstType, true>(dst[i * K + j * groupSize], src[i * K + j * groupSize],
                offsetValue, scaleValue, sharedTmpBuffer, groupSize);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename SrcType, typename DstType>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantImplScalar(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t N = src.GetSize() / K;
    uint32_t groupSize = K / (shapeInfo.scaleWidth == 0 ? scale.GetShapeInfo().shape[1] : shapeInfo.scaleWidth);
    uint32_t scaleLength = K / groupSize;

    SetMaskCount();
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < scaleLength; j++) {
            auto scaleValue = scale.GetValue(i * scaleLength + j);
            AntiQuantOuterLoop<SrcType, DstType, false>(dst[i * K + j * groupSize], src[i * K + j * groupSize],
                scaleValue, scaleValue, sharedTmpBuffer, groupSize);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename SrcType, typename DstType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantImplScalar(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    SetMaskCount();
    AntiQuantOuterLoop<SrcType, DstType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, calCount);
}

template <bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantFp16TransposeMainImpl(const LocalTensor<__cce_half> &dst, const LocalTensor<__cce_half> &src,
    const LocalTensor<__cce_half> &scale, const LocalTensor<__cce_half> &offset, const uint32_t srcN, const uint32_t K)
{
    SetMaskCount();



    uint32_t repStride = K * sizeof(__cce_half) / ONE_BLK_SIZE;
    BinaryRepeatParams binaryParams(1, 1, 0, repStride, repStride, 1);
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, srcN * B16_DATA_NUM_PER_REPEAT);
    const uint32_t loop = K / B16_DATA_NUM_PER_REPEAT;
    for (uint32_t i = 0; i < loop; ++i) {
        const uint32_t tmpOffset = i * B16_DATA_NUM_PER_REPEAT;
        if constexpr (withOffset) {
            Add<__cce_half, false>(dst[tmpOffset], src[tmpOffset], offset, MASK_PLACEHOLDER, srcN, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<__cce_half, false>(dst[tmpOffset], dst[tmpOffset], scale, MASK_PLACEHOLDER, srcN, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantFp16TransposeTailImpl(const LocalTensor<__cce_half> &dst, const LocalTensor<__cce_half> &src,
    const LocalTensor<__cce_half> &scale, const LocalTensor<__cce_half> &offset, const uint32_t srcN, const uint32_t K)
{
    SetMaskNorm();
    const uint32_t tailK = K % B16_DATA_NUM_PER_REPEAT;
    SetVectorMask<__cce_half, MaskMode::NORMAL>(tailK);



    const uint32_t repStride = K * sizeof(__cce_half) / ONE_BLK_SIZE;
    BinaryRepeatParams binaryParams(1, 1, 0, repStride, repStride, 1);
    const uint32_t loop = srcN / MAX_REPEAT_TIMES;
    for (uint32_t i = 0; i < loop; ++i) {
        const uint32_t srcOffset = MAX_REPEAT_TIMES * K * i;
        const uint32_t scaleOffset = MAX_REPEAT_TIMES * B16_DATA_NUM_PER_BLOCK * i;
        if constexpr (withOffset) {
            Add<__cce_half, false>(dst[srcOffset], dst[srcOffset], offset[scaleOffset], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<__cce_half, false>(dst[srcOffset], dst[srcOffset], scale[scaleOffset], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
            binaryParams);
        PipeBarrier<PIPE_V>();
    }
    const uint32_t tailN = srcN % MAX_REPEAT_TIMES;
    if (tailN != 0) {
        const uint32_t srcOffset = loop * MAX_REPEAT_TIMES * K;
        const uint32_t scaleOffset = loop * MAX_REPEAT_TIMES * B16_DATA_NUM_PER_BLOCK;
        if constexpr (withOffset) {
            Add<__cce_half, false>(dst[srcOffset], dst[srcOffset], offset[scaleOffset], MASK_PLACEHOLDER, tailN,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<__cce_half, false>(dst[srcOffset], dst[srcOffset], scale[scaleOffset], MASK_PLACEHOLDER, tailN, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}




template <typename SrcType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantFP16Transpose(const LocalTensor<__cce_half> &dst, const LocalTensor<SrcType> &src,
    LocalTensor<__cce_half> offset, const LocalTensor<__cce_half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t calCount = src.GetSize();
    uint32_t scaleN = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
    uint32_t scaleBrcbSize = ONE_BLK_SIZE / sizeof(__cce_half) * scaleN;
    uint32_t stackBufferSize = sharedTmpBuffer.GetSize() / sizeof(__cce_half);
    constexpr uint32_t tmpBufferCoeff = withOffset ? ANTIQUANT_TWO : 1;
    if (stackBufferSize < scaleBrcbSize * tmpBufferCoeff || K >= MAX_K_FOR_FP16_BRCB) {
        return withOffset ? AntiQuantImplScalar(dst, src, offset, scale, sharedTmpBuffer, calCount, K, shapeInfo) :
            AntiQuantImplScalar(dst, src, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    }


    SetMaskCount();
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, src.GetSize());
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        UnaryRepeatParams s42f16unaryParams;
        s42f16unaryParams.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
        Cast<__cce_half, SrcType, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, s42f16unaryParams);
    } else {
        UnaryRepeatParams unaryParams(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
        Cast<__cce_half, SrcType, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();

    LocalTensor<__cce_half> stackBuffer = sharedTmpBuffer.ReinterpretCast<__cce_half>();
    AntiquantParams<__cce_half> params;
    params.tempTensorScale = stackBuffer[0];
    if constexpr (withOffset) {
        params.tempTensorOffset = stackBuffer[B16_DATA_NUM_PER_BLOCK * scaleN];
    }
    AntiQuantFp16Brcb<withOffset>(scale, offset, params, scaleN);
    uint32_t srcN = src.GetSize() / K;
    if (K < B16_DATA_NUM_PER_REPEAT) {
        return AntiQuantFp16TransposeTailImpl<withOffset>(dst, dst, params.tempTensorScale, params.tempTensorOffset,
            srcN, K);
    }

    AntiQuantFp16TransposeMainImpl<withOffset>(dst, dst, params.tempTensorScale, params.tempTensorOffset, srcN, K);
    const uint32_t tailK = K % B16_DATA_NUM_PER_REPEAT;
    if (tailK != 0) {
        const uint32_t srcOffset = K - tailK;
        AntiQuantFp16TransposeTailImpl<withOffset>(dst[srcOffset], dst[srcOffset], params.tempTensorScale,
            params.tempTensorOffset, srcN, K);
    }
}


template <typename SrcType, typename DstType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    CheckApiDtypeValid<SrcType, DstType>();
    uint32_t calCount = src.GetSize();
    if constexpr (!isTranspose) {
        AscendAntiQuantNoTranspose(dst, src, scale, sharedTmpBuffer, src.GetSize(), K, shapeInfo);
    } else if constexpr (IsSameType<DstType, __cce_half>::value) {
        AscendAntiQuantFP16Transpose<SrcType, false>(dst, src, scale, scale, sharedTmpBuffer, K, shapeInfo);
    } else {
        AscendAntiQuantBF16Transpose(dst, src, scale, sharedTmpBuffer, K, shapeInfo);
    }
    SetMaskNorm();
    ResetMask();
}


template <typename SrcType, typename DstType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    CheckApiDtypeValid<SrcType, DstType>();
    if constexpr (!isTranspose) {
        AscendAntiQuantNoTranspose<SrcType, DstType, false>(dst, src, scale, scale, sharedTmpBuffer, src.GetSize(), K,
            shapeInfo);
    } else {
        AntiQuantImplScalar<SrcType, DstType, false>(dst, src, scale, scale, sharedTmpBuffer, src.GetSize(), K,
            shapeInfo);
    }
    SetMaskNorm();
    ResetMask();
}


template <typename SrcType, typename DstType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    CheckApiDtypeValid<SrcType, DstType>();
    if constexpr (!isTranspose) {
        AscendAntiQuantNoTranspose(dst, src, offset, scale, sharedTmpBuffer, src.GetSize(), K, shapeInfo);
    } else if constexpr (IsSameType<DstType, __cce_half>::value) {
        AscendAntiQuantFP16Transpose<SrcType, true>(dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
    } else {
        AscendAntiQuantBF16Transpose(dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename SrcType, typename DstType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                           ;
    AscendAntiQuantImpl<SrcType, DstType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, K);
}


template <typename SrcType, typename DstType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    CheckApiDtypeValid<SrcType, DstType>();
    if constexpr (!isTranspose) {
        AscendAntiQuantNoTranspose<SrcType, DstType, true>(
            dst, src, offset, scale, sharedTmpBuffer, src.GetSize(), K, shapeInfo);
    } else {
        AntiQuantImplScalar<SrcType, DstType, true>(
            dst, src, offset, scale, sharedTmpBuffer, src.GetSize(), K, shapeInfo);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename SrcType, typename DstType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                           ;
    AscendAntiQuantImpl<SrcType, DstType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, K);
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &offset, const LocalTensor<OutputDataType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, K,
        shapeInfo);
}
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, scale, sharedTmpBuffer, K, shapeInfo);
}
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &offset, const LocalTensor<OutputDataType> &scale, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, K, shapeInfo);
}
# 98 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const OutputDataType offset, const OutputDataType scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, K,
        shapeInfo);
}
# 120 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const OutputDataType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, scale, sharedTmpBuffer, K, shapeInfo);
}
# 141 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const OutputDataType offset, const OutputDataType scale, const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, K, shapeInfo);
}
#pragma end_pipe
}
# 71 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/logsoftmax.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/logsoftmax.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/logsoftmax_base_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/logsoftmax_base_impl.h"
namespace AscendC {
constexpr float SCALAR_NATURE_LOG_10 = 0.4342944819;
 [aicore] __inline__ __attribute__((always_inline)) void GenericLogNZImpl(LocalTensor<float>& dst, const LocalTensor<float>& src,
     const uint32_t originalSrcM, const uint32_t srcK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t blockStride = srcK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < blockStride; j++) {
            Ln<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK], MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK],
                static_cast<float>(SCALAR_NATURE_LOG_10), MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
        }
    } else {
        SetMaskCount();
        SetVectorMask<float>(0, srcK);
        for (int j = 0; j < originalSrcM; j++) {
            Ln<float, false>(dst[j * srcK], src[j * srcK], MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * srcK], src[j * srcK], static_cast<float>(SCALAR_NATURE_LOG_10),
                MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZImpl(LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    const UnaryRepeatParams unaryParams;
    const uint64_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint64_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    GenericLogNZImpl(dst, dst, reduceParam.originalSrcM, tiling.srcK);
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZReduceMaxImpl(const LocalTensor<float>& tmpBuffer0,
    const LocalTensor<float>& tmpBuffer1, const LocalTensor<__cce_half>& maxTensor, const uint32_t& offset2,
    const uint32_t& splitCount, uint64_t mask[2], const ReduceLastND& reduceParam)
{
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<__cce_half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZSubImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<float>& tmpBuffer1, const uint32_t& splitOffset,
    const uint32_t& lastSplitNZBlockOffset, uint64_t mask[2],
    const uint32_t& lastBlockMaskLen, const uint32_t& splitCount)
{
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZReduceSumImpl(const LocalTensor<float>& tmpBuffer0,
    const LocalTensor<float>& tmpBuffer1, const LocalTensor<__cce_half>& sumTensor, const uint32_t& offset2,
    const uint32_t& splitCount, uint64_t mask[2], const ReduceLastND& reduceParam)
{
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<__cce_half, float, false>(sumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZDivImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<float>& tmpBuffer1, const uint32_t& splitOffset,
    const uint32_t& lastSplitNZBlockOffset, uint64_t mask[2], const uint32_t& lastBlockMaskLen,
    const uint32_t& splitCount)
{
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Div<float>);
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZLogImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<__cce_half>& dst, const uint32_t& splitOffset,
    const uint32_t& splitCount, const SoftMaxTiling& tiling, const uint32_t& offset1)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Ln<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Muls<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j],
            static_cast<float>(SCALAR_NATURE_LOG_10), MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZExpImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<float>& tmpBuffer1, const uint32_t& splitOffset,
    const uint32_t& lastSplitNZBlockOffset, uint64_t mask[2], const uint32_t& lastBlockMaskLen,
    const uint32_t& splitCount)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& sumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint64_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint64_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint64_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, __cce_half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    LogSoftMaxGenericNZReduceMaxImpl(tmpBuffer0, tmpBuffer1, maxTensor, offset2, splitCount, mask, reduceParam);

    LogSoftMaxGenericNZSubImpl(splitNZBlockCount, tmpBuffer0, tmpBuffer1, splitOffset, lastSplitNZBlockOffset,
        mask, lastBlockMaskLen, splitCount);

    LogSoftMaxGenericNZExpImpl(splitNZBlockCount, tmpBuffer0, tmpBuffer1, splitOffset, lastSplitNZBlockOffset,
        mask, lastBlockMaskLen, splitCount);

    LogSoftMaxGenericNZReduceSumImpl(tmpBuffer0, tmpBuffer1, sumTensor, offset2, splitCount, mask, reduceParam);

    LogSoftMaxGenericNZDivImpl(splitNZBlockCount, tmpBuffer0, tmpBuffer1, splitOffset, lastSplitNZBlockOffset,
        mask, lastBlockMaskLen, splitCount);

    LogSoftMaxGenericNZLogImpl(splitNZBlockCount, tmpBuffer0, dst, splitCount, splitOffset, tiling, offset1);
}

[aicore] __inline__ __attribute__((always_inline)) bool LogSoftMaxTilingFunc(const uint32_t workLocalSize, const LastAxisShapeND& ndinfo,
    LogSoftMaxTiling& softmaxTiling, const uint32_t dataTypeSize1, const uint32_t dataTypeSize2,
    bool isDataFormatNZ = false)
{

                                                                                                 ;
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / dataTypeSize2;
    softmaxTiling.srcM = ndinfo.m;
    softmaxTiling.srcK = ndinfo.k;
    softmaxTiling.srcSize = ndinfo.m * ndinfo.k;
    softmaxTiling.outMaxM = ndinfo.m;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = ndinfo.m * elementNumPerBlk;
    if (isDataFormatNZ) {
        softmaxTiling.reduceM = workLocalSize / (SOFTMAX_SHAPE_NZ_BASIC_COUNT + ndinfo.k);
    } else {
        softmaxTiling.reduceM = CalculateNDSplitM(workLocalSize, dataTypeSize1, elementNumPerBlk, ndinfo);
    }

    if (softmaxTiling.reduceM < ndinfo.m && softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < ndinfo.m ? softmaxTiling.reduceM : ndinfo.m;
    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = ndinfo.k;
    softmaxTiling.splitSize = softmaxTiling.reduceM * ndinfo.k;

                                                                                              ;
    softmaxTiling.rangeM = ndinfo.m / softmaxTiling.reduceM;
    softmaxTiling.tailM = ndinfo.m % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * ndinfo.k;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

[aicore] __inline__ __attribute__((always_inline)) void GenericLogNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t originalSrcM, const uint32_t srcK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t blockStride = srcK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < blockStride; j++) {
            Ln<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK], MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK],
                static_cast<float>(SCALAR_NATURE_LOG_10), MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
        }
    } else {
        SetMaskCount();
        SetVectorMask<float>(0, srcK);
        for (int j = 0; j < originalSrcM; j++) {
            Ln<float, false>(dst[j * srcK], src[j * srcK], MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * srcK], src[j * srcK], static_cast<float>(SCALAR_NATURE_LOG_10), 1,
                MASK_PLACEHOLDER, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const LogSoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const UnaryRepeatParams unaryParams;

    NewReduceMaxLastNDImpl(maxTensor[offset2], src[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(dst[offset1], src[offset1], maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], dst[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(dst[offset1], dst[offset1], sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);
    PipeBarrier<PIPE_V>();

    GenericLogNDImpl(dst[offset1], dst[offset1], reduceParam.originalSrcM, tiling.srcK);
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNDImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& sumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const LogSoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer3 = workLocal[tiling.splitSize + tiling.reduceSize];
    const UnaryRepeatParams unaryParams;
    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);

    PipeBarrier<PIPE_V>();
    Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();

    NewReduceSumLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);
    PipeBarrier<PIPE_V>();

    Cast(sumTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();

    GenericLogNDImpl(tmpBuffer0, tmpBuffer0, reduceParam.originalSrcM, tiling.srcK);

    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNDImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const LogSoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const UnaryRepeatParams unaryParams;

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(maxTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);

    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();

    GenericLogNDImpl(tmpBuffer0, tmpBuffer0, reduceParam.originalSrcM, tiling.srcK);

    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxNZImpl(LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        LogSoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2,
            splitSize, reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxNZImpl(LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& sumTensor,
    const LocalTensor<__cce_half>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        LogSoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        LogSoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            tailReduceParam);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxNDImpl(const LocalTensor<__cce_half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const LogSoftMaxTiling& tiling)
{
    PipeBarrier<PIPE_V>();
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        LogSoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxNDImpl(LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const LogSoftMaxTiling& tiling)
{
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        LogSoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceSize, reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxImpl(LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LogSoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    SetMaskNorm();
    ResetMask();
    LocalTensor<float> tempBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tempBuffer.SetSize(sharedTmpBuffer.GetSize() / B32_BYTE_SIZE);
    ShapeInfo srcShape = src.GetShapeInfo();
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);




      ;
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }

    if constexpr (isDataFormatNZ) {
        if (srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM) {
            SoftMaxTiling newTiling;
            SoftMaxTilingFunc(tempBuffer.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T), sizeof(T), isDataFormatNZ);
            LogSoftMaxNZImpl(dst, sumTensor, maxTensor, src, tempBuffer, originalSrcShape, newTiling);
        }
    } else {

        if (srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM) {
            LogSoftMaxTiling newTiling = tiling;
            LogSoftMaxTilingFunc(tempBuffer.GetSize(), srcNDinfo, newTiling, sizeof(T), sizeof(T), isDataFormatNZ);
            LogSoftMaxNDImpl(dst, sumTensor, maxTensor, src, tempBuffer, originalSrcShape,
                newTiling);
        } else {
            LogSoftMaxNDImpl(dst, sumTensor, maxTensor, src, tempBuffer, originalSrcShape,
                tiling);
        }
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/logsoftmax.h" 2
#pragma begin_pipe(V)

namespace AscendC {
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/logsoftmax.h"
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void LogSoftMax(LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LogSoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                    ;
    LogSoftMaxImpl<T, isReuseSource, isDataFormatNZ>(dst, sumTensor, maxTensor, src, sharedTmpBuffer,
        tiling, softmaxShapeInfo);
                                   ;
}
}

#pragma end_pipe
# 72 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flash_base_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flash_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flash_base_impl/softmax_flash_nd_process_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flash_base_impl/softmax_flash_nd_process_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flash_base_impl/softmax_flash_basic_block_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flash_base_impl/softmax_flash_basic_block_impl.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashBasicBlock(const LocalTensor<T> &dst, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &src, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LocalTensor<float> &workLocal,
    const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / B16_BYTE_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        PipeBarrier<PIPE_V>();

        Cast<float, __cce_half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_BLK_NUM, HALF_DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_BLK_NUM, offset, offset });
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * i], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM, offset });
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1[halfSplitSize], tmpBuffer3, splitCeilM, { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        Brcb(tmpBuffer1[halfSplitSize + DEFAULT_BLK_NUM], tmpBuffer3, splitCeilM,
            { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i],
                tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM),
                { 1, 1, 0, offset, offset, B16_BYTE_SIZE });
        }
        PipeBarrier<PIPE_V>();

        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * i], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
        PipeBarrier<PIPE_V>();

        BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        Brcb(tmpBuffer1[DEFAULT_BLK_NUM], tmpBuffer3, splitCeilM, { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 2 });
        }
        PipeBarrier<PIPE_V>();
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Cast<float, __cce_half, false>(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Max<float, false>(tmpBuffer2, inMaxTmp, tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Cast<__cce_half, float, false>(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
        PipeBarrier<PIPE_V>();
        Sub<float, false>(tmpBuffer3, tmpBuffer1[halfSplitSize], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer3, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });
        Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });

        Cast<float, __cce_half, false>(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_BLK_NUM, HALF_DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        Mul<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Div<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
        Cast<__cce_half, float, false>(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
        Div<float, false>(tmpBuffer3, tmpBuffer3, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Mul<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.reduceM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
    }
}


[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashBasicBlockFloat(const LocalTensor<float> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<float> &src, const LocalTensor<float> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer1 = workLocal[0];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &inSumTmp = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfRepeatNum = DEFAULT_REPEAT_STRIDE / HALF_FACTOR;
    const uint32_t halfSplitSize = tiling.splitSize / HALF_FACTOR;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        __attribute__((cce_unif_buff)) float *tmpBufferAddr0 = (__attribute__((cce_unif_buff)) float *)src[offset1].GetPhyAddr();
        SetMaskNorm();
        ResetMask();
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            Copy<float, false>(tmpBuffer1, src[offset1], MASK_PLACEHOLDER, repeatTimes,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        } else {
            Max<float, false>(tmpBuffer1, src[offset1], src[offset1 + FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
            for (uint32_t j = 2; j < splitBlock; ++j) {
                PipeBarrier<PIPE_V>();
                Max<float, false>(tmpBuffer1, tmpBuffer1, src[offset1 + FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                    (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
            }
        }

        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer2[halfSplitSize], tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                tmpBuffer2[halfSplitSize], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(src[offset1], src[offset1], MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            Copy<float, false>(tmpBuffer1, src[offset1], MASK_PLACEHOLDER, repeatTimes,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        } else {
            Add<float, false>(tmpBuffer1, src[offset1], src[offset1 + FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
            for (uint32_t j = 2; j < splitBlock; ++j) {
                PipeBarrier<PIPE_V>();
                Add<float, false>(tmpBuffer1, tmpBuffer1, src[offset1 + FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                    (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
            }
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Copy<float, false>(inMaxTmp, inMaxTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer2, inMaxTmp, tmpBuffer2[halfSplitSize], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Copy<float, false>(maxTensor[offset2], tmpBuffer2, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Sub<float, false>(tmpBuffer3, tmpBuffer2[halfSplitSize], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer3, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        Copy<float, false>(inSumTmp, inSumTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        Mul<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Div<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();


        Copy<float, false>(expMaxTensor[offset2], inMaxTmp, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        Copy<float, false>(sumTensor[offset2], inSumTmp, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        Div<float, false>(tmpBuffer3, tmpBuffer3, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();

        ResetMask();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Mul<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.reduceM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
    }
}



[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashBasicBlock(const LocalTensor<__cce_half> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<__cce_half> &src, const LocalTensor<__cce_half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfRepeatNum = DEFAULT_REPEAT_STRIDE / B16_BYTE_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / B16_BYTE_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        PipeBarrier<PIPE_V>();
        Cast<float, __cce_half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, halfRepeatNum });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
        for (uint32_t j = 2; j < splitBlock; ++j) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1[halfSplitSize], tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j],
                tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
        for (uint32_t j = 2; j < splitBlock; ++j) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Copy<float, false>(inMaxTmp, inMaxTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer2, inMaxTmp, tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Copy<float, false>(maxTensor[offset2], tmpBuffer2, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Sub<float, false>(tmpBuffer3, tmpBuffer1[halfSplitSize], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer3, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        Copy<float, false>(inSumTmp, inSumTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        Mul<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Div<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();


        Copy<float, false>(tmpBuffer1, inMaxTmp, MASK_PLACEHOLDER, B16_BYTE_SIZE, { B16_BYTE_SIZE, 1, 1, 0 });
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize * B16_BYTE_SIZE);
        Cast<__cce_half, float, false>(expMaxTensor[offset2 * HALF_FACTOR], tmpBuffer1, FLOAT2HALF_ROUND_MODE,
            MASK_PLACEHOLDER, reduceCeilValue, { 1, 1, halfRepeatNum, DEFAULT_REPEAT_STRIDE });
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Copy<float, false>(sumTensor[offset2], inSumTmp, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        Div<float, false>(tmpBuffer3, tmpBuffer3, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();

        ResetMask();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Mul<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.reduceM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, halfRepeatNum, DEFAULT_REPEAT_STRIDE });
    }
}


}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flash_base_impl/softmax_flash_nd_process_impl.h" 2

namespace AscendC {

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashNDImpl(const LocalTensor<T> &dst, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &src, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LastAxisShapeND &originalSrcShape,
    const SoftMaxTiling &tiling)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    uint32_t workLocalSize = workLocal.GetSize();

    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &tmpBuffer4 =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp = workLocal[0];

    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    BroadCastLastND brcParam = { tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;


    if constexpr (isBasicBlock) {
        SoftmaxFlashBasicBlock<T>(dst, sumTensor, maxTensor, src, expMaxTensor, inSumTensor, inMaxTensor, workLocal,
            tiling);
    } else

    {
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset2 = i * tiling.reduceSize;
            offset1 = i * tiling.splitSize;
            Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer1, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceParam);
            PipeBarrier<PIPE_V>();

            Cast(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer4, tmpBuffer4, tiling.reduceSize);

            Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(inMaxTmp, inMaxTmp, tiling.reduceSize);

            Cast(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Cast(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);
            Cast(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);

            Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.splitSize);
        }
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset2 = tiling.rangeM * tiling.reduceSize;
        offset1 = tiling.rangeM * tiling.splitSize;

        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceParam);
        PipeBarrier<PIPE_V>();

        Cast(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.tailReduceSize);

        Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.tailReduceSize);
        Cast(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Cast(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);
        Cast(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);

        Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();
        Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.tailSplitSize);
    }
}


[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashNDImpl(const LocalTensor<float> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<float> &src, const LocalTensor<float> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const LastAxisShapeND &originalSrcShape, const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &tmpBuffer4 =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp = workLocal[0];

    const ReduceLastND reduceMainParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                                           tiling.splitK, tiling.reduceM, tiling.reduceK };
    const ReduceLastND reduceTailParam = { tiling.tailM, originalSrcShape.k, tiling.tailM,
                                           tiling.splitK, tiling.tailM, tiling.reduceK };
    const BroadCastLastND mainBrcParam = { tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK };
    const BroadCastLastND tailBrcParam = { tiling.tailM, tiling.splitK, tiling.tailM, tiling.reduceK };

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, src[offset1], reduceSumBuffer, reduceMainParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, mainBrcParam);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer1, src[offset1], tmpBuffer1, tiling.splitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.splitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceMainParam);
        PipeBarrier<PIPE_V>();

        DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        DataCopy(maxTensor[offset2], tmpBuffer2, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.reduceSize);

        Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.reduceSize);

        DataCopy(inSumTmp, inSumTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Add(sumTensor[offset2], inMaxTmp, reduceSumBuffer, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Div(expMaxTensor[offset2], inMaxTmp, sumTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        DataCopy(sumTensor[offset2], sumTensor[offset2], tiling.reduceSize);

        Div(tmpBuffer4, tmpBuffer4, sumTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, mainBrcParam);
        PipeBarrier<PIPE_V>();
        Mul(dst[offset1], tmpBuffer1, tmpBuffer0, tiling.splitSize);
    }

    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset2 = tiling.rangeM * tiling.reduceSize;
        offset1 = tiling.rangeM * tiling.splitSize;

        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, src[offset1], reduceSumBuffer, reduceTailParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, tailBrcParam);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer1, src[offset1], tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceTailParam);
        PipeBarrier<PIPE_V>();

        DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        DataCopy(maxTensor[offset2], tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.tailReduceSize);

        Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.tailReduceSize);
        DataCopy(inSumTmp, inSumTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Add(sumTensor[offset2], inMaxTmp, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Div(expMaxTensor[offset2], inMaxTmp, sumTensor[offset2], tiling.tailReduceSize);

        Div(tmpBuffer4, tmpBuffer4, sumTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, tailBrcParam);
        PipeBarrier<PIPE_V>();
        Mul(dst[offset1], tmpBuffer1, tmpBuffer0, tiling.tailSplitSize);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashPostProcess(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LocalTensor<float> &workLocal,
    const LastAxisShapeND &originalSrcShape, const SoftMaxTiling &tiling, bool isUpdate = false,
    const SoftMaxShapeInfo &softmaxShapeInfo = {})
{
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t workLocalSize = workLocal.GetSize();
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        if (!isUpdate) {
            SoftMaxNDImpl<T, T>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape, tiling);
        } else {
            SoftmaxFlashNDImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, originalSrcShape, tiling);
        }
    } else {
        if (!isUpdate) {
            SoftMaxNDImpl<T, T>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape, tiling);
        } else {

            if constexpr (isBasicBlock) {
                SoftmaxFlashBasicBlockFloat(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                    inMaxTensor, workLocal, tiling);
            } else

            {
                SoftmaxFlashNDImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
                    workLocal, originalSrcShape, tiling);
            }
        }
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashNDImpl(const LocalTensor<__cce_half> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<__cce_half> &src, const LocalTensor<__cce_half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const LastAxisShapeND &originalSrcShape, const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &inMaxTmp = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &tmpBuffer4 =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];

    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    BroadCastLastND brcParam = { tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;


    if constexpr (isBasicBlock) {
        SoftmaxFlashBasicBlock(dst, sumTensor, maxTensor, src, expMaxTensor, inSumTensor, inMaxTensor, workLocal,
            tiling);
    } else

    {
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset2 = i * tiling.reduceSize;
            offset1 = i * tiling.splitSize;
            Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer1, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, inMaxTmp, reduceParam);
            PipeBarrier<PIPE_V>();

            DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Max(maxTensor[offset2], inMaxTmp, tmpBuffer4, tiling.reduceSize);
            PipeBarrier<PIPE_V>();

            Sub(tmpBuffer4, tmpBuffer4, maxTensor[offset2], tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer4, tmpBuffer4, tiling.reduceSize);

            Sub(inMaxTmp, inMaxTmp, maxTensor[offset2], tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(inMaxTmp, inMaxTmp, tiling.reduceSize);

            DataCopy(inSumTmp, inSumTensor[offset2], tiling.reduceSize);

            PipeBarrier<PIPE_V>();
            Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            DataCopy(sumTensor[offset2], inSumTmp, tiling.reduceSize);


            BroadCastLastImpl(tmpBuffer0, inMaxTmp,
                { tiling.reduceM, HALF_NUM_PER_BLK, tiling.reduceM, tiling.reduceK });
            PipeBarrier<PIPE_V>();
            Cast(expMaxTensor[offset2 * HALF_FACTOR], tmpBuffer0, FLOAT2HALF_ROUND_MODE,
                tiling.reduceSize * HALF_FACTOR);

            Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.splitSize);
        }
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset2 = tiling.rangeM * tiling.reduceSize;
        offset1 = tiling.rangeM * tiling.splitSize;

        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, inMaxTmp, reduceParam);
        PipeBarrier<PIPE_V>();

        DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Max(maxTensor[offset2], inMaxTmp, tmpBuffer4, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer4, tmpBuffer4, maxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.tailReduceSize);

        Sub(inMaxTmp, inMaxTmp, maxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.tailReduceSize);
        DataCopy(inSumTmp, inSumTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();


        BroadCastLastImpl(tmpBuffer0, inMaxTmp,
            { tiling.reduceM, FLOAT_NUM_PER_BLK * B16_BYTE_SIZE, tiling.reduceM, tiling.reduceK });
        PipeBarrier<PIPE_V>();
        Cast(expMaxTensor[offset2 * HALF_FACTOR], tmpBuffer0, FLOAT2HALF_ROUND_MODE,
            tiling.tailReduceSize * B16_BYTE_SIZE);
        DataCopy(sumTensor[offset2], inSumTmp, tiling.tailReduceSize);

        Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();
        Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.tailSplitSize);
    }
}

}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flash_base_impl.h" 2

namespace AscendC {
template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const SoftMaxTiling &tiling,
    bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    const uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    uint32_t workLocalSize = workLocal.GetSize();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
        SoftMaxTiling new_tiling = tiling;
        SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, new_tiling, elementNumPerBlk, isUpdate, isBasicBlock);
        SoftmaxFlashPostProcess<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
            inMaxTensor, workLocal, originalSrcShape, new_tiling, isUpdate);
    } else {
        SoftmaxFlashPostProcess<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
            inMaxTensor, workLocal, originalSrcShape, tiling, isUpdate);
    }
}
template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashImpl(const LocalTensor<__cce_half> &dstTensor, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<__cce_half> &srcTensor, const LocalTensor<__cce_half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const SoftMaxTiling &tiling,
    bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    uint32_t workLocalSize = workLocal.GetSize();

    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if (srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM) {
        SoftMaxTiling newTiling = tiling;
        SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, newTiling, FLOAT_NUM_PER_BLK, isUpdate, isBasicBlock);
        if (!isUpdate) {
            SoftMaxNDImpl<__cce_half, float>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape,
                newTiling);
        } else {
            SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, workLocal, originalSrcShape, newTiling);
        }
    } else {
        if (!isUpdate) {
            SoftMaxNDImpl<__cce_half, float>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape, tiling);
        } else {
            SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, workLocal, originalSrcShape, tiling);
        }
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const SoftMaxTiling &tiling, bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{
    auto tempBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    const uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t workLocalSize = tempBuffer.GetSize();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
        SoftMaxTiling newTiling = tiling;
        SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, newTiling, elementNumPerBlk, isUpdate, isBasicBlock);
        SoftmaxFlashPostProcess<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
            inMaxTensor, tempBuffer, originalSrcShape, newTiling, isUpdate);
    } else {
        SoftmaxFlashPostProcess<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
            inMaxTensor, tempBuffer, originalSrcShape, tiling, isUpdate);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashImpl(const LocalTensor<__cce_half> &dstTensor, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<__cce_half> &srcTensor, const LocalTensor<__cce_half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SoftMaxTiling &tiling, bool isUpdate,
    const SoftMaxShapeInfo &softmaxShapeInfo)
{
    auto tempBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    uint32_t workLocalSize = tempBuffer.GetSize();
    if (srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM) {
        SoftMaxTiling newTiling = tiling;
        SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, newTiling, FLOAT_NUM_PER_BLK, isUpdate, isBasicBlock);
        if (!isUpdate) {
            SoftMaxNDImpl<__cce_half, float>(dstTensor, sumTensor, maxTensor, srcTensor, tempBuffer, originalSrcShape,
                newTiling);
        } else {
            SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, tempBuffer, originalSrcShape, newTiling);
        }
    } else {
        if (!isUpdate) {
            SoftMaxNDImpl<__cce_half, float>(dstTensor, sumTensor, maxTensor, srcTensor, tempBuffer, originalSrcShape,
                tiling);
        } else {
            SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, tempBuffer, originalSrcShape, tiling);
        }
    }
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlash(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const SoftMaxTiling &tiling,
    bool isUpdate = false, const SoftMaxShapeInfo &softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
# 89 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlash(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcTensor, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const SoftMaxTiling& tiling,
    bool isUpdate = false, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
# 130 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlash(const LocalTensor<T>& dstTensor, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<T>& expMaxTensor,
    const LocalTensor<T>& inSumTensor, const LocalTensor<T>& inMaxTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, bool isUpdate = false, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, sharedTmpBuffer, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
# 170 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlash(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<__cce_half>& srcTensor, const LocalTensor<__cce_half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling, bool isUpdate = false,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, sharedTmpBuffer, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
}
#pragma end_pipe
# 73 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_v220_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_2nd012.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_2nd012.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_2nz012.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_2nz012.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_0213.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_0213.h"
namespace AscendC {
const uint32_t CUBE_HALF_SIZE = CUBE_MAX_SIZE / 2;

template <typename T> struct ConfusionTranspose0213Params {
    [aicore] ConfusionTranspose0213Params(){};

    int32_t i;
    int32_t j;
    int32_t k;
    int32_t m;

    TransposeType transposeType;

    TransDataTo5HDParams mainTransDataParams;
    TransDataTo5HDParams transDataParams1;
    TransDataTo5HDParams tailTransDataParams;
    TransDataTo5HDParams transDataParams2;


    uint64_t mainDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t mainSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t tailDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t tailSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConfusionTranspose0213TransParams(ConfusionTranspose0213Tiling& tiling,
    ConfusionTranspose0213Params<T>& params, TransposeType transposeTypeIn)
{
    params.transposeType = transposeTypeIn;

    params.mainTransDataParams.repeatTimes = tiling.newPopSize / CUBE_MAX_SIZE;
    params.mainTransDataParams.dstRepStride = BLOCK_CUBE / tiling.blockSize;
    params.mainTransDataParams.srcRepStride = CUBE_MAX_SIZE / tiling.blockSize;
    if (params.mainTransDataParams.repeatTimes == 1) {
        params.mainTransDataParams.dstRepStride = 0;
        params.mainTransDataParams.srcRepStride = 0;
    }

    params.transDataParams1.repeatTimes = tiling.newPopSize / CUBE_MAX_SIZE;
    params.transDataParams1.dstRepStride = tiling.alignA3MulA1 * BLOCK_CUBE / tiling.blockSize;
    params.transDataParams1.srcRepStride = BLOCK_CUBE / tiling.blockSize;
    if (params.transDataParams1.repeatTimes == 1) {
        params.transDataParams1.dstRepStride = 0;
        params.transDataParams1.srcRepStride = 0;
    }

    params.tailTransDataParams.repeatTimes = tiling.tailSize / CUBE_MAX_SIZE;
    params.tailTransDataParams.dstRepStride = BLOCK_CUBE / tiling.blockSize;
    params.tailTransDataParams.srcRepStride = CUBE_MAX_SIZE / tiling.blockSize;
    if (params.tailTransDataParams.repeatTimes == 1) {
        params.tailTransDataParams.dstRepStride = 0;
        params.tailTransDataParams.srcRepStride = 0;
    }

    params.transDataParams2.repeatTimes = tiling.tailSize / CUBE_MAX_SIZE;
    params.transDataParams2.dstRepStride = tiling.alignA3MulA1 * BLOCK_CUBE / tiling.blockSize;
    params.transDataParams2.srcRepStride = BLOCK_CUBE / tiling.blockSize;
    if (params.transDataParams2.repeatTimes == 1) {
        params.transDataParams2.dstRepStride = 0;
        params.transDataParams2.srcRepStride = 0;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213MainHalf(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{

    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.mainDstLocalList[n] = (uint64_t)tmp1[(tiling.newPopH * n)].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.mainSrcLocalList[n] = (uint64_t)srcTensor[(tiling.newPopSize * params.m + params.j * tiling.needSize +
            params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)]
            .GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.mainDstLocalList, params.mainSrcLocalList, params.mainTransDataParams);

    if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList1[n] =
                (uint64_t)dstTensor[(tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * BLOCK_CUBE +
                params.i * tiling.alignA3 + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList1[n] =
                (uint64_t)
                    dstTensor[(tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * tiling.shapeA1BlockCube +
                params.i * BLOCK_CUBE + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList1[n] = (uint64_t)tmp1[(tiling.newPopH * n)].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213MainFloat(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{
    for (int16_t p = 0; p < 2; p++) {

        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
            params.mainDstLocalList[n] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2))].GetPhyAddr();
            params.mainDstLocalList[n + 1] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2)) + tiling.blockSize]
                .GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.mainSrcLocalList[n] = (uint64_t)
                srcTensor[(p * tiling.blockSize + tiling.newPopSize * params.m + params.j * tiling.needSize +
                params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.mainDstLocalList, params.mainSrcLocalList, params.mainTransDataParams);
    }
    for (int16_t p = 0; p < 2; p++) {

        if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList1[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * BLOCK_CUBE +
                    params.i * tiling.alignA3 + tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)]
                    .GetPhyAddr();
                params.dstLocalList1[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * BLOCK_CUBE +
                    params.i * tiling.alignA3 + tiling.alignA3MulA1 * (n / 2) + tiling.blockSize +
                    params.k * tiling.batchOffset)].GetPhyAddr();
            }
        } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList1[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * tiling.shapeA1BlockCube +
                    params.i * BLOCK_CUBE + tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)]
                    .GetPhyAddr();
                params.dstLocalList1[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * tiling.shapeA1BlockCube +
                    params.i * BLOCK_CUBE + tiling.alignA3MulA1 * (n / 2) + tiling.blockSize +
                    params.k * tiling.batchOffset)].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList1[n] = (uint64_t)tmp1[(p * tiling.blockSize + tiling.newPopH * n)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213TailHalf(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{

    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.tailDstLocalList[n] = (uint64_t)tmp1[tiling.newPopH * n].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.tailSrcLocalList[n] =
            (uint64_t)srcTensor[(tiling.newPopSize * tiling.mainBlocks + params.j * tiling.needSize +
            params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)]
            .GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.tailDstLocalList, params.tailSrcLocalList, params.tailTransDataParams);

    if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[(tiling.mainOffset + params.j * BLOCK_CUBE +
                params.i * tiling.alignA3 + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[(tiling.mainOffset + params.j * tiling.shapeA1BlockCube +
                params.i * BLOCK_CUBE + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList2[n] = (uint64_t)tmp1[(tiling.newPopH * n)].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213TailFloat(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{
    for (int16_t p = 0; p < 2; p++) {

        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
            params.tailDstLocalList[n] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2))].GetPhyAddr();
            params.tailDstLocalList[n + 1] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2)) + tiling.blockSize]
                .GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.tailSrcLocalList[n] = (uint64_t)srcTensor[(p * tiling.blockSize +
                tiling.newPopSize * tiling.mainBlocks + params.j * tiling.needSize +
                params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.tailDstLocalList, params.tailSrcLocalList, params.tailTransDataParams);
    }
    for (int16_t p = 0; p < 2; p++) {

        if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * BLOCK_CUBE + params.i * tiling.alignA3 +
                    tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)].GetPhyAddr();
                params.dstLocalList2[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * BLOCK_CUBE + params.i * tiling.alignA3 +
                    tiling.alignA3MulA1 * (n / 2) + tiling.blockSize + params.k * tiling.batchOffset)].GetPhyAddr();
            }
        } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * tiling.shapeA1BlockCube + params.i * BLOCK_CUBE +
                    tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)].GetPhyAddr();
                params.dstLocalList2[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * tiling.shapeA1BlockCube + params.i * BLOCK_CUBE +
                    tiling.alignA3MulA1 * (n / 2) + tiling.blockSize + params.k * tiling.batchOffset)].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList2[n] = (uint64_t)tmp1[(p * tiling.blockSize + tiling.newPopH * n)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
    }
}

}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_2nz012.h" 2

namespace AscendC {
template <typename T> struct ConfusionTranspose2NZ012NParams {
    [aicore] ConfusionTranspose2NZ012NParams(){};

    int32_t i;
    int32_t j;
    int32_t k;

    uint32_t tmp1RemainRowCount;
    uint32_t tmp2Count;
    uint32_t tmp2NeedRowCount;
    uint32_t transdataRepeat;
    uint32_t dstPrehnCount;
    uint32_t dstAllCount;

    TransposeType transposeType;


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];

    uint64_t dstLocalList3[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList3[NCHW_CONV_ADDR_LIST_SIZE];

    TransDataTo5HDParams transDataParams1;
    TransDataTo5HDParams transDataParams2;
    TransDataTo5HDParams transDataParams3;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConfusionTranspose2NZ012N(ConfusionTranspose2NZ012NParams<T> &params,
    ConfusionTranspose2NZ012NTiling &tiling)
{

    if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }

    params.tmp1RemainRowCount = 0;
    params.tmp2Count = 0;
    params.tmp2NeedRowCount = BLOCK_CUBE;
    params.transdataRepeat = 0;
    params.dstPrehnCount = 0;
    params.dstAllCount = 0;

    params.transDataParams1.repeatTimes = 1;
    params.transDataParams1.dstRepStride = 0;
    params.transDataParams1.srcRepStride = 0;

    params.transDataParams2.repeatTimes = 1;
    params.transDataParams2.dstRepStride = 0;
    params.transDataParams2.srcRepStride = 0;

    params.transDataParams3.repeatTimes = 1;
    params.transDataParams3.dstRepStride = 0;
    params.transDataParams3.srcRepStride = 0;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NTmp1RemainRowCountZero(const LocalTensor<T>& srcTensor,
    ConfusionTranspose2NZ012NTiling& tiling, ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(__cce_half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList1[n] = (uint64_t)tmp1[BLOCK_CUBE * n].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList1[n] = (uint64_t)srcTensor[params.i * tiling.alignsBlockCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                    .GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList1[n] = (uint64_t)tmp1[m * CUBE_HALF_SIZE + tiling.blockSize * n].GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList1[n] =
                        (uint64_t)srcTensor[params.i * tiling.alignsBlockCube + params.j * CUBE_MAX_SIZE +
                        m * tiling.blockSize + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                        .GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
            }
        }

        params.tmp1RemainRowCount = BLOCK_CUBE;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NTmp2RemainRowCountFirst(ConfusionTranspose2NZ012NTiling& tiling,
    ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{


    if (((params.i * BLOCK_CUBE) <= tiling.hnDiv) && (((params.i + 1) * BLOCK_CUBE) > tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE - tiling.gap;
    }

    if (params.tmp2NeedRowCount <= params.tmp1RemainRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams1.dstStride = 0;
            dataCopyParams1.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);

            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
            if (params.dstPrehnCount == tiling.hnDiv) {
                params.dstPrehnCount = 0;
            }
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NTmp2RemainRowCountZero(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2NZ012NTiling& tiling, ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(__cce_half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList2[n] =
                    (uint64_t)
                        dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsBlockCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList2[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList2[n] =
                        (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                        tiling.alignsBlockCube +
                        params.j * CUBE_MAX_SIZE + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                        params.k * tiling.dstBatchOffset]
                        .GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList2[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
            }
        }
        params.transdataRepeat += 1;



        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {


            params.tmp1RemainRowCount = 0;
        }

        params.tmp2Count = 0;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NCalcTmp2NeedRowCount(ConfusionTranspose2NZ012NTiling& tiling,
    ConfusionTranspose2NZ012NParams<T>& params)
{

    if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE;
    } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
        params.tmp2NeedRowCount =
            (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ? BLOCK_CUBE : (tiling.hnDiv - params.dstPrehnCount);
    } else if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NTmp2NeedRowCount(ConfusionTranspose2NZ012NTiling& tiling,
    ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{

    if (params.tmp1RemainRowCount >= params.tmp2NeedRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count = params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;


            if (params.dstPrehnCount == tiling.hnDiv) {
                params.dstPrehnCount = 0;
            }
        }
    } else {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp1RemainRowCount;
            params.dstAllCount += params.tmp1RemainRowCount;
            params.tmp2Count = params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
            if (params.dstPrehnCount == tiling.hnDiv) {
                params.dstPrehnCount = 0;
            }
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NTmp2NeedRowCountZero(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2NZ012NTiling& tiling, ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(__cce_half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList3[n] =
                    (uint64_t)
                        dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsBlockCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList3[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList3[n] =
                        (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                        tiling.alignsBlockCube +
                        params.j * CUBE_MAX_SIZE + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                        params.k * tiling.dstBatchOffset].GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList3[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
            }
        }
        params.transdataRepeat += 1;
        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {
            params.tmp1RemainRowCount = 0;
        }

        params.tmp2Count = 0;

        if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
            params.tmp2NeedRowCount = BLOCK_CUBE;
        } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
            params.tmp2NeedRowCount = (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ?
                BLOCK_CUBE : (tiling.hnDiv - params.dstPrehnCount);
        } else if (tiling.hnDiv < BLOCK_CUBE) {
            params.tmp2NeedRowCount = tiling.hnDiv;
        }
    }
}

}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_2nd012.h" 2

namespace AscendC {
template <typename T> struct ConfusionTranspose2ND012NParams {
    [aicore] ConfusionTranspose2ND012NParams(){};

    int32_t i;
    int32_t j;
    int32_t k;

    uint32_t tmp1RemainRowCount;
    uint32_t tmp2NeedRowCount;
    uint32_t transdataRepeat;
    uint32_t tmp2Count;
    uint32_t dstPrehnCount;
    uint32_t dstAllCount;
    uint32_t dstPrehnCountBefore;
    uint32_t PrehnCount;

    TransposeType transposeType;


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams1;


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams2;


    uint64_t dstLocalList3[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList3[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams3;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConfusionTranspose2ND012N(ConfusionTranspose2ND012NParams<T> &params,
    ConfusionTranspose2ND012NTiling &tiling)
{
    params.tmp1RemainRowCount = 0;
    params.tmp2NeedRowCount = BLOCK_CUBE;
    params.transdataRepeat = 0;
    params.tmp2Count = 0;
    params.dstPrehnCount = 0;

    params.dstAllCount = 0;
    params.dstPrehnCountBefore = 0;
    params.PrehnCount = 0;

    params.transDataParams1.repeatTimes = 1;
    params.transDataParams1.dstRepStride = 0;
    params.transDataParams1.srcRepStride = 0;

    params.transDataParams2.repeatTimes = 1;
    params.transDataParams2.dstRepStride = 0;
    params.transDataParams2.srcRepStride = 0;

    params.transDataParams3.repeatTimes = 1;
    params.transDataParams3.dstRepStride = 0;
    params.transDataParams3.srcRepStride = 0;

    if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp1RemainRowCount(const LocalTensor<T>& srcTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(__cce_half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList1[n] = (uint64_t)tmp1[BLOCK_CUBE * n].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList1[n] = (uint64_t)srcTensor[params.i * tiling.alignsCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                    .GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList1[n] = (uint64_t)tmp1[m * CUBE_HALF_SIZE + tiling.blockSize * n].GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList1[n] =
                        (uint64_t)srcTensor[params.i * tiling.alignsCube + params.j * CUBE_MAX_SIZE +
                        m * tiling.blockSize + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                        .GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
            }
        }

        params.tmp1RemainRowCount = BLOCK_CUBE;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp1RemainRowCountFirst(ConfusionTranspose2ND012NTiling& tiling,
    ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{


    if (((params.i * BLOCK_CUBE) <= tiling.hnDiv) && (((params.i + 1) * BLOCK_CUBE) > tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE - tiling.gap;
    }

    if (params.tmp2NeedRowCount <= params.tmp1RemainRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams1.dstStride = 0;
            dataCopyParams1.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);

            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp2NeedRowCountHalf(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    if (tiling.hnDiv <= BLOCK_CUBE) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] =
                (uint64_t)
                    dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsMulAlignHnDiv +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    } else {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] =
                (uint64_t)
                    dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                tiling.alignsMulAlignHnDiv +
                ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList2[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp2NeedRowCountFloat(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    for (uint16_t m = 0; m < 2; m++) {
        if (tiling.hnDiv <= BLOCK_CUBE) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList2[n] =
                    (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    params.j * tiling.alignHnDivCube + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
        } else {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
                params.dstLocalList2[n + 1] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    tiling.blockSize + params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList2[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp2NeedRowCount(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(__cce_half)) {
            ConfusionTranspose2ND012NTmp2NeedRowCountHalf(dstTensor, tiling, params, tmp2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            ConfusionTranspose2ND012NTmp2NeedRowCountFloat(dstTensor, tiling, params, tmp2);
        }
        params.transdataRepeat += 1;



        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {


            params.tmp1RemainRowCount = 0;
        }

        params.tmp2Count = 0;
        if (params.dstPrehnCount == tiling.hnDiv) {
            params.dstPrehnCount = 0;
        }
    }

    if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE;
    } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
        params.tmp2NeedRowCount =
            (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ? BLOCK_CUBE : (tiling.hnDiv - params.dstPrehnCount);
    } else if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp2NeedRowCountNotZero(ConfusionTranspose2ND012NTiling& tiling,
    ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount != 0) {
        if (params.tmp1RemainRowCount >= params.tmp2NeedRowCount) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count = params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        } else {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp1RemainRowCount;
            params.dstAllCount += params.tmp1RemainRowCount;
            params.tmp2Count = params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012Ntmp2NeedRowCountZeroHalf(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    if (tiling.hnDiv <= BLOCK_CUBE) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList3[n] =
                (uint64_t)
                    dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsMulAlignHnDiv +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    } else {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList3[n] =
                (uint64_t)
                    dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                tiling.alignsMulAlignHnDiv +
                ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList3[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012Ntmp2NeedRowCountZeroFloat(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    for (uint16_t m = 0; m < 2; m++) {
        if (tiling.hnDiv <= BLOCK_CUBE) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList3[n] =
                    (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    params.j * tiling.alignHnDivCube + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
        } else {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList3[n] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
                params.dstLocalList3[n + 1] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    tiling.blockSize + params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList3[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012Ntmp2NeedRowCountZero(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(__cce_half)) {
            ConfusionTranspose2ND012Ntmp2NeedRowCountZeroHalf(dstTensor, tiling, params, tmp2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            ConfusionTranspose2ND012Ntmp2NeedRowCountZeroFloat(dstTensor, tiling, params, tmp2);
        }
        params.transdataRepeat += 1;
        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {
            params.tmp1RemainRowCount = 0;
        }
        params.tmp2Count = 0;
        if (params.dstPrehnCount == tiling.hnDiv) {
            params.dstPrehnCount = 0;
        }

        if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
            params.tmp2NeedRowCount = BLOCK_CUBE;
        } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
            params.tmp2NeedRowCount = (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ?
                BLOCK_CUBE :
                (tiling.hnDiv - params.dstPrehnCount);
        } else if (tiling.hnDiv < BLOCK_CUBE) {
            params.tmp2NeedRowCount = tiling.hnDiv;
        }
    }
}

}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_impl.h" 2

namespace AscendC {
template <typename T> struct ConfusionTranspose012Params {
    [aicore] ConfusionTranspose012Params(){};

    int32_t i;
    int32_t j;
    int32_t k;

    TransposeType transposeType;
    TransDataTo5HDParams transDataParams1;
    TransDataTo5HDParams transDataParams2;

    uint32_t tmp1RemainRowCount;
    uint32_t tmp2NeedRowCount;
    uint32_t transdataRepeat;
    uint32_t tmp2Count;
    uint32_t tmp1Count;
    uint32_t dstAllCount;
    uint32_t dstPreHnCount;


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConfusionTranspose012TransParams(ConfusionTranspose012Tiling& tiling,
    ConfusionTranspose012Params<T>& params, TransposeType transposeTypeIn)
{
    if (tiling.shapeH < 16) {
        params.tmp2NeedRowCount = tiling.shapeH;
    }
    params.transposeType = transposeTypeIn;

    params.tmp1RemainRowCount = 0;
    params.tmp2NeedRowCount = 16;
    params.transdataRepeat = 0;
    params.tmp2Count = 0;
    params.tmp1Count = 0;
    params.dstAllCount = 0;
    params.dstPreHnCount = 0;

    params.transDataParams1.repeatTimes = 1;
    params.transDataParams1.dstRepStride = 0;
    params.transDataParams1.srcRepStride = 0;

    params.transDataParams2.repeatTimes = 1;
    params.transDataParams2.dstRepStride = 0;
    params.transDataParams2.srcRepStride = 0;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp1RemainRowCountZero(const LocalTensor<T>& srcTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount != 0) {
        return;
    }
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList1[n] = (uint64_t)tmp1[BLOCK_CUBE * n].GetPhyAddr();
            params.srcLocalList1[n] = (uint64_t)srcTensor[params.i * tiling.alignsCube +
                params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
    } else if constexpr (sizeof(T) == sizeof(float)) {
        for (uint16_t m = 0; m < 2; m++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList1[n] = (uint64_t)tmp1[m * CUBE_HALF_SIZE + tiling.blockSize * n].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList1[n] =
                    (uint64_t)srcTensor[params.i * tiling.alignsCube + params.j * CUBE_MAX_SIZE +
                        m * tiling.blockSize + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset].GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
        }
    }

    if (tiling.hnDiv > BLOCK_CUBE) {
        if ((params.dstPreHnCount < tiling.hnDiv) && (params.dstPreHnCount + BLOCK_CUBE) > tiling.hnDiv) {
            params.tmp1RemainRowCount = tiling.hnDiv - params.dstPreHnCount;
        } else {
            params.tmp1RemainRowCount = BLOCK_CUBE;
        }
    } else if (tiling.hnDiv <= BLOCK_CUBE) {
        params.tmp1RemainRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp1ToTmp2(const LocalTensor<T>& tmp2, ConfusionTranspose012Tiling& tiling,
    ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp2NeedRowCount <= params.tmp1RemainRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);
            params.tmp1Count += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.dstPreHnCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        }
    } else if (params.tmp2NeedRowCount > params.tmp1RemainRowCount) {
        if (params.tmp1RemainRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);
            params.tmp1Count += params.tmp1RemainRowCount;
            params.dstAllCount += params.tmp1RemainRowCount;
            params.dstPreHnCount += params.tmp1RemainRowCount;
            params.tmp2Count += params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp2ToDstHalf(const LocalTensor<T>& dstTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp2)
{
    if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITHOUT_N) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * BLOCK_CUBE +
                params.j * tiling.alignhBlockCube + tiling.alignH * n + params.k * tiling.dstBatchOffset].GetPhyAddr();
        }
    } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITHOUT_N) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * tiling.alignsCube +
                params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.dstBatchOffset].GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList2[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp2ToDstFloat(const LocalTensor<T>& dstTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp2)
{
    for (uint16_t m = 0; m < 2; m++) {
        if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITHOUT_N) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * BLOCK_CUBE +
                    params.j * tiling.alignhBlockCube + m * tiling.blockSizeMulAlignH +
                    tiling.alignH * (n / 2) + params.k * tiling.dstBatchOffset].GetPhyAddr();
                params.dstLocalList2[n + 1] = (uint64_t)dstTensor[params.transdataRepeat * BLOCK_CUBE +
                    params.j * tiling.alignhBlockCube + m * tiling.blockSizeMulAlignH + tiling.alignH * (n / 2) +
                    tiling.blockSize + params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
        } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITHOUT_N) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * tiling.alignsCube +
                    params.j * CUBE_MAX_SIZE + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                    params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList2[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp2ToDst(const LocalTensor<T>& dstTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp2)
{
    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(__cce_half)) {
            ConfusionTranspose012Tmp2ToDstHalf(dstTensor, tiling, params, tmp2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            ConfusionTranspose012Tmp2ToDstFloat(dstTensor, tiling, params, tmp2);
        }

        params.tmp2Count = 0;
        params.transdataRepeat += 1;
        if (params.dstAllCount == tiling.shapeH) {
            params.tmp1RemainRowCount = 0;
            params.dstAllCount = 0;
        }

        if ((params.transdataRepeat + 1) != tiling.hBlockNum) {
            params.tmp2NeedRowCount = BLOCK_CUBE;
        } else {
            params.tmp2NeedRowCount = tiling.shapeH - params.transdataRepeat * BLOCK_CUBE;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp1ToTmp2Remain(const LocalTensor<T>& tmp2,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount >= params.tmp2NeedRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1[(params.tmp1Count * BLOCK_CUBE)], dataCopyParams2);
            params.dstAllCount += params.tmp2NeedRowCount;
            params.dstPreHnCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        }
    } else {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1[(params.tmp1Count * BLOCK_CUBE)], dataCopyParams2);
            params.dstAllCount += params.tmp1RemainRowCount;
            params.dstPreHnCount += params.tmp1RemainRowCount;
            params.tmp2Count = params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
        }
    }
}






template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213Compute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose0213Tiling &tiling)
{
    ConfusionTranspose0213Params<T> params;
    InitConfusionTranspose0213TransParams<T>(tiling, params, transposeTypeIn);

    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        for (params.i = 0; params.i < tiling.shapeA1; params.i++) {
            for (params.j = 0; params.j < tiling.widthTiling; params.j++) {
                for (params.m = 0; params.m < tiling.mainBlocks; params.m++) {

                    if constexpr (sizeof(T) == sizeof(__cce_half)) {
                        ConfusionTranspose0213MainHalf(dstTensor, srcTensor, params, tiling, tmp1);
                    } else if constexpr (sizeof(T) == sizeof(float)) {
                        ConfusionTranspose0213MainFloat(dstTensor, srcTensor, params, tiling, tmp1);
                    } else {
                                     ;
                    }
                }
                if (tiling.tailSize) {
                    if constexpr (sizeof(T) == sizeof(__cce_half)) {
                        ConfusionTranspose0213TailHalf(dstTensor, srcTensor, params, tiling, tmp1);
                    } else if constexpr (sizeof(T) == sizeof(float)) {
                        ConfusionTranspose0213TailFloat(dstTensor, srcTensor, params, tiling, tmp1);
                    } else {
                                     ;
                    }
                }
            }
        }
    }
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NCompute(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer,
    ConfusionTranspose2NZ012NTiling &tiling)
{
    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();
    LocalTensor<T> tmp2 = tmp1[CUBE_MAX_SIZE];

    ConfusionTranspose2NZ012NParams<T> params;
    InitConfusionTranspose2NZ012N(params, tiling);

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        params.transdataRepeat = 0;
        for (params.j = 0; params.j < tiling.sBlockNum; params.j++) {
            for (params.i = 0; params.i < tiling.hBlockNum; params.i++) {
                ConfusionTranspose2NZ012NTmp1RemainRowCountZero(srcTensor, tiling, params, tmp1);
                ConfusionTranspose2NZ012NTmp2RemainRowCountFirst(tiling, params, tmp1, tmp2);
                ConfusionTranspose2NZ012NTmp2RemainRowCountZero(dstTensor, tiling, params, tmp2);
                ConfusionTranspose2NZ012NCalcTmp2NeedRowCount(tiling, params);


                while (params.tmp1RemainRowCount) {
                    ConfusionTranspose2NZ012NTmp2NeedRowCount(tiling, params, tmp1, tmp2);
                    ConfusionTranspose2NZ012NTmp2NeedRowCountZero(dstTensor, tiling, params, tmp2);
                }
            }
        }
    }
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NCompute(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer,
    ConfusionTranspose2ND012NTiling &tiling)
{
    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();
    LocalTensor<T> tmp2 = tmp1[CUBE_MAX_SIZE];

    ConfusionTranspose2ND012NParams<T> params;
    InitConfusionTranspose2ND012N(params, tiling);

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        params.transdataRepeat = 0;
        for (params.j = 0; params.j < tiling.sBlockNum; params.j++) {
            for (params.i = 0; params.i < tiling.hBlockNum; params.i++) {
                ConfusionTranspose2ND012NTmp1RemainRowCount(srcTensor, tiling, params, tmp1);
                ConfusionTranspose2ND012NTmp1RemainRowCountFirst(tiling, params, tmp1, tmp2);
                ConfusionTranspose2ND012NTmp2NeedRowCount(dstTensor, tiling, params, tmp2);


                while (params.tmp1RemainRowCount) {
                    ConfusionTranspose2ND012NTmp2NeedRowCountNotZero(tiling, params, tmp1, tmp2);
                    ConfusionTranspose2ND012Ntmp2NeedRowCountZero(dstTensor, tiling, params, tmp2);
                }
            }
        }
    }
}






template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Compute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose012Tiling &tiling)
{
    ConfusionTranspose012Params<T> params;
    InitConfusionTranspose012TransParams<T>(tiling, params, transposeTypeIn);
    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();
    LocalTensor<T> tmp2 = tmp1[CUBE_MAX_SIZE];

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        params.transdataRepeat = 0;
        for (params.j = 0; params.j < tiling.sBlockNum; params.j++) {
            params.transdataRepeat = 0;
            for (params.i = 0; params.i < (tiling.hnDivBlockNum * tiling.shapeN); params.i++) {

                params.tmp1Count = 0;
                ConfusionTranspose012Tmp1RemainRowCountZero(srcTensor, tiling, params, tmp1);

                if (params.dstPreHnCount == tiling.hnDiv) {
                    params.dstPreHnCount = 0;
                }
                ConfusionTranspose012Tmp1ToTmp2(tmp2, tiling, params, tmp1);
                ConfusionTranspose012Tmp2ToDst(dstTensor, tiling, params, tmp2);


                while (params.tmp1RemainRowCount) {

                    ConfusionTranspose012Tmp1ToTmp2Remain(tmp2, tiling, params, tmp1);

                    ConfusionTranspose012Tmp2ToDst(dstTensor, tiling, params, tmp2);
                }
            }
        }
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTransposeOnlyCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    ConfusionTransposeOnlyTiling &tiling)
{
    uint64_t dstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams;
    transDataParams.repeatTimes = tiling.repeat;
    transDataParams.dstRepStride = transDataParams.repeatTimes > 1 ? tiling.stride : 0;
    transDataParams.srcRepStride = transDataParams.repeatTimes > 1 ? 1 : 0;
    for (int32_t i = 0; i < tiling.highBlock; i++) {
        if constexpr (sizeof(T) == sizeof(__cce_half)) {
            for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
                dstLocalList[m] = (uint64_t)dstTensor[i * BLOCK_CUBE + tiling.height * m].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                srcLocalList[n] =
                    (uint64_t)srcTensor[i * tiling.width * BLOCK_CUBE + tiling.width * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(dstLocalList, srcLocalList, transDataParams);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + 2) {
                dstLocalList[m] = (uint64_t)dstTensor[i * BLOCK_CUBE + tiling.height * (m / 2)].GetPhyAddr();
                dstLocalList[m + 1] =
                    (uint64_t)dstTensor[i * BLOCK_CUBE + tiling.height * (m / 2) + tiling.blockSize].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                srcLocalList[n] =
                    (uint64_t)srcTensor[i * tiling.width * BLOCK_CUBE + tiling.width * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(dstLocalList, srcLocalList, transDataParams);
        }
    }
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_v220_impl.h" 2

namespace AscendC {





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose0213Tiling& tiling)
{
    ConfusionTranspose0213Compute(dstTensor, srcTensor, sharedTmpBuffer, transposeTypeIn, tiling);
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012N(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, ConfusionTranspose2NZ012NTiling& tiling)
{
    ConfusionTranspose2NZ012NCompute(dstTensor, srcTensor, sharedTmpBuffer, tiling);
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012N(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, ConfusionTranspose2ND012NTiling& tiling)
{
    ConfusionTranspose2ND012NCompute(dstTensor, srcTensor, sharedTmpBuffer, tiling);
}






template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose012Tiling& tiling)
{
    ConfusionTranspose012Compute(dstTensor, srcTensor, sharedTmpBuffer, transposeTypeIn, tiling);
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTransposeOnly(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTransposeOnlyTiling& tiling)
{
    ConfusionTransposeOnlyCompute(dstTensor, srcTensor, tiling);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_common_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTransposeImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeType, ConfusionTransposeTiling& tiling)
{





    if (transposeType == TransposeType::TRANSPOSE_NZ2ND_0213 || transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
        ConfusionTranspose0213(dstTensor, srcTensor, sharedTmpBuffer, transposeType,
            reinterpret_cast<ConfusionTranspose0213Tiling&>(tiling));
    }




    else if (transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITH_N) {
        ConfusionTranspose2NZ012N(dstTensor, srcTensor, sharedTmpBuffer,
            reinterpret_cast<ConfusionTranspose2NZ012NTiling &>(tiling));
    }




    else if (transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITH_N) {
        ConfusionTranspose2ND012N(dstTensor, srcTensor, sharedTmpBuffer,
            reinterpret_cast<ConfusionTranspose2ND012NTiling &>(tiling));
    }





    else if (transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITHOUT_N ||
        transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITHOUT_N) {
        ConfusionTranspose012(dstTensor, srcTensor, sharedTmpBuffer, transposeType,
            reinterpret_cast<ConfusionTranspose012Tiling &>(tiling));
    }



    else if (transposeType == TransposeType::TRANSPOSE_ND2ND_ONLY) {
        ConfusionTransposeOnly(dstTensor, srcTensor, reinterpret_cast<ConfusionTransposeOnlyTiling &>(tiling));
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeType, ConfusionTransposeTiling& tiling)
{
    ConfusionTransposeImpl<T>(dstTensor, srcTensor, sharedTmpBuffer, transposeType, tiling);
}
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    TransposeType transposeType, ConfusionTransposeTiling& tiling)
{
    LocalTensor<uint8_t> tmpBuffer;
    bool res = PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
                                                                               ;

    ConfusionTransposeImpl<T>(dstTensor, srcTensor, tmpBuffer, transposeType, tiling);
}
#pragma end_pipe
}
# 74 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/selectwithbytesmask.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/selectwithbytesmask.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_impl.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_common_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_common_impl.h"
namespace AscendC {

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void InitScalarSelectMask(const LocalTensor<T> &tmpMask, T scalar)
{
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, ONE_REPEAT_BYTE_SIZE / sizeof(T));
    Duplicate<T, false>(tmpMask, static_cast<T>(scalar), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();

    SetCmpMask(tmpMask);
    PipeBarrier<PIPE_V>();
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void CastMaskToHalfImpl(const LocalTensor<__cce_half> &localMaskTmp, const LocalTensor<U> &mask)
{
    if constexpr (sizeof(U) == 4) {
        LocalTensor<float> tmpTensor = mask.template ReinterpretCast<float>();
        Cast<__cce_half, float, false>(localMaskTmp, tmpTensor, RoundMode::CAST_ODD, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    } else if constexpr (sizeof(U) == 2) {
        LocalTensor<int16_t> tmpTensor = mask.template ReinterpretCast<int16_t>();
        Cast<__cce_half, int16_t, false>(localMaskTmp, tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    } else if constexpr (sizeof(U) == 1) {
        LocalTensor<uint8_t> tmpTensor = mask.template ReinterpretCast<uint8_t>();
        Cast<__cce_half, uint8_t, false>(localMaskTmp, tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    }
}

template <typename T, typename U, bool reverse = false>
[aicore] __inline__ __attribute__((always_inline)) void SelectWithBytesMaskPerAxisImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t srcAxisLen,
    const uint32_t bucketSize)
{
    const auto paddingLen = AlignUp(bucketSize / ONE_BYTE_BIT_SIZE, ONE_BLK_SIZE);
    const auto localMaskTmpOffset = paddingLen;
    const auto localScalarOffset = sizeof(__cce_half) * bucketSize;
    LocalTensor<uint8_t> localMask = sharedTmpBuffer;
    LocalTensor<__cce_half> localMaskTmp = sharedTmpBuffer.ReinterpretCast<__cce_half>();
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, srcAxisLen);
    CastMaskToHalfImpl<U>(localMaskTmp, mask);
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;
    constexpr auto loopSize = ONE_REPEAT_BYTE_SIZE / sizeof(__cce_half);
    const auto repeatTime = DivCeil(srcAxisLen, loopSize);

    if constexpr (!reverse) {
        CompareScalar<__cce_half, uint8_t, false>(localMask, localMaskTmp, static_cast<__cce_half>(0), CMPMODE::EQ,
            MASK_PLACEHOLDER, repeatTime, unaryParams);
    } else {
        CompareScalar<__cce_half, uint8_t, false>(localMask, localMaskTmp, static_cast<__cce_half>(0), CMPMODE::NE,
            MASK_PLACEHOLDER, repeatTime, unaryParams);
    }
    PipeBarrier<PIPE_V>();
    Select(dst, localMask, src0, 1, binaryParams);
}


template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void RemoveRedundantMask(const LocalTensor<U> &dst, const LocalTensor<U> &mask,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    LocalTensor<uint16_t> tmpDst = dst.template ReinterpretCast<uint16_t>();
    LocalTensor<uint16_t> tmpMask = mask.template ReinterpretCast<uint16_t>();

    uint64_t rsvdCnt;

    GatherMask<uint16_t>(tmpDst, tmpMask, REDUCEV2_MODE_SEVEN, true, info.srcLastAxis * sizeof(U) / sizeof(uint16_t),
        { DEFAULT_BLK_STRIDE, static_cast<uint16_t>(info.firstAxis),
        static_cast<uint16_t>(info.maskLastAxis * sizeof(U) / ONE_BLK_SIZE), 0 },
        rsvdCnt);
    SetMaskCount();
}

template <typename T, typename U, bool reverse = false>
[aicore] __inline__ __attribute__((always_inline)) void SelectWithBytesMaskLoopImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t loopSize,
    const uint32_t totalLen, const uint32_t srcOriginOffset, const uint32_t maskOriginOffset)
{
    for (uint32_t offset = 0; offset < totalLen; offset += loopSize) {
        auto calSize = offset + loopSize > totalLen ? totalLen - offset : loopSize;
        SelectWithBytesMaskPerAxisImpl<T, U, reverse>(dst[srcOriginOffset + offset], src0[srcOriginOffset + offset],
            src1, mask[maskOriginOffset + offset], sharedTmpBuffer, calSize, loopSize);
        PipeBarrier<PIPE_V>();
    }
}
}
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_v220_impl.h" 1
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) uint32_t ComputeMaskExtraBufSize(const uint32_t srcSize, const uint32_t typeSize)
{
    return AlignUp(srcSize * typeSize, ONE_BLK_SIZE);
}

template <typename T, typename U, bool reverse = false>
[aicore] __inline__ __attribute__((always_inline)) void SelectWithBytesMaskProcess(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<U> &tmpMask, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const SelectWithBytesMaskShapeInfo &info, const uint32_t tmpBufferOffset, const uint32_t loopSize)
{
    if (info.srcLastAxis != info.maskLastAxis) {
        RemoveRedundantMask(tmpMask, mask, sharedTmpBuffer, info);
        PipeBarrier<PIPE_V>();
    }

    SelectWithBytesMaskLoopImpl<T, U, reverse>(dst, src0, src1, tmpMask, sharedTmpBuffer[tmpBufferOffset], loopSize,
        src0.GetSize(), 0, 0);
}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_impl.h" 2




namespace AscendC {


template <typename T, typename U, bool isReuseMask, bool reverse = false>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void SelectWithBytesMaskImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0,
    T src1, const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const SelectWithBytesMaskShapeInfo &info)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    PipeBarrier<PIPE_V>();
    constexpr uint32_t MIN_REQUIRED_BUFFER = 1024;
    constexpr uint32_t RESERVED_BUFFER = 256;
    constexpr uint32_t MAX_CALC_BYTE_PER_LOOP = 255 * ONE_REPEAT_BYTE_SIZE;
    const uint32_t firstAxis = info.firstAxis;
    const uint32_t srcLastAxis = info.srcLastAxis;
    const uint32_t maskLastAxis = info.maskLastAxis;
    const uint32_t srcSize = src0.GetSize();


                                                                                                  ;

                                                                                                   ;

                                                                                                               ;
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
                                                                                                                   ;
    LocalTensor<U> tmpMask = mask;
    LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
    uint32_t tmpBufferOffset = 0;
    if constexpr (!isReuseMask) {
        if (srcLastAxis != maskLastAxis) {
            const uint32_t tmpMaskRequiredBuffer = ComputeMaskExtraBufSize(srcSize, sizeof(U));



              ;
            tmpMask = sharedTmpBuffer.template ReinterpretCast<U>();
            tmpMask.SetSize(tmpMaskRequiredBuffer / sizeof(U));
            bufferSize -= tmpMaskRequiredBuffer;
            tmpBufferOffset = tmpMaskRequiredBuffer;
        }
    }

    bufferSize -= RESERVED_BUFFER;
    uint32_t loopSize = bufferSize / sizeof(__cce_half) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    if (loopSize > MAX_CALC_BYTE_PER_LOOP / sizeof(__cce_half)) {
        loopSize = MAX_CALC_BYTE_PER_LOOP / sizeof(__cce_half);
    }

    SetMaskCount();
    InitScalarSelectMask(tmpTensor, src1);

    SelectWithBytesMaskProcess<T, U, reverse>(dst, src0, src1, mask, tmpMask, sharedTmpBuffer, info, tmpBufferOffset,
        loopSize);
    SetMaskNorm();
    ResetMask();
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/selectwithbytesmask.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/selectwithbytesmask.h"
template <typename T, typename U, bool isReuseMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SelectWithBytesMask(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    SelectWithBytesMaskImpl<T, U, isReuseMask, false>(dst, src0, src1, mask, sharedTmpBuffer, info);
}
# 71 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/selectwithbytesmask.h"
template <typename T, typename U, bool isReuseMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SelectWithBytesMask(const LocalTensor<T> &dst, T src0, const LocalTensor<T> &src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    SelectWithBytesMaskImpl<T, U, isReuseMask, true>(dst, src1, src0, mask, sharedTmpBuffer, info);
}
#pragma end_pipe
}
# 75 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sinh/sinh_common_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sinh/sinh_common_impl.h"
namespace AscendC {
constexpr float SINH_NEG_LN_TWO = -0.69314718055994530941723212145818;
constexpr float SINH_NEG_ONE = -1.0;
constexpr float SINH_POINT_FIVE = 0.5;
constexpr uint32_t SINH_HALF_CALC_PROC = 4;
constexpr uint32_t SINH_FLOAT_CALC_PROC = 1;


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SinhCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t offset)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<T>& tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<T>();


    Muls<T, false>(tmpFloatBuffer1, src, static_cast<T>(SINH_NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<T>(SINH_NEG_LN_TWO), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(dst, src, static_cast<T>(SINH_NEG_LN_TWO), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sub<T, false>(dst, dst, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}




template <>
[aicore] __inline__ __attribute__((always_inline)) void SinhCompute(const LocalTensor<__cce_half>& dst, const LocalTensor<__cce_half>& src,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t offset)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<float>& tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpFloatBuffer2 = tmpFloatBuffer1[offset];

    Cast<float, __cce_half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, SINH_NEG_LN_TWO, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, SINH_NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, SINH_NEG_LN_TWO, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<__cce_half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SinhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Sinh");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Sinh");


                                                                                                                       ;
    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(T);

    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        splitCount = splitCount / SINH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / SINH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    const uint32_t loopCount = calCount / splitCount;
    const uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        SinhCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer, splitCount);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        SinhCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer, splitCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SinhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    SinhImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SinhImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sinh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    SinhImpl(dstTensor, srcTensor, calCount);
}
# 98 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Sinh<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swiglu/swiglu_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swiglu/swiglu_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swiglu/swiglu_common_impl.h" 2


namespace AscendC {
constexpr float NUMBER_ONE = 1.0;
constexpr uint32_t REPEAT_TIME_SWIGLU = 1;
constexpr uint32_t SWIGLU_HALF_BUFFER_SIZE = 3;
constexpr uint32_t SWIGLU_FLOAT_TMP_BUFFER_SIZE = 0;
constexpr uint32_t SWIGLU_STRIDE_DIGITS = 2;

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const float &scalarValue, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const float &scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, srcTensor0.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl(LocalTensor<T> &dstTensor, LocalTensor<T> &srcTensor0, LocalTensor<T> &srcTensor1,
                              const float &scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, srcTensor0.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
                              const LocalTensor<T> &srcTensor1, const float &scalarValue,
                              const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }




                                                                            ;

                                                                                             ;

                                                                                                                    ;

                                                                                                            ;

                                                                                        ;

                                                                     ;

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));
    uint32_t stackSize = calCount;

    if (sizeof(T) == sizeof(__cce_half)) {


        stackSize = sharedTmpBuffer.GetSize() / sizeof(float) / SWIGLU_HALF_BUFFER_SIZE;
    }

    stackSize = ((stackSize * sizeof(T)) / ONE_BLK_SIZE * ONE_BLK_SIZE) / sizeof(T);

    if (stackSize <= 0) {
        stackSize = ONE_BLK_SIZE / sizeof(T);
    }

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    SetMaskCount();
    SetVectorMask<T>(0, stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        SwiGLUImpl(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], scalarValue, tmpBuffer, stackSize);
        offset = offset + stackSize;
    }
    if (tail != 0) {

        bool isTail32BAligned = (tail * sizeof(T) % ONE_BLK_SIZE == 0);
        auto tail32BAligned = (tail * sizeof(T) / ONE_BLK_SIZE + (isTail32BAligned ? 0 : 1)) *
                              ONE_BLK_SIZE / sizeof(T);
        SetVectorMask<T>(0, tail);
        SwiGLUImpl(dstTensor[offset], srcTensor0[offset], srcTensor1[offset],
                   scalarValue, tmpBuffer, tail32BAligned);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SwishCalcSimplified(
   const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const float &scalarValue)
{


    const UnaryRepeatParams unaryParams;

    Muls<float, false>(dstTensor, srcTensor, scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(dstTensor, dstTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, static_cast<T>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Div<float, false>(dstTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0, const LocalTensor<T> &src1,
                                  const float &beta, const LocalTensor<float> &sharedTmpBuffer, uint32_t calCount)
{

    float scalar = static_cast<float>(static_cast<float>(-1.0) * static_cast<float>(beta));
    SwishCalcSimplified(dst, src1, scalar);

    const BinaryRepeatParams binaryParams;

    Mul<float, false>(dst, src0, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl<__cce_half>(const LocalTensor<__cce_half> &dst, const LocalTensor<__cce_half> &src0,
                                        const LocalTensor<__cce_half> &src1, const float &beta,
                                        const LocalTensor<float> &sharedTmpBuffer, uint32_t calCount)
{
    LocalTensor<float> tmpSrc1FloatBuffer1 = sharedTmpBuffer;
    LocalTensor<float> tmpSrc1FloatBuffer2 = sharedTmpBuffer[calCount];
    LocalTensor<float> tmpSrc0FloatBuffer = sharedTmpBuffer[2 * calCount];


    Cast<float, __cce_half, false>(tmpSrc1FloatBuffer1, src1, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / SWIGLU_STRIDE_DIGITS });
    PipeBarrier<PIPE_V>();


    float scalar = static_cast<float>(static_cast<float>(-1.0) * static_cast<float>(beta));
    SwishCalcSimplified(tmpSrc1FloatBuffer2, tmpSrc1FloatBuffer1, scalar);


    Cast<float, __cce_half, false>(tmpSrc0FloatBuffer, src0, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / SWIGLU_STRIDE_DIGITS });
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;

    Mul<float, false>(tmpSrc1FloatBuffer2, tmpSrc0FloatBuffer, tmpSrc1FloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Cast<__cce_half, float, false>(dst, tmpSrc1FloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE / SWIGLU_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLU(LocalTensor<T>& dstTensor, LocalTensor<T>& srcTensor0, LocalTensor<T>& srcTensor1,
                              const float& scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, srcTensor0.GetSize());
}
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLU(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const float& scalarValue, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    SwiGLU<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, srcTensor0.GetSize());
}
# 72 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLU(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const float& scalarValue, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, calCount);
}
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLU(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
                              const LocalTensor<T>& srcTensor1, const float& scalarValue,
                              const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, calCount);
}
#pragma end_pipe
}
# 77 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/reglu.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/reglu.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/reglu/reglu_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/reglu/reglu_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/reglu/reglu_v220_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/reglu/reglu_v220_impl.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReGluCast(const LocalTensor<T> &dstTensor, const LocalTensor<float> &srcTensor)
{
    if constexpr (IsSameType<T, __cce_half>::value) {
        Cast<T, float, false>(dstTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    } else {
        Cast<T, float, false>(dstTensor, srcTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/reglu/reglu_common_impl.h" 2




namespace AscendC {
const uint8_t REGLU_HALF_CALC_PROCEDURE = 3;
const uint32_t REGLU_TEMP_BUFFER_OFFSET = 2U;

[aicore] __inline__ __attribute__((always_inline)) void Compute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const LocalTensor<float>& srcTensor1)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Maxs<float, false>(dstTensor, srcTensor1, static_cast<float>(0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(dstTensor, srcTensor0, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReGluCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    const LocalTensor<float>& x0CastBuffer = tmpTensor;
    const LocalTensor<float>& x1CastBuffer = tmpTensor[splitSize];
    const LocalTensor<float>& yCastBuffer = tmpTensor[splitSize * REGLU_TEMP_BUFFER_OFFSET];

    Cast<float, T, false>(x0CastBuffer, srcTensor0, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    Cast<float, T, false>(x1CastBuffer, srcTensor1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Compute(yCastBuffer, x0CastBuffer, x1CastBuffer);
    ReGluCast(dstTensor, yCastBuffer);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReGluImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

                                                                                ;
                                                                                                                  ;




                                                                                        ;

    SetMaskCount();
    if constexpr (IsSameType<T, float>::value) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
        Compute(dstTensor, srcTensor0, srcTensor1);
    } else {
        uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);
                                                                                                     ;
        LocalTensor<float> tmpBuffer;
        tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
        uint32_t stackSize = 0;

        stackSize = tmpBufferSize / REGLU_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                             ;

        const uint32_t round = calCount / stackSize;
        const uint32_t tail = calCount % stackSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, stackSize);
        uint32_t offset = 0;

        for (uint32_t i = 0; i < round; i++) {
            ReGluCompute(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], tmpBuffer, stackSize);
            offset = offset + stackSize;
        }

        if (tail != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, tail);
            ReGluCompute(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], tmpBuffer, stackSize);
        }
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReGluImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool hasStackBuffer = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                          ;
    ReGluImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/reglu.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/reglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReGlu(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if (g_coreType == AIC) {
        return;
    }
    ReGluImpl<T, false>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReGlu(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const uint32_t calCount)
{
    if (g_coreType == AIC) {
        return;
    }
    ReGluImpl<T, false>(dstTensor, srcTensor0, srcTensor1, calCount);
}

#pragma end_pipe
}
# 78 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h" 1
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_common_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void TanCast(
    const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor, RoundMode castType)
{
    Cast<float, float, false>(dstTensor, srcTensor, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_common_impl.h" 2




namespace AscendC {
constexpr uint32_t TAN_HALF_CALC_PROCEDURE = 10;
constexpr uint32_t TAN_FLOAT_CALC_PROCEDURE = 4;

constexpr float PI_FOR_X_TODIV = 0.3183098733425140380859375;
constexpr float KPI_FIRS_PI_MULS = 0.0009670257568359375;

constexpr float PI_V2 = 3.140625;

constexpr float PI_DOWN = 1.57079637050628662109375;
constexpr float PI_DOWN_NEG = -1.57079637050628662109375;

constexpr float KPI_TWI_PI_MULS = 6.2771141529083251953125e-7;
constexpr float PI_RESDOWN_ADDS = 0.00000004371139000189375;
constexpr float PI_RESDOWN_ADDS_NEG = -0.00000004371139000189375;

constexpr float KPI_THIR_PI_MULS = 1.21644916362129151821136474609375e-10;

constexpr float KPI_FOR_PI_MULS = -1.0291767438275201129727065563201904296875e-13;

constexpr float TAN_RES_MULIT_SCA = 0.0698520831551998762793;
constexpr float TAN_RES_ADDICT_UP = -6.8711573651634203789;
constexpr float TAN_2ADDS = 61.20362572811089435388;
constexpr float TAN_3ADDS = -24.8048928861126769186219;

[aicore] __inline__ __attribute__((always_inline)) void KPI_0(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, PI_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void KPI_1(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_FIRS_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor1, srcTensor, PI_DOWN, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor2, srcTensor, PI_DOWN_NEG, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void KPI_2(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_TWI_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor1, resTensor1, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor2, resTensor2, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor1, resTensor1, PI_RESDOWN_ADDS_NEG, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor2, resTensor2, PI_RESDOWN_ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void KPI_3(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_THIR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor1, resTensor1, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor2, resTensor2, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void KPI_4(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_FOR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor1, resTensor1, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor2, resTensor2, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void TanRound(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
# 170 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_common_impl.h"
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(roundTensor, srcTensor, PI_FOR_X_TODIV, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    TanCast(roundTensor, roundTensor, RoundMode::CAST_RINT);

    KPI_0(dstTensor, srcTensor, roundTensor);
    KPI_1(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
    KPI_2(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
    KPI_3(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
    KPI_4(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
}

[aicore] __inline__ __attribute__((always_inline)) void TanPolynomialApproximation(const LocalTensor<float>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<float>& roundTensor,
    const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
# 198 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_common_impl.h"
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;


    Mul<float, false>(roundTensor, srcTensor, srcTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, roundTensor, TAN_RES_MULIT_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, TAN_RES_ADDICT_UP, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, TAN_2ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(dstTensor, dstTensor, srcTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(roundTensor, roundTensor, TAN_3ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, resTensor1, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, resTensor2, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TanCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t calSize)
{
    const UnaryRepeatParams unaryParams;
    const LocalTensor<T>& tmpTensor1 = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<T>& tmpTensor2 = tmpTensor1[calSize];
    const LocalTensor<T>& tmpTensor3 = tmpTensor2[calSize];
    const LocalTensor<T>& tmpTensor4 = tmpTensor3[calSize];

    Adds<T, false>(tmpTensor4, srcTensor, static_cast<float>(0.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    TanRound(dstTensor, tmpTensor4, tmpTensor1, tmpTensor2, tmpTensor3);
    TanPolynomialApproximation(dstTensor, tmpTensor4, tmpTensor1, tmpTensor2, tmpTensor3);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TanCompute(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t calSize)
{
    const LocalTensor<float>& tempTensorConv = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpTensor1 = tempTensorConv[calSize];
    const LocalTensor<float>& tmpTensor2 = tmpTensor1[calSize];
    const LocalTensor<float>& tmpTensor3 = tmpTensor2[calSize];
    const LocalTensor<float>& tmpTensor4 = tmpTensor3[calSize];

    Cast<float, __cce_half, false>(tmpTensor1, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    TanRound(tempTensorConv, tmpTensor1, tmpTensor2, tmpTensor3, tmpTensor4);
    TanPolynomialApproximation(tempTensorConv, tmpTensor1, tmpTensor2, tmpTensor3, tmpTensor4);

    Cast<__cce_half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Tan");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Tan");


                                                                                                                       ;
    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(T);
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        splitCount = splitCount / TAN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / TAN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    const uint32_t loopCount = calCount / splitCount;
    const uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < loopCount; ++i) {
        TanCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer, splitCount);
    }

    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        TanCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer, tailCount);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    TanImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 70 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h"
 template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Tan<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    TanImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 113 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    TanImpl(dstTensor, srcTensor, calCount);
}
# 130 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Tan<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 79 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/round.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/round.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/round/round_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/round/round_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/round/round_v220_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/round/round_v220_impl.h"
namespace AscendC {
constexpr uint32_t STRIDE_OF_DIFFERENT_DIGITS = 2;

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void RoundComputeCount(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{}

template <>
[aicore] __inline__ __attribute__((always_inline)) void RoundComputeCount<float, false>(const LocalTensor<float> &dstTensor,
    const LocalTensor<float> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, (uint8_t)1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void RoundComputeCount<__cce_half, false>(const LocalTensor<__cce_half> &dstTensor,
    const LocalTensor<__cce_half> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{


    uint32_t sharedTmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = sharedTmpBufferSize / sizeof(float) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitCount, 0, sharedTmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    const LocalTensor<float> &tmpTensor = sharedTmpBuffer.ReinterpretCast<float>();

    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, splitCount);


    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, __cce_half, false>(tmpTensor, srcTensor[i * splitCount], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS });
        PipeBarrier<PIPE_V>();

        Cast<float, float, false>(tmpTensor, tmpTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, (uint8_t)1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dstTensor[i * splitCount], tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<__cce_half, MaskMode::COUNTER>(0, calcTail);
        Cast<float, __cce_half, false>(tmpTensor, srcTensor[loopCount * splitCount], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS });
        PipeBarrier<PIPE_V>();
        Cast<float, float, false>(tmpTensor, tmpTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, (uint8_t)1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(dstTensor[loopCount * splitCount], tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/round/round_common_impl.h" 2





namespace AscendC {

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void RoundImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Round");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Round");


                                                                                                                       ;
    SetMaskCount();
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        RoundComputeCount<__cce_half, false>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
    } else {
        RoundComputeCount<float, false>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void RoundImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    RoundImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/round.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/round.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Round(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    RoundImpl<T, false>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/round.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Round(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    RoundImpl<T, false>(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 80 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/trunc/trunc_common_impl.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/trunc/trunc_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/trunc/trunc_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/trunc/trunc_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void TruncCastForTrunc(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_TRUNC, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/trunc/trunc_common_impl.h" 2


namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void TruncCompute(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    const LocalTensor<float> floatTmpTensor = tmpTensor.ReinterpretCast<float>();

    Cast<float, __cce_half, false>(floatTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    TruncCastForTrunc(floatTmpTensor, floatTmpTensor, tmpTensor);

    Cast<__cce_half, float, false>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void TruncCompute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{

    TruncCastForTrunc(dstTensor, srcTensor, tmpTensor);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TruncImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Trunc");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Trunc");


                                                                                                                       ;

    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(T);
    constexpr uint32_t TRUNC_HALF_CALC_PROCEDURE = 2;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        splitCount = splitCount / TRUNC_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        TruncCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<__cce_half>(0, calcTail);
        TruncCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TruncImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    TruncImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    TruncImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    TruncImpl(dstTensor, srcTensor, calCount);
}
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Trunc<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Trunc<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swish.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swish.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swish/swish_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swish/swish_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swish/swish_common_impl.h" 2

namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SwishCalcSimplified(
    const LocalTensor<T> &dstAddr, const LocalTensor<T> &srcAddr, T &scalarValue, uint32_t repeatTimes)
{


    const UnaryRepeatParams unaryParams;
    Muls<T, false>(dstAddr, srcAddr, scalarValue, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(dstAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(dstAddr, dstAddr, static_cast<T>(1), MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Div<T, false>(dstAddr, srcAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void SwishCompute(
    const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, uint32_t dataSize, const T scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swish/swish_common_impl.h"
    T scalar = static_cast<T>(static_cast<float>(-1) * static_cast<float>(scalarValue));

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, dataSize);
    SwishCalcSimplified(dstLocal, srcLocal, scalar, 1);
    SetMaskNorm();
# 102 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swish/swish_common_impl.h"
    ResetMask();
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swish.h" 2

namespace AscendC {
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swish.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Swish(
    const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, uint32_t dataSize, const T scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SwishCompute<T, false>(dstLocal, srcLocal, dataSize, scalarValue);
}
}
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk_utils.h"
namespace AscendC {
struct TopKInfo {
    int32_t outter = 1;
    int32_t inner;
    int32_t n;
};


enum class TopKMode {
    TOPK_NORMAL,
    TOPK_NSMALL,
};


};
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h" 2





# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_utils.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_utils.h"
namespace {
constexpr uint16_t MIN_SORT32_SIZE = 32;
constexpr uint16_t MIN_RPSORT16_SIZE = 16;
constexpr uint32_t BUF_LIST_SIZE = 2;
constexpr uint32_t MRG_MAX_ARRAY_SIZE = 15;
constexpr uint32_t MRGSORT_VALID_QUEUE = 4;
constexpr uint32_t MRGSORT_VALID_TWO = 2;
constexpr uint32_t MRGSORT_VALID_TWO_OFFSET = 769;
constexpr uint32_t TWO = 2;
constexpr uint32_t THREE = 3;
constexpr uint32_t FOUR = 4;
constexpr uint32_t FIVE = 5;
constexpr uint32_t SIX = 6;
constexpr uint32_t SEVEN = 7;
constexpr uint32_t EIGHT = 8;
constexpr uint32_t NINE = 9;
constexpr uint32_t TWELVE = 12;
constexpr uint32_t SIXTEEN = 16;
constexpr uint32_t THIRTY_TWO = 32;
constexpr uint32_t FORTYEIGHT = 48;
constexpr uint32_t VREDUCEV2_HALF_MASK = 128;
constexpr uint32_t VREDUCEV2_FOUR_BYTE_MASK = 64;
constexpr uint32_t SRC1_STACK_TENSORSIZE = 10;
constexpr uint32_t SRC1_STACK_VAL_OFFSET = 16;
constexpr uint32_t TOPK_INNER_ALIGN_LEN = 32;
constexpr uint32_t TOPK_NORMAL_INNER_MAX_HALF_LEN = 2048;
constexpr uint32_t TOPK_NSMALL_INNER_LEN = 32;
constexpr uint32_t TOPK_NORMAL_INNER_MAX_LEN = 4096;
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_impl.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_v220_impl.h" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_v220_impl.h"
namespace AscendC {
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_v220_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgFourQueueSort(const LocalTensor<T> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo, uint16_t &z, int32_t &mrgFourQueueCount, int32_t &dstIdx)
{
    uint16_t innerU16Type = static_cast<uint16_t>(topKInfo.inner);
    for (; z * MRGSORT_VALID_QUEUE <= innerU16Type; z *= MRGSORT_VALID_QUEUE) {
        auto src = (mrgFourQueueCount % TWO == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
        dstIdx = (mrgFourQueueCount + 1) % TWO;
        auto dst = (dstIdx == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
        mrgFourQueueCount += 1;
        uint16_t elementLengths[MRG_SORT_ELEMENT_LEN] = {z, z, z, z};
        struct MrgSort4Info srcInfo(elementLengths, false, 0b1111, tilling.mrgSortRepeat / z);
        struct MrgSortSrcList<T> srcList(src,
            src[z * tilling.mrgSortSrc1offset],
            src[z * tilling.mrgSortSrc2offset],
            src[z * tilling.mrgSortSrc3offset]);
        MrgSort<T>(dst, srcList, srcInfo);
        PipeBarrier<PIPE_V>();
        const DataCopyParams intriParams = {static_cast<uint16_t>(tilling.copyUbToUbBlockCount), 1, 0, 0};
        DataCopy(src, dst, intriParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgTwoQueueSort(const LocalTensor<T> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo, const uint16_t z, const int32_t mrgFourQueueCount, int32_t &dstIdx, const int32_t k)
{
    int32_t arrayCount = 0;
    if (z < topKInfo.inner) {

        int32_t mrgArray[MRG_MAX_ARRAY_SIZE] = {0};
        int32_t tmpInner = topKInfo.inner;
        for (int32_t i = z; i >= MIN_SORT32_SIZE; i /= MRGSORT_VALID_QUEUE) {
            int32_t count;
            for (count = 0; count < tmpInner / i; ++count) {
                mrgArray[arrayCount++] = i;
            }
            tmpInner -= count * i;
        }
        uint16_t mrgSortedLen = 0;
        for (int32_t i = 0; i < arrayCount - 1; ++i) {
            auto src = ((mrgFourQueueCount + i) % TWO == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
            dstIdx = (mrgFourQueueCount + 1 + i) % TWO;
            auto dst = (dstIdx == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
            mrgSortedLen += static_cast<uint16_t>(mrgArray[i]);
            uint64_t tmpMrgSortedLen = mrgSortedLen;
            uint64_t tmpMrgArray = mrgArray[i + 1];
            if (mrgSortedLen > k) {
                tmpMrgSortedLen = k;
            }
            if (mrgArray[i + 1] > k) {
                tmpMrgArray = k;
            }
            uint16_t elementLengths[MRG_SORT_ELEMENT_LEN] = {
                static_cast<uint16_t>(tmpMrgSortedLen), static_cast<uint16_t>(tmpMrgArray), 0, 0};
            struct MrgSort4Info srcInfo(elementLengths, false, 0b0011, 1);
            struct MrgSortSrcList<T> srcList(src, src[mrgSortedLen * tilling.mrgSortTwoQueueSrc1Offset], src, src);
            MrgSort<T>(dst, srcList, srcInfo);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherDstValAndDstIdx(const LocalTensor<T> &dstValueLocal,
    const LocalTensor<int32_t> &dstIndexLocal, const LocalTensor<T> &tmpLocal, const TopkTiling &tilling,
    const int32_t dstIdx, const int32_t dstOffsetFourBytes, const int outterIdx)
{
    uint64_t rsvdCnt = 0;
    int32_t tmpLocalDstOffset = tilling.innerDataSize * dstIdx;
    if constexpr (sizeof(T) == sizeof(float)) {

        struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_STRIDE);
        GatherMask<T>(dstValueLocal[dstOffsetFourBytes], tmpLocal[tmpLocalDstOffset], REDUCEV2_MODE_ONE,
                      true, tilling.maskVreducev2FourBytes, reducev2Params, rsvdCnt);
    } else {
        int32_t dstOffsetTwoBytes = outterIdx * tilling.kAlignTwoBytes;

        struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_STRIDE);
        GatherMask<T>(dstValueLocal[dstOffsetTwoBytes], tmpLocal[tmpLocalDstOffset], REDUCEV2_MODE_THREE,
                      true, tilling.maskVreducev2TwoBytes, reducev2Params, rsvdCnt);
    }
    PipeBarrier<PIPE_V>();

    LocalTensor<float> tempBuffer = dstIndexLocal[dstOffsetFourBytes].template ReinterpretCast<float>();
    LocalTensor<float> tempBufferLocal = tmpLocal[tmpLocalDstOffset].template ReinterpretCast<float>();
    struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_STRIDE);
    GatherMask<float>(tempBuffer, tempBufferLocal, REDUCEV2_MODE_TWO, true,
                      tilling.maskVreducev2FourBytes, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
}

template <typename T, bool isInitIndex>
[aicore] __inline__ __attribute__((always_inline)) void TmpLocalSort32(const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal,
    const LocalTensor<T> &tmpLocal, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest,
    const int outterIdx, const UnaryRepeatParams unaryParams)
{


    int offset = outterIdx * topKInfo.inner;
    if (!isLargest) {
        SetVectorMask<T, MaskMode::COUNTER>(0, topKInfo.inner);
        Muls<T, false>(tmpLocal[tilling.innerDataSize], srcLocal[offset], T(-1), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        if constexpr (!isInitIndex) {
            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.srcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, tilling.sortRepeat);
        } else {
            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, tilling.sortRepeat);
        }
    } else {
        if constexpr (!isInitIndex) {
            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.srcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal[offset], tempBufferUint32, tilling.sortRepeat);
        } else {
            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal[offset], tempBufferUint32, tilling.sortRepeat);
        }
    }
    PipeBarrier<PIPE_V>();
    const DataCopyParams intriParams = {static_cast<uint16_t>(tilling.copyUbToUbBlockCount), 1, 0, 0};
    DataCopy(tmpLocal[tilling.innerDataSize], tmpLocal, intriParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isInitIndex, bool isHasfinish>
[aicore] __inline__ __attribute__((always_inline)) void TopKCompute(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<T> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest)
{
    const UnaryRepeatParams unaryParams;
    for (int j = 0; j < topKInfo.outter; ++j) {
        int32_t dstOffsetFourBytes = j * tilling.kAlignFourBytes;
        TmpLocalSort32<T, isInitIndex>(srcLocal, srcIndexLocal, tmpLocal, tilling, topKInfo, isLargest, j, unaryParams);

        int32_t mrgFourQueueCount = 0;
        uint16_t z = MIN_SORT32_SIZE;
        int32_t dstIdx = 0;
        MrgFourQueueSort<T>(tmpLocal, tilling, topKInfo, z, mrgFourQueueCount, dstIdx);
        MrgTwoQueueSort<T>(tmpLocal, tilling, topKInfo, z, mrgFourQueueCount, dstIdx, k);
        GatherDstValAndDstIdx(dstValueLocal, dstIndexLocal, tmpLocal, tilling, dstIdx, dstOffsetFourBytes, j);

        if constexpr (isHasfinish) {
            bool finishValue = finishLocal.GetValue(j);
            auto eventID = GetTPipePtr()->FetchEventID(HardEvent::S_V);
            SetFlag<HardEvent::S_V>(eventID);
            WaitFlag<HardEvent::S_V>(eventID);

            if (finishValue) {
                SetVectorMask<T, MaskMode::COUNTER>(0, k);
                Duplicate<int32_t, false>(dstIndexLocal[dstOffsetFourBytes],
                    static_cast<int32_t>(topKInfo.n),
                    MASK_PLACEHOLDER,
                    1,
                    1,
                    DEFAULT_REPEAT_STRIDE);
            }
        }
        PipeBarrier<PIPE_V>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void TopKNSmallGetFloatTopKValue(const LocalTensor<float> &dstValueLocal,
    const LocalTensor<int32_t> &dstIndexLocal, const LocalTensor<float> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo)
{
    LocalTensor<uint32_t> src1stackTensor = tmpLocal[tilling.topkMrgSrc1MaskSizeOffset].template
                                            ReinterpretCast<uint32_t>();
    auto eventID = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventID);
    WaitFlag<HardEvent::V_S>(eventID);
    src1stackTensor.SetSize(SRC1_STACK_TENSORSIZE);

    src1stackTensor.SetValue(0, tilling.vreduceValMask0);

    src1stackTensor.SetValue(1, tilling.vreduceValMask1);

    src1stackTensor.SetValue(EIGHT, tilling.vreduceIdxMask0);
    src1stackTensor.SetValue(NINE, tilling.vreduceIdxMask1);

    eventID = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventID);
    WaitFlag<HardEvent::S_V>(eventID);

    struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    uint64_t rsvdCnt = 0;
    GatherMask<float, uint32_t>(dstValueLocal, tmpLocal, src1stackTensor,
                                true, VREDUCEV2_FOUR_BYTE_MASK, reducev2Params, rsvdCnt);


    LocalTensor<float> tempBuffer = dstIndexLocal.template ReinterpretCast<float>();
    struct GatherMaskParams reducev2Params2(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    GatherMask<float, uint32_t>(tempBuffer, tmpLocal,
                                tmpLocal[tilling.topkMrgSrc1MaskSizeOffset + EIGHT].ReinterpretCast<uint32_t>(),
                                true, VREDUCEV2_FOUR_BYTE_MASK, reducev2Params2, rsvdCnt);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void TopKNSmallGetHalfTopKValue(const LocalTensor<__cce_half> &dstValueLocal,
    const LocalTensor<int32_t> &dstIndexLocal, const LocalTensor<__cce_half> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo)
{
    LocalTensor<uint16_t> src1stackTensor = tmpLocal[tilling.topkMrgSrc1MaskSizeOffset].template
                                            ReinterpretCast<uint16_t>();
    auto eventID = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventID);
    WaitFlag<HardEvent::V_S>(eventID);
    src1stackTensor.SetSize(EIGHT);
    src1stackTensor.SetValue(0, tilling.vreducehalfValMask0);
    src1stackTensor.SetValue(1, tilling.vreducehalfValMask1);
    src1stackTensor.SetValue(TWO, tilling.vreducehalfValMask2);
    src1stackTensor.SetValue(THREE, tilling.vreducehalfValMask3);
    src1stackTensor.SetValue(FOUR, tilling.vreducehalfValMask4);
    src1stackTensor.SetValue(FIVE, tilling.vreducehalfValMask5);
    src1stackTensor.SetValue(SIX, tilling.vreducehalfValMask6);
    src1stackTensor.SetValue(SEVEN, tilling.vreducehalfValMask7);
    LocalTensor<uint32_t> indexstackTensor = tmpLocal[tilling.topkMrgSrc1MaskSizeOffset + SRC1_STACK_VAL_OFFSET].
                                             template ReinterpretCast<uint32_t>();;
    indexstackTensor.SetSize(TWO);
    indexstackTensor.SetValue(0, tilling.vreduceIdxMask0);
    indexstackTensor.SetValue(1, tilling.vreduceIdxMask1);
    eventID = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventID);
    WaitFlag<HardEvent::S_V>(eventID);

    struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    uint64_t rsvdCnt = 0;
    GatherMask<__cce_half, uint16_t>(dstValueLocal, tmpLocal, src1stackTensor,
                               true, VREDUCEV2_HALF_MASK, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();

    LocalTensor<float> tempBufferIndex = dstIndexLocal.template ReinterpretCast<float>();
    LocalTensor<float> tempBufferLocal = tmpLocal.template ReinterpretCast<float>();
    struct GatherMaskParams reducev2Params2(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    GatherMask<float, uint32_t>(tempBufferIndex, tempBufferLocal,
        tempBufferLocal[(tilling.topkMrgSrc1MaskSizeOffset + SRC1_STACK_VAL_OFFSET) / TWO].ReinterpretCast<uint32_t>(),
        true, VREDUCEV2_FOUR_BYTE_MASK, reducev2Params2, rsvdCnt);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isInitIndex, bool isHasfinish>
[aicore] __inline__ __attribute__((always_inline)) void TopKNSmallCompute(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<T> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest)
{
    if (!isLargest) {
        if (!isInitIndex) {

            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.topkNSmallSrcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, topKInfo.outter);
        } else {

            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, topKInfo.outter);
        }
    } else {
        if (!isInitIndex) {

            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.topkNSmallSrcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal, tempBufferUint32, topKInfo.outter);
        } else {

            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal, tempBufferUint32, topKInfo.outter);
        }
    }
    PipeBarrier<PIPE_V>();

    if constexpr (sizeof(T) == sizeof(float)) {
        TopKNSmallGetFloatTopKValue(dstValueLocal, dstIndexLocal, tmpLocal, tilling, topKInfo);
    } else {
        TopKNSmallGetHalfTopKValue(dstValueLocal, dstIndexLocal, tmpLocal, tilling, topKInfo);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CopyData(LocalTensor<int32_t> indexLocalTmp, const TopKInfo &topKInfo)
{
    Copy(indexLocalTmp[topKInfo.inner], indexLocalTmp, THIRTY_TWO, topKInfo.outter - 1, {1, 1, FOUR, 0});
    PipeBarrier<PIPE_V>();
}

}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_impl.h" 2





namespace AscendC {
template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] __inline__ __attribute__((always_inline)) void TopKNormal(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<uint8_t> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest = true)
{
    LocalTensor<T> tempBuffer = tmpLocal.template ReinterpretCast<T>();

    if constexpr (!isInitIndex) {
        LocalTensor<int32_t> indexLocalTmp = tempBuffer[tilling.srcIndexOffset].template ReinterpretCast<int32_t>();
        ArithProgression(indexLocalTmp, static_cast<int32_t>(0), static_cast<int32_t>(1), topKInfo.inner);
        PipeBarrier<PIPE_V>();
    }

    SetMaskCount();
    TopKCompute<T, isInitIndex, isHasfinish>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal, finishLocal,
        tempBuffer, k, tilling, topKInfo, isLargest);

    if (!isLargest) {
        const UnaryRepeatParams unaryParams;
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.maskOffset);
        Muls<T, false>(dstValueLocal, dstValueLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] __inline__ __attribute__((always_inline)) void TopKNormal(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest = true)
{
    LocalTensor<uint8_t> stackTensor;
    PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);


                                             ;
    stackTensor.SetSize(tilling.tmpLocalSize * sizeof(T));
    TopKNormal<T, isInitIndex, isHasfinish, isReuseSrc>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal,
        finishLocal, stackTensor, k, tilling, topKInfo, isLargest);
}

template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] __inline__ __attribute__((always_inline)) void TopKNSmall(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<uint8_t> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest = true)
{
    LocalTensor<T> tempBuffer = tmpLocal.template ReinterpretCast<T>();

    if constexpr (!isInitIndex) {
        LocalTensor<int32_t> indexLocalTmp = tempBuffer[tilling.topkNSmallSrcIndexOffset].template
                                             ReinterpretCast<int32_t>();
        ArithProgression(indexLocalTmp, static_cast<int32_t>(0), static_cast<int32_t>(1), topKInfo.inner);
        PipeBarrier<PIPE_V>();
        if (topKInfo.outter > 1) {
            CopyData(indexLocalTmp, topKInfo);
        }
    }

    SetMaskCount();
    const UnaryRepeatParams unaryParams;

    if (!isLargest) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.allDataSize);
        Muls<T, false>(tempBuffer[tilling.innerDataSize], srcLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }
    TopKNSmallCompute<T, isInitIndex, isHasfinish>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal, finishLocal,
        tempBuffer, k, tilling, topKInfo, isLargest);

    if (!isLargest) {
        PipeBarrier<PIPE_V>();
        SetMaskCount();
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.maskOffset);
        Muls<T, false>(dstValueLocal, dstValueLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] __inline__ __attribute__((always_inline)) void TopKNSmall(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest = true)
{
    LocalTensor<uint8_t> stackTensor;
    PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);


                                             ;
    stackTensor.SetSize(tilling.tmpLocalSize * sizeof(T));

    TopKNSmall<T, isInitIndex, isHasfinish, isReuseSrc>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal,
        finishLocal, stackTensor, k, tilling, topKInfo, isLargest);
}

}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h" 2







namespace AscendC {
#pragma begin_pipe(V)
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h"
template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false,
    enum TopKMode topkMode = TopKMode::TOPK_NORMAL>
[aicore] __inline__ __attribute__((always_inline)) void TopK(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<uint8_t> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest = true)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }





    if constexpr (topkMode == TopKMode::TOPK_NORMAL) {
        TopKNormal<T, isInitIndex, isHasfinish, isReuseSrc>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal,
            finishLocal, tmpLocal, k, tilling, topKInfo, isLargest);
    }
    if constexpr (topkMode == TopKMode::TOPK_NSMALL) {
        TopKNSmall<T, isInitIndex, isHasfinish, isReuseSrc>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal,
            finishLocal, tmpLocal, k, tilling, topKInfo, isLargest);
    }
}
# 114 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h"
template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false,
    enum TopKMode topkMode = TopKMode::TOPK_NORMAL>
[aicore] __inline__ __attribute__((always_inline)) void TopK(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest = true)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }





    if constexpr (topkMode == TopKMode::TOPK_NORMAL) {
        TopKNormal<T, isInitIndex, isHasfinish, isReuseSrc>(
            dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal, finishLocal, k, tilling, topKInfo, isLargest);
    }
    if constexpr (topkMode == TopKMode::TOPK_NSMALL) {
        TopKNSmall<T, isInitIndex, isHasfinish, isReuseSrc>(
            dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal, finishLocal, k, tilling, topKInfo, isLargest);
    }
}
#pragma end_pipe
}
# 83 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/geglu/geglu_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/geglu/geglu_common_impl.h"
namespace AscendC {
constexpr float COEFF0 = -0.0713548162726;
constexpr float COEFF1 = 2.2363860002236e1;
constexpr uint32_t GEGLU_HALF_BUFFER_SIZE = 8;
constexpr uint32_t GEGLU_FLOAT_BUFFER_SIZE = 0;
constexpr uint32_t GEGLU_STRIDE_DIGITS = 2;
constexpr uint32_t GEGLU_ALGINED = 31;

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLUImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }
    LocalTensor<uint8_t> tmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
    GeGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, tmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLUImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const LocalTensor<uint8_t> &sharedTmpBuffer, uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/geglu/geglu_common_impl.h"
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));
    SetMaskCount();

    if (sizeof(T) == sizeof(__cce_half)) {
        auto tmpBufCount = sharedTmpBuffer.GetSize() / GEGLU_HALF_BUFFER_SIZE;
        tmpBufCount = tmpBufCount * sizeof(T) / ONE_BLK_SIZE * ONE_BLK_SIZE / sizeof(T);
        for (uint32_t offset = 0; offset < calCount; offset += tmpBufCount) {
            auto splitSize = (calCount - offset) > tmpBufCount ? tmpBufCount : (calCount - offset);
            SetVectorMask<T>(0, splitSize);
            splitSize = (splitSize * sizeof(T) + GEGLU_ALGINED) / ONE_BLK_SIZE * ONE_BLK_SIZE / sizeof(T);
            GeGLUCompute(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], tmpBuffer, splitSize);
        }
    } else {
        SetVectorMask<T>(0, calCount);
        GeGLUCompute(dstTensor, srcTensor0, srcTensor1, tmpBuffer, calCount);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GeGLUCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src0, const LocalTensor<T> &src1,
    const LocalTensor<float> &tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;


    Mul<T, false>(dst, src1, src1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(dst, dst, static_cast<T>(COEFF1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(dst, dst, src1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<T, false>(dst, dst, static_cast<T>(COEFF0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Exp<T, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(dst, dst, static_cast<T>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Div<T, false>(dst, src1, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(dst, src0, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}



template <>
[aicore] __inline__ __attribute__((always_inline)) void GeGLUCompute(const LocalTensor<__cce_half> &dst, const LocalTensor<__cce_half> &src0,
    const LocalTensor<__cce_half> &src1, const LocalTensor<float> &tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer2 = tmpBuffer[calSize];

    Cast<float, __cce_half, false>(tmpFloatBuffer1, src1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / GEGLU_STRIDE_DIGITS});
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, COEFF1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, COEFF0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Exp<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, __cce_half, false>(tmpFloatBuffer1, src0, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / GEGLU_STRIDE_DIGITS});
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<__cce_half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / GEGLU_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const LocalTensor<uint8_t> &sharedTmpBuffer, uint32_t calCount)
{
    GeGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
}
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }

    GeGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, calCount);
}
# 68 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    GeGLU<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, srcTensor0.GetSize());
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1)
{
    GeGLU<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, srcTensor0.GetSize());
}
#pragma end_pipe
}
# 84 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/lgamma.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/lgamma.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_utils.h"
namespace AscendC {
namespace {
constexpr float f05 = 0.5;
constexpr float f07 = 0.7;
constexpr float f15 = 1.5;
constexpr float f58 = 5.8;
constexpr float f1 = 1.0;
constexpr float f2 = 2.0;
constexpr float f3 = 3.0;
constexpr float fn05 = -0.5;
constexpr float fn1 = -1.0;
constexpr float fn2 = -2.0;
constexpr float fn3 = -3.0;
constexpr float fPi = 3.1415927410125732421875;
constexpr uint32_t FLOAT_NOREUSE_CALC_PROC = 8;
constexpr uint32_t FLOAT_REUSE_CALC_PROC = 7;
constexpr uint32_t LGAMMA_HALF_CALC_PROCEDURE = 13;
constexpr uint32_t i2 = 2;
constexpr uint32_t i4 = 4;
constexpr uint32_t i6 = 6;
constexpr uint32_t i7 = 7;
constexpr uint32_t i16 = 16;
constexpr float PI = 3.14159265358979323846264338327950288;
constexpr float t0 = 0.0f;
constexpr float t4 = 4.0f;
constexpr float t5 = 5.0f;
constexpr float t01 = 0.1f;
constexpr float t12 = 12.0f;
constexpr float N01 = -0.1f;

constexpr size_t params007Len = 7U;
constexpr float params007[params007Len] = {0.00358751555905,
    -0.00547128543258,
    -0.0446271263063,
    0.167317703366,
    -0.0421359799802,
    -0.655867278576,
    0.577215373516};

constexpr size_t params0715Len = 11U;
constexpr float params0715[params0715Len] = {0.0458826646209,
    0.103739671409,
    0.122803635895,
    0.127524212003,
    0.143216684461,
    0.169343575835,
    0.207407936454,
    0.27058750391,
    0.400685429573,
    0.82246696949,
    0.577215671539};

constexpr size_t params153Len = 10U;
constexpr float params153[params153Len] = {4.95984932058e-05,
    -0.000220894842641,
    0.000541314249858,
    -0.00120451697148,
    0.00288425176404,
    -0.00738275796175,
    0.0205813199282,
    -0.067352488637,
    0.322467029095,
    0.422784328461};

constexpr size_t params378X1Len = 4U;
constexpr size_t params378X2Len = 3U;
constexpr float params378X1[params378X1Len] = {-748.890319824, -12349.7421875, -41061.375, -48310.6640625};
constexpr float params378X2[params378X2Len] = {-259.250976562, -10777.1796875, -92685.046875};

constexpr size_t params58Len = 2U;
constexpr float params58[params58Len] = {0.000777830660809, -0.00277765537612};

constexpr size_t negParamsOddLen = 4U;
constexpr float negParamsOdd[negParamsOddLen] = {0.00002427957952022552490234375,
    -0.001388786011375486850738525390625,
    0.0416667275130748748779296875,
    -0.4999999701976776123046875};
constexpr size_t negParamsEvenLen = 3U;
constexpr float negParamsEven[negParamsEvenLen] = {
    -0.000195746586541645228862762451171875, 0.0083327032625675201416015625, -0.16666662693023681640625};
}

struct LGammaFParams {
    [aicore] LGammaFParams()
    {}
    LocalTensor<float> tmp1;
    LocalTensor<float> tmp2;
    LocalTensor<float> tmp3;
    LocalTensor<float> tmp4;
    LocalTensor<float> tmp5;
    LocalTensor<float> tmp6;
    LocalTensor<float> tmpScalar;
    LocalTensor<uint8_t> mask;
    LocalTensor<uint8_t> tmpMask1;
    LocalTensor<uint8_t> tmpMask2;
    LocalTensor<uint8_t> tmpMask3;
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    uint32_t splitSize;
};

}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_basic_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_basic_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_v220_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void LGammaFloor(const LocalTensor<float> &dst, const LocalTensor<float> &src)
{
    Cast<float, float, false>(
        dst, src, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_basic_impl.h" 2




namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void LGammaCalcMulAdd(const LocalTensor<float> &tmp, const LocalTensor<float> &src,
    const UnaryRepeatParams &unaryParams, const BinaryRepeatParams binaryParams, const float params[],
    const size_t paramLen)
{

    Muls<float, false>(tmp, src, params[0], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmp, tmp, params[1], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();



    for (size_t i = 2U; i < paramLen && i < params0715Len - 1U; ++i) {

        Mul<float, false>(tmp, tmp, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
        Adds<float, false>(tmp, tmp, params[i], MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }

    if (paramLen == params0715Len) {
        Mul<float, false>(tmp, tmp, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
        Adds<float, false>(tmp, tmp, params[params0715Len - 1U], MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }


    Mul<float, false>(tmp, tmp, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGamma007(const LocalTensor<float> &src, const LGammaFParams &params)
{

    LGammaCalcMulAdd(params.tmp1, src, params.unaryParams, params.binaryParams, params007, params007Len);


    Mul<float, false>(params.tmp1, params.tmp1, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp1, params.tmp1, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(params.tmp1, params.tmp1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGamma0715(const LocalTensor<float> &src, const LGammaFParams &params)
{

    Muls<float, false>(params.tmp1, src, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.tmp1, params.tmp1, f1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp2, params.tmp1, params.unaryParams, params.binaryParams, params0715, params0715Len);
}


[aicore] __inline__ __attribute__((always_inline)) void LGamma153(const LocalTensor<float> &src, const LGammaFParams &params)
{

    Adds<float, false>(params.tmp1, src, fn2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp2, params.tmp1, params.unaryParams, params.binaryParams, params153, params153Len);
}


[aicore] __inline__ __attribute__((always_inline)) void LGamma358(const LocalTensor<float> &src, const LGammaFParams &params)
{

    Adds<float, false>(params.tmp1, src, fn3, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp2, params.tmp1, params.unaryParams, params.binaryParams, params378X1, params378X1Len);

    constexpr float ftmp2 = -143033.40625;
    Adds<float, false>(params.tmp2, params.tmp2, ftmp2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp3, params.tmp1, params.unaryParams, params.binaryParams, params378X2, params378X2Len);

    constexpr float ftmp3 = -206353.578125;
    Adds<float, false>(params.tmp3, params.tmp3, ftmp3, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(params.tmp3, params.tmp2, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp3, params.tmp3, params.tmp1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGamma58(const LocalTensor<float> &src, const LGammaFParams &params)
{

    Ln<float, false>(params.tmp1, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, f05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(params.tmp2, src, fn05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(params.tmp2, params.tmp2, params.tmp1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Reciprocal<float, false>(params.tmp3, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmp4, params.tmp3, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp1, params.tmp4, params.unaryParams, params.binaryParams, params58, params58Len);
    constexpr float ftmp1 = 0.0833332762122;
    Adds<float, false>(params.tmp1, params.tmp1, ftmp1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(params.tmp1, params.tmp1, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    constexpr float ftmp2 = 0.91893851757;
    Adds<float, false>(params.tmp1, params.tmp1, ftmp2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(params.tmp1, params.tmp1, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaGenLTMask(
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src, const LGammaFParams &params, const float scalar)
{
    Duplicate<float, false>(params.tmp1, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, params.tmp1, CMPMODE::LT, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaGenGEMask(
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src, const LGammaFParams &params, const float scalar)
{
    Duplicate<float, false>(params.tmp1, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, params.tmp1, CMPMODE::GE, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaGenRangeMask(
    const LocalTensor<float> &src, const LGammaFParams &params, const float min, const float max)
{
    LGammaGenLTMask(params.mask, src, params, max);
    LGammaGenGEMask(params.tmpMask1, src, params, min);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(),
        params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    SetVectorMask<float>(0, params.splitSize);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaSelect(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const LocalTensor<uint8_t> &mask, const LGammaFParams &params)
{
    SetCmpMask<float>(params.tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(params.tmp1, mask, src, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(dst, params.tmp1, dst, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaPositive(const LGammaFParams &params)
{
    Duplicate<float, false>(params.tmp5, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();


    LGammaGenLTMask(params.mask, params.tmp6, params, f07);
    LGamma007(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp1, params.mask, params);


    LGammaGenRangeMask(params.tmp6, params, f07, f15);
    LGamma0715(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp2, params.mask, params);


    LGammaGenRangeMask(params.tmp6, params, f15, f3);
    LGamma153(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp2, params.mask, params);


    LGammaGenRangeMask(params.tmp6, params, f3, f58);
    LGamma358(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp3, params.mask, params);


    NotNumUnion notNum;
    notNum.i = F32_INF;
    LGammaGenRangeMask(params.tmp6, params, f58, notNum.f);
    LGamma58(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp1, params.mask, params);


    LGammaGenGEMask(params.mask, params.tmp6, params, notNum.f);
    LGammaSelect(params.tmp5, params.tmp6, params.mask, params);
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaCalNegTmp1(const LGammaFParams &params)
{

    Add<float, false>(params.tmp2, params.tmp6, params.tmp6, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.tmp2, params.tmp2, f05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    LGammaFloor(params.tmp2, params.tmp2);


    Muls<float, false>(params.tmp3, params.tmp2, f05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    LGammaFloor(params.tmp3, params.tmp3);
    Muls<float, false>(params.tmp3, params.tmp3, f2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(params.tmp3, params.tmp2, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    LGammaGenGEMask(params.mask, params.tmp3, params, f05);
    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));

    Not<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    SetVectorMask<float>(0, params.splitSize);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tmp2, params.tmp2, fn05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp2, params.tmp2, params.tmp6, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp2, params.tmp2, fPi, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaCalNegTmp2(const LGammaFParams &params)
{

    Mul<float, false>(params.tmp3, params.tmp2, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(
        params.tmp1, params.tmp3, params.unaryParams, params.binaryParams, negParamsEven, negParamsEvenLen);
    Mul<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmp2, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LGammaSelect(params.tmp2, params.tmp1, params.tmpMask1, params);


    LGammaCalcMulAdd(params.tmp1, params.tmp3, params.unaryParams, params.binaryParams, negParamsOdd, negParamsOddLen);
    Adds<float, false>(params.tmp1, params.tmp1, f1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    LGammaSelect(params.tmp2, params.tmp1, params.mask, params);
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaCalNegTmp3(const LGammaFParams &params)
{

    Abs<float, false>(params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(params.tmp3, params.tmp1, params.tmp6, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(params.tmp1, params.tmp3, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    constexpr float ftmp = 1.1447298526763916015625;
    Adds<float, false>(params.tmp1, params.tmp1, ftmp, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp2, params.tmp5, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp2, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaCalMinNeg(const LocalTensor<float> &src, const LGammaFParams &params)
{

    Ln<float, false>(params.tmp1, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaNegative(const LGammaFParams &params)
{
    Duplicate<float, false>(params.tmp4, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();


    constexpr float minf = 9.99999968266e-20;
    LGammaGenLTMask(params.mask, params.tmp6, params, minf);
    LGammaCalMinNeg(params.tmp6, params);
    LGammaSelect(params.tmp4, params.tmp1, params.mask, params);


    LGammaCalNegTmp1(params);

    LGammaCalNegTmp2(params);

    LGammaCalNegTmp3(params);


    LGammaFloor(params.tmp1, params.tmp6);


    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(
        params.mask, params.tmp1, params.tmp6, CMPMODE::NE, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();


    NotNumUnion notNum;
    notNum.i = F32_INF;
    Duplicate<float, false>(params.tmp1, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Compare<float, uint8_t, false>(
        params.tmpMask1, params.tmp3, params.tmp1, CMPMODE::LT, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();


    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(), params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);


    LGammaGenGEMask(params.tmpMask1, params.tmp6, params, minf);
    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(), params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
    LGammaSelect(params.tmp4, params.tmp2, params.mask, params);


    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(), params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);


    LGammaGenGEMask(params.mask, params.tmp6, params, minf);
    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(), params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);

    Duplicate<float, false>(params.tmp2, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LGammaSelect(params.tmp4, params.tmp2, params.tmpMask1, params);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LGammaInitFParams(
    const LocalTensor<float> &tmp, const uint32_t splitSize, const LocalTensor<float> &src, LGammaFParams &params)
{
    params.tmp1 = tmp;
    params.tmp2 = tmp[splitSize];
    params.tmp3 = params.tmp2[splitSize];
    params.tmp4 = params.tmp3[splitSize];
    params.tmp5 = params.tmp4[splitSize];
    params.tmp6 = params.tmp5[splitSize];
    if constexpr (isReuseSource) {
        params.mask = params.tmp6[splitSize].ReinterpretCast<uint8_t>();
        params.tmpScalar = src.ReinterpretCast<float>();
    } else {
        params.tmpScalar = params.tmp6[splitSize];
        params.mask = params.tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    }
    params.tmpMask1 = params.mask[splitSize];
    params.tmpMask2 = params.tmpMask1[splitSize];
    params.tmpMask3 = params.tmpMask2[splitSize];

    params.tmp1.SetSize(splitSize);
    params.tmp2.SetSize(splitSize);
    params.tmp3.SetSize(splitSize);
    params.tmp4.SetSize(splitSize);
    params.tmp5.SetSize(splitSize);
    params.tmp6.SetSize(splitSize);
    params.mask.SetSize(splitSize);
    params.tmpMask1.SetSize(splitSize);
    params.tmpMask2.SetSize(splitSize);
    params.tmpMask3.SetSize(splitSize);
    params.tmpScalar.SetSize(splitSize);

    params.splitSize = splitSize;
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_impl.h" 2



namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void Lgamma1Compute(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    LocalTensor<float> tmp1Tensor = tmpTensor;
    LocalTensor<float> tmp2Tensor = tmp1Tensor[splitSize];
    LocalTensor<float> tmp3Tensor = tmp2Tensor[splitSize];
    LocalTensor<float> tmp4Tensor = tmp3Tensor[splitSize];
    tmp1Tensor.SetSize(splitSize);
    tmp2Tensor.SetSize(splitSize);
    tmp3Tensor.SetSize(splitSize);
    tmp4Tensor.SetSize(splitSize);

    Adds<float, false>(tmp1Tensor, srcTensor, t4, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(dstTensor, f1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(dstTensor, dstTensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmp2Tensor, dstTensor, PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmp2Tensor, tmp2Tensor, f2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmp2Tensor, tmp2Tensor, f05, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();



    Muls<float, false>(tmp3Tensor, dstTensor, N01, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmp4Tensor, tmp1Tensor, t12, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmp4Tensor, tmp4Tensor, tmp3Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(dstTensor, f1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(tmp4Tensor, dstTensor, tmp4Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(tmp4Tensor, tmp4Tensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(tmp4Tensor, tmp4Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmp4Tensor, tmp4Tensor, fn1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmp4Tensor, tmp4Tensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(dstTensor, tmp4Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LgammaComputePosHalf(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    LocalTensor<float> tmp1Tensor = tmpTensor;
    LocalTensor<float> tmp2Tensor = tmpTensor[splitSize];
    LocalTensor<float> tmp3Tensor = tmpTensor[splitSize * 2];
    LocalTensor<float> tmp4Tensor = tmpTensor[splitSize * 3];

    tmp1Tensor.SetSize(splitSize);
    tmp2Tensor.SetSize(splitSize);
    tmp3Tensor.SetSize(splitSize);
    tmp4Tensor.SetSize(splitSize);


    Lgamma1Compute(dstTensor, srcTensor, tmpTensor, splitSize);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmp3Tensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmp2Tensor, srcTensor, f1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmp3Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmp2Tensor, srcTensor, f2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmp3Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmp2Tensor, srcTensor, f3, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmp3Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dstTensor, dstTensor, tmp3Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LgammaComputeNegHalf(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    LocalTensor<float> tmp1Tensor = tmpTensor;
    LocalTensor<float> tmp2Tensor = tmp1Tensor[splitSize];
    LocalTensor<float> tmp3Tensor = tmp2Tensor[splitSize];
    LocalTensor<float> tmp4Tensor = tmp3Tensor[splitSize];
    LocalTensor<float> tmp5Tensor = tmp4Tensor[splitSize];
    LocalTensor<float> tmp6Tensor = tmp5Tensor[splitSize];
    LocalTensor<float> tmp7Tensor = tmpTensor[splitSize * i2];
    tmp1Tensor.SetSize(splitSize);
    tmp2Tensor.SetSize(splitSize);
    tmp3Tensor.SetSize(splitSize);
    tmp4Tensor.SetSize(splitSize);
    tmp5Tensor.SetSize(splitSize);
    tmp6Tensor.SetSize(splitSize);
    tmp7Tensor.SetSize(splitSize * i4);


    Muls<float, false>(tmp1Tensor, srcTensor, fn1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmp1Tensor, tmp1Tensor, f1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LgammaComputePosHalf(dstTensor, tmp1Tensor, tmp7Tensor, splitSize);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, dstTensor, fn1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaFloor(tmp1Tensor, srcTensor);


    Sub<float, false>(tmp1Tensor, srcTensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmp1Tensor, tmp1Tensor, PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    SinCompute(tmp2Tensor, tmp1Tensor, tmp7Tensor, splitSize, false);
    PipeBarrier<PIPE_V>();

    Abs<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(tmp3Tensor, PI, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    Div<float, false>(tmp2Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, dstTensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaGenLTMaskHalf(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
    const LocalTensor<float> &tmptensor, const float scalar, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Duplicate<float, false>(tmptensor, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, tmptensor, CMPMODE::LT, MASK_PLACEHOLDER, repeat, binParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaGenGEMaskHalf(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
    const LocalTensor<float> &tmptensor, const float scalar, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Duplicate<float, false>(tmptensor, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, tmptensor, CMPMODE::GE, MASK_PLACEHOLDER, repeat, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LGammaSelectHalf(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &tmpTensor, const LocalTensor<float> &tmpScalar)
{
    const BinaryRepeatParams binParams;
    SetCmpMask<float>(tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(tmpTensor, mask, srcTensor, 1, binParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(dstTensor, tmpTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LGammaSelectINF(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &tmpTensor, const LocalTensor<float> &tmpScalar)
{
    const BinaryRepeatParams binParams;
    Duplicate<float, false>(tmpScalar, 655040.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetCmpMask<float>(tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(dstTensor, mask, srcTensor, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LgammaComputeImpl(const LocalTensor<__cce_half> &dstTensor, const LocalTensor<__cce_half> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t &splitSize, bool isReuseSource)
{
    (void)isReuseSource;
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;


    LocalTensor<float> restmpBuffer = tmpTensor;
    LocalTensor<float> srctmpBuffer = restmpBuffer[splitSize];
    Duplicate<float, false>(restmpBuffer, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Cast<float, __cce_half, false>(srctmpBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();

    LocalTensor<float> TensorPosRes = srctmpBuffer[splitSize];

    LocalTensor<float> TensorNegRes = TensorPosRes[splitSize];
    LocalTensor<float> tmp1Tensor = TensorNegRes[splitSize];

    LocalTensor<float> tmpScalar = tmp1Tensor[splitSize];
    Duplicate<float, false>(tmpScalar, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LocalTensor<uint8_t> MaskPos = tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    LocalTensor<uint8_t> MaskNeg = MaskPos[splitSize];
    LocalTensor<uint8_t> tmpMask1 = MaskNeg[splitSize];
    LocalTensor<uint8_t> tmpMask2 = tmpMask1[splitSize];
    LocalTensor<float> stackTensor = tmpScalar[splitSize*i2];

    restmpBuffer.SetSize(splitSize);
    srctmpBuffer.SetSize(splitSize);
    TensorPosRes.SetSize(splitSize);
    TensorNegRes.SetSize(splitSize);
    tmp1Tensor.SetSize(splitSize);
    tmpScalar.SetSize(splitSize);
    MaskNeg.SetSize(splitSize);
    MaskPos.SetSize(splitSize);
    tmpMask1.SetSize(splitSize);
    tmpMask2.SetSize(splitSize);
    stackTensor.SetSize(splitSize * i6);


    LgammaComputePosHalf(TensorPosRes, srctmpBuffer, stackTensor, splitSize);
    PipeBarrier<PIPE_V>();

    LGammaGenGEMaskHalf(MaskPos, srctmpBuffer, tmp1Tensor, 0.0f, splitSize);
    PipeBarrier<PIPE_V>();
    LGammaSelectHalf(restmpBuffer, TensorPosRes, MaskPos, tmp1Tensor, tmpScalar);
    PipeBarrier<PIPE_V>();


    LgammaComputeNegHalf(TensorNegRes, srctmpBuffer, stackTensor, splitSize);
    PipeBarrier<PIPE_V>();

    LGammaGenLTMaskHalf(MaskNeg, srctmpBuffer, tmp1Tensor, 0.0f, splitSize);
    PipeBarrier<PIPE_V>();
    LGammaSelectHalf(restmpBuffer, TensorNegRes, MaskNeg, tmp1Tensor, tmpScalar);
    PipeBarrier<PIPE_V>();


    SetVectorMask<float>(0, ConstCeil(splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(tmpMask1.ReinterpretCast<uint16_t>(), MaskPos.ReinterpretCast<uint16_t>(),
                         MASK_PLACEHOLDER, 1, unaryParams);
    Not<uint16_t, false>(tmpMask2.ReinterpretCast<uint16_t>(), MaskNeg.ReinterpretCast<uint16_t>(),
                         MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(tmpMask1.ReinterpretCast<uint16_t>(), tmpMask1.ReinterpretCast<uint16_t>(),
        tmpMask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binParams);

    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, splitSize);
    LGammaSelectHalf(restmpBuffer, srctmpBuffer, tmpMask1, tmp1Tensor, tmpScalar);
    PipeBarrier<PIPE_V>();


    Abs<float, false>(srctmpBuffer, srctmpBuffer, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LGammaGenGEMaskHalf(tmpMask1, srctmpBuffer, tmp1Tensor, 65504.0f, splitSize);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(tmpMask2.ReinterpretCast<uint16_t>(), tmpMask1.ReinterpretCast<uint16_t>(),
                         MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, splitSize);
    LGammaSelectINF(restmpBuffer, restmpBuffer, tmpMask2, tmp1Tensor, tmpScalar);
    PipeBarrier<PIPE_V>();


    Cast<__cce_half, float, false>(dstTensor, restmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
}

[aicore] __inline__ __attribute__((always_inline)) void LgammaComputeImpl(
    const LocalTensor<float> &dst, const LocalTensor<float> &src, LGammaFParams &params)
{

    LGammaGenGEMask(params.tmpMask2, src, params, 0.0f);
    LGammaGenLTMask(params.tmpMask3, src, params, 0.0f);



    Abs<float, false>(params.tmp6, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(params.tmpScalar, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();


    LGammaPositive(params);
    Duplicate<float, false>(dst, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LGammaSelect(dst, params.tmp5, params.tmpMask2, params);


    LGammaNegative(params);
    LGammaSelect(dst, params.tmp4, params.tmpMask3, params);


    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(),
        params.tmpMask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    Not<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.tmpMask3.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(),
        params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
    LGammaSelect(dst, params.tmp6, params.mask, params);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LgammaCompute(const LocalTensor<__cce_half> &dstTensor, const LocalTensor<__cce_half> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Lgamma");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Lgamma");

    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t tmpBufferSize = bufferSize / sizeof(float);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackSize = 0;

    stackSize = tmpBufferSize / LGAMMA_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(stackSize, 0, bufferSize);

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;
    SetMaskCount();
    SetVectorMask<__cce_half, MaskMode::COUNTER>(0, stackSize);
    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        LgammaComputeImpl(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize, isReuseSource);
        offset = offset + stackSize;
    }

    if (tail > 0) {
        SetVectorMask<__cce_half, MaskMode::COUNTER>(0, tail);
        LgammaComputeImpl(
            dstTensor[round * stackSize], srcTensor[round * stackSize], tmpBuffer, stackSize, isReuseSource);
    }
    SetMaskNorm();
    AscendCUtils::ResetMask();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LgammaCompute(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const LocalTensor<uint8_t> &tmp, const uint32_t calCount)
{
    CheckTensorPosition(dst, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(src, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(tmp, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", src, "srcTensor", "Lgamma");
    CheckCalCount(calCount, "calCount", dst, "dstTensor", "Lgamma");

    LocalTensor<float> tmpBuffer = tmp.ReinterpretCast<float>();
    uint32_t tmpBufferSize = tmpBuffer.GetSize();
    uint32_t splitSize = tmpBufferSize;
    if constexpr (isReuseSource) {
        splitSize = splitSize / FLOAT_REUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitSize = splitSize / FLOAT_NOREUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);


    LGammaFParams params;
    LGammaInitFParams<isReuseSource>(tmpBuffer, splitSize, src, params);

    const uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetMaskCount();
    SetVectorMask<float>(0, splitSize);
    for (uint32_t i = 0U; i < loopCount; ++i) {
        LgammaComputeImpl(dst[i * splitSize], src[i * splitSize], params);
    }
    if (calcTail > 0) {
        calcTail = (calcTail + ONE_BYTE_BIT_SIZE - 1U) / ONE_BYTE_BIT_SIZE * ONE_BYTE_BIT_SIZE;
        SetVectorMask<float>(0, calcTail);
        params.splitSize = calcTail;
        LgammaComputeImpl(dst[loopCount * splitSize], src[loopCount * splitSize], params);
    }
    SetMaskNorm();
    ResetMask();
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/lgamma.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/lgamma.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Lgamma(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LgammaCompute<isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/lgamma.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Lgamma(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> tmp;
    const bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(tmp);
                                                                                 ;
    LgammaCompute<isReuseSource>(dstTensor, srcTensor, tmp, calCount);
}
#pragma end_pipe
}
# 85 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/digamma.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/digamma.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_basic_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_basic_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_v220_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void DigammaCast(const LocalTensor<float> &dst, const LocalTensor<float> &src, RoundMode castType)
{
    Cast<float, float, false>(dst, src, castType, MASK_PLACEHOLDER, 1,
                              {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_basic_impl.h" 2





namespace AscendC {
namespace {
constexpr float MIN_NEG_WITH_FLOAT = -8388608.0;
constexpr float DIGAMMA_PI = 3.141592653589793238f;
constexpr float DIGAMMA_NEG_PI = -3.141592653589793238f;
constexpr uint32_t DIGAMMA_FLOAT_NOREUSE_CALC_PROC = 7;
constexpr uint32_t DIGAMMA_FLOAT_REUSE_CALC_PROC = 6;
constexpr uint32_t DIGAMMA_HALF_CALC_PROC = 8;
constexpr size_t DIGAMMA_MAX_LOOP = 5;

constexpr float posCalcConst[] = {2.10927960927960927961e-2, 7.57575757575757575758e-3, 4.16666666666666666667e-3,
                                  3.96825396825396825397e-3, 8.33333333333333333333e-3, 8.33333333333333333333e-2};
constexpr float tmp1CalcConst[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0};
constexpr float tmp1HalfCalcConst[] = {1.0, 2.0};
constexpr float picotCalcConst[] = {0.00326538085938f, 0.0242919921875f, 0.053466796875f,
                                    0.133377909660f, 0.333332300186f};
}

struct DigammaParams {
    [aicore] DigammaParams() {}
    LocalTensor<float> result;
    LocalTensor<float> tmpCal1;
    LocalTensor<float> tmpCal2;
    LocalTensor<float> tmpCal3;
    LocalTensor<float> tmpCal4;
    LocalTensor<float> tmpCal5;
    LocalTensor<float> tmpScalar;
    LocalTensor<uint8_t> mask;
    LocalTensor<uint8_t> mask1;
    LocalTensor<uint8_t> mask2;
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    uint32_t splitSize;
};
#pragma begin_pipe(V)

[aicore] __inline__ __attribute__((always_inline)) void DigammaGenCompareMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
                                             DigammaParams &params, const float scalar, CMPMODE cmpMode)
{
    Duplicate<float, false>(params.tmpScalar, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, params.tmpScalar, cmpMode,
                                   MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaGenNegIntMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
    DigammaParams &params, const float scalar)
{

    DigammaGenCompareMask(params.mask1, src, params, 0.0f, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, src, params, MIN_NEG_WITH_FLOAT, CMPMODE::GT);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask1.ReinterpretCast<uint16_t>(), params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);

    DigammaCast(params.tmpCal1, src, RoundMode::CAST_ROUND);
    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(params.mask2, src, params.tmpCal1, CMPMODE::EQ, MASK_PLACEHOLDER, repeat,
        params.binaryParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(mask.ReinterpretCast<uint16_t>(), params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaGenRangeMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
    DigammaParams &params, const float min, const float max)
{

    DigammaGenCompareMask(params.mask1, src, params, max, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, src, params, min, CMPMODE::GE);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(mask.ReinterpretCast<uint16_t>(), params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaGenNanMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
                                         DigammaParams &params)
{
    DigammaGenCompareMask(params.mask1, src, params, 0.0f, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, src, params, 0.0f, CMPMODE::GE);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.mask1.ReinterpretCast<uint16_t>(),
        params.mask1.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    Not<uint16_t, false>(params.mask2.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(mask.ReinterpretCast<uint16_t>(),
        params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaSelect(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                     const LocalTensor<uint8_t> &mask, const LocalTensor<float> &tmp,
                                     DigammaParams &params)
{
    Duplicate<float, false>(params.tmpScalar, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    SetCmpMask<float>(params.tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(tmp, mask, src, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dst, tmp, dst, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaNegativeRange(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                            DigammaParams &params)
{

    DigammaCast(params.tmpScalar, src, RoundMode::CAST_FLOOR);
    Sub<float, false>(params.tmpScalar, src, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpScalar, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    CosCompute<float>(params.tmpCal3, params.tmpScalar, params.result, params.splitSize, true);


    Muls<float, false>(src, src, DIGAMMA_NEG_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    SinCompute<float>(params.tmpScalar, src, params.result, params.splitSize, true);


    Muls<float, false>(params.tmpCal3, params.tmpCal3, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal3, params.tmpCal3, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dst, params.tmpCal2, params.tmpCal3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void DigammaInitParams(const LocalTensor<float> &tmp, const uint32_t &splitSize,
                                         const LocalTensor<__cce_half> &src, DigammaParams &params)
{
    params.result = tmp;
    params.tmpCal1 = params.result[splitSize];
    params.tmpCal2 = params.tmpCal1[splitSize];
    params.tmpCal3 = params.tmpCal2[splitSize];
    params.tmpCal4 = params.tmpCal3[splitSize];
    params.tmpCal5 = params.tmpCal4[splitSize];
    params.tmpScalar = params.tmpCal5[splitSize];
    params.mask = params.tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    params.mask1 = params.mask[splitSize];
    params.mask2 = params.mask1[splitSize];


    params.result.SetSize(splitSize * 4);
    params.tmpCal1.SetSize(splitSize);
    params.tmpCal2.SetSize(splitSize);
    params.tmpCal3.SetSize(splitSize);
    params.tmpCal4.SetSize(splitSize);
    params.tmpCal5.SetSize(splitSize);
    params.tmpScalar.SetSize(splitSize);
    params.mask.SetSize(splitSize);
    params.mask1.SetSize(splitSize);
    params.mask2.SetSize(splitSize);

    params.splitSize = splitSize;
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void DigammaInitParams(const LocalTensor<float> &tmp, const uint32_t &splitSize,
                                         const LocalTensor<float> &src, DigammaParams &params)
{
    params.result = tmp;
    params.tmpCal1 = tmp[splitSize];
    params.tmpCal2 = params.tmpCal1[splitSize];
    params.tmpCal3 = params.tmpCal2[splitSize];
    if constexpr (isReuseSource) {
        params.tmpCal4 = src;
        params.tmpScalar = params.tmpCal3[splitSize];
    } else {
        params.tmpCal4 = params.tmpCal3[splitSize];
        params.tmpScalar = params.tmpCal4[splitSize];
    }
    params.mask = params.tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    params.mask1 = params.mask[splitSize];
    params.mask2 = params.mask1[splitSize];

    params.result.SetSize(splitSize);
    params.tmpCal1.SetSize(splitSize);
    params.tmpCal2.SetSize(splitSize);
    params.tmpCal3.SetSize(splitSize);
    params.tmpCal4.SetSize(splitSize);
    params.tmpScalar.SetSize(splitSize);
    params.mask.SetSize(splitSize);
    params.mask1.SetSize(splitSize);
    params.mask2.SetSize(splitSize);

    params.splitSize = splitSize;
}
#pragma end_pipe
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_impl.h" 2




namespace AscendC {
#pragma begin_pipe(V)


[aicore] __inline__ __attribute__((always_inline)) void DigammaPositiveHalf(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Adds<float, false>(params.tmpCal1, src, 3.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tmpScalar, params.tmpCal1, 0.5f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpCal1, 0.0833333333333333f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpCal2, 0.0083333333333333f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal2, params.tmpCal2, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpCal2, 0.003968253968254f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, src, MASK_PLACEHOLDER, 1, params.binaryParams);

    constexpr size_t calcSize = 2;
    for (size_t i = 0U; i < calcSize; ++i) {

        Adds<float, false>(params.tmpCal2, src, tmp1HalfCalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();

        Div<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }


    Sub<float, false>(dst, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaNegativeHalf(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(params.tmpCal5, params.tmpScalar, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    DigammaPositiveHalf(dst, params.tmpCal5, params);


    Adds<float, false>(params.tmpCal2, dst, 0.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dst, dst, params.tmpCal3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DigammaComputeImpl(const LocalTensor<__cce_half> &dst, const LocalTensor<__cce_half> &src,
                                          DigammaParams &params)
{

    Cast<float, __cce_half, false>(params.tmpCal5, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                             {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();


    DigammaCast(params.tmpCal4, params.tmpCal5, RoundMode::CAST_FLOOR);
    Sub<float, false>(params.tmpCal4, params.tmpCal5, params.tmpCal4, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpCal4, params.tmpCal4, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    TanCompute<float>(params.tmpScalar, params.tmpCal4, params.result.ReinterpretCast<uint8_t>(), params.splitSize);

    Duplicate<float, false>(params.tmpCal1, DIGAMMA_PI, MASK_PLACEHOLDER, 1,
                            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal3, params.tmpCal1, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(params.tmpCal4, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    NotNumUnion notNum;
    notNum.i = F32_NAN;
    Duplicate<float, false>(params.result, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    DigammaGenCompareMask(params.mask, params.tmpCal5, params, MIN_NEG_WITH_FLOAT, CMPMODE::LE);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);


    DigammaGenNegIntMask(params.mask1, params.tmpCal5, params, MIN_NEG_WITH_FLOAT);
    DigammaSelect(params.tmpCal4, params.result, params.mask1, params.tmpCal1, params);


    DigammaGenNanMask(params.mask, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.tmpCal5, params.mask, params.tmpCal1, params);


    DigammaGenCompareMask(params.mask, params.tmpCal5, params, 0.0f, CMPMODE::GE);
    DigammaPositiveHalf(params.result, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);


    DigammaGenCompareMask(params.mask, params.tmpCal5, params, -0.0001f, CMPMODE::LT);
    DigammaNegativeHalf(params.result, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);

    Cast<float, __cce_half, false>(params.tmpCal5, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                             {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();

    DigammaGenRangeMask(params.mask, params.tmpCal5, params, -0.0001f, 0.0f);
    DigammaNegativeRange(params.result, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);

    Cast<__cce_half, float, false>(dst, params.tmpCal4, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                             {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
# 195 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_impl.h"
[aicore] __inline__ __attribute__((always_inline)) void DigammaPositiveTmp0(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Adds<float, false>(params.tmpCal1, src, 10.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tmpCal2, params.tmpCal1, 0.5f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);


    Duplicate<float, false>(params.tmpCal2, 8.33333333333333333333e-2, MASK_PLACEHOLDER, 1,
                            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    for (size_t i = 0U; i < DIGAMMA_MAX_LOOP; ++i) {
        Duplicate<float, false>(params.tmpScalar, posCalcConst[i], MASK_PLACEHOLDER, 1,
                                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


        Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Sub<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
    constexpr size_t calcSize = 6;
    for (size_t i = DIGAMMA_MAX_LOOP; i < calcSize; ++i) {
        Duplicate<float, false>(params.tmpScalar, posCalcConst[i], MASK_PLACEHOLDER, 1,
                                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


        Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
        Sub<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}




[aicore] __inline__ __attribute__((always_inline)) void DigammaPositiveTmp1(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(dst, params.tmpScalar, src, MASK_PLACEHOLDER, 1, params.binaryParams);

    for (size_t i = 0U; i < DIGAMMA_MAX_LOOP; ++i) {

        Adds<float, false>(params.tmpCal2, src, tmp1CalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();

        Div<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
    constexpr size_t calcSize = 9;
    for (size_t i = DIGAMMA_MAX_LOOP; i < calcSize; ++i) {

        Adds<float, false>(params.tmpCal2, src, tmp1CalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();

        Div<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaPositive(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                       DigammaParams &params)
{

    DigammaPositiveTmp0(dst, src, params);


    DigammaPositiveTmp1(params.tmpCal1, src, params);


    Sub<float, false>(dst, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DigammaNegPicotPix(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                          DigammaParams &params)
{

    Add<float, false>(params.tmpCal1, src, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    DigammaCast(params.tmpCal2, params.tmpCal1, RoundMode::CAST_ROUND);


    Sub<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpCal1, params.tmpCal1, 1.5707963267948966f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Cast<int32_t, float, false>(params.tmpCal2.ReinterpretCast<int32_t>(), params.tmpCal2, RoundMode::CAST_ROUND,
                                MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<int32_t, false>(params.tmpCal3.ReinterpretCast<int32_t>(), 1, MASK_PLACEHOLDER, 1,
                              DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize * (sizeof(float) / sizeof(uint16_t)));
    And<uint16_t, false>(params.tmpCal2.ReinterpretCast<uint16_t>(), params.tmpCal2.ReinterpretCast<uint16_t>(),
            params.tmpCal3.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);


    Cast<float, int32_t, false>(params.tmpCal2, params.tmpCal2.ReinterpretCast<int32_t>(), RoundMode::CAST_NONE,
                                MASK_PLACEHOLDER, 1, params.unaryParams);
    DigammaGenCompareMask(params.mask1, params.tmpCal2, params, 0.5f, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, params.tmpCal2, params, 0.5f, CMPMODE::GE);


    Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(dst, 0.0093383789065f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    constexpr size_t calcSize = 5;
    for (size_t i = 0U; i < calcSize; ++i) {
        Mul<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Adds<float, false>(dst, dst, picotCalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();
    }

    Mul<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dst, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmpCal1, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(dst, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    DigammaSelect(dst, params.tmpCal1, params.mask2, params.tmpCal3, params);


    Duplicate<float, false>(params.tmpScalar, -1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();

    DigammaSelect(dst, params.tmpCal1, params.mask1, params.tmpCal3, params);


    Muls<float, false>(dst, dst, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaNegative(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                       DigammaParams &params)
{

    Muls<float, false>(params.tmpCal3, src, -1.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.tmpCal3, params.tmpCal3, 1.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    DigammaPositive(dst, params.tmpCal3, params);


    DigammaNegPicotPix(params.tmpCal4, src, params);


    Add<float, false>(dst, dst, params.tmpCal4, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DigammaComputeImpl(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                          DigammaParams &params)
{
    Duplicate<float, false>(dst, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    NotNumUnion notNum;
    notNum.i = F32_NAN;
    Duplicate<float, false>(params.result, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    DigammaGenCompareMask(params.mask, src, params, MIN_NEG_WITH_FLOAT, CMPMODE::LE);
    DigammaSelect(dst, params.result, params.mask, params.tmpCal3, params);


    DigammaGenNegIntMask(params.mask1, src, params, MIN_NEG_WITH_FLOAT);
    DigammaSelect(dst, params.result, params.mask1, params.tmpCal3, params);


    DigammaGenNanMask(params.mask, src, params);
    DigammaSelect(dst, src, params.mask, params.tmpCal3, params);


    DigammaGenCompareMask(params.mask, src, params, 0.0f, CMPMODE::GE);
    DigammaPositive(params.result, src, params);
    DigammaSelect(dst, params.result, params.mask, params.tmpCal3, params);


    DigammaGenCompareMask(params.mask, src, params, 0.0f, CMPMODE::LT);
    DigammaNegative(params.result, src, params);
    DigammaSelect(dst, params.result, params.mask, params.tmpCal3, params);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void DigammaCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src,
                                      const LocalTensor<uint8_t> &tmp, const uint32_t calCount)
{
    CheckTensorPosition(dst, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(src, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(tmp, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", src, "srcTensor", "Digamma");
    CheckCalCount(calCount, "calCount", dst, "dstTensor", "Digamma");


                                                                                                                       ;

    LocalTensor<float> tmpBuffer = tmp.ReinterpretCast<float>();
    uint32_t tmpBufferSize = tmpBuffer.GetSize();
    uint32_t splitSize = tmpBufferSize;

    if (sizeof(T) == sizeof(float)) {
        if constexpr (isReuseSource) {
            splitSize = splitSize / DIGAMMA_FLOAT_REUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
        } else {
            splitSize = splitSize / DIGAMMA_FLOAT_NOREUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
        }
    } else {
        splitSize = splitSize / DIGAMMA_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);


    DigammaParams params;
    DigammaInitParams<isReuseSource>(tmpBuffer, splitSize, src, params);

    const uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetMaskCount();
    SetVectorMask<T>(0, splitSize);
    uint32_t offset = 0;
    for (uint32_t i = 0U; i < loopCount; ++i) {
        DigammaComputeImpl(dst[offset], src[offset], params);
        offset += splitSize;
    }

    if (calcTail > 0) {
        calcTail = (calcTail + ONE_BYTE_BIT_SIZE - 1U) / ONE_BYTE_BIT_SIZE * ONE_BYTE_BIT_SIZE;
        SetVectorMask<T>(0, calcTail);
        params.splitSize = calcTail;
        DigammaComputeImpl(dst[offset], src[offset], params);
    }
    SetMaskNorm();
    ResetMask();
}

#pragma end_pipe
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/digamma.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/digamma.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Digamma(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                               LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DigammaCompute<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/digamma.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Digamma(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> tmp;
    const bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(tmp);
                                                                                 ;
    DigammaCompute<T, isReuseSource>(dstTensor, srcTensor, tmp, calCount);
}
#pragma end_pipe
}
# 86 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sign/sign_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sign/sign_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sign/sign_common_impl.h" 2


#pragma begin_pipe(V)
namespace AscendC {
constexpr uint32_t SIGN_CALC_PROC = 3;
constexpr uint32_t SIGN_BIT = 8;

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SignComputeImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &tmpBuffer1, const LocalTensor<uint8_t> &tmpBuffer2, const LocalTensor<T> &tmpBuffer3,
    const LocalTensor<T> &tmpBuffer4, uint32_t calCount, uint32_t repeatTimes)
{
    BinaryRepeatParams binaryParams;

    Duplicate<T, false>(dstTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();





    Compare<T, uint8_t, false>(
        tmpBuffer1, srcTensor, dstTensor, CMPMODE::LT, MASK_PLACEHOLDER, repeatTimes, binaryParams);
    Compare<T, uint8_t, false>(
        tmpBuffer2, srcTensor, dstTensor, CMPMODE::GT, MASK_PLACEHOLDER, repeatTimes, binaryParams);





    Duplicate<T, false>(tmpBuffer3, static_cast<T>(1), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();

    SetCmpMask<T>(tmpBuffer3);
    Select<T, uint8_t>(tmpBuffer4, tmpBuffer1, dstTensor, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<T, false>(tmpBuffer3, static_cast<T>(-1), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();

    SetCmpMask<T>(tmpBuffer3);
    Select<T, uint8_t>(dstTensor, tmpBuffer2, dstTensor, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(dstTensor, tmpBuffer4, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SignCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    uint32_t sharedTmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = sharedTmpBufferSize / sizeof(T) / SIGN_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Sign");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Sign");

    CheckTmpBufferSize(splitCount, 0, sharedTmpBufferSize);


                                                                                                                       ;

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    __attribute__((cce_unif_buff)) uint8_t *tmpBuffer1 = (__attribute__((cce_unif_buff)) uint8_t *)sharedTmpBuffer.GetPhyAddr();
    uint32_t tmpLen = AlignUp(splitCount / SIGN_BIT, ONE_BLK_SIZE);
    __attribute__((cce_unif_buff)) uint8_t *tmpBuffer2 = tmpBuffer1 + tmpLen;
    LocalTensor<T> stackTensor = sharedTmpBuffer[tmpLen * 2].ReinterpretCast<T>();
    __attribute__((cce_unif_buff)) T *tmpBuffer3 = (__attribute__((cce_unif_buff)) T *)stackTensor.GetPhyAddr();
    __attribute__((cce_unif_buff)) T *tmpBuffer4 = tmpBuffer3 + splitCount;

    uint32_t offset = 0;
    uint32_t repeatTimes = (splitCount * sizeof(T) + ONE_REPEAT_BYTE_SIZE - 1) / ONE_REPEAT_BYTE_SIZE;
    for (uint32_t i = 0; i < loopCount; ++i) {
        SignComputeImpl(dstTensor[offset], srcTensor[offset],
            sharedTmpBuffer, sharedTmpBuffer[tmpLen], stackTensor, stackTensor[splitCount], splitCount, repeatTimes);
        offset = offset + splitCount;
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        repeatTimes = (calcTail * sizeof(T) + ONE_REPEAT_BYTE_SIZE - 1) / ONE_REPEAT_BYTE_SIZE;
        SignComputeImpl(dstTensor[offset], srcTensor[offset],
            sharedTmpBuffer, sharedTmpBuffer[tmpLen], stackTensor, stackTensor[splitCount], calcTail, repeatTimes);
    }
    SetMaskNorm();
    ResetMask();
}
}
#pragma end_pipe
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h" 2



#pragma begin_pipe(V)
namespace AscendC {
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sign(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    SignCompute<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sign(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sign<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sign(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    Sign<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 91 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sign(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{
    Sign<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
}
#pragma end_pipe
# 87 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean_utils.h"
namespace AscendC {
struct MeanParams {
    uint32_t outter = 1;
    uint32_t inner;
    uint32_t n;
};

};
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/mean/mean_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/mean/mean_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/mean/mean_common_impl.h" 2



namespace AscendC {
constexpr uint32_t HALF_NUM_PER = 128;
constexpr uint32_t FLOAT_NUM_PER = 64;

[aicore] __inline__ __attribute__((always_inline)) void MeanCast(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const MeanParams& meanParams)
{
    uint32_t elementNumPerRep = FLOAT_NUM_PER;
    uint32_t repeateTimes = (meanParams.n + elementNumPerRep - 1) / elementNumPerRep;
    const UnaryRepeatParams unaryParams;
    float scalarValue = static_cast<float>(1) / static_cast<float>(static_cast<int32_t>(meanParams.n));
    LocalTensor<float> TmpTensor = sharedTmpBuffer.ReinterpretCast<float>();
    LocalTensor<__cce_half> castTensor = sharedTmpBuffer.ReinterpretCast<__cce_half>();
    SetMaskCount();
    for (uint32_t row = 0; row < meanParams.outter; ++row) {
        SetVectorMask<__cce_half>(0, meanParams.n);
        Cast<float, __cce_half, false>(TmpTensor, srcTensor[row * meanParams.inner], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                                 1, {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        RepeatReduceSum<float, false>(TmpTensor[meanParams.inner], TmpTensor, 1,
                                      MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                      DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        uint32_t reduceNums = repeateTimes;
        while (reduceNums > 1) {
            SetVectorMask<__cce_half>(0, reduceNums);
            reduceNums = (reduceNums + elementNumPerRep - 1) / elementNumPerRep;
            RepeatReduceSum<float, false>(TmpTensor[meanParams.inner], TmpTensor[meanParams.inner], 1,
                                      MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                      DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);

            PipeBarrier<PIPE_V>();
        }
        SetVectorMask<__cce_half>(0, 1);
        Muls<float, false>(TmpTensor[meanParams.inner], TmpTensor[meanParams.inner],
                       scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Cast<__cce_half, float, false>(castTensor, TmpTensor[meanParams.inner], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                                 1, {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        RepeatReduceSum<__cce_half, false>(dstTensor[row], castTensor, 1, MASK_PLACEHOLDER,
                                     DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MeanForOneRepeatTime(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                            const MeanParams& meanParams, T scalarValue)
{
    SetVectorMask<T>(0, meanParams.n);
    for (uint32_t row = 0; row < meanParams.outter; ++row) {
        RepeatReduceSum<T, false>(dstTensor[row], srcTensor[row * meanParams.inner], 1, MASK_PLACEHOLDER,
                                  DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
    PipeBarrier<PIPE_V>();
    SetVectorMask<T>(0, meanParams.outter);
    const UnaryRepeatParams unaryParams;
    Muls<T, false>(dstTensor, dstTensor, scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    ResetMask();
}

template <typename T, typename accType, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void MeanCommon(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const MeanParams& meanParams)
{
    uint32_t elementNumPerRep = FLOAT_NUM_PER;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        elementNumPerRep = HALF_NUM_PER;
    }
    uint32_t repeateTimes = (meanParams.n + elementNumPerRep - 1) / elementNumPerRep;
    T scalarValue = static_cast<T>(static_cast<float>(1) / static_cast<float>(static_cast<int32_t>(meanParams.n)));
    SetMaskCount();
    if (repeateTimes == 1) {
        return MeanForOneRepeatTime(dstTensor, srcTensor, meanParams, scalarValue);
    }
    const UnaryRepeatParams unaryParams;
    LocalTensor<T> TmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
    for (uint32_t row = 0; row < meanParams.outter; ++row) {
        uint32_t reduceNums = repeateTimes;
        SetVectorMask<T>(0, meanParams.n);
        RepeatReduceSum<T, false>(TmpTensor,
            srcTensor[row * meanParams.inner],
            1,
            MASK_PLACEHOLDER,
            DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        while (reduceNums > 1) {
            SetVectorMask<T>(0, reduceNums);
            reduceNums = (reduceNums + elementNumPerRep - 1) / elementNumPerRep;
            if (reduceNums == 1) {
                RepeatReduceSum<T, false>(dstTensor[row], TmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                          DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            } else {
                RepeatReduceSum<T, false>(TmpTensor, TmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                          DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();
        }
    }
    SetVectorMask<T>(0, meanParams.outter);
    Muls<T, false>(dstTensor, dstTensor, scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
}

template <typename T, typename accType = T, bool isReuseSource = false, bool isBasicBlock = false,
          int32_t reduceDim = -1>
[aicore] __inline__ __attribute__((always_inline)) void MeanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const MeanParams& meanParams) {
    uint32_t elementNumPerRep = FLOAT_NUM_PER;
    if constexpr (sizeof(T) == sizeof(__cce_half) && sizeof(accType) == sizeof(float))
    {
        uint32_t repeateTimes = (meanParams.n + elementNumPerRep - 1) / elementNumPerRep;
        uint32_t finalWorkSize = meanParams.inner * sizeof(float) + (repeateTimes + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE * ONE_BLK_SIZE;

                                                        ;
        MeanCast(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
    } else {

        if constexpr (sizeof(T) == sizeof(__cce_half)) {
            elementNumPerRep = HALF_NUM_PER;
        }
        uint32_t repeateTimes = (meanParams.n + elementNumPerRep - 1) / elementNumPerRep;
        uint32_t finalWorkSize = (repeateTimes + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE * ONE_BLK_SIZE;


                                                        ;
        MeanCommon<T, accType, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
    }
}

#pragma end_pipe
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h"
template <typename T, typename accType = T, bool isReuseSource = false, bool isBasicBlock = false,
          int32_t reduceDim = -1>
[aicore] __inline__ __attribute__((always_inline)) void Mean(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                            const LocalTensor<uint8_t> &sharedTmpBuffer, const MeanParams &meanParams)
{
    if constexpr(g_coreType == AscendC::AIC)
    {
        return;
    }
    MeanImpl<T, accType, isReuseSource, isBasicBlock, reduceDim>(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
}
# 65 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h"
template <typename T, typename accType = T, bool isReuseSource = false, bool isBasicBlock = false,
          int32_t reduceDim = -1>
[aicore] __inline__ __attribute__((always_inline)) void Mean(
    const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const MeanParams &meanParams)
{
    if constexpr(g_coreType == AscendC::AIC)
    {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    Mean<T, accType>(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
}
#pragma end_pipe
}
# 88 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/axpy.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/axpy.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/axpy/axpy_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/axpy/axpy_common_impl.h"
namespace AscendC {
 template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AxpyIntrinsicsImpl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor,
    const U& scalarValue, LocalTensor<float> stackBuffer, uint32_t stackSize)
{


                                                                             ;
}





 template <>
[aicore] __inline__ __attribute__((always_inline)) void AxpyIntrinsicsImpl(const LocalTensor<__cce_half>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const __cce_half& scalarValue, LocalTensor<float> stackBuffer, uint32_t stackSize)
{
    LocalTensor<float> tmpSrc = stackBuffer[0];
    LocalTensor<float> tmpDst = stackBuffer[stackSize];

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Cast<float, __cce_half, false>(tmpSrc, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Cast<float, __cce_half, false>(tmpDst, dstTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpSrc, tmpSrc, (float)scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(tmpDst, tmpSrc, tmpDst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<__cce_half, float, false>(dstTensor, tmpDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}




 template <>
[aicore] __inline__ __attribute__((always_inline)) void AxpyIntrinsicsImpl(const LocalTensor<float>& dstTensor, const LocalTensor<__cce_half>& srcTensor,
    const __cce_half& scalarValue, LocalTensor<float> stackBuffer, uint32_t stackSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Cast<float, __cce_half, false>(stackBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Muls<float, false>(stackBuffer, stackBuffer, (float)scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, stackBuffer, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t axpyTmpCalc(uint32_t tmpBufferSize)
{
    uint32_t stackSize = tmpBufferSize;
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        stackSize = tmpBufferSize / 2 / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        stackSize = tmpBufferSize / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(stackSize, 0, tmpBufferSize);
    return stackSize;
}

template <typename T, typename U, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AxpySub(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor, const U& scalarValue,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    CheckTmpBufferSize(bufferSize, 0, bufferSize);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t tmpBufferSize = tmpBuffer.GetSize();

    uint32_t stackSize = axpyTmpCalc<T>(tmpBufferSize);

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AxpyIntrinsicsImpl(dstTensor[offset], srcTensor[offset], scalarValue, tmpBuffer, stackSize);
        offset = offset + stackSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AxpyIntrinsicsImpl(dstTensor[offset], srcTensor[offset], scalarValue, tmpBuffer, stackSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, typename U, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void AxpyImpl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor, const U scalarValue,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Axpy");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Axpy");



                                                                             ;

    if constexpr (sizeof(U) == sizeof(float)) {
        Axpy<T, U>(dstTensor, srcTensor, scalarValue, calCount);
    } else {
        AxpySub<T, U, isReuseSource>(dstTensor, srcTensor, scalarValue, sharedTmpBuffer, calCount);
    }
}

}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/axpy.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/axpy.h"
template <typename T, typename U, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor, const U scalarValue,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AxpyImpl<T, U, isReuseSource>(dstTensor, srcTensor, scalarValue, sharedTmpBuffer, calCount);
}

#pragma end_pipe
}
# 89 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/ceil.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/ceil.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/ceil/ceil_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/ceil/ceil_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/ceil/ceil_v220_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/ceil/ceil_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void CeilProcess(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<uint8_t> &tmpTensor)
{
    (void)tmpTensor;
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_CEIL, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/ceil/ceil_common_impl.h" 2





namespace AscendC {

constexpr uint32_t CEIL_HALF_CALC_PROCEDURE = 2;

[aicore] __inline__ __attribute__((always_inline)) void CeilProcess(const LocalTensor<__cce_half> &dstTensor, const LocalTensor<__cce_half> &srcTensor,
    const LocalTensor<uint8_t> &tmpTensor)
{
    const LocalTensor<float> floatTmpTensor = tmpTensor.ReinterpretCast<float>();


    Cast<float, __cce_half, false>(floatTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    CeilProcess(floatTmpTensor, floatTmpTensor, tmpTensor);
    Cast<__cce_half, float, false>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CeilImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", srcTensor, "srcTensor", "Ceil");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Ceil");


                                                                                                                       ;


    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(T);
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        splitCount = splitCount / CEIL_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        CeilProcess(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<T>(0, calcTail);
        CeilProcess(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CeilImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CeilImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/ceil.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/ceil.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Ceil(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CeilImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/ceil.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Ceil(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CeilImpl(dstTensor, srcTensor, calCount);
}

#pragma end_pipe
}
# 90 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_utils.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_utils.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_utils.h" 2

namespace AscendC {
constexpr uint32_t ONE_VOR_BLOCK_DIM = 8;
constexpr uint32_t ELEMENT_NUM_FOR_UINT16 = 16;
constexpr int32_t FLOAT_ELEMENT_NUM = 2;
constexpr uint32_t REPEAT_STRIDE_NUM = 8;
constexpr uint32_t MAX_REPEAT_NUM = 255;

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TwoDimBroadCastDimAlign(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &zeroTemp, const uint32_t firstDim, const uint32_t blockDim)
{
    int32_t dtypeCount = 1;
    if constexpr (sizeof(T) == sizeof(float)) {
        dtypeCount = FLOAT_ELEMENT_NUM;
    }
    uint32_t orCounts = firstDim / ONE_VOR_BLOCK_DIM;
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    uint8_t repeateTimes = blockDim / oneBlockElementNum;
    SetMaskNorm();
    SetVectorMask<uint16_t, MaskMode::NORMAL>(ONE_VOR_BLOCK_DIM * ELEMENT_NUM_FOR_UINT16);
    uint8_t dstBlkStride = blockDim * dtypeCount / ELEMENT_NUM_FOR_UINT16;
    BinaryRepeatParams binaryParams(dstBlkStride, 0, 0, 1, 1, 0);
    uint32_t transTmpBufferOffset = 0;
    for (uint32_t i = 0; i < orCounts; i++) {
        Or<uint16_t, false>(dstLocal[transTmpBufferOffset].template ReinterpretCast<uint16_t>(),
            srcLocal.template ReinterpretCast<uint16_t>(),
            zeroTemp.template ReinterpretCast<uint16_t>(),
            MASK_PLACEHOLDER,
            repeateTimes,
            binaryParams);
        transTmpBufferOffset += ONE_VOR_BLOCK_DIM * blockDim;
    }
    uint32_t orCountsTail = firstDim - orCounts * ONE_VOR_BLOCK_DIM;
    if (orCountsTail > 0) {
        SetMaskNorm();
        SetVectorMask<uint16_t, MaskMode::NORMAL>(orCountsTail * ELEMENT_NUM_FOR_UINT16);
        Or<uint16_t, false>(dstLocal[transTmpBufferOffset].template ReinterpretCast<uint16_t>(),
                            srcLocal.template ReinterpretCast<uint16_t>(),
                            zeroTemp.template ReinterpretCast<uint16_t>(),
                            MASK_PLACEHOLDER,
                            repeateTimes,
                            binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoopBroadCast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &zeroTemp, const uint32_t firstDim, const uint32_t blockDim)
{
    int32_t dtypeCount = 1;
    if constexpr (sizeof(T) == sizeof(float)) {
        dtypeCount = FLOAT_ELEMENT_NUM;
    }
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(firstDim * dtypeCount);
    BinaryRepeatParams binaryParams(1, 1, 0, REPEAT_STRIDE_NUM, REPEAT_STRIDE_NUM, 0);
    uint32_t temBufferOffset = 0;
    for (uint32_t i = 0; i < blockDim; i++) {
        Or<uint16_t, false>(dstLocal[temBufferOffset].template ReinterpretCast<uint16_t>(),
            srcLocal.template ReinterpretCast<uint16_t>(),
            zeroTemp.template ReinterpretCast<uint16_t>(),
            MASK_PLACEHOLDER,
            1,
            binaryParams);
        temBufferOffset += firstDim;
    }
    PipeBarrier<PIPE_V>();
}

}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_v220_impl.h" 2

namespace AscendC {
constexpr uint32_t BRCB_ONE_SIZE = 8;
constexpr uint32_t BRCB_HALF_MAX_REPEATE_TIMES = 254;
constexpr uint32_t BRCB_FLOAT_MAX_REPEATE_TIMES = 255;
constexpr uint8_t GATHER_MASK_PATTERN = 7;

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BrcbToOneBlock(const LocalTensor<T> &srcLocal, const uint32_t firstDim,
    uint32_t oneBlockElementNum, LocalTensor<T> &brcbOneBlockTempBuffer)
{
    const uint32_t brcbRepeatTime = (firstDim + BRCB_ONE_SIZE - 1) / BRCB_ONE_SIZE;
    uint32_t brcbMaxRepeateTimes = BRCB_HALF_MAX_REPEATE_TIMES;
    if constexpr (sizeof(T) == sizeof(float)) {
        brcbMaxRepeateTimes = BRCB_FLOAT_MAX_REPEATE_TIMES;
    }
    const uint32_t brcbCount = brcbRepeatTime / brcbMaxRepeateTimes;
    const uint32_t tailBrcbRepeateTime = brcbRepeatTime % brcbMaxRepeateTimes;
    uint32_t brcbSrcOffset = 0;
    uint32_t brcbOneBlockTempBufferOffset = 0;
    for (uint32_t i = 0; i < brcbCount; i++) {
        Brcb(brcbOneBlockTempBuffer[brcbOneBlockTempBufferOffset],
            srcLocal[brcbSrcOffset],
            brcbMaxRepeateTimes,
            {1, DEFAULT_REPEAT_STRIDE});
        brcbOneBlockTempBufferOffset += brcbMaxRepeateTimes * oneBlockElementNum * BRCB_ONE_SIZE;
        brcbSrcOffset += brcbMaxRepeateTimes * BRCB_ONE_SIZE;
    }
    if (tailBrcbRepeateTime != 0) {
        Brcb(brcbOneBlockTempBuffer[brcbOneBlockTempBufferOffset],
            srcLocal[brcbSrcOffset],
            tailBrcbRepeateTime,
            {1, DEFAULT_REPEAT_STRIDE});
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void TwoDimBroadCastLastDimAlign220(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    LocalTensor<T> &tmpBuffer, const uint32_t firstDim, const uint32_t blockDim)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    BrcbToOneBlock(srcLocal, firstDim, oneBlockElementNum, tmpBuffer);
    SetVectorMask<T, MaskMode::COUNTER>(blockDim);
    const CopyRepeatParams copyRepeatParams = {1, 0, (uint16_t)(blockDim / oneBlockElementNum), 1};
    uint32_t CopyCounts = firstDim / MAX_REPEAT_TIMES;
    uint32_t dstOffset = 0;
    uint32_t brcbOneBlockTempBufferOffset = 0;
    for (uint32_t i = 0; i < CopyCounts; i++) {
        Copy<T, false>(dstLocal[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            MAX_REPEAT_TIMES,
            copyRepeatParams);
        dstOffset += MAX_REPEAT_TIMES * blockDim;
        brcbOneBlockTempBufferOffset += MAX_REPEAT_TIMES * oneBlockElementNum;
    }
    uint32_t tailsCopyRepeateTimes = firstDim % MAX_REPEAT_TIMES;
    if (tailsCopyRepeateTimes != 0) {
        Copy<T, false>(dstLocal[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            tailsCopyRepeateTimes,
            copyRepeatParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void TwoDimBroadCastLastDimNotAlign220(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    LocalTensor<T> &tmpBuffer, const uint32_t firstDim, const uint32_t blockDim)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    BrcbToOneBlock(srcLocal, firstDim, oneBlockElementNum, tmpBuffer);
    const uint32_t blockDimAlignBlockNum = (blockDim + oneBlockElementNum - 1) / oneBlockElementNum;
    const uint32_t blockDimAlign = blockDimAlignBlockNum * oneBlockElementNum;
    SetVectorMask<T, MaskMode::COUNTER>(blockDimAlign);
    const CopyRepeatParams copyRepeatParams = {1, 0, (uint16_t)blockDimAlignBlockNum, 1};
    uint32_t CopyCounts = firstDim / MAX_REPEAT_TIMES;
    uint32_t dstOffset = 0;
    uint32_t brcbOneBlockTempBufferOffset = 0;
    auto copyTempBuffer = tmpBuffer[firstDim * oneBlockElementNum];
    for (uint32_t i = 0; i < CopyCounts; i++) {
        Copy<T, false>(copyTempBuffer[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            MAX_REPEAT_TIMES,
            copyRepeatParams);
        dstOffset += MAX_REPEAT_TIMES * blockDimAlign;
        brcbOneBlockTempBufferOffset += MAX_REPEAT_TIMES * oneBlockElementNum;
    }
    uint32_t tailsCopyRepeateTimes = firstDim % MAX_REPEAT_TIMES;
    if (tailsCopyRepeateTimes != 0) {
        Copy<T, false>(copyTempBuffer[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            tailsCopyRepeateTimes,
            copyRepeatParams);
    }
    PipeBarrier<PIPE_V>();
    const GatherMaskParams gatherMaskParams = {
        1, (uint16_t)firstDim, (uint16_t)blockDimAlignBlockNum, 0};
    uint64_t rsvdCnt = 0;
    GatherMask(dstLocal, copyTempBuffer, GATHER_MASK_PATTERN, true, blockDim, gatherMaskParams, rsvdCnt);
    SetMaskCount();
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetAlignLoopNumbers(const uint32_t firstDim, const uint32_t blockDim,
    const uint32_t tmpBufferSize, uint32_t &oneRepeateSize, uint32_t &rangeM, uint32_t &tailM)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t minBrcbTempBufferSize = oneBlockElementNum * oneBlockElementNum;
    constexpr uint32_t minTmpBufferSize = minBrcbTempBufferSize;





      ;
    oneRepeateSize = tmpBufferSize / minTmpBufferSize * oneBlockElementNum;
    rangeM = firstDim / oneRepeateSize;
    tailM = firstDim - oneRepeateSize * rangeM;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetNotAlignLoopNumbers(const uint32_t firstDim, const uint32_t blockDim,
    const uint32_t tmpBufferSize, uint32_t &oneRepeateSize, uint32_t &rangeM, uint32_t &tailM)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t minBrcbTempBufferSize = oneBlockElementNum * oneBlockElementNum;
    const uint32_t blockDimAlignBlockNum = (blockDim + oneBlockElementNum - 1) / oneBlockElementNum;
    const uint32_t blockDimAlign = blockDimAlignBlockNum * oneBlockElementNum;
    const uint32_t minCopyTempBufferSize = oneBlockElementNum * blockDimAlign;
    const uint32_t minTmpBufferSize = minBrcbTempBufferSize + minCopyTempBufferSize;





      ;
    oneRepeateSize = tmpBufferSize / minTmpBufferSize * oneBlockElementNum;
    rangeM = firstDim / oneRepeateSize;
    tailM = firstDim - oneRepeateSize * rangeM;
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TwoDimBroadCastLastDim(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<T> &tmpBuffer)
{
    const auto firstDim = dstShape[0];
    const auto blockDim = dstShape[axis];
    uint32_t oneRepeateSize = 0;
    uint32_t rangeM = 0;
    uint32_t tailM = 0;
    uint32_t dstLocalOffset = 0;
    uint32_t srcLocalOffset = 0;
    if (blockDim * sizeof(T) % ONE_BLK_SIZE == 0) {
        GetAlignLoopNumbers<T>(firstDim, blockDim, tmpBuffer.GetSize(), oneRepeateSize, rangeM, tailM);
        for (uint32_t i = 0; i < rangeM; i++) {
            TwoDimBroadCastLastDimAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, oneRepeateSize, blockDim);
            dstLocalOffset += oneRepeateSize * blockDim;
            srcLocalOffset += oneRepeateSize;
        }

        if (tailM != 0) {
            TwoDimBroadCastLastDimAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, tailM, blockDim);
        }
    } else {
        GetNotAlignLoopNumbers<T>(firstDim, blockDim, tmpBuffer.GetSize(), oneRepeateSize, rangeM, tailM);
        for (uint32_t i = 0; i < rangeM; i++) {
            TwoDimBroadCastLastDimNotAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, oneRepeateSize, blockDim);
            dstLocalOffset += oneRepeateSize * blockDim;
            srcLocalOffset += oneRepeateSize;
        }
        if (tailM != 0) {
            TwoDimBroadCastLastDimNotAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, tailM, blockDim);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void NoBroad(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, const uint32_t size)
{
    SetVectorMask<T, MaskMode::COUNTER>(size);
    Copy<T, false>(dstLocal, srcLocal, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}

}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_impl.h" 2





namespace AscendC {
constexpr uint32_t TWO_DIM = 2;
constexpr uint32_t HALF_ONE_BLK_SIZE = 16;

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TwoDimBroadCastFirstDim(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<T> &tmpBuffer)
{
    const uint32_t firstDim = dstShape[0];
    const uint32_t blockDim = dstShape[1];

                                                                                                                     ;

    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t FIRST_DIM_LOOP_LIMITE = MAX_REPEAT_NUM * oneBlockElementNum;

    auto zeroTemp = tmpBuffer;
    Duplicate(zeroTemp.template ReinterpretCast<uint16_t>(), (uint16_t)0, ONE_BLK_SIZE / sizeof(uint16_t));
    PipeBarrier<PIPE_V>();

    if (blockDim >= FIRST_DIM_LOOP_LIMITE) {
        LoopBroadCast<T>(dstLocal, srcLocal, zeroTemp, blockDim, firstDim);
        return;
    }

    TwoDimBroadCastDimAlign<T, isReuseSource>(dstLocal, srcLocal, zeroTemp, firstDim, blockDim);
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void BroadCastCompute(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<T> &tmpBuffer)
{
    uint32_t srcSize = 1;
    uint32_t dstSize = 1;
    for (uint32_t i = 0; i < dim; i++) {
        srcSize *= srcShape[i];
        dstSize *= dstShape[i];
    }

    if (srcSize == dstSize) {
        NoBroad(dstLocal, srcLocal, dstSize);
    } else if (srcSize == 1) {
        TEventID event1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(event1);
        WaitFlag<HardEvent::V_S>(event1);
        auto scalar = srcLocal.GetValue(0);
        TEventID event2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        SetFlag<HardEvent::S_V>(event2);
        WaitFlag<HardEvent::S_V>(event2);
        Duplicate(dstLocal, scalar, dstSize);
        PipeBarrier<PIPE_V>();
    } else {
        if constexpr (dim == TWO_DIM) {
            if constexpr (axis == 1) {
                TwoDimBroadCastLastDim<T, dim, axis, false>(dstLocal, srcLocal, dstShape, srcShape, tmpBuffer);
            } else {
                TwoDimBroadCastFirstDim<T, dim, axis, false>(dstLocal, srcLocal, dstShape, srcShape, tmpBuffer);
            }
        } else {
                                                                                                   ;
        }
    }
    SetMaskCount();
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void BroadCast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim])
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    BroadCast<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape, sharedTmpBuffer);
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void BroadCast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<uint8_t> &sharedTmpBuffer)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                   ;
    if constexpr (sizeof(T) == 1) {
        LocalTensor<__cce_half> tmpBuffer = sharedTmpBuffer.ReinterpretCast<__cce_half>();
        uint32_t srcSize = 1;
        uint32_t dstSize = 1;
        for (uint32_t i = 0; i < dim; i++) {
            srcSize *= srcShape[i];
            dstSize *= dstShape[i];
        }
        auto srcTempBuffer = tmpBuffer;
        const uint32_t alignSrcSize = ((srcSize + HALF_ONE_BLK_SIZE - 1) / HALF_ONE_BLK_SIZE) * HALF_ONE_BLK_SIZE;
        const uint32_t alignDstSize = ((dstSize + HALF_ONE_BLK_SIZE - 1) / HALF_ONE_BLK_SIZE) * HALF_ONE_BLK_SIZE;
        auto dstTempBuffer = tmpBuffer[alignSrcSize];
        auto tempTempBuffer = dstTempBuffer[alignDstSize];
        SetMaskCount();
        SetVectorMask<T, MaskMode::COUNTER>(srcSize);
        Cast<__cce_half, T, false>(srcTempBuffer,
            srcLocal,
            RoundMode::CAST_NONE,
            MASK_PLACEHOLDER,
            1,
            {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();

        BroadCastCompute<__cce_half, dim, axis, isReuseSource>(
            dstTempBuffer, srcTempBuffer, dstShape, srcShape, tempTempBuffer);
        SetVectorMask<T, MaskMode::COUNTER>(dstSize);
        Cast<T, __cce_half, false>(dstLocal,
            dstTempBuffer,
            RoundMode::CAST_NONE,
            MASK_PLACEHOLDER,
            1,
            {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
    } else {
        LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
        SetMaskCount();
        BroadCastCompute<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape, tmpBuffer);
        SetMaskNorm();
        ResetMask();
    }
                                  ;
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h"
template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Broadcast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<uint8_t> &sharedTmpBuffer)
{
    BroadCast<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape, sharedTmpBuffer);
}
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h"
template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Broadcast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim])
{
    BroadCast<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape);
}
#pragma end_pipe
}
# 91 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h" 2




# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_v220_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void CastInt162Float(const LocalTensor<float>& dst, const LocalTensor<int16_t>& src)
{
    Cast<float, int16_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                          {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
}

[aicore] __inline__ __attribute__((always_inline)) void CastFloat2Int16(const LocalTensor<int16_t>& dst, const LocalTensor<float>& src)
{
    Cast<int16_t, float, false>(dst, src, RoundMode::CAST_ROUND, MASK_PLACEHOLDER, 1,
                          {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h" 2






namespace AscendC {
namespace {
constexpr uint32_t REDUCE_XOR_SUM_REUSE_CALC_PROC = 2U;
constexpr uint32_t REDUCE_XOR_SUM_NOREUSE_CALC_PROC = 3U;
}
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h"
struct ReduceXorSumParam {
    [aicore] ReduceXorSumParam() {};
    LocalTensor<int16_t> tmpTensor1;
    LocalTensor<int16_t> tmpTensor2;
    LocalTensor<int16_t> tmpTensor3;
};

#pragma begin_pipe(V)
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceXorSumCompute(LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, LocalTensor<uint8_t>& tmp, const uint32_t calCount)
{
    static_assert(std::is_same<T, int16_t>::value, "ReduceXorSum only support int16_t data type on current device!");

                       ;

    uint32_t splitSize = 0;
    ReduceXorSumParam param;

    if constexpr (isReuseSource) {
        splitSize = tmp.GetSize() / sizeof(T) / REDUCE_XOR_SUM_REUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
        param.tmpTensor1 = tmp.ReinterpretCast<int16_t>();
        param.tmpTensor2 = param.tmpTensor1[splitSize];
        param.tmpTensor3 = src1;
    } else {
        splitSize = tmp.GetSize() / sizeof(T) / REDUCE_XOR_SUM_NOREUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
        param.tmpTensor1 = tmp.ReinterpretCast<int16_t>();
        param.tmpTensor2 = param.tmpTensor1[splitSize];
        param.tmpTensor3 = param.tmpTensor2[splitSize];
    }



      ;

    SetMaskCount();
    SetVectorMask<T>(0, calCount);
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    And<T, false>(param.tmpTensor1, src0, src1, MASK_PLACEHOLDER, 1, binaryParams);

    Or<T, false>(param.tmpTensor2, src0, src1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Not<T, false>(param.tmpTensor1, param.tmpTensor1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    And<T, false>(param.tmpTensor2, param.tmpTensor1, param.tmpTensor2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    CastInt162Float(param.tmpTensor1.ReinterpretCast<float>(), param.tmpTensor2);
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    ResetMask();

    ReduceSum<float>(param.tmpTensor1.ReinterpretCast<float>(), param.tmpTensor1.ReinterpretCast<float>(),
        param.tmpTensor3.ReinterpretCast<float>(), calCount);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<T>(0, 1);
    CastFloat2Int16(dst, param.tmpTensor1.ReinterpretCast<float>());
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}
#pragma end_pipe
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h" 2







namespace AscendC {
#pragma begin_pipe(V)
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceXorSum(LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    ReduceXorSumCompute<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceXorSum(LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
                                    const LocalTensor<T>&src1Tensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> tmp;
    const bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(tmp);
                                                                                 ;

    ReduceXorSumCompute<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, tmp, calCount);
}
#pragma end_pipe
}
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h" 1
# 13 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_common.h" 1
# 13 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_common.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_common_util_impl.h" 1
# 13 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_common_util_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_common_util_impl.h" 2



namespace AscendC {
namespace Pattern {
namespace Detail {
constexpr int32_t DIM_TWO = 2;
constexpr int32_t DIM_THREE = 3;
constexpr int32_t DIM_FOUR = 4;
constexpr int32_t DIM_FIVE = 5;
constexpr int32_t DIM_SIX = 6;
constexpr int32_t DIM_SEVEN = 7;
constexpr int32_t DIM_EIGHT = 8;
constexpr int32_t DIM_NINE = 9;
constexpr int32_t PATTERN_R = 0;
constexpr int32_t PATTERN_RA = 1;
constexpr int32_t PATTERN_AR = 2;
constexpr int32_t PATTERN_ARA = 3;
constexpr int32_t PATTERN_ARAR = 4;
constexpr int32_t PATTERN_ARARA = 5;
constexpr int32_t PATTERN_ARARAR = 6;
constexpr int32_t PATTERN_ARARARA = 7;
constexpr int32_t PATTERN_ARARARAR = 8;
constexpr int32_t PATTERN_ARARARARA = 9;
constexpr int32_t PATTERN_RAR = 10;
constexpr int32_t PATTERN_RARA = 11;
constexpr int32_t PATTERN_RARAR = 12;
constexpr int32_t PATTERN_RARARA = 13;
constexpr int32_t PATTERN_RARARAR = 14;
constexpr int32_t PATTERN_RARARARA = 15;

template <int32_t id, bool firstA, bool tailA, int32_t dim>
struct PatternConstInfo {
constexpr static int32_t ID = id;
constexpr static bool FirstA = firstA;
constexpr static bool TailA = tailA;
constexpr static int32_t Dim = dim;
};
}
}

namespace Internal {

enum class ApiMode : uint8_t {
    API_MODE_SUM = 0,
    API_MODE_MIN,
    API_MODE_MAX,
    API_MODE_ANY,
    API_MODE_ALL
};


[aicore] __inline__ __attribute__((always_inline)) uint32_t FindClosestPowerOfTwo(uint32_t n)
{
                                                                                      ;
    constexpr uint32_t totalShiftBits = 63;
    return totalShiftBits - ScalarCountLeadingZero(n);
}

template <class T>
[aicore] __inline__ __attribute__((always_inline)) void ComputeMaskBit(uint32_t oneBlkMask, uint32_t oneBlkElems, uint32_t blkNum,
    uint64_t& maskLow, uint64_t& maskHigh)
{


    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        uint32_t maskLBlkNum = blkNum > HALF_DEFAULT_REPEAT_STRIDE ? HALF_DEFAULT_REPEAT_STRIDE : blkNum;
        uint32_t maskHBlkNum = blkNum - maskLBlkNum;
        for (int32_t k = 0; k < maskLBlkNum; k++) {
            maskLow += (((1ULL << oneBlkMask) - 1ULL) << (k * oneBlkElems));
        }
        for (int32_t k = 0; k < maskHBlkNum; k++) {
            maskHigh += (((1ULL << oneBlkMask) - 1ULL) << (k * oneBlkElems));
        }
    } else if constexpr (sizeof(T) == sizeof(float)) {
        for (int32_t k = 0; k < blkNum; k++) {
            maskLow += (((1ULL << oneBlkMask) - 1ULL) << (k * oneBlkElems));
        }
    }
}

template <class T, ApiMode apiMode, MaskMode maskMode = MaskMode::NORMAL>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const int32_t repeat, const uint64_t mask[], const int32_t blkStride, const int32_t repStride)
{
    if constexpr (maskMode == MaskMode::NORMAL) {
        if constexpr (apiMode == ApiMode::API_MODE_SUM) {
            BlockReduceSum(dstTensor, srcTensor, repeat, mask, 1, 1, DEFAULT_REPEAT_STRIDE);
        } else if constexpr (apiMode == ApiMode::API_MODE_MIN || apiMode == ApiMode::API_MODE_ALL) {
            BlockReduceMin(dstTensor, srcTensor, repeat, mask, 1, 1, DEFAULT_REPEAT_STRIDE);
        } else if constexpr (apiMode == ApiMode::API_MODE_MAX || apiMode == ApiMode::API_MODE_ANY) {
            BlockReduceMax(dstTensor, srcTensor, repeat, mask, 1, 1, DEFAULT_REPEAT_STRIDE);
        }
    } else {
        if constexpr (apiMode == ApiMode::API_MODE_MIN || apiMode == ApiMode::API_MODE_ALL) {
            BlockReduceMin<T, false>(dstTensor, srcTensor, 1, MASK_PLACEHOLDER, 1, blkStride, repStride);
        } else if constexpr (apiMode == ApiMode::API_MODE_MAX || apiMode == ApiMode::API_MODE_ANY) {
            BlockReduceMax<T, false>(dstTensor, srcTensor, 1, MASK_PLACEHOLDER, 1, blkStride, repStride);
        }
    }
}

template <class T, ApiMode apiMode>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const int32_t repeat, const int32_t mask, const int32_t repStride)
{
    if constexpr (apiMode == ApiMode::API_MODE_MIN || apiMode == ApiMode::API_MODE_ALL) {
        WholeReduceMin(dstTensor, srcTensor, mask, repeat, 1, 1, repStride, ReduceOrder::ORDER_ONLY_VALUE);
    } else if constexpr (apiMode == ApiMode::API_MODE_MAX || apiMode == ApiMode::API_MODE_ANY) {
        WholeReduceMax(dstTensor, srcTensor, mask, repeat, 1, 1, repStride, ReduceOrder::ORDER_ONLY_VALUE);
    }
}
}
}
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_common.h" 2

namespace AscendC {
namespace Pattern {
namespace Reduce {





struct R : private Detail::PatternConstInfo<Detail::PATTERN_R, true, false, 1> {};
struct RA : private Detail::PatternConstInfo<Detail::PATTERN_RA, false, true, Detail::DIM_TWO> {};
struct AR : private Detail::PatternConstInfo<Detail::PATTERN_AR, true, false, Detail::DIM_TWO> {};
struct ARA : private Detail::PatternConstInfo<Detail::PATTERN_ARA, true, true, Detail::DIM_THREE> {};
struct ARAR : private Detail::PatternConstInfo<Detail::PATTERN_ARAR, true, false, Detail::DIM_FOUR> {};
struct ARARA : private Detail::PatternConstInfo<Detail::PATTERN_ARARA, true, true, Detail::DIM_FIVE> {};
struct ARARAR : private Detail::PatternConstInfo<Detail::PATTERN_ARARAR, true, false, Detail::DIM_SIX> {};
struct ARARARA : private Detail::PatternConstInfo<Detail::PATTERN_ARARARA, true, true, Detail::DIM_SEVEN> {};
struct ARARARAR : private Detail::PatternConstInfo<Detail::PATTERN_ARARARAR, true, false, Detail::DIM_EIGHT> {};
struct ARARARARA : private Detail::PatternConstInfo<Detail::PATTERN_ARARARARA, true, true, Detail::DIM_NINE> {};
struct RAR : private Detail::PatternConstInfo<Detail::PATTERN_RAR, false, false, Detail::DIM_THREE> {};
struct RARA : private Detail::PatternConstInfo<Detail::PATTERN_RARA, false, true, Detail::DIM_FOUR> {};
struct RARAR : private Detail::PatternConstInfo<Detail::PATTERN_RARAR, false, false, Detail::DIM_FIVE> {};
struct RARARA : private Detail::PatternConstInfo<Detail::PATTERN_RARARA, false, true, Detail::DIM_SIX> {};
struct RARARAR : private Detail::PatternConstInfo<Detail::PATTERN_RARARAR, false, false, Detail::DIM_SEVEN> {};
struct RARARARA : private Detail::PatternConstInfo<Detail::PATTERN_RARARARA, false, true, Detail::DIM_EIGHT> {};
}
}
}
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_prod/reduce_prod_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_prod/reduce_prod_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_prod/reduce_prod_v220_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_prod/../reduce_common_util_v220_impl.h" 1
# 13 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_prod/../reduce_common_util_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_prod/../reduce_common_util_v220_impl.h" 2



namespace AscendC {
namespace Internal {

template <typename T, ApiMode apiMode>
[aicore] __inline__ __attribute__((always_inline)) void DoReduceLessThanBlk(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t firstAxis, uint32_t lastAxis)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t firstBlkRepeat = DivCeil(firstAxis, DEFAULT_BLK_NUM);
    uint32_t blkMaxRepeat = DivCeil(firstBlkRepeat, MAX_REPEAT_TIMES);
    uint32_t blkRepeatTail =
        firstBlkRepeat % MAX_REPEAT_TIMES == 0 ? MAX_REPEAT_TIMES : firstBlkRepeat % MAX_REPEAT_TIMES;
    uint32_t mainBlkNum = firstAxis < DEFAULT_BLK_NUM ? firstAxis : DEFAULT_BLK_NUM;
    uint64_t mainMaskLow = 0;
    uint64_t mainMaskHigh = 0;
    ComputeMaskBit<T>(lastAxis, elePerBlk, mainBlkNum, mainMaskLow, mainMaskHigh);
    uint64_t mainMask[] = { mainMaskLow, mainMaskHigh };
    uint32_t tailBlkNum = firstAxis % DEFAULT_BLK_NUM;
    if (tailBlkNum == 0 || firstAxis < DEFAULT_BLK_NUM) {
        uint32_t blkMainRepeat = MAX_REPEAT_TIMES;
        for (int32_t i = 0; i < blkMaxRepeat; i++) {
            blkMainRepeat = i == blkMaxRepeat - 1 ? blkRepeatTail : MAX_REPEAT_TIMES;
            BlockReduceCompute<T, apiMode>(dstTensor[i * MAX_REPEAT_TIMES * DEFAULT_BLK_NUM],
                srcTensor[i * MAX_REPEAT_TIMES * elePerRep], blkMainRepeat, mainMask, 1,
                DEFAULT_REPEAT_STRIDE);
            PipeBarrier<PIPE_V>();
        }
    } else {
        uint64_t tailMaskLow = 0;
        uint64_t tailMaskHigh = 0;
        ComputeMaskBit<T>(lastAxis, elePerBlk, tailBlkNum, tailMaskLow, tailMaskHigh);
        uint64_t tailMask[] = { tailMaskLow, tailMaskHigh };
        for (int32_t i = 0; i < blkMaxRepeat; i++) {
            if (i == blkMaxRepeat - 1) {
                BlockReduceCompute<T, apiMode>(dstTensor[i * MAX_REPEAT_TIMES * DEFAULT_BLK_NUM],
                    srcTensor[i * MAX_REPEAT_TIMES * elePerRep], blkRepeatTail - 1, mainMask, 1,
                    DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
                BlockReduceCompute<T, apiMode>(dstTensor[(i * MAX_REPEAT_TIMES + blkRepeatTail - 1) * DEFAULT_BLK_NUM],
                    srcTensor[(i * MAX_REPEAT_TIMES + blkRepeatTail - 1) * elePerRep], 1, tailMask, 1,
                    DEFAULT_REPEAT_STRIDE);
            } else {
                BlockReduceCompute<T, apiMode>(dstTensor[i * MAX_REPEAT_TIMES * DEFAULT_BLK_NUM],
                    srcTensor[i * MAX_REPEAT_TIMES * elePerRep], MAX_REPEAT_TIMES, mainMask, 1,
                    DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
            }
        }
    }
}

template <typename T, ApiMode apiMode>
[aicore] __inline__ __attribute__((always_inline)) void DoReduceOneBlk(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t firstAxis, uint32_t lastAxis)
{
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * lastAxis);
    BlockReduceCompute<T, apiMode, MaskMode::COUNTER>(dstTensor, srcTensor, 1, MASK_PLACEHOLDER_LIST, 1,
        DEFAULT_REPEAT_STRIDE);
}

template <typename T, void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t,
    const uint8_t, const BinaryRepeatParams &)>
[aicore] __inline__ __attribute__((always_inline)) void AccValOnBlk(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, const BinaryRepeatParams& mainParams, const BinaryRepeatParams& tailParams,
    uint32_t firstAxis, uint32_t lastAxis, uint32_t tmpOffset, uint32_t padLast)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t firstRepeat = DivCeil(firstAxis, MAX_REPEAT_TIMES);
    uint32_t firstRepeatTail = firstAxis % MAX_REPEAT_TIMES == 0 ? MAX_REPEAT_TIMES : firstAxis % MAX_REPEAT_TIMES;
    uint32_t blkCount = lastAxis / elePerBlk;
    uint32_t blkTail = lastAxis % elePerBlk;
    for (int32_t i = 1; i < blkCount; i++) {
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerBlk);
        func(tmpTensor, tmpTensor, srcTensor[i * elePerBlk], MASK_PLACEHOLDER, 1, mainParams);
        PipeBarrier<PIPE_V>();
    }
    if (blkTail != 0) {
        SetMaskNorm();
        SetVectorMask<T, MaskMode::NORMAL>(blkTail);
        uint32_t blkRepeat = MAX_REPEAT_TIMES;
        for (int32_t i = 0; i < firstRepeat; i++) {
            blkRepeat = i == firstRepeat - 1 ? firstRepeatTail : MAX_REPEAT_TIMES;
            func(tmpTensor[i * MAX_REPEAT_TIMES * tmpOffset], tmpTensor[i * MAX_REPEAT_TIMES * tmpOffset],
                srcTensor[i * MAX_REPEAT_TIMES * padLast + blkCount * elePerBlk], blkTail, blkRepeat, tailParams);
            PipeBarrier<PIPE_V>();
        }
        SetMaskCount();
    }
}

template <typename T, bool isReuseSource, ApiMode apiMode,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
        const BinaryRepeatParams &)>
[aicore] __inline__ __attribute__((always_inline)) void DoReduceByBlk(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t lastAxis, uint32_t padLast)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint8_t blkStridePerRow = padLast / elePerBlk;
    uint8_t blkStridePerRep = (padLast / elePerBlk) * DEFAULT_BLK_NUM;
    SetMaskCount();
    if constexpr (!isReuseSource) {
        UnaryRepeatParams blockUnaryParams{ 1, blkStridePerRow, DEFAULT_REPEAT_STRIDE, blkStridePerRep };
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerBlk);
        Adds<T, false>(tmpTensor, srcTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1, blockUnaryParams);
        PipeBarrier<PIPE_V>();
        BinaryRepeatParams blockMainParams{ 1, 1, blkStridePerRow, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, blkStridePerRep};
        BinaryRepeatParams blockTailParams{ 1, 1, 1, 1, 1, blkStridePerRow };
        AccValOnBlk<T, func>(dstTensor, srcTensor, tmpTensor, blockMainParams, blockTailParams, firstAxis, lastAxis, elePerBlk, padLast);
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerBlk);
        BlockReduceCompute<T, apiMode, MaskMode::COUNTER>(dstTensor, tmpTensor, 1, MASK_PLACEHOLDER_LIST, 1,
            DEFAULT_REPEAT_STRIDE);
    } else {
        BinaryRepeatParams blockMainParams{ blkStridePerRow, blkStridePerRow, blkStridePerRow, blkStridePerRep,
            blkStridePerRep, blkStridePerRep};
        BinaryRepeatParams blockTailParams{ 1, 1, 1, blkStridePerRow, blkStridePerRow, blkStridePerRow };
        AccValOnBlk<T, func>(dstTensor, srcTensor, srcTensor, blockMainParams, blockTailParams, firstAxis, lastAxis, padLast, padLast);
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerBlk);
        BlockReduceCompute<T, apiMode, MaskMode::COUNTER>(dstTensor, srcTensor, 1, MASK_PLACEHOLDER_LIST, blkStridePerRow,
            blkStridePerRep);
    }
}

template <typename T, ApiMode apiMode>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceValOnRep(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t tmpOffset, uint32_t repStride)
{
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t firstRepeat = DivCeil(firstAxis, MAX_REPEAT_TIMES);
    uint32_t firstRepeatTail = firstAxis % MAX_REPEAT_TIMES == 0 ? MAX_REPEAT_TIMES : firstAxis % MAX_REPEAT_TIMES;
    if constexpr (IsSameType<T, __cce_half>::value) {
        SetMaskNorm();
        uint32_t blockRepeat = MAX_REPEAT_TIMES;
        for (int32_t i = 0; i < firstRepeat; i++) {
            blockRepeat = i == firstRepeat - 1 ? firstRepeatTail : MAX_REPEAT_TIMES;
            WholeReduceCompute<T, apiMode>(dstTensor[i * MAX_REPEAT_TIMES], tmpTensor[i * MAX_REPEAT_TIMES * tmpOffset],
                blockRepeat, elePerRep, repStride);
            PipeBarrier<PIPE_V>();
        }
    } else {
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerRep);
        BlockReduceCompute<T, apiMode, MaskMode::COUNTER>(tmpTensor, tmpTensor, 1, MASK_PLACEHOLDER_LIST, 1,
            repStride);
        PipeBarrier<PIPE_V>();
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * DEFAULT_BLK_NUM);
        BlockReduceCompute<T, apiMode, MaskMode::COUNTER>(dstTensor, tmpTensor, 1, MASK_PLACEHOLDER_LIST, 1,
            DEFAULT_REPEAT_STRIDE);
    }
}

template <typename T, ApiMode apiMode,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
    const BinaryRepeatParams &)>
[aicore] __inline__ __attribute__((always_inline)) void AccValOnRep(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, const BinaryRepeatParams& binaryParams, uint32_t firstAxis, uint32_t lastAxis,
    uint32_t tmpOffset, uint32_t repStride, uint32_t padLast)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t firstRepeat = DivCeil(firstAxis, MAX_REPEAT_TIMES);
    uint32_t firstRepeatTail = firstAxis % MAX_REPEAT_TIMES == 0 ? MAX_REPEAT_TIMES : firstAxis % MAX_REPEAT_TIMES;
    uint32_t repCount = lastAxis / elePerRep;
    uint32_t repTail = lastAxis % elePerRep;
    for (int32_t i = 1; i < repCount; i++) {
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerRep);
        func(tmpTensor, tmpTensor, srcTensor[i * elePerRep], MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    if (repTail != 0) {
        SetMaskNorm();
        SetVectorMask<T, MaskMode::NORMAL>(repTail);
        uint32_t repRepeat = MAX_REPEAT_TIMES;
        for (int32_t i = 0; i < firstRepeat; i++) {
            repRepeat = i == firstRepeat - 1 ? firstRepeatTail : MAX_REPEAT_TIMES;
            func(tmpTensor[i * MAX_REPEAT_TIMES * tmpOffset], tmpTensor[i * MAX_REPEAT_TIMES * tmpOffset],
                srcTensor[i * MAX_REPEAT_TIMES * padLast + repCount * elePerRep], repTail, repRepeat, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        SetMaskCount();
    }
    GetReduceValOnRep<T, apiMode>(dstTensor, srcTensor, tmpTensor, firstAxis, tmpOffset, repStride);
}

template <typename T, bool isReuseSource, ApiMode apiMode,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
        const BinaryRepeatParams &)>
[aicore] __inline__ __attribute__((always_inline)) void DoReduceByRep(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t lastAxis, uint32_t padLast)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint8_t repStridePerRow = padLast / elePerBlk;
    SetMaskCount();
    if constexpr (!isReuseSource) {
        UnaryRepeatParams repeatUnaryParams{ 1, 1, DEFAULT_REPEAT_STRIDE, repStridePerRow };
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerRep);
        Adds<T, false>(tmpTensor, srcTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1, repeatUnaryParams);
        PipeBarrier<PIPE_V>();
        BinaryRepeatParams binaryParams{ 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, repStridePerRow};
        AccValOnRep<T, apiMode, func>(dstTensor, srcTensor, tmpTensor, binaryParams, firstAxis, lastAxis, elePerRep,
            DEFAULT_REPEAT_STRIDE, padLast);
    } else {
        BinaryRepeatParams binaryParams{ 1, 1, 1, repStridePerRow, repStridePerRow, repStridePerRow};
        AccValOnRep<T, apiMode, func>(dstTensor, srcTensor, srcTensor, binaryParams, firstAxis, lastAxis, padLast,
            repStridePerRow, padLast);
    }
}

template <typename T, bool isReuseSource, ApiMode apiMode,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
        const BinaryRepeatParams &)>
[aicore] __inline__ __attribute__((always_inline)) void DoLongLastReduce(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t lastAxis, uint32_t padLast)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t repCount = DivCeil(lastAxis, elePerRep);
    uint32_t repTail = lastAxis % elePerRep == 0 ? elePerRep : lastAxis % elePerRep;
    BinaryRepeatParams defaultParams;
    UnaryRepeatParams defaultUnaryParams;
    SetMaskCount();
    if constexpr (!isReuseSource) {
        SetVectorMask<T, MaskMode::COUNTER>(0, elePerRep);
        for (int32_t i = 0; i < firstAxis; i++) {
            Adds<T, false>(tmpTensor[i * elePerRep], srcTensor[i * padLast], static_cast<T>(0), MASK_PLACEHOLDER,
                1, defaultUnaryParams);
            PipeBarrier<PIPE_V>();
        }
        uint32_t mask = elePerRep;
        for (int32_t i = 1; i < repCount; i++) {
            mask = i == repCount - 1 ? repTail : elePerRep;
            SetVectorMask<T, MaskMode::COUNTER>(0, mask);
            for (int32_t j = 0; j < firstAxis; j++) {
                func(tmpTensor[j * elePerRep], tmpTensor[j * elePerRep],
                    srcTensor[j * padLast + i * elePerRep], MASK_PLACEHOLDER, 1, defaultParams);
                PipeBarrier<PIPE_V>();
            }
        }
        GetReduceValOnRep<T, apiMode>(dstTensor, srcTensor, tmpTensor, firstAxis, elePerRep, DEFAULT_REPEAT_STRIDE);
    } else {
        uint32_t mask = elePerRep;
        for (int32_t i = 0; i < firstAxis; i++) {
            for (int32_t j = 1; j < repCount; j++) {
                mask = j == repCount - 1 ? repTail : elePerRep;
                SetVectorMask<T, MaskMode::COUNTER>(0, mask);
                if (j == 1) {
                    func(srcTensor[i * elePerRep], srcTensor[i * padLast], srcTensor[i * padLast + j * elePerRep],
                        MASK_PLACEHOLDER, 1, defaultParams);
                    PipeBarrier<PIPE_V>();
                } else {
                    func(srcTensor[i * elePerRep], srcTensor[i * elePerRep], srcTensor[i * padLast + j * elePerRep],
                        MASK_PLACEHOLDER, 1, defaultParams);
                    PipeBarrier<PIPE_V>();
                }
            }
        }
        GetReduceValOnRep<T, apiMode>(dstTensor, srcTensor, srcTensor, firstAxis, elePerRep, DEFAULT_REPEAT_STRIDE);
    }
}

template <typename T, bool isReuseSource, ApiMode apiMode,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
    const BinaryRepeatParams &)>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceByLastAxis(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t lastAxis, uint32_t padLast)
{



      ;
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (lastAxis < elePerBlk) {
        DoReduceLessThanBlk<T, apiMode>(dstTensor, srcTensor, firstAxis, lastAxis);
    } else if (lastAxis == elePerBlk) {
        DoReduceOneBlk<T, apiMode>(dstTensor, srcTensor, firstAxis, lastAxis);
    } else if (lastAxis > elePerBlk && lastAxis < elePerRep) {
        DoReduceByBlk<T, isReuseSource, apiMode, func>(dstTensor, srcTensor, tmpTensor, firstAxis, lastAxis, padLast);
    } else if (lastAxis >= elePerRep && lastAxis <= MAX_REPEAT_TIMES * elePerBlk) {
        DoReduceByRep<T, isReuseSource, apiMode, func>(dstTensor, srcTensor, tmpTensor, firstAxis, lastAxis, padLast);
    } else {
        DoLongLastReduce<T, isReuseSource, apiMode, func>(dstTensor, srcTensor, tmpTensor, firstAxis, lastAxis,
            padLast);
    }
}

struct ReduceParams {
public:
    [aicore] ReduceParams() {}
    [aicore] ReduceParams(uint32_t first, uint32_t last,
        uint32_t padLast, uint32_t splitK, uint32_t tail, uint32_t elePerBlk)
    {
        this->first = first;
        this->last = last;
        this->padLast = padLast;
        this->splitK = splitK;
        this->tail = tail;
        this->elePerBlk = elePerBlk;
    }
    uint32_t first = 0;
    uint32_t last = 0;
    uint32_t padLast = 0;
    uint32_t splitK = 0;
    uint32_t tail = 0;
    uint32_t elePerBlk = 0;
    BinaryRepeatParams defaultParam = { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE };
    UnaryRepeatParams defaultUnaryParam = { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE };
};

template <class T, ApiMode apiMode>
[aicore] __inline__ __attribute__((always_inline)) void BlkReduceForLoop(const LocalTensor<T>& dst,
    const LocalTensor<T>& tmp, uint32_t srcOffset, uint32_t first, uint32_t last) {
    constexpr uint32_t blkReduceDstStride = 8;
    uint32_t srcPerBlkElements = ONE_BLK_SIZE/sizeof(T);
    uint64_t maskHigh = 0;
    uint32_t oneRepElements = srcPerBlkElements * DEFAULT_BLK_NUM;
    uint32_t nMaxRepBlkNum = first / (MAX_REPEAT_TIMES * DEFAULT_BLK_NUM);
    uint32_t tailMaxRepBlkNum = first % (MAX_REPEAT_TIMES * DEFAULT_BLK_NUM);
    uint32_t tailNBlkNum = tailMaxRepBlkNum / DEFAULT_BLK_NUM;
    uint32_t tailRemainOfBlkNum = tailMaxRepBlkNum % DEFAULT_BLK_NUM;
    uint32_t dstOffset = 0;
    uint32_t blkReduceSrcOffset = 0;
    uint32_t oneBlkMask = last > srcPerBlkElements ? srcPerBlkElements : last;
    uint64_t maskLow = 0;
    ComputeMaskBit<T>(oneBlkMask, srcPerBlkElements, DEFAULT_BLK_NUM, maskLow, maskHigh);

    uint64_t blkReduceMask[] = { maskLow, maskHigh };
    for (int k = 0; k < nMaxRepBlkNum; k++) {
        BlockReduceCompute<T, apiMode>(dst[dstOffset], tmp[srcOffset], MAX_REPEAT_TIMES, blkReduceMask,
            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
    }
    if (tailNBlkNum > 0) {
        dstOffset = nMaxRepBlkNum * MAX_REPEAT_TIMES * blkReduceDstStride;
        blkReduceSrcOffset = srcOffset + nMaxRepBlkNum * MAX_REPEAT_TIMES * srcPerBlkElements;
        BlockReduceCompute<T, apiMode>(dst[dstOffset], tmp[blkReduceSrcOffset], tailNBlkNum, blkReduceMask,
            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
    }
    if (tailRemainOfBlkNum > 0) {
        maskLow = 0;
        maskHigh = 0;
        uint32_t tailBlkReduceRep = 1;
        ComputeMaskBit<T>(oneBlkMask, srcPerBlkElements, tailRemainOfBlkNum, maskLow, maskHigh);
        uint64_t tailMask[] = { maskLow, maskHigh };
        dstOffset = tailNBlkNum * blkReduceDstStride + (nMaxRepBlkNum * MAX_REPEAT_TIMES * blkReduceDstStride);
        blkReduceSrcOffset = srcOffset + tailNBlkNum * oneRepElements + (nMaxRepBlkNum * MAX_REPEAT_TIMES * oneRepElements);
        BlockReduceCompute<T, apiMode>(dst[dstOffset], tmp[blkReduceSrcOffset], tailBlkReduceRep, tailMask,
            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T, bool isReuseSource, ApiMode apiMode,
            void (*func)(const LocalTensor<__cce_half> &, const LocalTensor<__cce_half> &,
                       const LocalTensor<__cce_half> &, uint64_t, const uint8_t,
                       const BinaryRepeatParams &)>
[aicore] __inline__ __attribute__((always_inline)) void BinaryReduceAnyAllCompute(
    const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<T> &tmp, const ReduceParams &params)
{
    __cce_half halfZero = 0.0;
    LocalTensor<__cce_half> tmpBuf = tmp.template ReinterpretCast<__cce_half>();
    uint32_t tmpK;
    constexpr uint32_t halfBlkElements = 16;
    SetMaskCount();
    for (int i = 0; i < params.first; i++) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(params.padLast);
        Cast<__cce_half, uint8_t, false>(tmpBuf, src[i*params.padLast], RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        if (params.tail > 0 && params.splitK > 0) {
            SetVectorMask<__cce_half, MaskMode::COUNTER>(params.tail);
            func(tmpBuf, tmpBuf, tmpBuf[params.splitK], MASK_PLACEHOLDER, 1, params.defaultParam);
            PipeBarrier<PIPE_V>();
        }
        tmpK = params.splitK;
        while (tmpK > halfBlkElements) {
            tmpK >>= 1;
            SetVectorMask<__cce_half, MaskMode::COUNTER>(tmpK);
            func(tmpBuf, tmpBuf, tmpBuf[tmpK], MASK_PLACEHOLDER, 1, params.defaultParam);
            PipeBarrier<PIPE_V>();
        }
        SetVectorMask<__cce_half, MaskMode::COUNTER>(halfBlkElements);
        Adds<__cce_half, false>(tmpBuf[params.padLast + i * halfBlkElements], tmpBuf, halfZero,
            MASK_PLACEHOLDER, 1, params.defaultUnaryParam);
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
    BlkReduceForLoop<__cce_half, apiMode>(tmpBuf, tmpBuf, params.padLast, params.first, params.last);
    SetMaskCount();
    SetVectorMask<__cce_half, MaskMode::COUNTER>(params.first);
    Cast<uint8_t, __cce_half, false>(dst, tmpBuf, RoundMode::CAST_NONE,
        MASK_PLACEHOLDER, 1,
        { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
    const BinaryRepeatParams &)>
[aicore] __inline__ __attribute__((always_inline)) void BinaryReduceByFirstAxis(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t lastAxis, uint32_t padLast)
{



      ;
    BinaryRepeatParams defaultParam;
    UnaryRepeatParams defaultUnaryParam;
    uint32_t k = FindClosestPowerOfTwo(firstAxis);
    uint32_t splitK = 1 << k;
    uint32_t remain = firstAxis - splitK;
    SetMaskCount();
    if constexpr (isReuseSource) {

        if (remain != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, padLast * remain);
            func(srcTensor, srcTensor, srcTensor[splitK * padLast], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        }
    } else {
        CheckTmpBufferSize(tmpTensor.GetSize(), 0, tmpTensor.GetSize());

        if (remain != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, splitK * padLast);
            Adds<T, false>(tmpTensor, srcTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1,
                           defaultUnaryParam);
            PipeBarrier<PIPE_V>();
            SetVectorMask<T, MaskMode::COUNTER>(0, padLast * remain);
            func(tmpTensor, tmpTensor, srcTensor[splitK * padLast], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        } else if (splitK > 1) {
            splitK >>= 1;
            SetVectorMask<T, MaskMode::COUNTER>(0, padLast * splitK);
            func(tmpTensor, srcTensor, srcTensor[splitK * padLast], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        } else {
            SetVectorMask<T, MaskMode::COUNTER>(0, lastAxis);
            Adds<T, false>(dstTensor, srcTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1, defaultUnaryParam);
            PipeBarrier<PIPE_V>();
            return;
        }
    }

    LocalTensor<T> currBuff = isReuseSource ? srcTensor : tmpTensor;
    while (splitK > 1) {
        splitK >>= 1;
        SetVectorMask<T, MaskMode::COUNTER>(0, padLast * splitK);
        func(currBuff, currBuff, currBuff[splitK * padLast], MASK_PLACEHOLDER, 1, defaultParam);
        PipeBarrier<PIPE_V>();
    }
    SetVectorMask<T, MaskMode::COUNTER>(0, lastAxis);
    Adds<T, false>(dstTensor, currBuff, static_cast<T>(0), MASK_PLACEHOLDER, 1, defaultUnaryParam);
    PipeBarrier<PIPE_V>();
}
}
}
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_prod/reduce_prod_v220_impl.h" 2



namespace AscendC {
namespace Internal {

template <typename T, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void PreProcessReduceForAR(const LocalTensor<T>& src, const LocalTensor<T>& currBuff,
    uint32_t row, uint32_t last, uint32_t padLast, uint32_t remain, uint32_t& splitK)
{
    BinaryRepeatParams defaultParam;
    UnaryRepeatParams defaultUnaryParam;
    constexpr uint32_t bytePerBlk = 32;
    constexpr uint32_t elePerBlk = bytePerBlk / sizeof(T);
    if constexpr (isReuseSource) {
        if (last >= elePerBlk && remain != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, remain);
            Mul<T, false>(currBuff, currBuff, currBuff[splitK], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        }
    } else {
        if (last >= elePerBlk && remain != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, splitK);
            Adds<T, false>(currBuff, src[row * padLast], static_cast<T>(0), MASK_PLACEHOLDER, 1, defaultUnaryParam);
            PipeBarrier<PIPE_V>();

            SetVectorMask<T, MaskMode::COUNTER>(0, remain);
            Mul<T, false>(currBuff, currBuff, src[row * padLast + splitK], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        } else if (splitK > elePerBlk) {
            splitK >>= 1;
            SetVectorMask<T, MaskMode::COUNTER>(0, splitK);

            Mul<T, false>(currBuff, src[row * padLast], src[row * padLast + splitK], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        } else {
            SetVectorMask<T, MaskMode::COUNTER>(0, last);
            Adds<T, false>(currBuff, src[row * padLast], static_cast<T>(0), MASK_PLACEHOLDER, 1, defaultUnaryParam);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void ReduceProdByLastAxis(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmp, uint32_t first, uint32_t last, uint32_t padLast)
{
    constexpr uint32_t bytePerBlk = 32;
    constexpr uint32_t elePerBlk = bytePerBlk / sizeof(T);
    constexpr uint32_t bytePerRep = 256;
    constexpr uint32_t elePerRep = bytePerRep / sizeof(T);

    BinaryRepeatParams defaultParam;
    UnaryRepeatParams defaultUnaryParam;
    BrcbRepeatParams defaultBrcbParam;;
    LocalTensor<T> resBeforeGather = tmp[elePerRep];
    LocalTensor<T> finalResStored = isReuseSource ? src : resBeforeGather;

    uint32_t k = FindClosestPowerOfTwo(last);
    uint32_t splitK = 1 << k;
    uint32_t remain = last - splitK;
    SetMaskCount();
    if constexpr (!isReuseSource) {
        CheckTmpBufferSize(tmp.GetSize(), 0, tmp.GetSize());
    }
    for (uint32_t j = 0; j < first; j++) {
        uint32_t splitKCopy = splitK;
        LocalTensor<T> tmpRowRes = isReuseSource ? src[j * padLast] : resBeforeGather[j * elePerBlk];
        LocalTensor<T> tmpDst = isReuseSource ? src[j * elePerBlk] : resBeforeGather[j * elePerBlk];

        PreProcessReduceForAR<T, isReuseSource>(src, tmpRowRes, j, last, padLast, remain, splitKCopy);

        while (splitKCopy > elePerBlk) {
            splitKCopy >>= 1;
            SetVectorMask<T, MaskMode::COUNTER>(0, splitKCopy);

            Mul<T, false>(tmpRowRes, tmpRowRes, tmpRowRes[splitKCopy], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        }

        Brcb(tmp, tmpRowRes, 1, defaultBrcbParam);
        PipeBarrier<PIPE_V>();
        uint32_t finalTail = last < elePerBlk ? splitK : elePerBlk;
        if (splitK != last && finalTail < elePerBlk) {
            SetVectorMask<T, MaskMode::COUNTER>(0, remain * elePerBlk);
            Mul<T, false>(tmp, tmp, tmp[splitK * elePerBlk], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        }
        while (finalTail > 1) {
            finalTail >>= 1;
            SetVectorMask<T, MaskMode::COUNTER>(0, finalTail * elePerBlk);

            Mul<T, false>(tmp, tmp, tmp[finalTail * elePerBlk], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        }
        SetVectorMask<T, MaskMode::COUNTER>(0, elePerBlk);
        Adds<T, false>(tmpDst, tmp, static_cast<T>(0), MASK_PLACEHOLDER, 1, defaultUnaryParam);
        PipeBarrier<PIPE_V>();
    }
    LocalTensor<uint32_t> tmpInt = tmp.template ReinterpretCast<uint32_t>();
    Duplicate(tmpInt, 1u, elePerRep);
    PipeBarrier<PIPE_V>();
    GatherMaskParams gatherMaskParam = {1, static_cast<uint16_t>(first), 1, 0};
    uint64_t rsvdCnt;
    GatherMask(dst, finalResStored, tmpInt, true, elePerBlk, gatherMaskParam, rsvdCnt);
}

template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceProdImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                      const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[],
                                      bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, float>(), "failed to check the data type, current api supports data type is float!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                               ;
    LocalTensor<T> tmpDst = sharedTmpBuffer.ReinterpretCast<T>();
    if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
        ReduceProdByLastAxis<T, isReuseSource>(dstTensor, srcTensor,tmpDst, first, last, padLast);
    } else {
        BinaryReduceByFirstAxis<T, isReuseSource, Mul<T, false>>(
            dstTensor, srcTensor, tmpDst, first, last, padLast);
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_max/reduce_max_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_max/reduce_max_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_max/reduce_max_v220_impl.h" 2





namespace AscendC {
namespace Internal {
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                      const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[],
                                      bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, __cce_half, float>(), "failed to check the data type, current api supports data type is half/float!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                              ;
    LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();

    if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
        BlockReduceByLastAxis<T, isReuseSource, ApiMode::API_MODE_MAX, Max<T, false>>(
            dstTensor, srcTensor, tmpTensor, first, last, padLast);
    } else {
        BinaryReduceByFirstAxis<T, isReuseSource, Max<T, false>>(
            dstTensor, srcTensor, tmpTensor, first, last, padLast);
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_min/reduce_min_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_min/reduce_min_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_min/reduce_min_v220_impl.h" 2





namespace AscendC {
namespace Internal {
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                      const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[],
                                      bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, __cce_half, float>(), "failed to check the data type, current api supports data type is half/float!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                              ;

    LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();

    if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
        BlockReduceByLastAxis<T, isReuseSource, ApiMode::API_MODE_MIN, Min<T, false>>(
            dstTensor, srcTensor, tmpTensor, first, last, padLast);
    } else {
        BinaryReduceByFirstAxis<T, isReuseSource, Min<T, false>>(
            dstTensor, srcTensor, tmpTensor, first, last, padLast);
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_sum/reduce_sum_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_sum/reduce_sum_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_sum/reduce_sum_v220_impl.h" 2





namespace AscendC {
namespace Internal {

template <class T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumComputeSliceAAxis(const LocalTensor<T>& dst, const LocalTensor<T>& src,
        const LocalTensor<T>& tmpBuf, const uint32_t repeat,
        const uint32_t perRowReduceSize, const uint32_t tailLen, const ReduceParams &params)
{
    uint32_t curOffset;
    if constexpr (isReuseSource) {
        SetVectorMask<T, MaskMode::COUNTER>(tailLen);
        for (uint32_t i = 0; i < repeat; i++) {
            curOffset = i * params.padLast;
            Add<T, false>(tmpBuf[curOffset], tmpBuf[curOffset],
                src[curOffset + perRowReduceSize], MASK_PLACEHOLDER, 1, params.defaultParam);
        }
        PipeBarrier<PIPE_V>();

        uint16_t blockCount = repeat;
        uint16_t blockLen = perRowReduceSize / B32_DATA_NUM_PER_BLOCK;
        uint16_t srcStride = (params.padLast - perRowReduceSize) / B32_DATA_NUM_PER_BLOCK;
        uint16_t dstStride = 0;
        DataCopy(tmpBuf, tmpBuf, { blockCount, blockLen, srcStride, dstStride });
        PipeBarrier<PIPE_V>();
    } else {
        for (uint32_t i = 0; i < repeat; i++) {
            curOffset = i * params.padLast;
            SetVectorMask<T, MaskMode::COUNTER>(tailLen);
            Add<T, false>(tmpBuf[i * perRowReduceSize], tmpBuf[i * perRowReduceSize],
                src[curOffset + perRowReduceSize], MASK_PLACEHOLDER, 1, params.defaultParam);
        }
        PipeBarrier<PIPE_V>();
    }
}

template <class T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumComputeSliceRAxis(const LocalTensor<T>& dst, const LocalTensor<T>& src,
        const LocalTensor<T>& tmpBuf, const uint32_t repeat,
        const uint32_t perRowReduceSize, const uint32_t tailLenN, const uint32_t tailLenRemain, const ReduceParams &params)
{
    SetMaskNorm();
    uint32_t srcOffset;
    uint32_t tmpOffset;
    uint32_t perRowSize = perRowReduceSize;
    if constexpr (isReuseSource) {
        perRowSize = params.padLast;
    }
    uint8_t dstRepStride = static_cast<uint8_t>(perRowSize / B32_DATA_NUM_PER_BLOCK);
    uint8_t src0RepStride = dstRepStride;
    uint8_t src1RepStride = static_cast<uint8_t>(params.padLast / B32_DATA_NUM_PER_BLOCK);
    SetVectorMask<T, MaskMode::NORMAL>(B32_DATA_NUM_PER_REPEAT);
    for (uint32_t i = 0; i < tailLenN; i++) {
        srcOffset = perRowReduceSize + i * B32_DATA_NUM_PER_REPEAT;
        tmpOffset = i * B32_DATA_NUM_PER_REPEAT;
        Add<T, false>(tmpBuf[tmpOffset], tmpBuf[tmpOffset], src[srcOffset], MASK_PLACEHOLDER, repeat,
            {1, 1, 1, dstRepStride, src0RepStride, src1RepStride});
    }
    if (tailLenRemain > 0) {
        tmpOffset = tailLenN * B32_DATA_NUM_PER_REPEAT;
        srcOffset = perRowReduceSize + tailLenN * B32_DATA_NUM_PER_REPEAT;
        SetVectorMask<T, MaskMode::NORMAL>(tailLenRemain);
        Add<T, false>(tmpBuf[tmpOffset], tmpBuf[tmpOffset], src[srcOffset], MASK_PLACEHOLDER, repeat,
            {1, 1, 1, dstRepStride, src0RepStride, src1RepStride});
    }
    PipeBarrier<PIPE_V>();
    if constexpr (isReuseSource) {
        uint16_t blockCount = repeat;
        uint16_t blockLen = perRowReduceSize / B32_DATA_NUM_PER_BLOCK;
        uint16_t srcStride = (params.padLast - perRowReduceSize) / B32_DATA_NUM_PER_BLOCK;
        uint16_t dstStride = 0;
        DataCopy(tmpBuf, tmpBuf, { blockCount, blockLen, srcStride, dstStride });
        PipeBarrier<PIPE_V>();
    }
}

template <class T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumComputeTail(const LocalTensor<T>& dst, const LocalTensor<T>& src,
        const LocalTensor<T>& tmpBuf, const uint32_t repeat,
        const uint32_t perRowReduceSize, const ReduceParams &params)
{
    uint32_t tailLen = params.tail;
    if (params.tail == 0 && perRowReduceSize > 0) {
        tailLen = params.padLast - perRowReduceSize;
    }
    uint32_t tailLenN = tailLen / B32_DATA_NUM_PER_REPEAT;
    uint32_t tailLenRemain = tailLen % B32_DATA_NUM_PER_REPEAT;
    bool isLastAxisSliceLarge = (tailLenN + (tailLenRemain > 0)) < repeat;
    uint32_t vaddMaxRepStrideVal = 255;

    bool isLeMaxRepStride = (perRowReduceSize / B32_DATA_NUM_PER_BLOCK) <= vaddMaxRepStrideVal;
    bool performanceSlice = isLastAxisSliceLarge && params.last > B32_DATA_NUM_PER_REPEAT && isLeMaxRepStride;
    if (performanceSlice) {
        ReduceSumComputeSliceRAxis<T, isReuseSource>(dst, src, tmpBuf,
            repeat, perRowReduceSize, tailLenN, tailLenRemain, params);
    } else {
        ReduceSumComputeSliceAAxis<T, isReuseSource>(dst, src, tmpBuf, repeat, perRowReduceSize, tailLen, params);
    }
}

template <class T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumInLargeLast(const LocalTensor<T>& dst, const LocalTensor<T>& src,
        const LocalTensor<T>& tmpBuf, const uint32_t repeat,
        const uint32_t perRowReduceSize, const ReduceParams &params)
{
    if constexpr (!isReuseSource) {
        uint16_t blockCount = repeat;
        uint16_t blockLen = perRowReduceSize / B32_DATA_NUM_PER_BLOCK;
        uint16_t srcStride = (params.padLast - perRowReduceSize) / B32_DATA_NUM_PER_BLOCK;
        uint16_t dstStride = 0;
        DataCopy(tmpBuf, src, { blockCount, blockLen, srcStride, dstStride });
        PipeBarrier<PIPE_V>();
    }
    ReduceSumComputeTail<T, isReuseSource>(dst, src, tmpBuf, repeat, perRowReduceSize, params);
    ResetMask();
    SetMaskCount();
    uint32_t tmpK = perRowReduceSize;
    while (tmpK > B32_DATA_NUM_PER_REPEAT) {
        SetVectorMask<T, MaskMode::COUNTER>(repeat * tmpK);
        PairReduceSum<T, false>(tmpBuf, tmpBuf, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        tmpK >>= 1;
    }
    SetVectorMask<T, MaskMode::COUNTER>(repeat * B32_DATA_NUM_PER_REPEAT);
    BlockReduceSum<T, false>(tmpBuf, tmpBuf, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<T, MaskMode::COUNTER>(repeat * B32_DATA_NUM_PER_BLOCK);
    BlockReduceSum<T, false>(dst, tmpBuf, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
}

template <class T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumArCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuf, const uint32_t perRowReduceSize, const ReduceParams &params)
{
    constexpr uint32_t maxRepeatTimes = 248;
    uint32_t maxRepOffsetN = params.first / maxRepeatTimes;
    uint32_t maxRepeatOffsetTail = params.first % maxRepeatTimes;
    uint32_t srcMaxRepNOffset;
    uint32_t dstMaxRepNOffset;
    uint32_t srcMaxRepTailOffset;
    uint32_t dstMaxRepTailOffset;
    if (params.last <= B32_DATA_NUM_PER_REPEAT) {
        SetMaskNorm();
        SetVectorMask<T, MaskMode::NORMAL>(params.last);
        for (uint32_t i = 0; i < maxRepOffsetN; i++) {
            srcMaxRepNOffset = maxRepeatTimes * i * params.padLast;
            dstMaxRepNOffset = maxRepeatTimes * i;
            WholeReduceSum<T, false>(dst[dstMaxRepNOffset], src[srcMaxRepNOffset],
                MASK_PLACEHOLDER, maxRepeatTimes, 1, 1, params.padLast / params.elePerBlk);
        }
        if (maxRepeatOffsetTail > 0) {
            srcMaxRepTailOffset = (params.first - maxRepeatOffsetTail) * params.padLast;
            dstMaxRepTailOffset = params.first - maxRepeatOffsetTail;
            SetVectorMask<T, MaskMode::NORMAL>(params.last);
            WholeReduceSum<T, false>(dst[dstMaxRepTailOffset], src[srcMaxRepTailOffset],
                MASK_PLACEHOLDER, maxRepeatOffsetTail, 1, 1, params.padLast / params.elePerBlk);
        }
    } else {
        SetMaskCount();
        uint32_t tmpBufMaxRepNOffset;

        for (uint32_t i = 0; i < maxRepOffsetN; i++) {
            srcMaxRepNOffset = maxRepeatTimes * i * params.padLast;
            dstMaxRepNOffset = maxRepeatTimes * i;
            tmpBufMaxRepNOffset = maxRepeatTimes * i * B32_DATA_NUM_PER_REPEAT;
            ReduceSumInLargeLast<T, isReuseSource>(dst[dstMaxRepNOffset], src[srcMaxRepNOffset],
                tmpBuf[tmpBufMaxRepNOffset], maxRepeatTimes, perRowReduceSize, params);
        }
        if (maxRepeatOffsetTail > 0) {
            srcMaxRepTailOffset = (params.first - maxRepeatOffsetTail) * params.padLast;
            dstMaxRepTailOffset = params.first - maxRepeatOffsetTail;
            tmpBufMaxRepNOffset = (params.first - maxRepeatOffsetTail) * B32_DATA_NUM_PER_REPEAT;
            ReduceSumInLargeLast<T, isReuseSource>(dst[dstMaxRepTailOffset],
                src[srcMaxRepTailOffset], tmpBuf[tmpBufMaxRepNOffset],
                maxRepeatOffsetTail, perRowReduceSize, params);
        }
        SetMaskNorm();
    }
    PipeBarrier<PIPE_V>();
    ResetMask();
}

template <class T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumArReusedSrc(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    ReduceParams params)
{
    if (params.last <= B32_DATA_NUM_PER_BLOCK) {
        BlkReduceForLoop<T, ApiMode::API_MODE_SUM>(dst, src, 0, params.first, params.last);
    } else {
        uint32_t perRowReduceSize = params.elePerBlk > params.splitK ? params.elePerBlk : params.splitK;
        if (params.last == params.splitK) {
            perRowReduceSize >>= 1;
        }
        ReduceSumArCompute<T, isReuseSource>(dst, src, src, perRowReduceSize, params);
    }
}

template <class T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumArUnReusedSrc(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuf, const ReduceParams &params)
{
    uint32_t perRowReduceSize = params.elePerBlk > params.splitK ? params.elePerBlk : params.splitK;
    if (params.last == params.splitK) {
        perRowReduceSize >>= 1;
    }

    if (params.last <= B32_DATA_NUM_PER_BLOCK) {
        BlkReduceForLoop<T, ApiMode::API_MODE_SUM>(dst, src, 0, params.first, params.last);
    } else {
        ReduceSumArCompute<T, isReuseSource>(dst, src, tmpBuf, perRowReduceSize, params);
    }
}

template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumCommon(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad, const ReduceParams &reduceParams)
{
    uint32_t first = reduceParams.first;
    uint32_t last = reduceParams.last;
    uint32_t padLast = reduceParams.padLast;
    uint32_t elePerBlk = reduceParams.elePerBlk;
    LocalTensor<T> tmpBuf = sharedTmpBuffer.ReinterpretCast<T>();
    if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {





          ;
        uint32_t splitK = 1 << FindClosestPowerOfTwo(last);
        if (last <= elePerBlk) {
            splitK = 0;
        }
        uint32_t tail = last - splitK;
        if constexpr (isReuseSource) {
            ReduceSumArReusedSrc<T, true>(dstTensor, srcTensor, ReduceParams(
                first, last, padLast, splitK, tail, elePerBlk));
        } else {
            ReduceSumArUnReusedSrc<T, false>(dstTensor, srcTensor, tmpBuf,
                ReduceParams(first, last, padLast, splitK, tail, elePerBlk));
        }
    } else {



          ;
        BinaryReduceByFirstAxis<T, isReuseSource, Add<T, false>>(
                dstTensor, srcTensor, tmpBuf, first, last, padLast);
    }
}

template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad) {
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, float>(), "failed to check the data type, current api supports data type is float!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                              ;
    ReduceParams reduceParams = ReduceParams(first, last, padLast, 0, 0, elePerBlk);
    ReduceSumCommon<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad, reduceParams);
}

}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_mean/reduce_mean_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_mean/reduce_mean_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_mean/reduce_mean_v220_impl.h" 2





namespace AscendC {
namespace Internal {
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMeanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, __cce_half, float>(), "failed to check the data type, current api supports data type is half/float!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");

                                                         ;
    ReduceParams reduceParams = ReduceParams(first, last, padLast, 0, 0, elePerBlk);
    ReduceSumCommon<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad, reduceParams);
    SetMaskCount();
    UnaryRepeatParams defaultUnaryParam;
    if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
        float lastAxisValReciprocal = 1.0f/static_cast<int32_t>(last);
        SetVectorMask<T, MaskMode::COUNTER>(first);
        Muls<T, false>(dstTensor, dstTensor, lastAxisValReciprocal, MASK_PLACEHOLDER, 1, defaultUnaryParam);
        PipeBarrier<PIPE_V>();
    } else {
        float firstAxisValReciprocal = 1.0f/static_cast<int32_t>(first);
        SetVectorMask<T, MaskMode::COUNTER>(last);
        Muls<T, false>(dstTensor, dstTensor, firstAxisValReciprocal, MASK_PLACEHOLDER, 1, defaultUnaryParam);
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_any/reduce_any_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_any/reduce_any_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_any/reduce_any_v220_impl.h" 2





namespace AscendC {
namespace Internal {
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceAnyImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, float, uint8_t>(), "failed to check the data type, current api supports data type is float/uint8_t!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                              ;
    if constexpr (SupportType<T, uint8_t>()) {
        if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
            uint32_t splitK = 1 << FindClosestPowerOfTwo(last);
            if (last < elePerBlk) {
                splitK = 0;
            }
            uint32_t tail = last - splitK;
            BinaryReduceAnyAllCompute<T, isReuseSource, ApiMode::API_MODE_MAX, Max<__cce_half, false>>(
                    dstTensor, srcTensor, sharedTmpBuffer, ReduceParams(first, last, padLast, splitK, tail, elePerBlk));
        } else {

            padLast >>= 1;
            last = (last + 1) >> 1;
            LocalTensor<int16_t> srcTmpBuff = srcTensor.template ReinterpretCast<int16_t>();
            LocalTensor<int16_t> dstTmpBuff = dstTensor.template ReinterpretCast<int16_t>();
            LocalTensor<int16_t> tmpBuf = sharedTmpBuffer.template ReinterpretCast<int16_t>();
            BinaryReduceByFirstAxis<int16_t, isReuseSource, Or<int16_t, false>>(
                dstTmpBuff, srcTmpBuff, tmpBuf, first, last, padLast);
        }
    } else {
        LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();

        if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
            BlockReduceByLastAxis<T, isReuseSource, ApiMode::API_MODE_ANY, Max<T, false>>(
                dstTensor, srcTensor, tmpTensor, first, last, padLast);
        } else {
            BinaryReduceByFirstAxis<T, isReuseSource, Max<T, false>>(
                dstTensor, srcTensor, tmpTensor, first, last, padLast);
        }
        SetMaskNorm();
        ResetMask();
    }
}
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_all/reduce_all_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_all/reduce_all_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_all/reduce_all_v220_impl.h" 2





namespace AscendC {
namespace Internal {
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceAllImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                      const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[],
                                      bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, float, uint8_t>(), "failed to check the data type, current api supports data type is float/uint8_t!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                              ;
    if constexpr (SupportType<T, uint8_t>()) {
        if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
            uint32_t splitK = 1 << FindClosestPowerOfTwo(last);
            if (last < elePerBlk) {
                splitK = 0;
            }
            uint32_t tail = last - splitK;
            BinaryReduceAnyAllCompute<T, isReuseSource, ApiMode::API_MODE_MIN, Min<__cce_half, false>>(
                dstTensor, srcTensor, sharedTmpBuffer, ReduceParams(first, last, padLast, splitK, tail, elePerBlk));

        } else {

            padLast >>= 1;
            last = (last + 1) >> 1;
            LocalTensor<int16_t> srcTmpBuff = srcTensor.template ReinterpretCast<int16_t>();
            LocalTensor<int16_t> dstTmpBuff = dstTensor.template ReinterpretCast<int16_t>();
            LocalTensor<int16_t> tmpBuf = sharedTmpBuffer.template ReinterpretCast<int16_t>();
            BinaryReduceByFirstAxis<int16_t, isReuseSource, And<int16_t, false>>(
                dstTmpBuff, srcTmpBuff, tmpBuf, first, last, padLast);
        }
    } else {
        LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
        if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
            BlockReduceByLastAxis<T, isReuseSource, ApiMode::API_MODE_ALL, Min<T, false>>(
                dstTensor, srcTensor, tmpTensor, first, last, padLast);
        } else {
            BinaryReduceByFirstAxis<T, isReuseSource, Min<T, false>>(
                dstTensor, srcTensor, tmpTensor, first, last, padLast);
        }
        SetMaskNorm();
        ResetMask();
    }
}
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceProd(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceProdImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 72 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceProd(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceProd<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 99 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceMaxImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 124 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceMax<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 151 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceMinImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 176 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceMin<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 203 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceAny(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceAnyImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 228 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceAny(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceAny<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 255 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceAll(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceAllImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 280 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceAll(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceAll<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 307 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceSumImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 332 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceSum<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 359 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMean(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceMeanImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 384 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMean(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceMean<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}

#pragma end_pipe
}
# 93 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cumsum/cumsum_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cumsum/cumsum_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cumsum/cumsum_common_impl.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum_utils.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum_utils.h"
namespace AscendC {

struct CumSumConfig {
    bool isLastAxis{true};
    bool isReuseSource{false};
    bool outputLastRow{false};
};

struct CumSumInfo {
    uint32_t outter{0};
    uint32_t inner{0};
};

};
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cumsum/cumsum_common_impl.h" 2

namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) TransDataTo5HDParams ExtractTransDataParam(uint8_t repeatTimes, uint32_t inner, uint16_t alignOutter,
    uint32_t oneBlockElementNum, uint16_t dstRepStride, uint32_t srcRepStride)
{
    repeatTimes = inner / oneBlockElementNum;
    if (repeatTimes > 1) {


        return TransDataTo5HDParams(false, false, repeatTimes, alignOutter, 1);
    } else {
        return TransDataTo5HDParams(false, false, repeatTimes, dstRepStride, srcRepStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CumSumLastDim(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    LocalTensor<T> tempBuffer, const CumSumInfo &cumSumInfo)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    uint16_t alignOutter =
        (cumSumInfo.outter + NCHW_CONV_ADDR_LIST_SIZE - 1) / NCHW_CONV_ADDR_LIST_SIZE * NCHW_CONV_ADDR_LIST_SIZE;
    uint64_t transDataTo5HDDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t transDataTo5HDSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint8_t repeatTimes = 1;
    uint16_t dstRepStride = 0;
    uint16_t srcRepStride = 0;


    if (cumSumInfo.outter == alignOutter && alignOutter > cumSumInfo.inner) {
        repeatTimes = alignOutter / NCHW_CONV_ADDR_LIST_SIZE;
        if (repeatTimes > 1) {


            dstRepStride = 1;
            srcRepStride = cumSumInfo.inner;
        }
        TransDataTo5HDParams params(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < cumSumInfo.inner / oneBlockElementNum; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)srcTensor[i * oneBlockElementNum + n * cumSumInfo.inner].GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer[i * oneBlockElementNum * alignOutter + alignOutter * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    } else {
        TransDataTo5HDParams params = ExtractTransDataParam(repeatTimes, cumSumInfo.inner, alignOutter,
            oneBlockElementNum, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] = (uint64_t)srcTensor[((i * NCHW_CONV_ADDR_LIST_SIZE +
                    n % (cumSumInfo.outter - i * NCHW_CONV_ADDR_LIST_SIZE)) * cumSumInfo.inner)].GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + alignOutter * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(alignOutter * cumSumInfo.inner);
    LocalTensor<float> floatTempBuffer = tempBuffer[alignOutter * cumSumInfo.inner].template ReinterpretCast<float>();
    Cast<float, T, false>(floatTempBuffer, tempBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();

    SetVectorMask<float>(0, alignOutter);
    const BinaryRepeatParams binaryParams;
    for (uint32_t row = 1; row < cumSumInfo.inner; ++row) {
        Add<float, false>(floatTempBuffer[row * alignOutter], floatTempBuffer[(row - 1) * alignOutter],
            floatTempBuffer[row * alignOutter], MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }

    SetVectorMask<T, MaskMode::COUNTER>(alignOutter * cumSumInfo.inner);
    Cast<T, float, false>(tempBuffer, floatTempBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();

    auto tempBuffer2 = tempBuffer[alignOutter * cumSumInfo.inner];
    if (alignOutter > cumSumInfo.inner) {
        repeatTimes = alignOutter / oneBlockElementNum;
        if (repeatTimes > 1) {
            dstRepStride = cumSumInfo.inner;
            srcRepStride = 1;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < cumSumInfo.inner / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[(i * NCHW_CONV_ADDR_LIST_SIZE + n) * alignOutter].GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer2[i * NCHW_CONV_ADDR_LIST_SIZE + n * cumSumInfo.inner].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }
    } else {
        repeatTimes = cumSumInfo.inner / oneBlockElementNum;
        if (repeatTimes > 1) {
            srcRepStride = 1;
            dstRepStride = alignOutter;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, srcRepStride, dstRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + alignOutter * n].GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer2[(i * NCHW_CONV_ADDR_LIST_SIZE + n) * cumSumInfo.inner].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<T>(0, cumSumInfo.outter * cumSumInfo.inner);
    Adds<T, false>(
        dstTensor, tempBuffer2, 0, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void CumSumLastDim(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    LocalTensor<float> tempBuffer, const CumSumInfo &cumSumInfo)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(float);
    uint8_t repeatTimes = 1;
    uint16_t dstRepStride = 0;
    uint16_t srcRepStride = 0;
    uint16_t alignOutter =
        (cumSumInfo.outter + NCHW_CONV_ADDR_LIST_SIZE - 1) / NCHW_CONV_ADDR_LIST_SIZE * NCHW_CONV_ADDR_LIST_SIZE;
    uint64_t transDataTo5HDDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t transDataTo5HDSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];


    if (cumSumInfo.outter == alignOutter && alignOutter > cumSumInfo.inner) {
        repeatTimes = alignOutter / NCHW_CONV_ADDR_LIST_SIZE;
        if (repeatTimes > 1) {


            dstRepStride = 2;
            srcRepStride = cumSumInfo.inner * 2;
        }
        TransDataTo5HDParams params(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < cumSumInfo.inner / oneBlockElementNum; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)srcTensor[i * oneBlockElementNum + n * cumSumInfo.inner].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer[(i * oneBlockElementNum + n) * alignOutter].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer[(i * oneBlockElementNum + n) * alignOutter + oneBlockElementNum].GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    } else {
        TransDataTo5HDParams params = ExtractTransDataParam(repeatTimes, cumSumInfo.inner, alignOutter,
            oneBlockElementNum, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] = (uint64_t)srcTensor[((i * NCHW_CONV_ADDR_LIST_SIZE +
                    n % (cumSumInfo.outter - i * NCHW_CONV_ADDR_LIST_SIZE)) * cumSumInfo.inner)].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter + oneBlockElementNum]
                        .GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float>(0, alignOutter);
    const BinaryRepeatParams binaryParams;
    uint32_t addOffset = alignOutter;
    for (uint32_t row = 1; row < cumSumInfo.inner; ++row) {
        Add<float, false>(tempBuffer[addOffset],
            tempBuffer[addOffset - alignOutter],
            tempBuffer[addOffset],
            MASK_PLACEHOLDER,
            1,
            binaryParams);
        addOffset += alignOutter;
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();

    auto tempBuffer2 = tempBuffer[alignOutter * cumSumInfo.inner];
    if (alignOutter > cumSumInfo.inner) {
        repeatTimes = alignOutter / NCHW_CONV_ADDR_LIST_SIZE;
        if (repeatTimes > 1) {


            dstRepStride = cumSumInfo.inner * 2;
            srcRepStride = 2;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < cumSumInfo.inner / oneBlockElementNum; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[i * oneBlockElementNum * alignOutter + n * alignOutter].GetPhyAddr();
                transDataTo5HDSrcLocalList[n + NCHW_CONV_ADDR_LIST_SIZE / 2] =
                    (uint64_t)tempBuffer[i * oneBlockElementNum * alignOutter + n * alignOutter + oneBlockElementNum]
                        .GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer2[i * oneBlockElementNum + n * cumSumInfo.inner].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer2[i * oneBlockElementNum + (n + oneBlockElementNum) * cumSumInfo.inner]
                        .GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }

    } else {
        repeatTimes = cumSumInfo.inner / oneBlockElementNum;
        if (repeatTimes > 1) {


            dstRepStride = alignOutter;
            srcRepStride = 1;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, srcRepStride, dstRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter].GetPhyAddr();
                transDataTo5HDSrcLocalList[n + NCHW_CONV_ADDR_LIST_SIZE / 2] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter + oneBlockElementNum]
                        .GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer2[(i * NCHW_CONV_ADDR_LIST_SIZE + n) * cumSumInfo.inner].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer2[(i * NCHW_CONV_ADDR_LIST_SIZE + (n + NCHW_CONV_ADDR_LIST_SIZE / 2)) *
                                          cumSumInfo.inner]
                        .GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float>(0, cumSumInfo.outter * cumSumInfo.inner);
    Adds<float, false>(
        dstTensor, tempBuffer2, 0, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CumSumFirstDim(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    LocalTensor<uint8_t> &sharedTmpBuffer, const CumSumInfo &cumSumInfo)
{
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        const uint32_t minTmpBufferSize = cumSumInfo.outter * cumSumInfo.inner * sizeof(float);
        const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();






        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(cumSumInfo.outter * cumSumInfo.inner);
        LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
        Cast<float, T, false>(tmpBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();

        SetVectorMask<T>(0, cumSumInfo.inner);
        const BinaryRepeatParams binaryParams;
        for (uint32_t row = 1; row < cumSumInfo.outter; ++row) {
            Add<float, false>(tmpBuffer[row * cumSumInfo.inner], tmpBuffer[(row - 1) * cumSumInfo.inner],
                tmpBuffer[row * cumSumInfo.inner], MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        }

        SetVectorMask<T, MaskMode::COUNTER>(cumSumInfo.outter * cumSumInfo.inner);
        Cast<T, float, false>(dstTensor, tmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            1, {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();

    } else {
        SetMaskCount();
        SetVectorMask<T>(0, cumSumInfo.inner);
        Adds<T, false>(
            dstTensor, srcTensor, 0, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        const BinaryRepeatParams binaryParams;
        for (uint32_t row = 1; row < cumSumInfo.outter; ++row) {
            Add<T, false>(dstTensor[row * cumSumInfo.inner],
                dstTensor[(row - 1) * cumSumInfo.inner],
                srcTensor[row * cumSumInfo.inner],
                MASK_PLACEHOLDER,
                1,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
        SetMaskNorm();
        ResetMask();
    }
}

template <typename T, const CumSumConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void CumSumValid(LocalTensor<T> &dstTensor, LocalTensor<T> &lastRowTensor,
    const LocalTensor<T> &srcTensor, LocalTensor<uint8_t> &sharedTmpBuffer, const CumSumInfo &cumSumInfo)
{
    CheckTensorPosition(dstTensor, "dstLocal", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(lastRowTensor, "lastRowTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(srcTensor, "srcLocal", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");
# 390 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cumsum/cumsum_common_impl.h"
}

template <typename T, const CumSumConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void CumSumImpl(LocalTensor<T> &dstTensor, LocalTensor<T> &lastRowTensor,
    const LocalTensor<T> &srcTensor, LocalTensor<uint8_t> &sharedTmpBuffer, const CumSumInfo &cumSumInfo)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    CumSumValid<T, config>(dstTensor, lastRowTensor, srcTensor, sharedTmpBuffer, cumSumInfo);


                                                                                                                       ;
    if constexpr (config.isLastAxis) {
        uint32_t minCastTempBufferSize = 0;
        if constexpr (sizeof(T) == sizeof(__cce_half)) {
            minCastTempBufferSize = cumSumInfo.inner * NCHW_CONV_ADDR_LIST_SIZE * sizeof(__cce_half);
        }

        const uint32_t minTmpBufferSize = minCastTempBufferSize +
                                          NCHW_CONV_ADDR_LIST_SIZE * cumSumInfo.inner * sizeof(T) * 2;
        const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();







        const uint32_t oneRepeateSize = tmpBufferSize / minTmpBufferSize * NCHW_CONV_ADDR_LIST_SIZE;
        const uint32_t rangeM = cumSumInfo.outter / oneRepeateSize;
        const uint32_t tailM = cumSumInfo.outter - oneRepeateSize * rangeM;
        uint32_t dstLocalOffset = 0;
        uint32_t srcLocalOffset = 0;
        LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
        for (uint32_t i = 0; i < rangeM; i++) {
            CumSumLastDim<T>(
                dstTensor[dstLocalOffset], srcTensor[srcLocalOffset], tmpBuffer, {oneRepeateSize, cumSumInfo.inner});
            dstLocalOffset += cumSumInfo.inner * oneRepeateSize;
            srcLocalOffset += cumSumInfo.inner * oneRepeateSize;
        }

        if (tailM != 0) {
            CumSumLastDim<T>(
                dstTensor[dstLocalOffset], srcTensor[srcLocalOffset], tmpBuffer, {tailM, cumSumInfo.inner});
        }
    } else {
        CumSumFirstDim<T>(dstTensor, srcTensor, sharedTmpBuffer, cumSumInfo);
    }

    if constexpr (config.outputLastRow) {
        SetMaskCount();
        SetVectorMask<T>(0, cumSumInfo.inner);
        Adds<T, false>(lastRowTensor, dstTensor[(cumSumInfo.outter - 1) * cumSumInfo.inner], 0, MASK_PLACEHOLDER,
            1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
    }
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h" 2






namespace AscendC {
#pragma begin_pipe(V)

constexpr CumSumConfig defaultCumSumConfig = {true, false, true};
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h"
template <typename T, const CumSumConfig &config = defaultCumSumConfig>
[aicore] __inline__ __attribute__((always_inline)) void CumSum(LocalTensor<T> &dstTensor, LocalTensor<T> &lastRowTensor, const LocalTensor<T> &srcTensor,
    LocalTensor<uint8_t> &sharedTmpBuffer, const CumSumInfo &cumSumInfo)
{
    CumSumImpl<T, config>(dstTensor, lastRowTensor, srcTensor, sharedTmpBuffer, cumSumInfo);
}
# 65 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h"
template <typename T, const CumSumConfig &config = defaultCumSumConfig>
[aicore] __inline__ __attribute__((always_inline)) void CumSum(LocalTensor<T> &dstTensor, LocalTensor<T> &lastRowTensor, const LocalTensor<T> &srcTensor,
    const CumSumInfo &cumSumInfo)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CumSum<T, config>(dstTensor, lastRowTensor, srcTensor, sharedTmpBuffer, cumSumInfo);
}

#pragma end_pipe
}
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/fmod.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/fmod.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/fmod/fmod_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/fmod/fmod_common_impl.h"
namespace AscendC {
namespace {
constexpr uint32_t SRC0_IDX = 1;
constexpr uint32_t SRC1_IDX = 2;
constexpr uint32_t TRUNC_IDX = 3;
}

[aicore] __inline__ __attribute__((always_inline)) void FmodCompute(const LocalTensor<float> &dstTensor, const LocalTensor<float> &src0Tensor,
    const LocalTensor<float> &src1Tensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t stackSize,
    const uint32_t calCount)
{
    PipeBarrier<PIPE_V>();

    Div(dstTensor, src0Tensor, src1Tensor, calCount);
    PipeBarrier<PIPE_V>();

    Trunc(dstTensor, dstTensor, sharedTmpBuffer, calCount);
    PipeBarrier<PIPE_V>();

    Mul(dstTensor, dstTensor, src1Tensor, calCount);
    PipeBarrier<PIPE_V>();

    Sub(dstTensor, src0Tensor, dstTensor, calCount);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void FmodCompute(const LocalTensor<__cce_half> &dstTensor, const LocalTensor<__cce_half> &src0Tensor,
    const LocalTensor<__cce_half> &src1Tensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t stackSize,
    const uint32_t calCount)
{


    LocalTensor<float> floatTmpTensor = sharedTmpBuffer.ReinterpretCast<float>();
    LocalTensor<float> tmpSrc0 = floatTmpTensor[SRC0_IDX * stackSize];
    LocalTensor<float> tmpSrc1 = floatTmpTensor[SRC1_IDX * stackSize];

    PipeBarrier<PIPE_V>();

    Cast<float, __cce_half>(tmpSrc0, src0Tensor, RoundMode::CAST_NONE, calCount);

    Cast<float, __cce_half>(tmpSrc1, src1Tensor, RoundMode::CAST_NONE, calCount);
    PipeBarrier<PIPE_V>();

    FmodCompute(floatTmpTensor, tmpSrc0, tmpSrc1, sharedTmpBuffer[TRUNC_IDX * stackSize * sizeof(float)], stackSize, calCount);

    Cast<__cce_half, float>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, calCount);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FmodImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CheckTensorPosition(dstTensor, "dstTensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(src0Tensor, "src0Tensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(src1Tensor, "src1Tensor", "VECIN, VECOUT, VECCALC");
    CheckTensorPosition(sharedTmpBuffer, "sharedTmpBuffer", "VECIN, VECOUT, VECCALC");

    CheckCalCount(calCount, "calCount", src0Tensor, "src0Tensor", "Fmod");
    CheckCalCount(calCount, "calCount", src1Tensor, "src1Tensor", "Fmod");
    CheckCalCount(calCount, "calCount", dstTensor, "dstTensor", "Fmod");


                                                                                                                       ;


                                                                                                        ;

    if constexpr (sizeof(T) == sizeof(float)) {
        FmodCompute(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize(), calCount);
        return;
    }

    constexpr uint32_t maxLiveNodeCnt = 8;
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t stackSize =
        bufferSize / sizeof(T) / maxLiveNodeCnt / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(stackSize, 0, bufferSize);
                                                                                                          ;
    stackSize = stackSize > src0Tensor.GetSize() ? src0Tensor.GetSize() : stackSize;

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    for (uint32_t i = 0; i < round; ++i) {
        FmodCompute(dstTensor[i * stackSize], src0Tensor[i * stackSize], src1Tensor[i * stackSize], sharedTmpBuffer,
            stackSize, stackSize);
    }
    if (tail > 0) {
        FmodCompute(dstTensor[round * stackSize], src0Tensor[round * stackSize], src1Tensor[round * stackSize],
            sharedTmpBuffer, stackSize, tail);
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/fmod.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/fmod.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Fmod(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    FmodImpl<T>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/fmod.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Fmod(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    FmodImpl<T>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/fmod.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Fmod(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    FmodImpl<T>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize());
}
# 98 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/fmod.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Fmod(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    FmodImpl<T>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize());
}

#pragma end_pipe
}
# 95 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/groupnorm.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/groupnorm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/groupnorm/groupnorm_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/groupnorm/groupnorm_common_impl.h"
namespace AscendC {
    namespace {
        constexpr uint32_t GROUPNORM_MASK_MAX_VAL = 64;
        constexpr uint32_t GROUPNORM_MASK_SMALLEST_VAL = 8;
        constexpr uint32_t GROUPNORM_MASK_STEP_VAL = 8;
        constexpr uint32_t GROUPNORM_ONE_BLK_SIZE = 8;
    }

template <typename T> struct GroupNormParams
{
    [aicore] GroupNormParams(){};
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetGroupNormWholeReduceMask1(const GroupNormTiling& tiling)
{
    uint32_t mask1{0};
    if (tiling.dhwAlignSize > GROUPNORM_MASK_MAX_VAL) {
        mask1 = GROUPNORM_MASK_MAX_VAL;
        while (mask1 != 0 && tiling.dhwAlignSize % mask1 != 0) {
            mask1 -= GROUPNORM_MASK_STEP_VAL;
        }
        return mask1;
    }
    return tiling.dhwAlignSize;
}

[aicore] __inline__ __attribute__((always_inline)) void GetGroupNormOutputMean(const LocalTensor<float>& x_in,
    const LocalTensor<float>& tmp, const LocalTensor<float>& mean,
    const GroupNormTiling& tiling)
{
    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.dhwAlignSize;
        ReduceSum<float>(mean[i], x_in[buffIndex], tmp[buffIndex], tiling.dhwAlignSize);
    }
    PipeBarrier<PIPE_V>();

    Muls(mean, mean, tiling.factor, tiling.bsCurLength);


    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
}

[aicore] __inline__ __attribute__((always_inline)) void GetGroupNormOutputVar(const LocalTensor<float>& x_in,
    const LocalTensor<float>& tmp1, const LocalTensor<float>& tmp2,
    const LocalTensor<float>& mean, const LocalTensor<float>& var, const GroupNormTiling& tiling)
{
    for (uint32_t i = 0; i < tiling.d * tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.hwAlignSize;
        Adds(tmp1[buffIndex], x_in[buffIndex], -1.0f * mean.GetValue(i / tiling.d), tiling.hw);
    }
    PipeBarrier<PIPE_V>();

    Mul(tmp2, tmp1, tmp1, tiling.bshCurLength);
    PipeBarrier<PIPE_V>();

    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.dhwAlignSize;
        ReduceSum<float>(var[i], tmp2[buffIndex], tmp2[buffIndex], tiling.dhwAlignSize);
    }
    PipeBarrier<PIPE_V>();

    Muls(var, var, tiling.factor, tiling.bsCurLength);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetGroupNormOutputPre(const LocalTensor<float>& inout,
    const LocalTensor<float>& tmp, const LocalTensor<float>& tempOnes, const LocalTensor<float>& variance,
    const GroupNormTiling& tiling, const float epsilon)
{
    Adds(tmp, variance, epsilon, tiling.bsCurLength);
    PipeBarrier<PIPE_V>();

    Duplicate(tempOnes, 1.0f, tiling.bsCurLength);
    PipeBarrier<PIPE_V>();

    Sqrt(tmp, tmp, tiling.bsCurLength);
    PipeBarrier<PIPE_V>();

    Div(tmp, tempOnes, tmp, tiling.bsCurLength);
    PipeBarrier<PIPE_V>();

    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);


    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.dhwAlignSize;
        Muls(inout[buffIndex], inout[buffIndex], tmp.GetValue(i), tiling.dhwAlignSize);
    }


    auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::V_S>(eventIdSToV);
    WaitFlag<HardEvent::V_S>(eventIdSToV);

    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetGroupNormOutput(const LocalTensor<float>& inout,
    const LocalTensor<float>& gamma, const LocalTensor<float>& beta,
    const GroupNormTiling& tiling, const int32_t loopCount)
{
    size_t channelIndex = loopCount * tiling.meanVarRoundSize * tiling.d;
    for (uint32_t channel_offset = 0; channel_offset < tiling.bsCurLength * tiling.d; ++channel_offset) {
        Muls(inout[channel_offset * tiling.hwAlignSize], inout[channel_offset * tiling.hwAlignSize],
        gamma.GetValue(channelIndex % tiling.c), tiling.hw);
        channelIndex += 1;
    }
    PipeBarrier<PIPE_V>();

    channelIndex = loopCount * tiling.meanVarRoundSize * tiling.d;
    for (uint32_t channel_offset = 0; channel_offset < tiling.bsCurLength * tiling.d; ++channel_offset) {
        Adds(inout[channel_offset * tiling.hwAlignSize], inout[channel_offset * tiling.hwAlignSize],
        beta.GetValue(channelIndex % tiling.c), tiling.hw);
        channelIndex += 1;
    }
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GroupNormExe(const LocalTensor<__cce_half>& inputX,
    const LocalTensor<__cce_half>& gamma, const LocalTensor<__cce_half>& beta,
    const LocalTensor<__cce_half>& output, const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const __cce_half epsilon, const GroupNormTiling& tiling, const GroupNormParams<float>& params, const int32_t loopCount)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;
    Duplicate(tempTensorA, 0.0f, tiling.bshCurLength);
    PipeBarrier<PIPE_V>();
    Cast<float, __cce_half>(tempTensorB, inputX, RoundMode::CAST_NONE, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();

    GetGroupNormOutputMean(tempTensorB, tempTensorC, outputMean, tiling);

    GetGroupNormOutputVar(tempTensorB, tempTensorB, tempTensorC, outputMean, outputVariance, tiling);

    GetGroupNormOutputPre(tempTensorB, tempTensorA, tempTensorC, outputVariance, tiling, static_cast<float>(epsilon));

    Cast<float, __cce_half>(tempTensorA, gamma, RoundMode::CAST_NONE, tiling.c);
    PipeBarrier<PIPE_V>();
    Cast<float, __cce_half>(tempTensorC, beta, RoundMode::CAST_NONE, tiling.c);
    PipeBarrier<PIPE_V>();

    GetGroupNormOutput(tempTensorB, tempTensorA, tempTensorC, tiling, loopCount);

    Cast<__cce_half, float>(output, tempTensorB, RoundMode::CAST_NONE, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void GroupNormExe(const LocalTensor<float>& inputX,
    const LocalTensor<float>& gamma, const LocalTensor<float>& beta,
    const LocalTensor<float>& output, const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const float epsilon, const GroupNormTiling& tiling, const GroupNormParams<float>& params, const int32_t loopCount)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    GetGroupNormOutputMean(inputX, output, outputMean, tiling);

    Duplicate(output, 0.0f, tiling.bshCurLength);
    PipeBarrier<PIPE_V>();

    GetGroupNormOutputVar(inputX, output, tempTensorC, outputMean, outputVariance, tiling);

    GetGroupNormOutputPre(output, tempTensorA, tempTensorB, outputVariance, tiling, epsilon);

    GetGroupNormOutput(output, gamma, beta, tiling, loopCount);
}

[aicore] __inline__ __attribute__((always_inline)) void GroupNormExeSmallShape(const LocalTensor<__cce_half>& inputX,
    const LocalTensor<__cce_half>& gamma, const LocalTensor<__cce_half>& beta,
    const LocalTensor<__cce_half>& output, const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const __cce_half epsilon, const GroupNormTiling& tiling, const GroupNormParams<float>& params, const int32_t loopCount)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;
    Duplicate(tempTensorA, 0.0f, tiling.inputRoundSize * tiling.numberOfTmpBuf);
    PipeBarrier<PIPE_V>();

    Cast<float, __cce_half>(tempTensorB, inputX, RoundMode::CAST_NONE, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();

    uint32_t mask1 = GetGroupNormWholeReduceMask1(tiling);
                                                                                 ;

    uint32_t repeat1 = tiling.dhwAlignSize / mask1 * tiling.meanVarRoundSize;
    uint32_t mask2 = tiling.dhwAlignSize / mask1 * GROUPNORM_MASK_SMALLEST_VAL;
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(tempTensorC, tempTensorB, mask1, repeat1, GROUPNORM_MASK_SMALLEST_VAL, DEFAULT_BLK_STRIDE, mask1 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(outputMean, tempTensorC, mask2, tiling.bsCurLength, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, mask2 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    Muls(outputMean, outputMean, tiling.factor, tiling.bsCurLength);
    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.dhwAlignSize;
        Adds(tempTensorB[buffIndex], tempTensorB[buffIndex], -1.0f * outputMean.GetValue(i), tiling.hw, tiling.d,
        {1, 1, static_cast<uint8_t>(tiling.hwAlignSize / GROUPNORM_ONE_BLK_SIZE), static_cast<uint8_t>(tiling.hwAlignSize / GROUPNORM_ONE_BLK_SIZE)});
    }
    PipeBarrier<PIPE_V>();

    Mul(tempTensorC, tempTensorB, tempTensorB, tiling.bshCurLength);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(tempTensorA, tempTensorC, mask1, repeat1, GROUPNORM_MASK_SMALLEST_VAL, DEFAULT_BLK_STRIDE, mask1 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(outputVariance, tempTensorA, mask2, tiling.bsCurLength, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, mask2 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    Muls(outputVariance, outputVariance, tiling.factor, tiling.bsCurLength);
    PipeBarrier<PIPE_V>();

    GetGroupNormOutputPre(tempTensorB, tempTensorA, tempTensorC, outputVariance, tiling, static_cast<float>(epsilon));

    Cast<float, __cce_half>(tempTensorA, gamma, RoundMode::CAST_NONE, tiling.c);
    PipeBarrier<PIPE_V>();
    Cast<float, __cce_half>(tempTensorC, beta, RoundMode::CAST_NONE, tiling.c);
    PipeBarrier<PIPE_V>();

    GetGroupNormOutput(tempTensorB, tempTensorA, tempTensorC, tiling, loopCount);

    Cast<__cce_half, float>(output, tempTensorB, RoundMode::CAST_NONE, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GroupNormExeSmallShape(const LocalTensor<float>& inputX,
    const LocalTensor<float>& gamma, const LocalTensor<float>& beta,
    const LocalTensor<float>& output, const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const float epsilon, const GroupNormTiling& tiling, const GroupNormParams<float>& params, const int32_t loopCount)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;
    Duplicate(output, 0.0f, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();
    Duplicate(tempTensorC, 0.0f, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();
    uint32_t mask1 = GetGroupNormWholeReduceMask1(tiling);
                                                                                 ;

    uint32_t repeat1 = tiling.dhwAlignSize / mask1 * tiling.meanVarRoundSize;
    uint32_t mask2 = tiling.dhwAlignSize / mask1 * GROUPNORM_MASK_SMALLEST_VAL;
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(tempTensorC, inputX, mask1, repeat1, GROUPNORM_MASK_SMALLEST_VAL, DEFAULT_BLK_STRIDE, mask1 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(outputMean, tempTensorC, mask2, tiling.bsCurLength, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, mask2 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    Muls(outputMean, outputMean, tiling.factor, tiling.bsCurLength);
    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    auto repeatStride = tiling.hwAlignSize / GROUPNORM_ONE_BLK_SIZE;
    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.dhwAlignSize;
        Adds(output[buffIndex], inputX[buffIndex], -1.0f * outputMean.GetValue(i), tiling.hw, tiling.d,
        {1, 1, static_cast<uint8_t>(repeatStride), static_cast<uint8_t>(repeatStride)});
    }
    PipeBarrier<PIPE_V>();

    Mul(tempTensorC, output, output, tiling.bshCurLength);
    PipeBarrier<PIPE_V>();

    Duplicate(tempTensorA, 0.0f, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(tempTensorA, tempTensorC, mask1, repeat1, GROUPNORM_MASK_SMALLEST_VAL, DEFAULT_BLK_STRIDE, mask1 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(outputVariance, tempTensorA, mask2, tiling.bsCurLength, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, mask2 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    Muls(outputVariance, outputVariance, tiling.factor, tiling.bsCurLength);
    PipeBarrier<PIPE_V>();
    GetGroupNormOutputPre(output, tempTensorA, tempTensorB, outputVariance, tiling, epsilon);

    GetGroupNormOutput(output, gamma, beta, tiling, loopCount);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetGroupNormNDTensorInfo(const LocalTensor<__cce_half>& inputX,
    const LocalTensor<__cce_half>& outputMean, const LocalTensor<__cce_half>& outputVariance,
    const LocalTensor<float>& stackBuffer, const GroupNormTiling& tiling, GroupNormParams<float>& params)
{
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];




      ;



      ;
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetGroupNormNDTensorInfo(const LocalTensor<float>& inputX,
    const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& stackBuffer, const GroupNormTiling& tiling, GroupNormParams<float>& params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    if constexpr (isReuseSource) {
        params.tempTensorA = inputX;
        params.tempTensorB = stackBuffer[tiling.firstTmpStartPos];
        params.tempTensorC = stackBuffer[tiling.secondTmpStartPos];




          ;
    } else {
        params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
        params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
        params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];




          ;
    }




      ;
}

[aicore] __inline__ __attribute__((always_inline)) void GetOutputMeanVariance(const LocalTensor<__cce_half>& outputMean,
    const LocalTensor<__cce_half>& outputVariance, const GroupNormTiling& tiling, const GroupNormParams<float>& params)
{
    Cast<__cce_half, float>(outputMean, params.meanTmpTensor, RoundMode::CAST_NONE, tiling.n * tiling.g);
    Cast<__cce_half, float>(outputVariance, params.varianceTmpTensor, RoundMode::CAST_NONE, tiling.n * tiling.g);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GroupNormNDCommon(const LocalTensor<T>& inputX,
    const LocalTensor<T>& gamma, const LocalTensor<T>& beta,
    const LocalTensor<T>& output, const LocalTensor<T>& outputMean, const LocalTensor<T>& outputVariance,
    const T epsilon, GroupNormTiling& tiling, const GroupNormParams<float>& params)
{
    uint32_t inputOffset = 0;
    uint32_t mvOffset = 0;

    if (tiling.smallShape) {
        for (uint32_t index = 0; index < tiling.loopRound; index++) {
            GroupNormExeSmallShape(inputX[inputOffset], gamma, beta, output[inputOffset],
            params.meanTmpTensor[mvOffset],
            params.varianceTmpTensor[mvOffset], epsilon, tiling, params, index);

            inputOffset += tiling.inputRoundSize;
            mvOffset += tiling.meanVarRoundSize;
        }
    } else {
        for (uint32_t index = 0; index < tiling.loopRound; index++) {
            GroupNormExe(inputX[inputOffset], gamma, beta, output[inputOffset],
            params.meanTmpTensor[mvOffset],
            params.varianceTmpTensor[mvOffset], epsilon, tiling, params, index);

            inputOffset += tiling.inputRoundSize;
            mvOffset += tiling.meanVarRoundSize;
        }
    }

    if (tiling.inputTailSize > 0) {
        tiling.bshCurLength = tiling.inputTailSize;
        tiling.bsCurLength = tiling.meanVarTailSize;

        inputOffset = tiling.inputTailPos;
        mvOffset = tiling.meanVarTailPos;

        if (tiling.smallShape) {
            GroupNormExeSmallShape(inputX[inputOffset], gamma, beta, output[inputOffset],
            params.meanTmpTensor[mvOffset],
            params.varianceTmpTensor[mvOffset], epsilon, tiling, params, tiling.loopRound);
        } else {
            GroupNormExe(inputX[inputOffset], gamma, beta, output[inputOffset],
            params.meanTmpTensor[mvOffset],
            params.varianceTmpTensor[mvOffset], epsilon, tiling, params, tiling.loopRound);
        }


        tiling.bshCurLength = tiling.inputRoundSize;
        tiling.bsCurLength = tiling.meanVarRoundSize;
    }

    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        GetOutputMeanVariance(outputMean, outputVariance, tiling, params);
    }
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GroupNormImpl(const LocalTensor<T>& output,
    const LocalTensor<T>& outputMean, const LocalTensor<T>& outputVariance,
    const LocalTensor<T>& inputX, const LocalTensor<T>& gamma, const LocalTensor<T>& beta,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, GroupNormTiling& tiling)
{
                                                                                                         ;

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
                                                                                                               ;

    GroupNormParams<float> params;
    GetGroupNormNDTensorInfo<isReuseSource>(inputX, outputMean, outputVariance, stackBuffer, tiling, params);

    GroupNormNDCommon<T>(inputX, gamma, beta, output, outputMean, outputVariance, epsilon, tiling, params);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GroupNormImpl(const LocalTensor<T>& output,
    const LocalTensor<T>& outputMean, const LocalTensor<T>& outputVariance,
    const LocalTensor<T>& inputX, const LocalTensor<T>& gamma, const LocalTensor<T>& beta,
    const T epsilon, GroupNormTiling& tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    GroupNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, sharedTmpBuffer, epsilon, tiling);
}

}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/groupnorm.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/groupnorm.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GroupNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, GroupNormTiling& tiling)
{
    GroupNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, sharedTmpBuffer, epsilon,
        tiling);
}
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/groupnorm.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GroupNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const T epsilon, GroupNormTiling& tiling)
{
    GroupNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, epsilon, tiling);
}
#pragma end_pipe
}
# 96 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/init_global_memory.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/init_global_memory.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/../../impl/utils/init_global_memory/init_global_memory_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/../../impl/utils/init_global_memory/init_global_memory_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/../../impl/utils/init_global_memory/init_global_memory_v220_impl.h" 2

namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitGlobalMemoryImpl(GlobalTensor<T> &gmWorkspaceAddr, const uint64_t size, const T value)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<T> popBuffer;
    constexpr uint32_t MAX_REPEAT_LEN = 256;
    bool ret = PopStackBuffer<T, TPosition::LCM>(popBuffer);
                                                                                                     ;
    constexpr uint32_t maxBurstSize = (MAX_REPEAT_TIMES * MAX_REPEAT_LEN) / sizeof(T);
    const uint32_t popSize = popBuffer.GetSize() >= maxBurstSize ? maxBurstSize : popBuffer.GetSize();
    const uint32_t round = size / popSize;
    const uint32_t tail = size % popSize;
    const uint32_t roundSize = round != 0 ? popSize : 0;
    Duplicate<T>(popBuffer, value, popSize);
    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    struct DataCopyExtParams repeatParams;
    repeatParams.blockCount = 1;
    uint32_t comOffset = 0;

    repeatParams.blockLen = static_cast<uint32_t>(roundSize * sizeof(T));
    for (uint32_t index = 0; index < round; ++index) {
        DataCopyPad(gmWorkspaceAddr[comOffset], popBuffer, repeatParams);
        comOffset += roundSize;
    }

    repeatParams.blockLen = static_cast<uint32_t>(tail * sizeof(T));
    if (tail != 0) {
        comOffset = round * roundSize;
        DataCopyPad(gmWorkspaceAddr[comOffset], popBuffer, repeatParams);
    }
    PipeBarrier<PIPE_MTE3>();
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/init_global_memory.h" 2


namespace AscendC {
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/init_global_memory.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("V")))
    __attribute__((out_pipe("MTE3"))) void InitGlobalMemory(GlobalTensor<T> &gmWorkspaceAddr, const uint64_t size, const T value)
{
    InitGlobalMemoryImpl<T>(gmWorkspaceAddr, size, value);
}

}
# 97 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 54 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
namespace AscendC {
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void __attribute__((inout_pipe("MTE2"))) DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
    const DataCopyParams& repeatParams)
{
    using PrimType = PrimT<T>;
    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());






    if (dstHWPos == Hardware::UB) {

        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            repeatParams);
    } else if (dstHWPos == Hardware::L1) {

        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            repeatParams);
    } else {


                                                                                                   ;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CheckNd2NzParams(Nd2NzParams params, const __attribute__((cce_global)) char *msg)
{
    constexpr uint16_t ND2NZ_LIMIT = 16384;
                                                                        ;
                                                                           ;
                                                                                         ;
                                                                                       ;
}
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
    const Nd2NzParams& intriParams)
{
    CheckNd2NzParams(intriParams, "DataCopy with Nd2NzParams");
    using PrimType = PrimT<T>;
    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());
                                                                                                  ;
    if (dstHWPos == Hardware::L1) {

        DataCopyGM2L1ND2NZImpl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            intriParams);
    } else if (dstHWPos == Hardware::UB) {
        DataCopyGM2UBND2NZImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            intriParams);
    } else {


                                                                                                   ;
    }
}
# 130 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const Nd2NzParams &intriParams)
{
    CheckNd2NzParams(intriParams, "DataCopy with Nd2NzParams");
    using PrimType = PrimT<T>;
    CheckTensorPos<T>(srcLocal, Hardware::UB, "srcLocal", "VECIN / VECCALC / VECOUT",
        "DataCopy from LocalTensor to LocalTensor with Nd2NzParams");
    CheckTensorPos<T>(dstLocal, Hardware::L1, "dstLocal", "TSCM",
        "DataCopy from LocalTensor to LocalTensor with Nd2NzParams");
                                                                                                 ;
    DataCopyUB2L1ND2NZImpl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        intriParams);
}
# 155 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const DataCopyParams& repeatParams)
{
    using PrimType = PrimT<T>;
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());
# 172 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
    if (srcHWPos == Hardware::UB) {

        DataCopyUB2GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
            repeatParams);
    } else if (srcHWPos == Hardware::L1) {

        DataCopyL12GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)srcLocal.GetPhyAddr(),
            repeatParams);
    } else {







                                                                                                   ;

    }







}
# 210 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &repeatParams)
{
    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());

                                                                                                  ;
    if (srcHWPos == Hardware::UB) {
        if (dstHWPos == Hardware::UB) {






            DataCopyUB2UBIntf(dstLocal, srcLocal, repeatParams);
        } else if (dstHWPos == Hardware::L1) {

            DataCopyUB2L1Intf(dstLocal, srcLocal, repeatParams);
        } else {



                                                                                                       ;





        }
    } else if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL12UBIntf(dstLocal, srcLocal, repeatParams);
        } else if (dstHWPos == Hardware::BIAS) {
            CheckTensorAlign<T>(dstLocal, 64, "dstLocal", "DataCopy from C1 to C2");
            CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "DataCopy from C1 to C2");
            DataCopyL12BTIntf(dstLocal, srcLocal, repeatParams);

        } else if (dstHWPos == Hardware::FIXBUF) {
            CheckTensorAlign<T>(dstLocal, 128, "dstLocal", "DataCopy from A1 / B1 / C1 to C2PIPE2GM");
            CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "DataCopy from C1 to C2");
            DataCopyL12FBIntf(dstLocal, srcLocal, repeatParams);

        } else {


                                                                                                       ;
        }
    } else {


                                                                                                   ;
    }
}
# 277 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<dst_T> &dstLocal, const LocalTensor<src_T> &srcLocal,
    const DataCopyParams &repeatParams)
{
    using PrimDstType = PrimT<dst_T>;
    using PrimSrcType = PrimT<src_T>;
    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());

                                                                                                                  ;
    if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::BIAS) {

            CheckTensorAlign<dst_T>(dstLocal, 64, "dstLocal", "DataCopy from C1 to C2");
            CheckTensorAlign<src_T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "DataCopy from C1 to C2");
            if constexpr (IsSameType<PrimDstType, PrimSrcType>::value) {
                DataCopyL12BTImpl((uint64_t)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimSrcType*)srcLocal.GetPhyAddr(),
                    (uint16_t)0, repeatParams);
            } else if constexpr (IsSameType<PrimDstType, float>::value && IsSameType<PrimSrcType, __cce_half>::value) {
                DataCopyL12BTImpl((uint64_t)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) __cce_half *)srcLocal.GetPhyAddr(), (uint16_t)1,
                    repeatParams);
            } else {

                                                                                                          ;
            }
        } else {


                                                                                                       ;
        }
    } else {


                                                                                                   ;
    }
}
# 327 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T, bool IsSetMask>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint64_t mask[], const uint8_t repeatTimes, const CopyRepeatParams &repeatParams)
{
    using PrimType = PrimT<T>;






    CopyImpl<PrimType, IsSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}


template <typename T, bool IsSetMask>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint64_t mask, const uint8_t repeatTimes, const CopyRepeatParams &repeatParams)
{
    using PrimType = PrimT<T>;






    CopyImpl<PrimType, IsSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 367 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T> &dstLocal, const GlobalTensor<T> &srcGlobal,
    const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[], const uint32_t dimValue)
{
    using PrimType = PrimT<T>;
    static_assert(IsSameType<PrimType, T>::value, "TensorTrait is not supported by DataCopy with SliceInfo!");






    uint32_t srcStartIndex = 0;
    uint32_t dstStartIndex = 0;
    uint32_t srcOffsetListSize = 0;
    uint32_t dstOffsetListSize = 0;
    uint32_t srcShapeInfo[8];
    uint32_t dstShapeInfo[8];
    bool useShapeValue = !(srcSliceInfo[0].shapeValue == 0);
    for (int i = 0; i < dimValue; i++) {
        srcShapeInfo[i] = useShapeValue ? srcSliceInfo[i].shapeValue : srcGlobal.GetShapeInfo().shape[i];
        dstShapeInfo[i] = useShapeValue ? dstSliceInfo[i].shapeValue : dstLocal.GetShapeInfo().shape[i];
    }

    srcStartIndex = DataCopyGetPhyStartIndex(srcSliceInfo, srcShapeInfo, dimValue);
    dstStartIndex = DataCopyGetPhyStartIndex(dstSliceInfo, dstShapeInfo, dimValue);
    uint32_t srcOffsetList[MAX_SLICE_SIZE];
    uint32_t dstOffsetList[MAX_SLICE_SIZE];
    DataCopyGetOffsetList(srcSliceInfo, srcShapeInfo, dimValue, &srcOffsetListSize, srcOffsetList);
    DataCopyGetOffsetList(dstSliceInfo, dstShapeInfo, dimValue, &dstOffsetListSize, dstOffsetList);
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = srcSliceInfo[0].burstLen;
    uint32_t oneSliceLen = srcSliceInfo[0].burstLen * AscendCUtils::GetC0Count(sizeof(T)) + srcSliceInfo[0].stride;
    repeatParams.blockCount =
        (srcSliceInfo[0].endIndex - srcSliceInfo[0].startIndex + 1 + srcSliceInfo[0].stride) / oneSliceLen;
    repeatParams.dstStride = dstSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();

    if ((srcSliceInfo[0].stride * sizeof(T)) % AscendCUtils::GetC0Size() == 0) {
        repeatParams.srcStride = srcSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {
            DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_global)) T *)srcGlobal.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);
        }
    } else {
        repeatParams.srcStride = srcSliceInfo[0].stride * sizeof(T);
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {
            DataCopySliceGm2UBImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_global)) T *)srcGlobal.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);
        }
    }
}
# 428 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T> &dstGlobal, const LocalTensor<T> &srcLocal,
    const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[], const uint32_t dimValue)
{
    using PrimType = PrimT<T>;
    static_assert(IsSameType<PrimType, T>::value, "TensorTrait is not supported by DataCopy with SliceInfo!");






    uint32_t srcStartIndex = 0;
    uint32_t dstStartIndex = 0;
    uint32_t srcOffsetListSize = 0;
    uint32_t dstOffsetListSize = 0;
    uint32_t srcShapeInfo[8];
    uint32_t dstShapeInfo[8];
    bool useShapeValue = !(srcSliceInfo[0].shapeValue == 0);
    for (int i = 0; i < dimValue; i++) {
        srcShapeInfo[i] = useShapeValue ? srcSliceInfo[i].shapeValue : srcLocal.GetShapeInfo().shape[i];
        dstShapeInfo[i] = useShapeValue ? dstSliceInfo[i].shapeValue : dstGlobal.GetShapeInfo().shape[i];
    }

    srcStartIndex = DataCopyGetPhyStartIndex(srcSliceInfo, srcShapeInfo, dimValue);
    dstStartIndex = DataCopyGetPhyStartIndex(dstSliceInfo, dstShapeInfo, dimValue);
    uint32_t dstOffsetList[MAX_SLICE_SIZE];
    uint32_t srcOffsetList[MAX_SLICE_SIZE];
    DataCopyGetOffsetList(srcSliceInfo, srcShapeInfo, dimValue, &srcOffsetListSize, srcOffsetList);
    DataCopyGetOffsetList(dstSliceInfo, dstShapeInfo, dimValue, &dstOffsetListSize, dstOffsetList);

    struct DataCopyParams repeatParams;
    repeatParams.blockLen = srcSliceInfo[0].burstLen;
    uint32_t oneSliceLen = srcSliceInfo[0].burstLen * AscendCUtils::GetC0Count(sizeof(T)) + srcSliceInfo[0].stride;
    repeatParams.blockCount =
        (srcSliceInfo[0].endIndex - srcSliceInfo[0].startIndex + 1 + srcSliceInfo[0].stride) / oneSliceLen;
    repeatParams.srcStride = srcSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();

    if ((dstSliceInfo[0].stride * sizeof(T)) % AscendCUtils::GetC0Size() == 0) {
        repeatParams.dstStride = dstSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {
            DataCopyUB2GMImpl((__attribute__((cce_global)) T *)dstGlobal.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);
        }
    } else {
        repeatParams.dstStride = dstSliceInfo[0].stride * sizeof(T);
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {
            DataCopySliceUB2GMImpl((__attribute__((cce_global)) T *)dstGlobal.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);
        }
    }
}
# 488 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
    const uint32_t calCount)
{
    using PrimType = PrimT<T>;
                                                                                                 ;
    struct DataCopyParams repeatParams;
    {



                         ;
        repeatParams.blockLen = calCount / AscendCUtils::GetC0Count(sizeof(PrimType));
    }
    DataCopy(dstLocal, srcGlobal, repeatParams);
}
# 512 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const uint32_t calCount)
{
    using PrimType = PrimT<T>;
                                                                                                 ;
    struct DataCopyParams repeatParams;
    {



                         ;
        repeatParams.blockLen = calCount / AscendCUtils::GetC0Count(sizeof(PrimType));
    }
    DataCopy(dstGlobal, srcLocal, repeatParams);
}
# 536 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, const uint32_t calCount)
{
    using PrimType = PrimT<T>;



                     ;
                                                                                                ;
    struct DataCopyParams repeatParams;

    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());
    if (srcHWPos != Hardware::L1) {
        repeatParams.blockLen = calCount / AscendCUtils::GetC0Count(sizeof(PrimType));
    } else {
        if (dstHWPos == Hardware::UB) {
            repeatParams.blockLen = calCount / AscendCUtils::GetC0Count(sizeof(PrimType));
        } else if (dstHWPos == Hardware::BIAS) {
            repeatParams.blockLen = calCount / (64 / sizeof(PrimType));
        } else if (dstHWPos == Hardware::FIXBUF) {
            repeatParams.blockLen = calCount / (128 / sizeof(PrimType));
        }
    }
    DataCopy(dstLocal, srcLocal, repeatParams);
}

[aicore] __inline__ __attribute__((always_inline)) void CheckNz2NdParams(const Nz2NdParamsFull& params)
{
    constexpr uint16_t NZ2ND_LIMIT = 8192;
                                                                                                    ;
                                                                                                       ;
                                                                                                       ;
                                                                                                                           ;
                                                                                                              ;
}







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const Nz2NdParamsFull& intriParams)
{
    CheckNz2NdParams(intriParams);
    using PrimType = PrimT<T>;
# 593 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());
    if (srcHWPos != Hardware::UB) {





                                                                                                   ;

    }
    DataCopyUB2GMNZ2NDImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        intriParams);






}
# 631 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    using PrimType = PrimT<T>;
    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());
                                                                                                                    ;

    if (dstHWPos == Hardware::UB) {

        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            intriParams);
    } else if (dstHWPos == Hardware::L1) {

        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            intriParams);
    } else {


                                                                                                   ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    using PrimType = PrimT<T>;
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());
                                                                                                                    ;

    if (srcHWPos == Hardware::UB) {

        DataCopyUB2GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
            intriParams);
    } else if (srcHWPos == Hardware::L1) {

        DataCopyL12GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)srcLocal.GetPhyAddr(),
            intriParams);
    } else {


                                                                                                   ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());
                                                                                                                   ;

    if (srcHWPos == Hardware::UB) {
        if (dstHWPos == Hardware::L1) {

            DataCopyUB2L1Intf(dstLocal, srcLocal, intriParams);
        } else if (dstHWPos == Hardware::L0C) {

            DataCopyUB2L0CIntf(dstLocal, srcLocal, intriParams, enhancedParams);
        } else if (dstHWPos == Hardware::UB) {

            DataCopyUB2UBIntf(dstLocal, srcLocal, intriParams);
        } else {


                                                                                                       ;
        }
    } else if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL12UBIntf(dstLocal, srcLocal, intriParams);
        } else if (dstHWPos == Hardware::L0C) {

            DataCopyL12L0CIntf(dstLocal, srcLocal, intriParams, enhancedParams);
        } else {


                                                                                                       ;
        }
    } else if (srcHWPos == Hardware::L0C) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL0C2UBIntf(dstLocal, srcLocal, intriParams, enhancedParams);
        } else {


                                                                                                       ;
        }
    } else {







                                                                                                   ;

    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
    const DataCopyCO12DstParams& intriParams)
{
    CheckTensorPos<U>(srcLocal, Hardware::L0C, "srcLocal", "CO1",
        "DataCopy from LocalTensor to LocalTensor with DataCopyCO12DstParams");
    CheckTensorPos<T>(dstLocal, Hardware::L1, "dstLocal", "A1",
        "DataCopy from LocalTensor to LocalTensor with DataCopyCO12DstParams");
                                                                                                   ;

    DataCopyL0C2L1Impl((__attribute__((cce_cube_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) PrimT<U>*)srcLocal.GetPhyAddr(), intriParams);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<U>& srcLocal,
    const DataCopyCO12DstParams& intriParams)
{
    CheckTensorPos<U>(srcLocal, Hardware::L0C, "srcLocal", "CO1",
        "DataCopy from LocalTensor to GlobalTensor with DataCopyCO12DstParams");
                                                                                                    ;

    DataCopyL0C2GMImpl((__attribute__((cce_global)) PrimT<T>*)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_c)) PrimT<U>*)srcLocal.GetPhyAddr(), intriParams);
}



template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, bfloat16_t>::value &&
    IsSameType<PrimT<U>, float>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());
                                                                                                                   ;
    if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::L0C) {

            DataCopyL12L0CImpl((__attribute__((cce_cube_c)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<U>*)srcLocal.GetPhyAddr(),
                intriParams, enhancedParams);
        } else {


                                                                                                       ;
        }
    } else {


                                                                                                   ;
    }
}



template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, __cce_half>::value &&
    IsSameType<PrimT<U>, float>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());
                                                                                                                   ;
    if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::L0C) {

            DataCopyL12L0CImpl((__attribute__((cce_cube_c)) __cce_half*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) float*)srcLocal.GetPhyAddr(), intriParams,
                enhancedParams);
        } else {


                                                                                                       ;
        }
    } else if (srcHWPos == Hardware::L0C) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) __cce_half*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) float*)srcLocal.GetPhyAddr(), intriParams,
                enhancedParams);
        } else {


                                                                                                       ;
        }
    } else {


                                                                                                   ;
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void CheckTensorL0C2UB(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal)
{
    CheckTensorPos<U>(srcLocal, Hardware::L0C, "srcLocal", "CO1",
        "DataCopy from LocalTensor(CO1) to LocalTensor(CO2) with DataCopyEnhancedParams");
    CheckTensorPos<T>(dstLocal, Hardware::UB, "dstLocal", "CO2",
        "DataCopy from LocalTensor(CO1) to LocalTensor(CO2) with DataCopyEnhancedParams");
}


template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, __cce_half>::value &&
    IsSameType<PrimT<U>, int32_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<U> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dstLocal, srcLocal);
                                                                                                                   ;
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) __cce_half*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)srcLocal.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, int16_t>::value &&
    IsSameType<PrimT<U>, int32_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<U> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dstLocal, srcLocal);
                                                                                                                   ;
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) int16_t*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)srcLocal.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value &&
    IsSameType<PrimT<U>, int32_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<U> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dstLocal, srcLocal);
                                                                                                                   ;
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) int8_t*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)srcLocal.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, uint8_t>::value &&
    IsSameType<PrimT<U>, int32_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<U> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dstLocal, srcLocal);
                                                                                                                   ;
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) uint8_t*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)srcLocal.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, float>::value &&
    IsSameType<PrimT<U>, __cce_half>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<U> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorPos<U>(srcLocal, Hardware::UB, "srcLocal", "CO2",
        "DataCopy from LocalTensor(CO2) to LocalTensor(CO1) with DataCopyEnhancedParams");
    CheckTensorPos<T>(dstLocal, Hardware::L0C, "dstLocal", "CO1",
        "DataCopy from LocalTensor(CO2) to LocalTensor(CO1) with DataCopyEnhancedParams");
                                                                                                                   ;
    DataCopyUB2L0CImpl((__attribute__((cce_cube_c)) float*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) __cce_half*)srcLocal.GetPhyAddr(), intriParams,
        enhancedParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T> &dstLocal,
    const GlobalTensor<T> &srcGlobal, const DataCopyParams &dataCopyParams, const DataCopyPadParams &padParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }






    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstHWPos == Hardware::UB) {
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            dataCopyParams, padParams);
    } else if (dstHWPos == Hardware::L1) {
        DataCopyPadGM2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            dataCopyParams, padParams);
    } else {







                                                                                                   ;

    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T> &dstGlobal,
    const LocalTensor<T> &srcLocal, const DataCopyParams &dataCopyParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                          ;
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());
    if (srcHWPos == Hardware::UB) {
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
            dataCopyParams);
    } else if (srcHWPos == Hardware::L1) {
        DataCopyPadL12GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)srcLocal.GetPhyAddr(),
            dataCopyParams);
    } else {







                                                                                                   ;

    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPad(const LocalTensor<T> &dstLocal,
    const LocalTensor<T> &srcLocal, const DataCopyParams &dataCopyParams, const Nd2NzParams &nd2nzParams)
{
    CheckNd2NzParams(nd2nzParams, "DataCopyPad with Nd2NzParams");
    using PrimType = PrimT<T>;
    CheckTensorPos<T>(dstLocal, Hardware::L1, "dstLocal", "TSCM", "DataCopyPad with Nd2NzParams");
    CheckTensorPos<T>(srcLocal, Hardware::UB, "srcLocal", "VECIN / VECOUT", "DataCopyPad with Nd2NzParams");
                                                                                                                      ;
    DataCopyPadUB2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        dataCopyParams, nd2nzParams);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T> &dstLocal,
    const GlobalTensor<T> &srcGlobal, const DataCopyExtParams &dataCopyParams, const DataCopyPadExtParams<T> &padParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }






    const Hardware dstHWPos = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstHWPos == Hardware::UB) {
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)srcGlobal.GetPhyAddr(),
            dataCopyParams, padParams);
    } else if (dstHWPos == Hardware::L1) {
        DataCopyPadGM2L1Impl((__attribute__((cce_cube_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)srcGlobal.GetPhyAddr(),
            dataCopyParams, padParams);
    } else {







                                                                                                   ;

    }
}



template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, U>::value &&
    (!IsSameType<T, U>::value), bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T> &dstLocal,
    const GlobalTensor<T> &srcGlobal, const DataCopyExtParams &dataCopyParams, const DataCopyPadExtParams<U> &padParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }






    CheckTensorPos<T>(dstLocal, Hardware::UB, "dstLocal", "VECIN / VECOUT", "DataCopyPad from GM to VECIN / VECOUT");
    DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
        dataCopyParams, padParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T> &dstGlobal,
    const LocalTensor<T> &srcLocal, const DataCopyExtParams &dataCopyParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                          ;
    const Hardware srcHWPos = GetPhyType((TPosition)srcLocal.GetPosition());
    if (srcHWPos == Hardware::UB) {
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
            dataCopyParams);
    } else if (srcHWPos == Hardware::L1) {
        DataCopyPadL12GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)srcLocal.GetPhyAddr(),
            dataCopyParams);
    } else {







                                                                                                   ;

    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPad(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyExtParams &dataCopyParams, const Nd2NzParams &nd2nzParams)
{
    CheckNd2NzParams(nd2nzParams, "DataCopyPad with Nd2NzParams");
    using PrimType = PrimT<T>;
    CheckTensorPos<T>(dstLocal, Hardware::L1, "dstLocal", "TSCM", "DataCopyPad with Nd2NzParams");
    CheckTensorPos<T>(srcLocal, Hardware::UB, "srcLocal", "VECIN / VECOUT", "DataCopyPad with Nd2NzParams");
                                                                                                                      ;
    DataCopyPadUB2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        dataCopyParams, nd2nzParams);
}

template <typename T, TPosition pos>
[aicore] __inline__ __attribute__((always_inline)) void SetPadValue(T paddingValue)
{

    if (g_coreType == AIC) {
        return;
    }
    set_mov_pad_val(GetScalarBitcodeValue((T)paddingValue));
# 1088 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_dump_tensor_intf.cppm" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_dump_tensor_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_dump_tensor_impl.h" 1
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_dump_tensor_impl.h"
namespace AscendC {
[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_dumpWorkspaceReserved;

template <typename T> [aicore] __inline__ __attribute__((always_inline)) uint32_t GetDataType(T data)
{
    uint32_t type;

    if (IsSameType<T, uint8_t>::value) {
        return 4;
    } else if (IsSameType<T, int8_t>::value) {
        return 2;
    } else if (IsSameType<T, int16_t>::value) {
        return 6;
    } else if (IsSameType<T, uint16_t>::value) {
        return 7;
    } else if (IsSameType<T, int32_t>::value) {
        return 3;
    } else if (IsSameType<T, uint32_t>::value) {
        return 8;
    } else if (IsSameType<T, uint64_t>::value) {
        return 10;
    } else if (IsSameType<T, int64_t>::value) {
        return 9;
    } else if (IsSameType<T, float>::value) {
        return 0;
    } else if (IsSameType<T, __cce_half>::value) {
        return 1;
    } else if (IsSameType<T, bfloat16_t>::value) {
        return 27;
    } else {
        return 42;
    }
}

[aicore] __inline__ __attribute__((always_inline)) uint8_t GetDumpBlockIdx()
{
    if constexpr(g_coreType == AscendC::AIV) {
        return GetBlockIdxImpl();
    } else {
        return GetBlockIdxImpl() + AIV_CORE_NUM;
    }
}


[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockNum();
[aicore] __inline__ __attribute__((always_inline)) void InitDumpImpl(bool mixFlag, uint32_t gmLen)
{
    uint64_t firstTimeStamp = static_cast<uint64_t>(GetSystemCycle());
    uint32_t totalBlockNum;

    if (g_dumpWorkspaceReserved == nullptr) {

                                                                                        ;
        return;
    }
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;

    if (mixFlag == true) {
        totalBlockNum = GetBlockNum() * (1 + MIX_NUM);
    } else {
        totalBlockNum = GetBlockNum();
    }
    uint32_t blockDumpSize = DUMP_UINTSIZE;

    uint32_t blockDim = GetDumpBlockIdx();
    if (blockDim >= DUMP_CORE_COUNT) {
        return;
    }



    uint32_t blkInfoLen = sizeof(BlockInfo) + sizeof(DumpMeta);

    uint64_t blockInfoStart = dumpWorkspaceStart + blockDim * DUMP_UINTSIZE;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_LEN_POS) = blockDumpSize;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_CORE_POS) = blockDim;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_BLOCKNUM_POS) = totalBlockNum;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_DUMPOFFSET_POS) = blockDumpSize - blkInfoLen;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_MAGIC_POS) = 0x5aa5bccd;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_RSV_POS) = 0;
    *((__attribute__((cce_global)) uint64_t*)((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_DUMP_ADDR)) = blockInfoStart + blkInfoLen;

    blockInfoStart = blockInfoStart + sizeof(BlockInfo);
    *(__attribute__((cce_global)) uint32_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_TYPE_POS) =
        static_cast<uint32_t>(DumpType::DUMP_META);
    *(__attribute__((cce_global)) uint32_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_LEN_POS) = 8;
    *(__attribute__((cce_global)) uint16_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_BLOCK_DIM_POS) =
        static_cast<uint16_t>(GetBlockNum());
    *(__attribute__((cce_global)) uint8_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_CORE_TYPE_POS) =
        static_cast<uint8_t>(g_coreType);
    *(__attribute__((cce_global)) uint8_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_TASK_RATION) =
        static_cast<uint8_t>(mixFlag);
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + DUMP_META_RSV_POS) = 0;
# 143 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_dump_tensor_impl.h"
    dcci((__attribute__((cce_global)) uint64_t*)blockInfoStart, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}
[aicore] __inline__ __attribute__((always_inline)) DataCopyParams GetDataCopyParamImpl(uint32_t offset)
{
    DataCopyParams repeatParams;
    repeatParams.blockCount = 1;
    repeatParams.blockLen = offset / ONE_BLK_SIZE;
    repeatParams.srcStride = 0;
    repeatParams.dstStride = 0;
    return repeatParams;
}
[aicore] __inline__ __attribute__((always_inline)) FixpipeInfoParams<float> GetFixpipeParamImpl(uint32_t dumpSize)
{
    FixpipeParams<float> fixpipeParams;
    uint16_t align = (dumpSize % DEFAULT_BLOCK_SIZE == 0) ? 0 : 1;
    uint16_t cout_blocks = align + dumpSize / DEFAULT_BLOCK_SIZE;
    fixpipeParams = { cout_blocks, static_cast<uint16_t>(16 * 16 * sizeof(float) / 32), 0, 0};
    FixpipeInfoParams<float> fixpipeInfo(fixpipeParams, sizeof(float));
    uint16_t cburstNum = fixpipeInfo.tiling.nSize / 16;
    return fixpipeInfo;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t CheckValidPosition(const LocalTensor<T>& tensor)
{

    uint32_t position = 0;
    if ((Hardware)GetPhyType((TPosition)tensor.GetPosition()) == Hardware::UB) {
        position = static_cast<uint32_t>(AscendC::Hardware::UB);
        return position;
    } else if ((Hardware)GetPhyType((TPosition)tensor.GetPosition()) == Hardware::L1) {
        position = static_cast<uint32_t>(AscendC::Hardware::L1);
        return position;
    } else if ((Hardware)GetPhyType((TPosition)tensor.GetPosition()) == Hardware::L0C) {
        position = static_cast<uint32_t>(AscendC::Hardware::L0C);
        return position;
    } else {
        return false;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void DumpShapeImpl(const ShapeInfo &shapeInfo)
{
    uint8_t core = GetDumpBlockIdx();
    if (core >= DUMP_CORE_COUNT) {
        return;
    }
    uint32_t valueSize = sizeof(DumpShapeMessageHead);
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    uint32_t tlvSize = valueSize + DUMP_SHAPE_MESSAGE_TL_LEN;
    if (ptr->dumpOffset < tlvSize) {


                          ;
        *((__attribute__((cce_global)) uint32_t*)ptr + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return;
    }
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_TYPE_POS) = static_cast<uint32_t>(DumpType::DUMP_SHAPE);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_LEN_POS) = valueSize;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_DIM_POS) = shapeInfo.shapeDim;
    for (uint32_t idx = 0; idx < shapeInfo.shapeDim && idx < 8; idx++) {
        *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_SHAPE_START_POS + idx) = shapeInfo.shape[idx];
    }
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_RSV_POS) = 0;

    ptr->dumpAddr += tlvSize;
    ptr->dumpOffset -= tlvSize;
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensorLocal2GMEntityImpl(const LocalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize)
{
    uint32_t position = CheckValidPosition(tensor);

    if (position == 0) {

                                                                                                          ;
        return;
    }

    T data;
    uint8_t core = GetDumpBlockIdx();
    if (core >= DUMP_CORE_COUNT) {
        return;
    }
    uint32_t offset = dumpSize * sizeof(T);

    if (offset % ONE_BLK_SIZE != 0) {

                                                                                                       ;
        return;
    }

    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;

    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    if (ptr->dumpOffset < (offset + sizeof(DumpMessageHead))) {


                                                   ;
        *((__attribute__((cce_global)) uint32_t*)ptr + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return;
    }

    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_TYPE_POS) = static_cast<uint32_t>(DumpType::DUMP_TENSOR);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_LEN_POS) = offset + DUMP_MSG_HEAD_SIZE;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_ADDR_POS) =
        static_cast<uint32_t>(reinterpret_cast<uintptr_t>(tensor.GetPhyAddr()));
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DATA_TYPE_POS) = GetDataType(data);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DESC_POS) = desc;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_BUFFERID_POS) = 0;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_POSITION_POS) = position;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_RSV_POS) = 0;

    ptr->dumpAddr += sizeof(DumpMessageHead);
    ptr->dumpOffset -= sizeof(DumpMessageHead);
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    DataCopyParams repeatParams = GetDataCopyParamImpl(offset);
    const Hardware srcHWPos = GetPhyType((TPosition)tensor.GetPosition());

    PipeBarrier<PIPE_ALL>();
    if (srcHWPos == Hardware::UB) {
        DataCopyUB2GMImpl((__attribute__((cce_global)) T*)(ptr->dumpAddr), (__attribute__((cce_unif_buff)) T*)tensor.GetPhyAddr(), repeatParams);
    } else if (srcHWPos == Hardware::L1) {
        DataCopyL12GMImpl((__attribute__((cce_global)) T*)(ptr->dumpAddr), (__attribute__((cce_cube_buff)) T*)tensor.GetPhyAddr(), repeatParams);
    } else if (srcHWPos == Hardware::L0C) {
        if constexpr(g_coreType != AscendC::AIC) {
            return;
        }

        FixpipeInfoParams<float> fixpipeInfo = GetFixpipeParamImpl(dumpSize);
        copy_matrix_cc_to_gm((__attribute__((cce_global)) float*)(ptr->dumpAddr), (__attribute__((cce_cube_c)) float*)(tensor.GetPhyAddr()),
                    fixpipeInfo.sid, fixpipeInfo.n, fixpipeInfo.m, fixpipeInfo.dstStride, fixpipeInfo.srcStride,
                    fixpipeInfo.unitFlag, QuantMode_t::NoQuant, static_cast<uint8_t>(fixpipeInfo.reluEn),
                    fixpipeInfo.channelSplit, fixpipeInfo.nz2ndEn);
    }
    PipeBarrier<PIPE_ALL>();
    ptr->dumpOffset -= offset;
    ptr->dumpAddr += offset;
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensorLocal2GMImpl(const LocalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize)
{
    uint64_t ctrlValue = get_ctrl();
    set_atomic_none();
    DumpTensorLocal2GMEntityImpl(tensor, desc, dumpSize);
    set_ctrl(ctrlValue);
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetLoopCount(uint32_t offset)
{
    uint32_t loopCount = 0;
    if (offset % ONE_DUMP_BACKUP_SIZE != 0) {
        loopCount = 1 + offset / ONE_DUMP_BACKUP_SIZE;
    } else {
        loopCount = offset / ONE_DUMP_BACKUP_SIZE;
    }
    return loopCount;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitTmpTensor(LocalTensor<T>& tmpLocal, uint8_t quePos)
{
    TBuffAddr tbuf_tmpLocal;
    tbuf_tmpLocal.logicPos = quePos;
    tmpLocal.SetAddr(tbuf_tmpLocal);



    tmpLocal.address_.bufferAddr = (uint64_t)(0);

    tmpLocal.address_.dataLen = ONE_DUMP_BACKUP_SIZE;
}
[aicore] __inline__ __attribute__((always_inline)) bool CheckDumpValid(uint32_t offset)
{
    if (offset % ONE_BLK_SIZE != 0) {
                                                                                      ;
        return false;
    }
    uint8_t core = GetDumpBlockIdx();
    if (core >= DUMP_CORE_COUNT) {
        return false;
    }
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    if (reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) < DUMP_WORKSPACE_SIZE) {
                                                                                                   ;
        return false;
    }
    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    if (ptr->dumpOffset < (offset + sizeof(DumpMessageHead) + ONE_DUMP_BACKUP_SIZE)) {

                                                                      ;
        *((__attribute__((cce_global)) uint32_t*)ptr + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return false;
    }

    return true;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpBlockInfoImpl(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize)
{
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    uint32_t position = static_cast<uint32_t>(AscendC::Hardware::GM);
    T data;
    uint32_t offset = dumpSize * sizeof(T);

    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * GetDumpBlockIdx());
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_TYPE_POS) = static_cast<uint32_t>(DumpType::DUMP_TENSOR);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_LEN_POS) = offset + DUMP_MSG_HEAD_SIZE;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_ADDR_POS) =
        static_cast<uint32_t>(reinterpret_cast<uintptr_t>(tensor.GetPhyAddr()));
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DATA_TYPE_POS) = GetDataType(data);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DESC_POS) = desc;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_BUFFERID_POS) = 0;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_POSITION_POS) = position;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_RSV_POS) = 0;

    ptr->dumpAddr += sizeof(DumpMessageHead);
    ptr->dumpOffset -= sizeof(DumpMessageHead);
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpGMTailImpl(LocalTensor<T>& tmpLocal, uint32_t alignSize, uint64_t tmpAddr,
                                      uint64_t gmAddr, uint32_t offset)
{
    DataCopyParams tailParams = GetDataCopyParamImpl((alignSize + ONE_BLK_SIZE - 1) & (~(ONE_BLK_SIZE - 1)));
    if (g_coreType == AIV) {
        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(),
                          (__attribute__((cce_global)) T*)(tmpAddr + offset - alignSize), tailParams);
        PipeBarrier<PIPE_ALL>();
        DataCopyUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(), tailParams);
    } else if (g_coreType == AIC) {
        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(),
                          (__attribute__((cce_global)) T*)(tmpAddr + offset - alignSize), tailParams);
        PipeBarrier<PIPE_ALL>();
        DataCopyL12GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(), tailParams);
    }
    PipeBarrier<PIPE_ALL>();
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensorGM2GMEntityImpl(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize)
{
    uint32_t position = static_cast<uint32_t>(AscendC::Hardware::GM);
    T data;
    uint32_t offset = dumpSize * sizeof(T);
    if (!CheckDumpValid(offset)) {
        return;
    }
    DumpBlockInfoImpl(tensor, desc, dumpSize);
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * GetDumpBlockIdx());
    DataCopyParams backupParams = GetDataCopyParamImpl(ONE_DUMP_BACKUP_SIZE);
    LocalTensor<T> tmpLocal;
    uint64_t gmBackAddr = dumpWorkspaceStart + DUMP_UINTSIZE * (GetDumpBlockIdx() + 1) - ONE_DUMP_BACKUP_SIZE;


    PipeBarrier<PIPE_ALL>();
    if (g_coreType == AIV) {
        InitTmpTensor(tmpLocal, (uint8_t)TPosition::VECIN);
        DataCopyUB2GMImpl((__attribute__((cce_global)) T*)(gmBackAddr), (__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(), backupParams);
    } else if (g_coreType == AIC) {
        InitTmpTensor(tmpLocal, (uint8_t)TPosition::A1);
        DataCopyL12GMImpl((__attribute__((cce_global)) T*)(gmBackAddr), (__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(), backupParams);
    }
    PipeBarrier<PIPE_ALL>();
    dcci((__attribute__((cce_global)) uint64_t*)gmBackAddr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);

    uint32_t alignSize = offset % ONE_DUMP_BACKUP_SIZE;
    uint64_t tmpAddr = static_cast<uint64_t>(reinterpret_cast<uintptr_t>(tensor.GetPhyAddr()));
    uint64_t gmAddr = ptr->dumpAddr;
    for (int i = 0; i < offset / ONE_DUMP_BACKUP_SIZE; i++) {
        if (g_coreType == AIV) {
            DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(),
                              (__attribute__((cce_global)) T*)(tmpAddr + ONE_DUMP_BACKUP_SIZE * i), backupParams);
            PipeBarrier<PIPE_ALL>();
            DataCopyUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(), backupParams);
            gmAddr += ONE_DUMP_BACKUP_SIZE;
        } else if (g_coreType == AIC) {
            DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(),
                              (__attribute__((cce_global)) T*)(tmpAddr + ONE_DUMP_BACKUP_SIZE * i), backupParams);
            PipeBarrier<PIPE_ALL>();
            DataCopyL12GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(), backupParams);
            gmAddr += ONE_DUMP_BACKUP_SIZE;
        }
        PipeBarrier<PIPE_ALL>();
    }
    if (alignSize != 0) {
        DumpGMTailImpl(tmpLocal, alignSize, tmpAddr, gmAddr, offset);
    }
    if (g_coreType == AIV) {
        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)gmBackAddr, backupParams);
    } else if (g_coreType == AIC) {
        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)gmBackAddr, backupParams);
    }
    PipeBarrier<PIPE_ALL>();
    ptr->dumpOffset -= offset;
    ptr->dumpAddr += offset;
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensorGM2GMImpl(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize)
{
    uint64_t ctrlValue = get_ctrl();
    set_atomic_none();
    DumpTensorGM2GMEntityImpl(tensor, desc, dumpSize);
    set_ctrl(ctrlValue);
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsNum()
{
    return 0;
}

template <typename T, typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsNum(T scalar, Args... args)
{
    return 1 + GetArgsNum(args...);
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetStringLength(__attribute__((cce_global)) const char* s)
{
    uint32_t i = 0;
    while (*(s + i) != '\0') {
        i++;
    }
    return i + 1;
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsSize()
{
    return 0;
}

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsSize(Args&&... args);

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsSizeImpl(__attribute__((cce_global)) const char* s, Args&&... args)
{
    uint32_t strLen = GetStringLength(s);
    uint32_t strParamSize = ONE_PARAM_SIZE + strLen;
    return strParamSize + GetArgsSize(args...);
}

template <typename T, typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsSizeImpl(T scalar, Args&&... args)
{
    return ONE_PARAM_SIZE + GetArgsSize(args...);
}

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsSize(Args&&... args)
{
    return GetArgsSizeImpl(args...);
}

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetParamSize(__attribute__((cce_global)) const char* fmt, Args&&... args)
{
    uint32_t fmtSize = GetStringLength(fmt);
    uint32_t argsSize = GetArgsSize(args...);
    return fmtSize + argsSize + ONE_PARAM_SIZE;
}

[aicore] __attribute__((cce_global)) __inline__ __attribute__((always_inline)) BlockInfo *GetBlockInfo()
{
    uint8_t core = GetDumpBlockIdx();
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    __attribute__((cce_global)) BlockInfo *blockInfo = (__attribute__((cce_global)) BlockInfo *)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    return blockInfo;
}

[aicore] __inline__ __attribute__((always_inline)) void WriteString(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, __attribute__((cce_global)) const char* s, uint32_t& offset)
{
    __attribute__((cce_global)) uint64_t *stringAddr = reinterpret_cast<__attribute__((cce_global)) uint64_t *>(paramAddr) + paramIdx;
    __attribute__((cce_global)) uint64_t *dstStrAddr = reinterpret_cast<__attribute__((cce_global)) uint64_t *>(paramAddr + offset);


    *((__attribute__((cce_global)) uint64_t *)stringAddr) = static_cast<uint64_t>(offset - ONE_PARAM_SIZE * paramIdx);
    dcci((__attribute__((cce_global)) uint64_t*)stringAddr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);


    __attribute__((cce_global)) char *d = (__attribute__((cce_global)) char *)(dstStrAddr);
    uint32_t strLen = GetStringLength(s);

    for (uint32_t i = 0; i < strLen; i++) {
        *(d + i) = *(s + i);
        dcci((__attribute__((cce_global)) uint64_t*)d, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    }
    offset += strLen;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void WriteScalar(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, T scalar)
{
    __attribute__((cce_global)) uint64_t *scalarAddr = (__attribute__((cce_global)) uint64_t *)paramAddr + paramIdx;
    *scalarAddr = 0;

    static_assert(!SupportType<T, double>(), "printf unsupport double type");

    if constexpr (SupportType<T, __cce_half, float>()) {
        *((__attribute__((cce_global)) float *)scalarAddr) = static_cast<float>(scalar);
    } else if constexpr(SupportType<T, bfloat16_t>()) {
        *((__attribute__((cce_global)) float *)scalarAddr) = ToFloat(scalar);
    } else if constexpr (std::is_signed<T>::value) {
        *((__attribute__((cce_global)) int64_t *)scalarAddr) = static_cast<int64_t>(scalar);
    } else if constexpr(std::is_unsigned<T>::value) {
        *((__attribute__((cce_global)) uint64_t *)scalarAddr) = static_cast<uint64_t>(scalar);
    } else if constexpr(std::is_pointer<T>::value) {
        *((__attribute__((cce_global)) uint64_t *)scalarAddr) = (uintptr_t)scalar;
    } else if constexpr(std::is_enum<T>::value) {
        *((__attribute__((cce_global)) uint64_t *)scalarAddr) = static_cast<uint64_t>(scalar);
    }

    dcci((__attribute__((cce_global)) uint64_t*)scalarAddr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

[aicore] __inline__ __attribute__((always_inline)) void SetParam(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset)
{
    return;
}

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) void SetParam(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset, Args&&... args);

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) void SetParamImpl(__attribute__((cce_global)) uint8_t *paramAddr, uint32_t paramIdx, uint32_t &offset,
                                    __attribute__((cce_global)) const char *s, Args&&... args)
{
    WriteString(paramAddr, paramIdx, s, offset);
    SetParam(paramAddr, paramIdx + 1, offset, args...);
}

template <typename T, typename... Args>
[aicore] __inline__ __attribute__((always_inline)) void SetParamImpl(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset, T scalar,
                                    Args&&... args)
{
    WriteScalar(paramAddr, paramIdx, scalar);
    SetParam(paramAddr, paramIdx + 1, offset, args...);
}

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) void SetParam(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset, Args&&... args)
{
    SetParamImpl(paramAddr, paramIdx, offset, args...);
}

[aicore] __inline__ __attribute__((always_inline)) void WriteTLHead(DumpType printType, __attribute__((cce_global)) uint8_t *tlv, uint32_t valueSize)
{
    *((__attribute__((cce_global)) uint32_t *)tlv) = static_cast<uint32_t>(printType);
    *((__attribute__((cce_global)) uint32_t *)tlv + 1) = valueSize;
    dcci((__attribute__((cce_global)) uint64_t*)tlv, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}
[aicore] __inline__ __attribute__((always_inline)) void UpdateBlockInfo(uint32_t tlvSize)
{
    __attribute__((cce_global)) BlockInfo *blockInfo = GetBlockInfo();
    uint32_t remainSize = blockInfo->dumpOffset;
    uint64_t lastDumpAddr = blockInfo->dumpAddr;

    __attribute__((cce_global)) uint8_t *blockInfoStart = (__attribute__((cce_global)) uint8_t *)blockInfo;
    *((__attribute__((cce_global)) uint32_t *)blockInfoStart + BLOCK_INFO_DUMPOFFSET_POS) = remainSize - tlvSize;
    *((__attribute__((cce_global)) uint64_t *)((__attribute__((cce_global)) uint32_t *)blockInfoStart + BLOCK_INFO_DUMP_ADDR)) = lastDumpAddr + tlvSize;
    dcci((__attribute__((cce_global)) uint64_t*)blockInfoStart, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void PrintfEntityImpl(DumpType printType, __attribute__((cce_global)) const char* fmt, Args&&... args)
{

    uint8_t blockIdx = GetDumpBlockIdx();
    if (blockIdx >= DUMP_CORE_COUNT) {
        return;
    }
    __attribute__((cce_global)) BlockInfo *blockInfo = GetBlockInfo();
    uint32_t remainSize = blockInfo->dumpOffset;
    uint64_t dumpAddr = blockInfo->dumpAddr;

    uint32_t paramSize = GetParamSize(fmt, args...);
    uint32_t paramNum = GetArgsNum(args...) + 1;
    paramSize = (paramSize + ONE_PARAM_SIZE - 1) & (~(ONE_PARAM_SIZE - 1));

    uint32_t tlvSize = paramSize + ONE_PARAM_SIZE;
    if (tlvSize > remainSize) {
        __attribute__((cce_global)) uint8_t *blockInfoStart = (__attribute__((cce_global)) uint8_t *)blockInfo;
        *((__attribute__((cce_global)) uint32_t *)blockInfoStart + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)blockInfoStart, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return;
    }

    __attribute__((cce_global)) uint8_t *tlvAddr = (__attribute__((cce_global)) uint8_t *)dumpAddr;
    WriteTLHead(printType, tlvAddr, paramSize);
    __attribute__((cce_global)) uint8_t *paramAddr = tlvAddr + ONE_PARAM_SIZE;
    uint32_t offset = paramNum * ONE_PARAM_SIZE;
    WriteString(paramAddr, 0, fmt, offset);
    uint32_t paramIdx = 1;
    SetParam(paramAddr, paramIdx, offset, args...);


    UpdateBlockInfo(tlvSize);

}

template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void PrintfImpl(DumpType printType, __attribute__((cce_global)) const char* fmt, Args&&... args)
{
    uint64_t ctrlValue = get_ctrl();
    set_atomic_none();
    PrintfEntityImpl(printType, fmt, args...);
    set_ctrl(ctrlValue);
}

[aicore] __inline__ __attribute__((always_inline)) void DumpTimeStampImpl(uint32_t descId)
{
# 678 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_dump_tensor_impl.h"
}

[aicore] __inline__ __attribute__((always_inline)) void AscendCTimeStamp(uint32_t descId, uint64_t pcPtr = 0)
{



}

[aicore] __inline__ __attribute__((always_inline)) void InitDump(bool mixFlag, uint32_t gmLen)
{

    g_dumpWorkspaceReserved = GetSysWorkSpacePtr();
    InitDumpImpl(mixFlag, gmLen);



}
[aicore] __inline__ __attribute__((always_inline)) void InitDump(bool mixFlag, __attribute__((cce_global)) uint8_t* dumpStartAddr, uint32_t gmLen)
{

    g_dumpWorkspaceReserved = dumpStartAddr + DUMP_WORKSPACE_SIZE;
    InitDumpImpl(mixFlag, gmLen);



}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_dump_tensor_intf.cppm" 2
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_dump_tensor_intf.cppm"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const LocalTensor<T> &tensor, uint32_t desc, uint32_t dumpSize)
{
                                                                                                    ;

    DumpTensorLocal2GMImpl(tensor, desc, dumpSize);

    return;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize)
{
                                                                                                    ;

    DumpTensorGM2GMImpl(tensor, desc, dumpSize);

    return;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize, const ShapeInfo& shapeInfo)
{
                                                                                                    ;

    DumpShapeImpl(shapeInfo);
    DumpTensorGM2GMImpl(tensor, desc, dumpSize);

    return;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const LocalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize, const ShapeInfo& shapeInfo)
{
                                                                                                    ;

    DumpShapeImpl(shapeInfo);
    DumpTensorLocal2GMImpl(tensor, desc, dumpSize);

    return;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpAccChkPoint(const LocalTensor<T> &tensor, uint32_t index, uint32_t countOff, uint32_t dumpSize)
{
                                                                                                         ;

    if (countOff > tensor.GetSize()) {


                                                       ;
        return;
    }
    LocalTensor<T> tmpTensor = tensor[countOff];
    DumpTensorLocal2GMImpl(tmpTensor, index, dumpSize);

    return;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpAccChkPoint(const GlobalTensor<T> &tensor, uint32_t index, uint32_t countOff, uint32_t dumpSize)
{
                                                                                                         ;

    if (countOff > tensor.GetSize()) {


                                                       ;
        return;
    }
    GlobalTensor<T> tmpTensor = tensor[countOff];
    DumpTensorGM2GMImpl(tmpTensor, index, dumpSize);

    return;
}
# 155 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_dump_tensor_intf.cppm"
template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void PRINTF(__attribute__((cce_global)) const char* fmt, Args&&... args)
{

    PrintfImpl(DumpType::DUMP_SCALAR, fmt, args...);

}

template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void printf(__attribute__((cce_global)) const char* fmt, Args&&... args)
{

    PrintfImpl(DumpType::DUMP_SCALAR, fmt, args...);

}



template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void AssertImpl(__attribute__((cce_global)) const char* fmt, Args&&... args)
{

    PrintfImpl(DumpType::DUMP_ASSERT, fmt, args...);



}
# 213 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_dump_tensor_intf.cppm"
[aicore] __inline__ __attribute__((always_inline)) void PrintTimeStamp(uint32_t descId)
{



}


}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
namespace AscendC {
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParams& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dstLocal, srcLocal, loadDataParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcLocal,
    const LoadData2DParams& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dstLocal, srcLocal, loadDataParams);
}
# 77 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParamsV2& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dstLocal, srcLocal, loadDataParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcLocal,
    const LoadData2DParamsV2& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dstLocal, srcLocal, loadDataParams);
}
# 121 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T, const IsResetLoad3dConfig &defaultConfig,
    typename U, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV1<U>& loadDataParams)
{
    CheckLoadData3dParams(loadDataParams.l1H, loadDataParams.l1W, loadDataParams.strideW, loadDataParams.strideH);

                                           ;

                                                           ;

                                                           ;

                                           ;

                                           ;

                                           ;
                                                                                                                ;
                                                                                                      ;
    LoadDataImpl<T, defaultConfig>(dstLocal, srcLocal, loadDataParams);
}
# 171 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T, const IsResetLoad3dConfig &defaultConfig,
    typename U, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2<U>& loadDataParams)
{






    LoadDataImpl<T, defaultConfig>(dstLocal, srcLocal, loadDataParams);
}
# 211 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    LoadDataImpl<T>(dstLocal, srcLocal, loadDataParams);
}
# 233 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTranspose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParams& loadDataParams)
{
    LoadDataWithTransposeImpl(dstLocal, srcLocal, loadDataParams);
}
# 253 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTranspose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParamsV2& loadDataParams)
{
    LoadDataWithTransposeImpl(dstLocal, srcLocal, loadDataParams);
}
# 280 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename DstT, typename Src0T, typename Src1T>
[aicore] __inline__ __attribute__((always_inline)) void Mmad(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const MmadParams& mmadParams)
{
    MmadImpl(dstLocal, fmLocal, filterLocal, mmadParams);
}

template <typename DstT, typename Src0T, typename Src1T, typename BiasT>
[aicore] __inline__ __attribute__((always_inline)) void Mmad(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const LocalTensor<BiasT>& biasLocal, const MmadParams& mmadParams)
{
    MmadImpl(dstLocal, fmLocal, filterLocal, biasLocal, mmadParams);
}


template <typename T, typename U,
    typename std::enable_if<IsSameType<PrimT<T>, int32_t>::value, bool>::type,
    typename std::enable_if<IsSameType<PrimT<U>, int8_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void MmadWithSparse(const LocalTensor<T>& dstLocal, const LocalTensor<U>& fmLocal,
    const LocalTensor<U>& filterLocal, const MmadParams& mmadParams)
{
    MmadSpImpl(dstLocal, fmLocal, filterLocal, mmadParams);
}

template <typename T, typename U,
    typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value, bool>::type,
    typename std::enable_if<IsSameType<PrimT<U>, uint8_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithSparse(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<U> &idxLocal, const LoadData2dParams &loadDataParam)
{
    LoadDataWithSparseImpl(dstLocal, srcLocal, idxLocal, loadDataParam);
}
# 325 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void BroadCastVecToMM(const LocalTensor<T> &dstLocal,
    const LocalTensor<U> &srcLocal, const int32_t blockCount, const uint8_t blockLen, const uint8_t srcGap,
    const uint8_t dstGap)
{
    BroadCastVecToMMImpl(dstLocal, srcLocal, blockCount, blockLen, srcGap, dstGap);
}
# 345 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void InitConstValue(const LocalTensor<T> &dstLocal,
    const InitConstValueParams<U>& initConstValueParams)
{
# 362 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
    InitConstValueImpl(dstLocal, initConstValueParams);
}
# 372 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataPaddingValue(const T padValue)
{
    Load3DSetPaddingImpl(padValue);
}
# 388 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
[aicore] __inline__ __attribute__((always_inline)) void SetFmatrix(uint16_t l1H, uint16_t l1W, const uint8_t padList[4], const FmatrixMode& fmatrixMode)
{
                                                                                     ;
                                                                                     ;
    SetFmatrixImpl(l1H, l1W, padList, fmatrixMode);
}
# 403 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataBoundary(uint32_t boundaryValue)
{
    SetLoadDataBoundaryImpl(boundaryValue);
}




[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataRepeat(const LoadDataRepeatParam& repeatParams)
{
                                                                                               ;
    SetLoadDataRepeatImpl(repeatParams);
}
# 434 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadImageToLocal(const LocalTensor<T>& dstLocal,
    const LoadImageToLocalParams& loadDataParams)
{





                                                                                                       ;
                                                                                                     ;
                                                                                                               ;
                                                                                                             ;
                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
                                                                                                   ;
                                                                                                     ;
    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L1) {
        LoadImageToLocalCal((__attribute__((cce_cube_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), loadDataParams);
    } else {

                                                                                               ;
    }
}
# 470 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzip(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal)
{
    LoadDataUnzipImpl(dstLocal, srcGlobal);
}
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm"
namespace AscendC {

template <typename T>
[[deprecated("NOTICE: GetGemmTiling has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) GemmTiling GetGemmTiling(uint32_t m, uint32_t k, uint32_t n)
{
    uint32_t c0 = 0;
    uint32_t dSize = 1;
    if (IsSameType<T, uint8_t>::value || IsSameType<T, int8_t>::value) {
        c0 = 32;
        dSize = 1;
    } else {
        c0 = 16;
        dSize = 2;
    }
    GemmTiling tilling;
    tilling.c0Size = c0;
    tilling.dtypeSize = dSize;
    tilling.mNum = m;
    tilling.nNum = n;
    tilling.kNum = k;
    tilling.roundM = DivCeil(m, tilling.blockSize) * tilling.blockSize;
    tilling.roundN = DivCeil(n, tilling.blockSize) * tilling.blockSize;
    tilling.roundK = DivCeil(k, tilling.c0Size) * tilling.c0Size;
    uint32_t k0a = TOTAL_L0A_SIZE / 2 / (tilling.roundM * dSize);
    uint32_t k0b = TOTAL_L0B_SIZE / 2 / (tilling.roundN * dSize);
    uint32_t k0 = k0a > k0b ? k0b : k0a;
    k0 = k0 > k ? k : k0;

    tilling.kTileBlock = k0 / tilling.c0Size;
    if (tilling.kTileBlock == 0) {
        tilling.kTileBlock = 1;
    }
    tilling.loopMode = LoopMode::MODE_NM;

    tilling.mBlockNum = DivCeil(m, tilling.blockSize);
    tilling.nBlockNum = DivCeil(n, tilling.blockSize);
    tilling.kBlockNum = DivCeil(k, tilling.c0Size);

    CalculateGemmTiling(tilling);

    return tilling;
}
# 99 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm"
template <typename dst_T, typename src0_T, typename src1_T>
[[deprecated("NOTICE: Gemm has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Gemm(const LocalTensor<dst_T>& dstLocal, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, const uint32_t m, const uint32_t k, const uint32_t n, GemmTiling tilling,
    bool partialsum, int32_t initValue)
{
# 120 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm"
    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    LocalTensor<dst_T> L0c;
    if (dstScope == Hardware::L0C) {
        L0c = dstLocal[0];
    } else {
# 134 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm"
    }

    if (tilling.loopMode == LoopMode::MODE_NM) {
        GemmExecNm(L0c, src0Local, src1Local, tilling, initValue);
    } else if (tilling.loopMode == LoopMode::MODE_MN) {
        GemmExecMn(L0c, src0Local, src1Local, tilling, initValue);
    } else {

    }
# 151 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm"
}
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_fixpipe_intf.cppm" 1
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_fixpipe_intf.cppm"
namespace AscendC {
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_fixpipe_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfig(const LocalTensor<T> &reluPre, const LocalTensor<T> &quantPre,
    bool isUnitFlag)
{
    SetFixPipeConfigImpl<T>(reluPre, quantPre, isUnitFlag);
}

template <typename T, bool setRelu>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfig(const LocalTensor<T> &preTensor, bool isUnitFlag)
{
    SetFixPipeConfigImpl<T, setRelu>(preTensor, isUnitFlag);
}

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipeNz2ndFlag(uint16_t ndNum, uint16_t srcNdStride, uint16_t dstNdStride)
{
    SetFixpipeNz2ndFlagImpl(ndNum, srcNdStride, dstNdStride);
}

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipePreQuantFlag(uint64_t config)
{
    SetFixpipePreQuantFlagImpl(config);
}

[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeClipRelu(uint64_t config)
{
    SetFixPipeClipReluImpl(config);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeAddr(const LocalTensor<T> &eleWiseTensor, uint16_t c0ChStride)
{
    SetFixPipeAddrImpl(eleWiseTensor, c0ChStride);
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT> &dstLocal, const LocalTensor<SrcT> &srcLocal,
    const FixpipeParamsV220 &intriParams)
{
    CheckTensorPos<SrcT>(srcLocal, Hardware::L0C, "srcLocal", "CO1", "Fixpipe");


                                                                                               ;

    if ((GetPhyType((TPosition)dstLocal.GetPosition()) == Hardware::L1)) {
        FixpipeL0C2L1Impl<PrimT<DstT>, PrimT<SrcT>, config>((__attribute__((cce_cube_buff)) PrimT<DstT>*)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_c)) PrimT<SrcT>*)srcLocal.GetPhyAddr(), intriParams);
    } else if ((GetPhyType((TPosition)dstLocal.GetPosition()) == Hardware::UB)) {
        FixpipeL0C2UBImpl<PrimT<DstT>, PrimT<SrcT>, config>((__attribute__((cce_unif_buff)) PrimT<DstT>*)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_c)) PrimT<SrcT>*)srcLocal.GetPhyAddr(), intriParams);
    }
}


template <typename DstT, typename SrcT, const FixpipeConfig& config, typename BufT,
    typename std::enable_if<IsSameType<PrimT<BufT>, uint64_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT>& dstLocal, const LocalTensor<SrcT>& srcLocal,
    const LocalTensor<BufT>& cbufWorkspace, const FixpipeParamsV220& intriParams)
{
    CheckTensorPos<SrcT>(srcLocal, Hardware::L0C, "srcLocal", "CO1", "Fixpipe");


                                                                                               ;
    CheckTensorPos<BufT>(cbufWorkspace, Hardware::L1, "cbufWorkspace", "A1", "Fixpipe");


                                                                                                       ;
    if ((GetPhyType((TPosition)dstLocal.GetPosition()) == Hardware::L1)) {
        FixpipeL0C2L1Impl<PrimT<DstT>, PrimT<SrcT>, config>((__attribute__((cce_cube_buff)) PrimT<DstT>*)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_c)) PrimT<SrcT>*)srcLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) uint64_t*)cbufWorkspace.GetPhyAddr(), intriParams);
    } else if ((GetPhyType((TPosition)dstLocal.GetPosition()) == Hardware::UB)) {
        FixpipeL0C2UBImpl<PrimT<DstT>, PrimT<SrcT>, config>((__attribute__((cce_unif_buff)) PrimT<DstT>*)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_c)) PrimT<SrcT>*)srcLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) uint64_t*)cbufWorkspace.GetPhyAddr(), intriParams);
    }
}


template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT> &dstGlobal, const LocalTensor<SrcT> &srcLocal,
    const FixpipeParamsV220 &intriParams)
{







    CheckTensorPos<SrcT>(srcLocal, Hardware::L0C, "srcLocal", "CO1", "Fixpipe");
    FixpipeL0C2GMImpl<PrimT<DstT>, PrimT<SrcT>, config>((__attribute__((cce_global)) PrimT<DstT>*)dstGlobal.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<SrcT>*)srcLocal.GetPhyAddr(),
        intriParams);






}


template <typename DstT, typename SrcT, const FixpipeConfig &config, typename BufT,
    typename std::enable_if<IsSameType<PrimT<BufT>, uint64_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT> &dstGlobal, const LocalTensor<SrcT> &srcLocal,
    const LocalTensor<BufT> &cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
    CheckTensorPos<SrcT>(srcLocal, Hardware::L0C, "srcLocal", "CO1", "Fixpipe");
    CheckTensorPos<BufT>(cbufWorkspace, Hardware::L1, "cbufWorkspace", "A1", "Fixpipe");


                                                                                                       ;
    FixpipeL0C2GMImpl<PrimT<DstT>, PrimT<SrcT>, config>((__attribute__((cce_global)) PrimT<DstT>*)dstGlobal.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<SrcT>*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_cube_buff)) uint64_t*)cbufWorkspace.GetPhyAddr(), intriParams);
}
}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm"
namespace AscendC {

template <typename T>
[[deprecated("NOTICE: GetConv2dTiling has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) Conv2dTilling GetConv2dTiling(Conv2dParams& conv2dParams)
{
    Conv2dTilling tilling;
    GetTypeforC0<T>(conv2dParams, tilling);

    tilling.loopMode = LoopMode::MODE_NM;

    tilling.strideH = conv2dParams.stride[0];
    tilling.strideW = conv2dParams.stride[1];
    tilling.dilationH = conv2dParams.dilation[0];
    tilling.dilationW = conv2dParams.dilation[1];
    tilling.hi = conv2dParams.imgShape[0];
    tilling.wi = conv2dParams.imgShape[1];
    tilling.ho = (tilling.hi + conv2dParams.padList[2] + conv2dParams.padList[3] -
        tilling.dilationH * (conv2dParams.kernelShape[0] - 1) - 1) /
        tilling.strideH +
        1;
    tilling.wo = (tilling.wi + conv2dParams.padList[0] + conv2dParams.padList[1] -
        tilling.dilationW * (conv2dParams.kernelShape[1] - 1) - 1) /
        tilling.strideW +
        1;

    tilling.height = conv2dParams.kernelShape[0];
    tilling.width = conv2dParams.kernelShape[1];

    tilling.howo = tilling.ho * tilling.wo;

    tilling.mNum = tilling.howo;
    tilling.nNum = conv2dParams.cout;
    tilling.kNum = conv2dParams.cin * tilling.height * tilling.width;

    CalculateConv2dTiling(tilling);

    return tilling;
}
# 87 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm"
template <typename dst_T, typename src_T>
[[deprecated("NOTICE: Conv2D has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("MTE2")))
    __attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<dst_T> &dstLocal, const LocalTensor<src_T> &featureMap,
    const LocalTensor<src_T> &weight, Conv2dParams &conv2dParams, Conv2dTilling &tilling)
{
    if (conv2dParams.initY == 2) {
        return;
    }

    Conv2D(dstLocal, dstLocal, featureMap, weight, conv2dParams, tilling);
}

template <typename dst_T, typename src_T>
[[deprecated("NOTICE: Conv2D has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("MTE2")))__attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<dst_T> &dstLocal,
    const LocalTensor<dst_T> &bias, const LocalTensor<src_T> &featureMap, const LocalTensor<src_T> &weight,
    Conv2dParams &conv2dParams, Conv2dTilling &tilling)
{
# 121 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm"
    const Hardware dstScope = GetPhyType((TPosition)dstLocal.GetPosition());
    LocalTensor<dst_T> L0c;
    if (dstScope == Hardware::L0C) {
        L0c = dstLocal[0];
    } else {
# 135 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm"
    }

    if (tilling.loopMode == LoopMode::MODE_NM) {
        Conv2DExecNm(L0c, bias, featureMap, weight, conv2dParams, tilling);
    } else if (tilling.loopMode == LoopMode::MODE_MN) {
        Conv2DExecMn(L0c, bias, featureMap, weight, conv2dParams, tilling);
    } else {

    }
# 152 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm"
}
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_common_intf.cppm" 1
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_common_intf.cppm"
namespace AscendC {
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_common_intf.cppm"
[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockNum();
template <bool isAIVOnly>
[aicore] __inline__ __attribute__((always_inline)) void IBSet(const GlobalTensor<int32_t> &gmWorkspace,
    const LocalTensor<int32_t> &ubWorkspace, int32_t blockIdx, int32_t eventID)
{
    int32_t blockNum = GetBlockNum();

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    if (!isAIVOnly) {
        blockNum = GetBlockNum() * 2;
    }


    __ib_set_stub(blockIdx, eventID, isAIVOnly);

    auto localSyncGM = gmWorkspace[blockNum * 8 * eventID + blockIdx * 8];
    pipe_barrier(PIPE_ALL);

    while (true) {
        DataCopy(ubWorkspace, localSyncGM, ONE_BLK_SIZE / sizeof(int32_t));
        event_t eventIdMte2ToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
        SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        if (ubWorkspace.GetValue(0) == 0) {
            ubWorkspace.SetValue(0, 1);
            event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
            SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            DataCopy(localSyncGM, ubWorkspace, ONE_BLK_SIZE / sizeof(int32_t));
            break;
        }
    }
    pipe_barrier(PIPE_ALL);

    __ib_set_stub(blockIdx, eventID, isAIVOnly);

}

template <bool isAIVOnly>
[aicore] __inline__ __attribute__((always_inline)) void IBWait(const GlobalTensor<int32_t> &gmWorkspace,
    const LocalTensor<int32_t> &ubWorkspace, int32_t blockIdx, int32_t eventID)
{
    int32_t blockNum = GetBlockNum();

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    if (!isAIVOnly) {
        blockNum = GetBlockNum() * 2;
    }


    __ib_wait_stub(blockIdx, eventID, isAIVOnly);

    auto localSyncGM = gmWorkspace[blockNum * 8 * eventID + blockIdx * 8];
    pipe_barrier(PIPE_ALL);

    while (true) {
        DataCopy(ubWorkspace, localSyncGM, ONE_BLK_SIZE / sizeof(int32_t));
        event_t eventIdMte2ToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
        SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        if (ubWorkspace.GetValue(0) == 1) {
            ubWorkspace.SetValue(0, 0);
            event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
            SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            DataCopy(localSyncGM, ubWorkspace, ONE_BLK_SIZE / sizeof(int32_t));
            break;
        }
    }
    pipe_barrier(PIPE_ALL);

    __ib_wait_stub(blockIdx, eventID, isAIVOnly);

}





template<pipe_t AIV_PIPE, pipe_t AIC_PIPE>
[aicore] __inline__ __attribute__((always_inline)) void SetNextTaskStart()
{



}

[aicore] __inline__ __attribute__((always_inline)) void WaitPreTaskEnd()
{



}







template <bool isAIVOnly>
[aicore] __inline__ __attribute__((always_inline)) void SyncAll(const GlobalTensor<int32_t> &gmWorkspace,
    const LocalTensor<int32_t> &ubWorkspace, const int usedCores)
{




    SoftSyncAllImpl<isAIVOnly>((__attribute__((cce_global)) int32_t*)gmWorkspace.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t*)ubWorkspace.GetPhyAddr(), usedCores);

}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockIdx()
{
    return GetBlockIdxImpl();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockNum()
{



    return get_block_num();

}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockIdx()
{
    return GetSubBlockIdxImpl();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetTaskRation()
{
    return GetTaskRationImpl();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("V")))
    __attribute__((out_pipe("MTE3"))) void InitOutput(GlobalTensor<T> gmWorkspaceAddr, uint32_t size, T value)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<T> popBuffer;
    bool ret = PopStackBuffer<T, TPosition::LCM>(popBuffer);
    uint32_t maxBurstSize = (MAX_REPEAT_TIMES * ONE_BLK_SIZE) / sizeof(T);
    uint32_t popSize = popBuffer.GetSize() >= maxBurstSize ? maxBurstSize : popBuffer.GetSize();
    uint32_t round = size / popSize;
    uint32_t tail = size % popSize;
    uint32_t roundSize = round != 0 ? popSize : 0;
    DuplicateImpl<T>((__attribute__((cce_unif_buff)) T*)popBuffer.GetPhyAddr(), value, popSize);
    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    struct DataCopyExtParams repeatParams;
    uint32_t comOffset = 0;

    repeatParams = { 1, static_cast<uint32_t>(roundSize * sizeof(T)), 0, 0, 0 };
    for (int index = 0; index < round; ++index) {
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmWorkspaceAddr.GetPhyAddr() + comOffset,
            (__attribute__((cce_unif_buff)) T*)popBuffer.GetPhyAddr(),
            repeatParams);
        comOffset += roundSize;
    }

    repeatParams = {1, static_cast<uint32_t>(tail * sizeof(T)), 0, 0, 0};
    if (tail != 0) {
        comOffset = round * roundSize;
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmWorkspaceAddr.GetPhyAddr() + comOffset,
            (__attribute__((cce_unif_buff)) T*)popBuffer.GetPhyAddr(),
            repeatParams);
    }

}

template<bool isAIVOnly>
[aicore] __inline__ __attribute__((always_inline)) void SyncAll()
{
    SyncAllImpl<isAIVOnly>();
}

template <AtomicDtype type, AtomicOp op>
[aicore] __inline__ __attribute__((always_inline)) void SetStoreAtomicConfig()
{
    SetStoreAtomicConfigImpl<static_cast<atomic_type_t>(type), static_cast<atomic_op_t>(op)>();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetStoreAtomicConfig()
{
    return GetStoreAtomicConfigImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void GetStoreAtomicConfig(uint16_t &atomicType, uint16_t &atomicOp)
{
    GetStoreAtomicConfigImpl(atomicType, atomicOp);
}

template <pipe_t pipe>
[aicore] __inline__ __attribute__((always_inline)) void NotifyEvent(uint16_t flagId)
{
    constexpr uint8_t subBlockSyncMode = 0x02;
    NotifyEventImpl<subBlockSyncMode, pipe>(flagId);
}

template <pipe_t pipe=PIPE_S>
[aicore] __inline__ __attribute__((always_inline)) void WaitEvent(uint16_t flagId)
{
    constexpr uint8_t mode = 0;
    WaitEventImpl<mode, pipe>(flagId);
}

template<uint8_t modeId, pipe_t pipe>
[aicore] __inline__ __attribute__((always_inline)) void CrossCoreSetFlag(uint16_t flagId)
{
    NotifyEventImpl<modeId, pipe>(flagId);
}

template <uint8_t modeId, pipe_t pipe>
[aicore] __inline__ __attribute__((always_inline)) void CrossCoreWaitFlag(uint16_t flagId)
{
    WaitEventImpl<modeId, pipe>(flagId);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCachePreload(const GlobalTensor<uint64_t> &srcTensor, const T cacheOffset)
{
    DataCachePreloadImpl(srcTensor, cacheOffset);
}

[aicore] __inline__ __attribute__((always_inline)) void ICachePreLoad(const int64_t preFetchLen)
{
    PreLoad(preFetchLen);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetICachePreloadStatus()
{
    return GetICachePreloadStatusImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void CheckLocalMemoryIA(const CheckLocalMemoryIAParam& checkParams)
{
    CheckLocalMemoryIAImpl(checkParams);
}


template <HardEvent event, MemoryT memT, bool isVirtual> [aicore] __inline__ __attribute__((always_inline)) void HSetFlag(int32_t eventID)
{
    if (g_coreType == AIV) {
        return;
    }
    HSetFlagImpl<event, memT, isVirtual>(eventID);
}

template <HardEvent event, MemoryT memT, bool isVirtual> [aicore] __inline__ __attribute__((always_inline)) void HWaitFlag(int32_t eventID)
{
    if (g_coreType == AIV) {
        return;
    }
    HWaitFlagImpl<event, memT, isVirtual>(eventID);
}

}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm" 1
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
namespace AscendC {


constexpr int32_t REGION_PROPOSAL_LABEL_POSITION = 5;
constexpr int32_t REGION_PROPOSAL_Y1_POSITION = 1;
constexpr uint8_t GATHER_MASK_MODE_FOR_INDEX_EVEN = 1;
constexpr uint8_t GATHER_MASK_MODE_FOR_INDEX_ODD = 2;

constexpr uint8_t GATHER_MASK_MODE_FOR_EXTRACT_INDEX = 4;
constexpr int32_t REGION_PROPOSAL_SCORE_POSITION = 4;

#pragma begin_pipe(V)
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort4(const LocalTensor<T>& dstLocal, const MrgSortSrcList<T>& srcLocal,
    const MrgSort4Info& params)
{


                                            ;
    for (int8_t i = 0; i < MRG_SORT_ELEMENT_LEN; ++i) {
                                                                                                  ;
    }

                                                                                                                   ;





    uint64_t config = 0;
    config |= (params.repeatTimes & 0xFF);
    config |= (uint64_t(params.elementLengths[0] & 0xFFF) << 8);
    config |= (uint64_t(params.elementLengths[1] & 0xFFF) << 20);
    config |= (uint64_t(params.elementLengths[2] & 0xFFF) << 32);
    config |= (uint64_t(params.elementLengths[3] & 0xFFF) << 44);
    config |= (uint64_t(params.ifExhaustedSuspension & 0x1) << 59);
    config |= (uint64_t(params.validBit & 0xF) << 60);

    __attribute__((cce_unif_buff)) T *addrArray[MRG_SORT_ELEMENT_LEN] = {(__attribute__((cce_unif_buff)) T *)srcLocal.src1.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src2.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src3.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src4.GetPhyAddr()};
    Vmrgsort4Cal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), addrArray, config);
}
# 108 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void RpSort16(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes)
{


                                            ;
                                                                             ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTimes;
    VbitsortCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeatParams);
}
# 138 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort(const LocalTensor<T>& dstLocal, const MrgSortSrcList<T>& srcLocal,
    const MrgSort4Info& params)
{


                                            ;
    for (int8_t i = 0; i < MRG_SORT_ELEMENT_LEN; ++i) {
                                                                                                 ;
    }

                                                                                                                  ;





    uint64_t config = 0;
    config |= (params.repeatTimes & 0xFF);
    config |= (uint64_t(params.validBit & 0xF) << 8);
    config |= (uint64_t(params.ifExhaustedSuspension & 0x1) << 12);

    uint64_t src1 = 0;
    src1 |= (uint64_t(params.elementLengths[0] & 0xFFFF));
    src1 |= (uint64_t(params.elementLengths[1] & 0xFFFF) << 16);
    src1 |= (uint64_t(params.elementLengths[2] & 0xFFFF) << 32);
    src1 |= (uint64_t(params.elementLengths[3] & 0xFFFF) << 48);

    __attribute__((cce_unif_buff)) T *addrArray[MRG_SORT_ELEMENT_LEN] = {(__attribute__((cce_unif_buff)) T *)srcLocal.src1.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src2.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src3.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src4.GetPhyAddr()};

    Vmrgsort4Cal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), addrArray, src1, config);
}
# 183 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sort32(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<uint32_t>& src1Local, const int32_t repeatTimes)
{


                                            ;
                                                                           ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTimes;
    VbitsortCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t*)src1Local.GetPhyAddr(), repeatParams);
}
# 212 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ProposalConcat(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes, const int32_t modeNumber)
{

                                                                                      ;
                                                                                   ;
                                                                               ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTimes;
    repeatParams.modeNumber = modeNumber;
    VconcatCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeatParams);
}
# 240 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ProposalExtract(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes, const int32_t modeNumber)
{

                                                                                                      ;
                                                                                    ;
                                                                                ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTimes;
    repeatParams.modeNumber = modeNumber;
    VextractCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeatParams);
}
# 268 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Concat(LocalTensor<T> &concatLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &tmpLocal, const int32_t repeatTimes)
{

                                                                                     ;
                                                                           ;

    concatLocal = srcLocal;
# 286 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
}
# 297 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Extract(const LocalTensor<T> &dstValueLocal, const LocalTensor<uint32_t> &dstIndexLocal,
    const LocalTensor<T> &sortedLocal, const int32_t repeatTimes)
{

                                                                                     ;
                                                                            ;






    uint64_t rsvdCnt;
    if constexpr (IsSameType<T, __cce_half>::value) {
        constexpr uint8_t GATHER_MASK_PATTERN_3 = 3;
        constexpr uint8_t GATHER_MASK_PATTERN_2 = 2;
        GatherMaskCal((__attribute__((cce_unif_buff)) T *)dstValueLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)sortedLocal.GetPhyAddr(),
            GATHER_MASK_PATTERN_3, false, (uint32_t)0, { 1, (uint16_t)repeatTimes, DEFAULT_REPEAT_STRIDE, 0 }, rsvdCnt);
        PipeBarrier<PIPE_V>();
        GatherMaskCal((__attribute__((cce_unif_buff)) uint32_t *)dstIndexLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint32_t *)sortedLocal.GetPhyAddr(),
            GATHER_MASK_PATTERN_2, false, (uint32_t)0, { 1, (uint16_t)(repeatTimes * 2), 8, 0 }, rsvdCnt);
    } else {
        constexpr uint8_t GATHER_MASK_PATTERN_1 = 1;
        constexpr uint8_t GATHER_MASK_PATTERN_2 = 2;
        GatherMaskCal((__attribute__((cce_unif_buff)) T *)dstValueLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)sortedLocal.GetPhyAddr(),
            GATHER_MASK_PATTERN_1, false, (uint32_t)0, { 1, (uint16_t)repeatTimes, DEFAULT_REPEAT_STRIDE, 0 }, rsvdCnt);
        PipeBarrier<PIPE_V>();
        GatherMaskCal((__attribute__((cce_unif_buff)) uint32_t *)dstIndexLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint32_t *)sortedLocal.GetPhyAddr(),
            GATHER_MASK_PATTERN_2, false, (uint32_t)0, { 1, (uint16_t)repeatTimes, 8, 0 }, rsvdCnt);
    }
# 344 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
}
# 357 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T, bool isExhaustedSuspension>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort(const LocalTensor<T> &dstLocal, const MrgSortSrcList<T> &sortList,
    const uint16_t elementCountList[4], uint32_t sortedNum[4], uint16_t validBit, const int32_t repeatTimes)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


                                            ;
    MrgSort4Info mrgSortInfo(elementCountList, isExhaustedSuspension, validBit, (uint16_t)repeatTimes);

    MrgSort(dstLocal, sortList, mrgSortInfo);



    if (isExhaustedSuspension) {

        constexpr uint32_t validBitMask = 0xFFFF;
        constexpr uint32_t shiftBase = 16;







        auto res = get_vms4_sr();
        sortedNum[0] = res & validBitMask;
        sortedNum[1] = (res >> shiftBase) & validBitMask;
        sortedNum[2] = (res >> (2 * shiftBase)) & validBitMask;
        sortedNum[3] = (res >> (3 * shiftBase)) & validBitMask;
    }
}
# 402 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T, bool isFullSort>
[aicore] __inline__ __attribute__((always_inline)) void Sort(const LocalTensor<T> &dstLocal, const LocalTensor<T> &concatLocal,
    const LocalTensor<uint32_t> &indexLocal, LocalTensor<T> &tmpLocal, const int32_t repeatTimes)
{

                                                                             ;
                                                                         ;






    Sort32(dstLocal, concatLocal, indexLocal, repeatTimes);
# 446 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
    if constexpr (isFullSort) {
        PipeBarrier<PIPE_V>();
        DoFullSort(dstLocal, concatLocal, indexLocal, tmpLocal, repeatTimes);
    }
}

constexpr uint32_t halfSortedDataSize = 4;
constexpr uint32_t floatSortedDataSize = 2;






template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetSortOffset(const uint32_t elemOffset)
{


                          ;

    if constexpr (IsSameType<T, __cce_half>::value) {
        return elemOffset * halfSortedDataSize;
    } else {
        return elemOffset * floatSortedDataSize;
    }



}







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetSortLen(const uint32_t elemCount)
{


                          ;

    if constexpr (IsSameType<T, __cce_half>::value) {
        return elemCount * halfSortedDataSize;
    } else {
        return elemCount * floatSortedDataSize;
    }



}
#pragma end_pipe
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetMrgSortResult(
    uint16_t &mrgSortList1, uint16_t &mrgSortList2, uint16_t &mrgSortList3, uint16_t &mrgSortList4)
{

    if (g_coreType == AIC) {
        return;
    }

    GetMrgSortResultImpl(mrgSortList1, mrgSortList2, mrgSortList3, mrgSortList4);
}
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_bilinearinterpalation_intf.cppm" 1
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_bilinearinterpalation_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_bilinearinterpalation_impl.h" 1
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_bilinearinterpalation_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
constexpr uint32_t brcbEleNum = 8;

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolationCalc(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask, uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    auto sharedTmpBufferT = sharedTmpBuffer.ReinterpretCast<T>();
    GatherRepeatParams gatherbRepeatParams;
    uint8_t innerRepeatTimes = hRepeat * vRepeat;
    constexpr uint32_t eleCntOfOneRep = DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(T);

    GatherbImpl((__attribute__((cce_unif_buff)) uint16_t *)sharedTmpBufferT.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint16_t *)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)src0OffsetLocal.GetPhyAddr(), src0Local.GetSize(), innerRepeatTimes, gatherbRepeatParams);
    uint32_t posSrc1Brcb = hRepeat * vRepeat * DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(T);
    BrcbRepeatParams brcbRepeatParams;
    Brcb(sharedTmpBufferT[posSrc1Brcb], src1Local, src1Local.GetSize() / brcbEleNum, brcbRepeatParams);
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams mulRepeatParams;
    if (repeatMode == false) {
        mulRepeatParams.src0RepStride = 1;
        mulRepeatParams.src0BlkStride = 0;
    }

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, innerRepeatTimes * eleCntOfOneRep);
    Mul<T, false>(sharedTmpBufferT, sharedTmpBufferT[posSrc1Brcb], sharedTmpBufferT, mask, innerRepeatTimes,
        mulRepeatParams);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams addRepeatParams;
    addRepeatParams.dstRepStride = 0;
    addRepeatParams.src1RepStride = 0;
    for (int i = 0; i < vRepeat; i++) {
        if (hRepeat > 1) {
            Add(sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], sharedTmpBufferT[(i * hRepeat + 1) * eleCntOfOneRep],
                sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], mask, hRepeat - 1, addRepeatParams);
        }
    }
    PipeBarrier<PIPE_V>();

    UnaryRepeatParams addsRepeatParams;
    addsRepeatParams.srcRepStride = hRepeat * DEFAULT_REPEAT_STRIDE;
    addsRepeatParams.dstBlkStride = dstBlkStride;
    addsRepeatParams.dstRepStride = vROffset * sizeof(T) / ONE_BLK_SIZE;
    Adds(dstLocal, sharedTmpBufferT, (T)0, mask, vRepeat, addsRepeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolationCalc(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask[], uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    auto sharedTmpBufferT = sharedTmpBuffer.ReinterpretCast<T>();
    GatherRepeatParams gatherbRepeatParams;
    uint8_t innerRepeatTimes = hRepeat * vRepeat;
    constexpr uint32_t eleCntOfOneRep = DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(T);

    GatherbImpl((__attribute__((cce_unif_buff)) uint16_t *)sharedTmpBufferT.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint16_t *)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)src0OffsetLocal.GetPhyAddr(), src0Local.GetSize(), innerRepeatTimes, gatherbRepeatParams);
    uint32_t posSrc1Brcb = hRepeat * vRepeat * DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(T);
    BrcbRepeatParams brcbRepeatParams;
    Brcb(sharedTmpBufferT[posSrc1Brcb], src1Local, src1Local.GetSize() / brcbEleNum, brcbRepeatParams);
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams mulRepeatParams;
    if (repeatMode == false) {
        mulRepeatParams.src0RepStride = 1;
        mulRepeatParams.src0BlkStride = 0;
    }

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, innerRepeatTimes * eleCntOfOneRep);
    Mul<T, false>(sharedTmpBufferT, sharedTmpBufferT[posSrc1Brcb], sharedTmpBufferT, mask, innerRepeatTimes,
        mulRepeatParams);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams addRepeatParams;
    addRepeatParams.dstRepStride = 0;
    addRepeatParams.src1RepStride = 0;
    for (int i = 0; i < vRepeat; i++) {
        if (hRepeat > 1) {
            Add(sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], sharedTmpBufferT[(i * hRepeat + 1) * eleCntOfOneRep],
                sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], mask, hRepeat - 1, addRepeatParams);
        }
    }
    PipeBarrier<PIPE_V>();

    UnaryRepeatParams addsRepeatParams;
    addsRepeatParams.srcRepStride = hRepeat * DEFAULT_REPEAT_STRIDE;
    addsRepeatParams.dstBlkStride = dstBlkStride;
    addsRepeatParams.dstRepStride = vROffset * sizeof(T) / ONE_BLK_SIZE;
    Adds(dstLocal, sharedTmpBufferT, (T)0, mask, vRepeat, addsRepeatParams);
}
}
#pragma end_pipe
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_bilinearinterpalation_intf.cppm" 2






#pragma begin_pipe(V)
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolation(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask, uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_bilinearinterpalation_intf.cppm"
                                                                            ;
                                                                                             ;
    BilinearInterpolationCalc(dstLocal, src0Local, src0OffsetLocal, src1Local, mask, hRepeat,
        repeatMode, dstBlkStride, vROffset, vRepeat, sharedTmpBuffer);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolation(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask[], uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_bilinearinterpalation_intf.cppm"
                                                                            ;
                                                                                             ;
    BilinearInterpolationCalc(dstLocal, src0Local, src0OffsetLocal, src1Local, mask, hRepeat,
        repeatMode, dstBlkStride, vROffset, vRepeat, sharedTmpBuffer);
}
}
#pragma end_pipe
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_createvecindex_intf.cppm" 1
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_createvecindex_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_createvecindex_impl.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_createvecindex_impl.h"
namespace AscendC {
constexpr int32_t maskBitNum = 64;

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateVecIndexOneBlk(const LocalTensor<T> &dstLocal, const T &firstValue, uint32_t calCount)
{
    for (int32_t i = 0; i < static_cast<int32_t>(calCount); i++) {
        dstLocal.SetValue(i, static_cast<float>(firstValue) + static_cast<float>(i));
    }
    auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateVecIndexOneRep(const LocalTensor<T> &dstLocal, const T &firstValue, uint64_t mask[],
    uint16_t dstBlkStride)
{
    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));
    if constexpr (sizeof(T) == sizeof(__cce_half)) {
        for (int i = 0; i < 2; i++) {
            uint64_t maskValue = 1;
            for (int j = 0; j < maskBitNum; j++) {
                if (mask[i] & maskValue) {
                    uint32_t index = i * maskBitNum + j;
                    uint32_t blkIndex = index / eleCntOfOneBlk;
                    uint32_t eleIndex = blkIndex * eleCntOfOneBlk * dstBlkStride + index % eleCntOfOneBlk;
                    dstLocal.SetValue(eleIndex, (float)firstValue + (float)(i * maskBitNum + j));
                }
                maskValue <<= 1;
            }
        }
    } else {
        uint64_t maskValue = 1;
        for (int j = 0; j < maskBitNum; j++) {
            if (mask[0] & maskValue) {
                uint32_t blkIndex = j / eleCntOfOneBlk;
                uint32_t eleIndex = blkIndex * eleCntOfOneBlk * dstBlkStride + j % eleCntOfOneBlk;
                dstLocal.SetValue(eleIndex, (float)firstValue + (float)j);
            }
            maskValue <<= 1;
        }
    }
    auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateVecIndexCalc(LocalTensor<T> &dstLocal, const T &firstValue, uint64_t mask,
    uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride)
{

    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    if (mask < eleCntOfOneBlk) {
        CreateVecIndexOneBlk(dstLocal, firstValue, mask);
    } else {
        CreateVecIndexOneBlk(dstLocal, firstValue, eleCntOfOneBlk);
    }
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));
    UnaryRepeatParams addsParams;

    int32_t loopN = mask / eleCntOfOneBlk - 1;
    int32_t tailSize = mask % eleCntOfOneBlk;
    int32_t blkEleStride = dstBlkStride * eleCntOfOneBlk;
    int32_t repEleStride = dstRepStride * eleCntOfOneBlk;
    for (int i = 0; i < loopN; i++) {
        Adds(dstLocal[(i + 1) * blkEleStride], dstLocal[i * blkEleStride], (T)(eleCntOfOneBlk), eleCntOfOneBlk, 1,
            addsParams);
        PipeBarrier<PIPE_V>();
    }
    addsParams.dstBlkStride = dstBlkStride;
    addsParams.srcBlkStride = dstBlkStride;
    int32_t offsetTailDst = mask / eleCntOfOneBlk * eleCntOfOneBlk * dstBlkStride;
    int32_t offsetTailSrc = offsetTailDst - eleCntOfOneBlk * dstBlkStride;
    if (tailSize > 0) {
        Adds(dstLocal[offsetTailDst], dstLocal[offsetTailSrc], (T)eleCntOfOneBlk, tailSize, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }


    for (int i = 0; i < repeatTimes - 1; i++) {
        Adds(dstLocal[(i + 1) * repEleStride], dstLocal[i * repEleStride], (T)(eleCntOfOneRep), mask, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateVecIndexCalc(LocalTensor<T> &dstLocal, const T &firstValue, uint64_t mask[],
    uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride)
{

    CreateVecIndexOneRep(dstLocal, firstValue, mask, dstBlkStride);

    UnaryRepeatParams addsParams;
    addsParams.dstBlkStride = dstBlkStride;
    addsParams.srcBlkStride = dstBlkStride;
    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));
    int32_t blkEleStride = dstBlkStride * eleCntOfOneBlk;
    int32_t repEleStride = dstRepStride * eleCntOfOneBlk;
    for (int i = 0; i < repeatTimes - 1; i++) {
        Adds(dstLocal[(i + 1) * repEleStride], dstLocal[i * repEleStride], (T)(eleCntOfOneRep), mask, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateVecIndexCalc(LocalTensor<T> dstLocal, const T &firstValue, uint32_t calCount)
{

    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    if (calCount <= eleCntOfOneBlk) {
        CreateVecIndexOneBlk(dstLocal, firstValue, (uint32_t)eleCntOfOneBlk);
        return;
    }
    CreateVecIndexOneBlk(dstLocal, firstValue, (uint32_t)eleCntOfOneBlk);

    UnaryRepeatParams addsParams;
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));

    int32_t loopN = 0, tailSize = 0, offsetTailDst, offsetTailSrc;
    if (calCount >= eleCntOfOneRep) {
        loopN = DEFAULT_REPEAT_STRIDE - 1;
    } else {
        loopN = calCount / eleCntOfOneBlk - 1;
        tailSize = calCount % eleCntOfOneBlk;
    }
    for (int i = 0; i < loopN; i++) {
        Adds(dstLocal[(i + 1) * eleCntOfOneBlk], dstLocal[i * eleCntOfOneBlk], (T)eleCntOfOneBlk, eleCntOfOneBlk, 1,
            addsParams);
        PipeBarrier<PIPE_V>();
    }
    offsetTailDst = calCount / eleCntOfOneBlk * eleCntOfOneBlk;
    offsetTailSrc = offsetTailDst - eleCntOfOneBlk;
    if (tailSize > 0) {
        Adds(dstLocal[offsetTailDst], dstLocal[offsetTailSrc], (T)eleCntOfOneBlk, tailSize, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }
    if (calCount <= eleCntOfOneRep) {
        return;
    }

    loopN = calCount / eleCntOfOneRep - 1;
    tailSize = calCount % eleCntOfOneRep;
    for (int i = 0; i < loopN; i++) {
        Adds(dstLocal[(i + 1) * eleCntOfOneRep], dstLocal[i * eleCntOfOneRep], (T)(eleCntOfOneRep), eleCntOfOneRep, 1,
            addsParams);
        PipeBarrier<PIPE_V>();
    }
    offsetTailDst = calCount / eleCntOfOneRep * eleCntOfOneRep;
    offsetTailSrc = offsetTailDst - eleCntOfOneRep;
    if (tailSize > 0) {
        Adds(dstLocal[offsetTailDst], dstLocal[offsetTailSrc], (T)(eleCntOfOneRep), tailSize, 1, addsParams);
    }
}
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_createvecindex_intf.cppm" 2






namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dstLocal, const T &firstValue,
    uint64_t mask, uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride)
{

                                                                                                                     ;





    CreateVecIndexCalc(dstLocal, firstValue, mask, repeatTimes, dstBlkStride, dstRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dstLocal, const T &firstValue,
    uint64_t mask[], uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride)
{

                                                                                                                     ;





    CreateVecIndexCalc(dstLocal, firstValue, mask, repeatTimes, dstBlkStride, dstRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> dstLocal, const T &firstValue,
    uint32_t calCount)
{

                                                                                                                     ;





    CreateVecIndexCalc(dstLocal, firstValue, calCount);
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_mulcast_intf.cppm" 1
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_mulcast_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_mulcast_impl.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_mulcast_impl.h"
#pragma begin_pipe(V)
namespace AscendC {

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCastIntrinsicsImpl(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams)
{
    if constexpr (IsSameType<T, int8_t>::value) {
        vmulconv_f162s8((__attribute__((cce_unif_buff)) int8_t *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) __cce_half *)src0Local.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) __cce_half *)src1Local.GetPhyAddr(), repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride);
    } else {
        vmulconv_f162u8((__attribute__((cce_unif_buff)) uint8_t *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) __cce_half *)src0Local.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) __cce_half *)src1Local.GetPhyAddr(), repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride);
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCastCalc(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<U>(mask);
        MulCastIntrinsicsImpl(dstLocal, src0Local, src1Local, repeatTimes, repeatParams);
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCastCalc(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        MulCastIntrinsicsImpl(dstLocal, src0Local, src1Local, repeatTimes, repeatParams);
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCastCalc(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        BinaryRepeatParams repeatParams;
        repeatParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
        set_mask_count();
        set_vector_mask(0, calCount);
        MulCastIntrinsicsImpl(dstLocal, src0Local, src1Local, 1, repeatParams);
        set_mask_norm();
        ResetMask();
    }
}
}
#pragma end_pipe
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_mulcast_intf.cppm" 2






#pragma begin_pipe(V)
namespace AscendC {
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams)
{

                                                                                                                  ;





    MulCastCalc(dstLocal, src0Local, src1Local, mask, repeatTimes, repeatParams);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams &repeatParams)
{

                                                                                                                  ;





    MulCastCalc(dstLocal, src0Local, src1Local, mask, repeatTimes, repeatParams);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint32_t calCount)
{

                                                                                                                  ;





    MulCastCalc(dstLocal, src0Local, src1Local, calCount);
}
}
#pragma end_pipe
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_determine_compute_sync_intf.cppm" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_determine_compute_sync_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_determine_compute_sync_impl.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_determine_compute_sync_intf.cppm" 2




namespace AscendC {





[aicore] __inline__ __attribute__((always_inline)) void InitDetermineComputeWorkspace(GlobalTensor<int32_t> &gmWorkspace,
    LocalTensor<int32_t> &ubWorkspace)
{
    InitDetermineComputeWorkspaceCalc(gmWorkspace, ubWorkspace);
}





[aicore] __inline__ __attribute__((always_inline)) void WaitPreBlock(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    WaitPreBlockCalc(gmWorkspace, ubWorkspace);
}






[aicore] __inline__ __attribute__((always_inline)) void NotifyNextBlock(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    NotifyNextBlockCalc(gmWorkspace, ubWorkspace);
}
}
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm"
namespace AscendC {
#pragma begin_pipe(V)
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm"
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void Transpose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal)
{


                                                       ;





    TransposeImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr());
}
# 75 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HD(const LocalTensor<T> (&dstLocalList)[NCHW_CONV_ADDR_LIST_SIZE],
    const LocalTensor<T> (&srcLocalList)[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams)
{


                                                                                                  ;





    __attribute__((cce_unif_buff)) T* dstList[NCHW_CONV_ADDR_LIST_SIZE];
    __attribute__((cce_unif_buff)) T* srcList[NCHW_CONV_ADDR_LIST_SIZE];

    for (int32_t i = 0; i < NCHW_CONV_ADDR_LIST_SIZE; i++) {
        dstList[i] = (__attribute__((cce_unif_buff)) T*)dstLocalList[i].GetPhyAddr();
        srcList[i] = (__attribute__((cce_unif_buff)) T*)srcLocalList[i].GetPhyAddr();
    }

    TransDataTo5HDImpl(dstList, srcList, nchwconvParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HD(uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams)
{


                                                                                                     ;
# 119 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm"
    TransDataTo5HDImpl<T>(dstList, srcList, nchwconvParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Transpose(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const TransposeParamsExt &transposeParams)
{





    if ((transposeParams.transposeType == TransposeType::TRANSPOSE_ND2ND_B16) &&
        (transposeParams.hSize == NCHW_CONV_ADDR_LIST_SIZE) && (transposeParams.wSize == NCHW_CONV_ADDR_LIST_SIZE)) {


                                                                                     ;
        TransposeImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr());
    } else if (transposeParams.transposeType == TransposeType::TRANSPOSE_NCHW2NHWC ||
        transposeParams.transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {



                                                                 ;
        if (transposeParams.cSize == 1) {
            struct DataCopyParams repeatParams;
            repeatParams.blockLen = transposeParams.nSize * transposeParams.cSize * transposeParams.hSize *
                transposeParams.wSize / AscendCUtils::GetC0Count(sizeof(T));
            DataCopyUB2UBImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(), repeatParams);
        } else {
# 157 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm"
            Transpose4DImpl(dstLocal, srcLocal, sharedTmpBuffer, transposeParams);
        }
    }
}
#pragma end_pipe
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void TransDataTo5HD(const LocalTensor<uint64_t> &dstLocal,
    const LocalTensor<uint64_t> &srcLocal, const TransDataTo5HDParams &nchwconvParams)
{


                                                                                                  ;







    constexpr uint32_t vaRegSize = VA_REG_ARRAY_LEN / HALF_FACTOR;
    constexpr uint32_t vaOne = 1;
    constexpr uint32_t vaTwo = 2;
    constexpr uint32_t vaThree = 3;
    constexpr uint64_t vaAddr = 5;
    constexpr uint64_t vaMask = 0x1fff;
    constexpr uint64_t vaBit1 = 16;
    constexpr uint64_t vaBit2 = 32;
    constexpr uint64_t vaBit3 = 48;

    for (uint32_t i = 0; i < vaRegSize; i++)
    {
        uint64_t dstAddrConfig = (((dstLocal.GetValue(vaRegSize * i) >> vaAddr) & vaMask) |
                                  (((dstLocal.GetValue(vaRegSize * i + vaOne) >> vaAddr) & vaMask) << vaBit1) |
                                  (((dstLocal.GetValue(vaRegSize * i + vaTwo) >> vaAddr) & vaMask) << vaBit2) |
                                  (((dstLocal.GetValue(vaRegSize * i + vaThree) >> vaAddr) & vaMask) << vaBit3));
        dstLocal.SetValue(i, dstAddrConfig);

        uint64_t srcAddrConfig = (((srcLocal.GetValue(vaRegSize * i) >> vaAddr) & vaMask) |
                                  (((srcLocal.GetValue(vaRegSize * i + vaOne) >> vaAddr) & vaMask) << vaBit1) |
                                  (((srcLocal.GetValue(vaRegSize * i + vaTwo) >> vaAddr) & vaMask) << vaBit2) |
                                  (((srcLocal.GetValue(vaRegSize * i + vaThree) >> vaAddr) & vaMask) << vaBit3));
        srcLocal.SetValue(i, srcAddrConfig);
    }

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    TransDataTo5HDVldVaRegImpl<T>(
        (__attribute__((cce_unif_buff)) uint64_t*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint64_t*)srcLocal.GetPhyAddr(), nchwconvParams);

}
}
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gatherb(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<uint32_t>& offsetLocal, const uint8_t repeatTimes, const GatherRepeatParams& repeatParams)
{





    uint32_t srcLength = src0Local.GetSize();
    GatherbImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t*)offsetLocal.GetPhyAddr(), srcLength, repeatTimes, repeatParams);
}
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint64_t mask,
    const uint8_t repeatTimes, const uint16_t dstRepStride)
{







                                                                                           ;
# 99 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
    const uint32_t srcLength = srcLocal.GetSize();
    GatherImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)srcOffsetLocal.GetPhyAddr(), srcLength, srcBaseAddr, mask, repeatTimes, dstRepStride);
}
# 115 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint64_t mask[],
    const uint8_t repeatTimes, const uint16_t dstRepStride)
{







                                                                                           ;
# 138 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
    const uint32_t srcLength = srcLocal.GetSize();
    GatherImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)srcOffsetLocal.GetPhyAddr(), srcLength, srcBaseAddr, mask, repeatTimes, dstRepStride);
}
# 152 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint32_t count)
{
# 168 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
    uint32_t elementCountSingleRepeat;
    if constexpr (sizeof(T) == sizeof(uint16_t)) {
        elementCountSingleRepeat = 128;
    } else {
        elementCountSingleRepeat = 64;
    }
    const uint32_t elementCountTail = count % elementCountSingleRepeat;
    const uint8_t repeatTimes = count / elementCountSingleRepeat;
    if (repeatTimes > 0) {
        Gather(dstLocal, srcLocal, srcOffsetLocal, srcBaseAddr, (uint64_t)elementCountSingleRepeat, repeatTimes,
            DEFAULT_REPEAT_STRIDE);
    }
    if (elementCountTail > 0) {
        const uint32_t offset = count - elementCountTail;
        Gather(dstLocal[offset], srcLocal, srcOffsetLocal[offset], srcBaseAddr, (uint64_t)elementCountTail, 1,
            DEFAULT_REPEAT_STRIDE);
    }

}
}
#pragma end_pipe
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint64_t mask,
    const uint8_t repeatTimes, const uint8_t srcRepStride)
{







                                                                              ;






    const uint32_t dstLength = dstLocal.GetSize();
    ScatterImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)dstOffsetLocal.GetPhyAddr(), dstLength, dstBaseAddr, mask, repeatTimes, srcRepStride);
}
# 84 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint64_t mask[],
    const uint8_t repeatTimes, const uint8_t srcRepStride)
{







                                                                              ;






    const uint32_t dstLength = dstLocal.GetSize();
    ScatterImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)dstOffsetLocal.GetPhyAddr(), dstLength, dstBaseAddr, mask, repeatTimes, srcRepStride);
}
# 117 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint32_t count)
{







                                                                              ;
# 139 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm"
    uint32_t elementCountSingleRepeat;
    if constexpr (sizeof(T) == sizeof(uint16_t)) {
        elementCountSingleRepeat = 128;
    } else {
        elementCountSingleRepeat = 64;
    }
    const uint32_t elementCountTail = count % elementCountSingleRepeat;
    const uint8_t repeatTimes = count / elementCountSingleRepeat;
    if (repeatTimes > 0) {
        Scatter(dstLocal, srcLocal, dstOffsetLocal, dstBaseAddr, (uint64_t)elementCountSingleRepeat, repeatTimes,
            DEFAULT_REPEAT_STRIDE);
    }
    if (elementCountTail > 0) {
        const uint32_t offset = count - elementCountTail;
        Scatter(dstLocal, srcLocal[offset], dstOffsetLocal[offset], dstBaseAddr, (uint64_t)elementCountTail, 1,
            DEFAULT_REPEAT_STRIDE);
    }

}
}
#pragma end_pipe
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_brcb_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_brcb_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_brcb_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Brcb(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local, const uint8_t repeatTimes,
    const BrcbRepeatParams& repeatParams)
{





    BrcbImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), repeatTimes, repeatParams);
}
}
#pragma end_pipe
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 95 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    AddImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 127 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    SubImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    SubImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 166 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    SubImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 198 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 237 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    MulImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 269 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    DivImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    DivImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 308 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    DivImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 340 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
    const LocalTensor<U>& src1Local, const uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;






    MulAddDstImpl<PrimDstType, PrimSrcType, isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
    const LocalTensor<U>& src1Local, uint64_t mask, const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;






    MulAddDstImpl<PrimDstType, PrimSrcType, isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}
# 384 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
    const LocalTensor<U>& src1Local, const int32_t& calCount)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;





    MulAddDstImpl((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), calCount);
}
# 417 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 456 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    MaxImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 488 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 527 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    MinImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 559 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AndImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AndImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 598 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    AndImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 630 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    OrImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    OrImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 669 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    OrImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 701 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    AddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    AddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    AddReluImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 781 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<int32_t>& src0Local,
    const LocalTensor<int32_t>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) __cce_half*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) int32_t*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
    const LocalTensor<U>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
    static_assert((IsSameType<PrimDstType, __cce_half>::value && IsSameType<PrimSrcType, int32_t>::value) &&
        "Failed to check dtype in AddDeqRelu, current api support dtype combination is src: int32_t, dst: half.");






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<__cce_half> &dstLocal, const LocalTensor<int32_t> &src0Local,
    const LocalTensor<int32_t> &src1Local, uint64_t mask, const uint8_t repeatTimes,
    const BinaryRepeatParams &repeatParams)
{






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) __cce_half*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) int32_t*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask, const uint8_t repeatTimes,
    const BinaryRepeatParams &repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
    static_assert((IsSameType<PrimDstType, __cce_half>::value && IsSameType<PrimSrcType, int32_t>::value) &&
        "Failed to check dtype in AddDeqRelu, current api support dtype combination is src: int32_t, dst: half.");






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}
# 859 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<__cce_half>& dstLocal, const LocalTensor<int32_t>& src0Local,
    const LocalTensor<int32_t>& src1Local, const int32_t& calCount)
{





    AddDeqReluImpl((__attribute__((cce_unif_buff)) __cce_half *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) int32_t *)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t *)src1Local.GetPhyAddr(), calCount);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
    const LocalTensor<U>& src1Local, const int32_t& calCount)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
    static_assert((IsSameType<PrimDstType, __cce_half>::value && IsSameType<PrimSrcType, int32_t>::value) &&
        "Failed to check dtype in AddDeqRelu, current api support dtype combination is src: int32_t, dst: half.");





    AddDeqReluImpl((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), calCount);
}
# 906 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}
# 957 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    FusedMulAddImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 994 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}
# 1045 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    FusedMulAddReluImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 1082 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    SubReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    SubReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    SubReluImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
}
#pragma end_pipe
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 54 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    AddsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType< PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    AddsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType< PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}
# 120 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    AddsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType< PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    AddsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, calCount);
}
# 165 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType< PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}
# 231 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType< PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    MulsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, calCount);
}
# 276 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MaxsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MaxsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}
# 342 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    MaxsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    MaxsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, calCount);
}
# 387 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MinsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MinsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}
# 453 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    MinsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    MinsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, calCount);
}
# 498 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    ShiftLeftImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type >
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    ShiftLeftImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    ShiftLeftImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    ShiftLeftImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams);
}
# 564 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    ShiftLeftImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    ShiftLeftImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, calCount);
}
# 609 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn)
{






    ShiftRightImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams, roundEn);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn)
{
    using PrimType = PrimT<T>;






    ShiftRightImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams, roundEn);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn)
{






    ShiftRightImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams, roundEn);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn)
{
    using PrimType = PrimT<T>;






    ShiftRightImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams, roundEn);
}
# 675 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    ShiftRightImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    ShiftRightImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, calCount);
}
# 720 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    LeakyReluImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams);
}

template < typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type >
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LeakyReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    LeakyReluImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LeakyReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams);
}
# 786 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    LeakyReluImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template < typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type >
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    LeakyReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, calCount);
}
}
#pragma end_pipe
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm" 1
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{







                                                            ;







    VcmpvImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), cmpMode, mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{







                                                            ;







    VcmpvImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), cmpMode, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, CMPMODE cmpMode,
    const uint64_t mask[], const BinaryRepeatParams& repeatParams)
{


                                                                         ;







    VcmpImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), cmpMode, mask, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, CMPMODE cmpMode,
    const uint64_t mask, const BinaryRepeatParams& repeatParams)
{


                                                                         ;







    VcmpImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), cmpMode, mask,
        repeatParams);
}
# 152 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, uint32_t calCount)
{







                                                            ;






    VcmpvImpl((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), cmpMode, calCount);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetCmpMask(const LocalTensor<T>& dst)
{

    if (g_coreType == AIC) {
        return;
    }






    GetCmpMaskImpl((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr());
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetCmpMask(const LocalTensor<T>& src)
{

    if (g_coreType == AIC) {
        return;
    }






    SetCmpMaskImpl((__attribute__((cce_unif_buff)) T*)src.GetPhyAddr());

}
# 224 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{







                                                         ;







    VcmpvsImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), src1Scalar,
        cmpMode, mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{







                                                         ;







    VcmpvsImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), src1Scalar,
        cmpMode, mask, repeatTimes, repeatParams);
}
# 281 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, uint32_t calCount)
{







                                                         ;






    VcmpvsImpl((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), src1Scalar, cmpMode, calCount);
}
# 327 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint64_t mask[],
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{






    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), selMode, mask, repeatTimes, repeatParams);
}


template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint64_t mask,
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{






    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), selMode, mask, repeatTimes, repeatParams);
}
# 369 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint32_t calCount)
{





    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), selMode, calCount);
}
# 401 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint64_t mask[], uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{






    CheckTensorPos<U>(selMask, Hardware::UB, "selMask", "VECIN / VECCALC / VECOUT", "Select");
    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        src1Local, selMode, mask, repeatTimes, repeatParams);
}


template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint64_t mask, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{






    CheckTensorPos<U>(selMask, Hardware::UB, "selMask", "VECIN / VECCALC / VECOUT", "Select");
    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        src1Local, selMode, mask, repeatTimes, repeatParams);
}
# 445 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint32_t calCount)
{





    CheckTensorPos<U>(selMask, Hardware::UB, "selMask", "VECIN / VECCALC / VECOUT", "Select");
    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        src1Local, selMode, calCount);
}
}
#pragma end_pipe
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_duplicate_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_duplicate_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_duplicate_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, uint64_t mask,
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    CheckDuplicateSupportedType<T>();






    DuplicateImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, dstBlockStride,
        dstRepeatStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, uint64_t mask[],
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    CheckDuplicateSupportedType<T>();






    DuplicateImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, dstBlockStride,
        dstRepeatStride);
}
# 88 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_duplicate_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, const int32_t& calCount)
{
    CheckDuplicateSupportedType<T>();





    DuplicateImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), scalarValue, calCount);
}
}
#pragma end_pipe
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
namespace AscendC {
#pragma begin_pipe(V)
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceMaxImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}
# 111 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceMinImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}
# 141 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                    ;
                                                                        ;







    PairReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceMaxImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceMinImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                    ;
                                                                        ;







    PairReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void RepeatReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t mask, const int32_t dstBlkStride, const int32_t srcBlkStride,
    const int32_t dstRepStride, const int32_t srcRepStride)
{

                                                                                                      ;
                                                                          ;







    RepeatReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstBlkStride, srcBlkStride, dstRepStride, srcRepStride);
}
# 267 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                                   ;







    WholeReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), mask,
        repeatTimes, dstRepStride, srcBlkStride, srcRepStride);
}
# 297 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{

                                                                                                     ;
                                                                                   ;

                                                                          ;
# 324 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
    WholeReduceMaxImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), mask,
        repeatTimes, dstRepStride, srcBlkStride, srcRepStride, order);
}
# 339 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{

                                                                                                     ;
                                                                                   ;

                                                                          ;
# 366 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
    WholeReduceMinImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, dstRepStride, srcBlkStride, srcRepStride, order);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                                   ;







    WholeReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{

                                                                                                     ;
                                                                                   ;

                                                                          ;
# 416 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
    WholeReduceMaxImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, dstRepStride, srcBlkStride, srcRepStride, order);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{

                                                                                                     ;
                                                                                   ;

                                                                          ;
# 446 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
    WholeReduceMinImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, dstRepStride, srcBlkStride, srcRepStride, order);
}
# 462 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex)
{






                                                                                     ;
    ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MAX);
}
# 491 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex)
{






                                                                                     ;
    struct ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MIN);
}
# 519 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride)
{






                                                                                     ;
    ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, 0, ReduceMode::REDUCE_SUM);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex)
{






                                                                                     ;
    struct ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MAX);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex)
{






                                                                                     ;
    struct ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MIN);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride)
{






                                                                                     ;
    struct ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, 0, ReduceMode::REDUCE_SUM);
}
# 598 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count, bool calIndex)
{

                                                                                     ;
                                                                                        ;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t repeatTimes = count / elementNumPerRep;
    int32_t tailCount = count % elementNumPerRep;
    int32_t bodyCount = elementNumPerRep;

    if (repeatTimes == 0) {
        repeatTimes = 1;
        bodyCount = count;
        tailCount = 0;
    }





    struct ReduceRepeatParams params(bodyCount, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);
    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MIN);

    if (tailCount != 0) {
        ReduceTailCompute(dstLocal, srcLocal, workLocal, count, calIndex, ReduceMode::REDUCE_MIN);
    }
}
# 639 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count, bool calIndex)
{

                                                                                     ;
                                                                                        ;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t repeatTimes = count / elementNumPerRep;
    int32_t tailCount = count % elementNumPerRep;
    int32_t bodyCount = elementNumPerRep;

    if (repeatTimes == 0) {
        repeatTimes = 1;
        bodyCount = count;
        tailCount = 0;
    }






    struct ReduceRepeatParams params(bodyCount, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);
    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MAX);

    if (tailCount != 0) {
        ReduceTailCompute(dstLocal, srcLocal, workLocal, count, calIndex, ReduceMode::REDUCE_MAX);
    }
}
# 680 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count)
{
                                                                                        ;


                                                                                     ;
    ReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), count);
# 741 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
}
#pragma end_pipe
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCount(uint32_t &maxMinValue, uint32_t &maxMinIndex)
{

    if (g_coreType == AIC) {
        return;
    }

    GetReduceMaxMinCountImpl<T>(maxMinValue, maxMinIndex);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCount(uint32_t &maxMinValue)
{
    GetReduceMaxMinCountImpl<T>(maxMinValue);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetAccVal()
{

    if (g_coreType == AIC) {
        return 0;
    }

    return get_acc_val();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue, T &maxMinIndex)
{


                   ;

    if (g_coreType == AIC) {
        return;
    }

    GetReduceMaxMinCountImpl<T>(maxMinValue, maxMinIndex);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue)
{

                                                                                                      ;
    GetReduceMaxMinCountImpl<T>(maxMinValue);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) T GetAccVal()
{

                                                                   ;

    if (g_coreType == AIC) {
        return 0;
    }

    return GetAccValImpl<T>();
}
}
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_mask_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_mask_intf.cppm"
namespace AscendC {
#pragma begin_pipe(V)
template <typename T, typename U, GatherMaskMode mode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<U>& src1Pattern, const bool reduceMode, const uint32_t mask,
    const GatherMaskParams& gatherMaskParams, uint64_t& rsvdCnt)
{
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_mask_intf.cppm"
    GatherMaskCal<T, mode>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) U*)src1Pattern.GetPhyAddr(), reduceMode, mask, gatherMaskParams, rsvdCnt);

}

template <typename T, GatherMaskMode mode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const uint8_t src1Pattern, const bool reduceMode, const uint32_t mask, const GatherMaskParams& gatherMaskParams,
    uint64_t& rsvdCnt)
{
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_mask_intf.cppm"
    GatherMaskCal<T, mode>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), src1Pattern,
        reduceMode, mask, gatherMaskParams, rsvdCnt);

}

template <typename T>
[[deprecated("NOTICE: This GatheMask in this form has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const uint8_t patternMode, const GatherMaskParams& gatherMaskParams)
{
    GatherMaskImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), patternMode, gatherMaskParams);
}

template <typename T>
[[deprecated("NOTICE: This GatheMask in this form has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const uint8_t patternMode, const GatherMaskParams& gatherMaskParams)
{
    GatherMaskImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), patternMode,
        gatherMaskParams);
}
#pragma end_pipe

[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) int64_t GetGatherMaskRemainCount()
{

    if (g_coreType == AIC) {
        return 0;
    }

    return GetGatherMaskRemainCountImpl();
}
}
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_ternary_scalar_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_ternary_scalar_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_ternary_scalar_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    AxpyImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    AxpyImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}
# 88 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_ternary_scalar_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{





    AxpyImpl<T, U>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)srcLocal.GetPhyAddr(), scalarValue, calCount);
}
}
#pragma end_pipe
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    ReluImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 119 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ExpImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ExpImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 157 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    ExpImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 182 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LnImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LnImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 218 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    LnImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 243 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    AbsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    AbsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 281 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    AbsImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 306 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReciprocalImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReciprocalImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 344 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    ReciprocalImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 370 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    RsqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    RsqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 408 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    RsqrtImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 433 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    SqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    SqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 471 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    SqrtImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 496 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    NotImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    NotImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 534 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    NotImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
}
#pragma end_pipe
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm" 1
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
namespace AscendC {
#pragma begin_pipe(V)
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint64_t mask[], const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    CastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(), round_mode,
        mask, repeatTimes, repeatParams);
}


template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
# 98 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    CastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(), round_mode,
        mask, repeatTimes, repeatParams);
}
# 110 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint32_t calCount)
{
# 125 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    if constexpr (!(IsSameType<T1, int4b_t>::value) && !(IsSameType<T2, int4b_t>::value)) {



                                                                  ;
    }
    CastImpl((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(), round_mode, calCount);
}
# 145 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
# 160 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    CastDeqImpl<T1, T2, isSetMask, isVecDeq, halfBlock>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
template <typename T1, typename T2, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const int32_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
# 177 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    CastDeqImpl<T1, T2, isSetMask, isVecDeq, halfBlock>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 192 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const uint32_t calCount)
{
# 205 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    CastDeqImpl<T1, T2, isVecDeq, halfBlock>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(),
        calCount);
}
# 228 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{

    if (g_coreType == AIC) {
        return;
    }







    AddReluCastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}


template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{

    if (g_coreType == AIC) {
        return;
    }







    AddReluCastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 276 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, const uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }






    AddReluCastImpl((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), calCount);
}
# 313 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{

    if (g_coreType == AIC) {
        return;
    }







    SubReluCastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}


template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{

    if (g_coreType == AIC) {
        return;
    }







    SubReluCastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 361 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, const uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }






    SubReluCastImpl((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), calCount);
}

#pragma end_pipe
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(__cce_half scale)
{

    if (g_coreType == AIC) {
        return;
    }




    SetDeqScaleImpl(scale);
}

[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(float scale, int16_t offset, bool signMode)
{

    if (g_coreType == AIC) {
        return;
    }




    SetDeqScaleImpl(scale, offset, signMode);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(const LocalTensor<T>& vdeqTensor, const VdeqInfo& vdeqInfo)
{

    if (g_coreType == AIC) {
        return;
    }
# 421 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    SetDeqScaleImpl<T>(vdeqTensor, vdeqInfo);
}

template <bool castMode>
[aicore] __inline__ __attribute__((always_inline)) void SetCastOverflowMode()
{
    SetCastOverflowModeImpl<castMode>();
}
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vpadding_intf.cppm" 1
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vpadding_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_vpadding_impl.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_vpadding_impl.h"
namespace AscendC {
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void VectorPaddingImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t padMode, bool padSide,
    const uint64_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
                                                      ;
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void VectorPaddingImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t padMode, bool padSide,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
                                                      ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VectorPaddingImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t padMode, bool padSide,
    const uint32_t calCount)
{
                                                      ;
}
}
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vpadding_intf.cppm" 2






#pragma begin_pipe(V)
namespace AscendC {
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint64_t mask, const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{






    VectorPaddingImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(), padMode,
        padSide, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint64_t mask[], const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{






    VectorPaddingImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(), padMode,
        padSide, mask, repeatTimes, repeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint32_t calCount)
{





    VectorPaddingImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(), padMode, padSide,
        calCount);
}
}
#pragma end_pipe
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_scalar_intf.cppm" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_scalar_intf.cppm"
namespace AscendC {
template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetCountOfValue(uint64_t valueIn)
{
    return ScalarGetCountOfValueImpl<countValue>(valueIn);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarCountLeadingZero(uint64_t valueIn)
{
    return ScalarCountLeadingZeroImpl(valueIn);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t CountBitsCntSameAsSignBit(int64_t valueIn)
{
    return CountBitsCntSameAsSignBitImpl(valueIn);
}

template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetSFFValue(uint64_t valueIn)
{
    return ScalarGetSFFValueImpl<countValue>(valueIn);
}

template <typename srcT, typename dstT, RoundMode roundMode>
[aicore] __inline__ __attribute__((always_inline)) dstT ScalarCast(srcT valueIn)
{
    return ScalarCastImpl<srcT, dstT, roundMode>(valueIn);
}
}
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_sys_var_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_sys_var_intf.cppm"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void GetArchVersion(uint32_t& coreVersion)
{
    GetArchVersionImpl(coreVersion);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockNum()
{
    return GetSubBlockNumImpl();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetProgramCounter()
{
    return GetProgramCounterImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void Trap()
{
    TrapImpl();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSystemCycle()
{
    return GetSystemCycleImpl();
}
}
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_set_atomic_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_set_atomic_intf.cppm"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicType()
{
    SetAtomicTypeImpl<T>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicAdd()
{
    SetAtomicAddImpl<T>();
}

[aicore] __inline__ __attribute__((always_inline)) void SetAtomicNone()
{
    SetAtomicNoneImpl();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMax()
{
    SetAtomicMaxImpl<T>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMin()
{
    SetAtomicMinImpl<T>();
}
}
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_prof_trace_intf.cppm" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_prof_trace_intf.cppm"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void MetricsProfStart()
{
    ProfStartImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void MetricsProfStop()
{
    ProfStopImpl();
}
}
# 54 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 2
# 2 "/root/yjw/HANS/HANS-NPU/hans_merge.cpp" 2






using namespace AscendC;

constexpr uint32_t DATA_BLOCK_BYTE_NUM = 4096;
constexpr int32_t BUFFER_NUM = 2;
constexpr int32_t BLOCK_NUM = 256;
constexpr uint32_t HISTOGRAM_BINS = 256;
constexpr uint32_t TILE_LEN = 16;
constexpr uint32_t TILE_NUM = DATA_BLOCK_BYTE_NUM / sizeof(uint32_t) / TILE_LEN;




template<typename T>
class PrefixKernel {
public:
    [aicore] __inline__ __attribute__((always_inline)) PrefixKernel() {}

    [aicore] __inline__ __attribute__((always_inline)) void Init(TPipe* pipe,
                                uint32_t datablockNum,
                                __attribute__((cce_global)) uint8_t* tilePrefix,
                                __attribute__((cce_global)) uint8_t* compressedSize,
                                __attribute__((cce_global)) uint8_t* compressedSizePrefix


    ) {
        this->pipe = pipe;
        this->DATA_BLOCK_NUM = datablockNum;
        tileprefix.SetGlobalBuffer((__attribute__((cce_global)) T*)(tilePrefix));
        compSize.SetGlobalBuffer((__attribute__((cce_global)) T*)(compressedSize));
        output.SetGlobalBuffer((__attribute__((cce_global)) T*)(compressedSizePrefix));

        pipe->InitBuffer(inQueue, BUFFER_NUM, TILE_NUM * sizeof(T));
        pipe->InitBuffer(outQueue, BUFFER_NUM, ((DATA_BLOCK_NUM + 31) / 32) * 32 * sizeof(T));
    }

    [aicore] __inline__ __attribute__((always_inline)) void Process() {
        pipe->InitBuffer(prefixTemp, ((DATA_BLOCK_NUM + 31) / 32) * 32 * sizeof(T));
        LocalTensor<T> prefixLocal = prefixTemp.Get<T>();

        for(int i = 0; i < DATA_BLOCK_NUM; i ++){
            int offset0 = i * TILE_NUM;
            CopyIn(offset0);
            Compute(i, prefixLocal);
        }

        ComputePrefix(prefixLocal);
        CopyOut(prefixLocal);
    }

private:
    [aicore] __inline__ __attribute__((always_inline)) void CopyIn(int32_t offset) {
        LocalTensor<T> inLocal = inQueue.AllocTensor<T>();
        DataCopy(inLocal, tileprefix[offset], TILE_NUM);
        inQueue.EnQue(inLocal);
    }

    [aicore] __inline__ __attribute__((always_inline)) void Compute(uint32_t i, LocalTensor<int32_t> prefixLocal) {
        LocalTensor<T> inLocal = inQueue.DeQue<T>();
        prefixLocal(i) =

        ((inLocal(TILE_NUM - 1) + 32 - 1) / 32) * 32;




        inQueue.FreeTensor(inLocal);
    }

    [aicore] __inline__ __attribute__((always_inline)) void ComputePrefix(LocalTensor<int32_t> prefixLocal){
        LocalTensor<T> outLocal = outQueue.AllocTensor<T>();
        outLocal(0) = 0;
        for(int l = 1; l < DATA_BLOCK_NUM; l ++){
            outLocal(l) = outLocal(l - 1) + prefixLocal(l - 1);
        }


        outQueue.EnQue(outLocal);
    }

    [aicore] __inline__ __attribute__((always_inline)) void CopyOut(LocalTensor<int32_t> prefixLocal) {
        LocalTensor<T> outLocal = outQueue.DeQue<T>();
        DataCopy(output, outLocal, ((DATA_BLOCK_NUM + 31) / 32) * 32);
        DataCopy(compSize, prefixLocal, ((DATA_BLOCK_NUM + 31) / 32) * 32);
        outQueue.FreeTensor(outLocal);
    }

private:
    TPipe* pipe;
    TQue<QuePosition::VECIN, 1> inQueue;
    TQue<QuePosition::VECOUT, 1> outQueue;
    TBuf<TPosition::VECCALC> prefixTemp;

    GlobalTensor<T> tileprefix;
    GlobalTensor<T> compSize;
    GlobalTensor<T> output;

    uint32_t DATA_BLOCK_NUM;
};

template<typename T>
class CoalesceKernel {
public:
    [aicore] __inline__ __attribute__((always_inline)) CoalesceKernel() {}



    [aicore] __inline__ __attribute__((always_inline)) void Init(TPipe* pipe,
                                uint32_t dataBlockNum,
                                __attribute__((cce_global)) uint8_t* finalCompressedExp,
                                __attribute__((cce_global)) uint8_t* compressedSize,
                                __attribute__((cce_global)) uint8_t* compressedSizePrefix,
                                uint32_t totalUncompressedBytes) {
        this->pipe = pipe;
        this->dataBlockNum = dataBlockNum;
        this->blockId = GetBlockIdx();
        this->blockNum = GetBlockNum();

        input.SetGlobalBuffer((__attribute__((cce_global)) T*)(finalCompressedExp + DATA_BLOCK_BYTE_NUM / 2 * dataBlockNum));
        output.SetGlobalBuffer((__attribute__((cce_global)) T*)(finalCompressedExp));
        compressedsize.SetGlobalBuffer((__attribute__((cce_global)) T*)(compressedSize));
        compressedsizePrefix.SetGlobalBuffer((__attribute__((cce_global)) T*)(compressedSizePrefix));

        pipe->InitBuffer(queBind, BUFFER_NUM, DATA_BLOCK_BYTE_NUM / 2);
    }

public:
    [aicore] __inline__ __attribute__((always_inline)) void Process()
    {
        pipe->InitBuffer(compSize, dataBlockNum * sizeof(T));
        LocalTensor<T> compSizeLocal = compSize.Get<T>();
        DataCopy(compSizeLocal, compressedsize, dataBlockNum);

        pipe->InitBuffer(compPrefix, dataBlockNum * sizeof(T));
        LocalTensor<T> compPrefixLocal = compPrefix.Get<T>();
        DataCopy(compPrefixLocal, compressedsizePrefix, dataBlockNum);



        auto bindLocal = queBind.AllocTensor<T>();
        for(int i = blockId; i < dataBlockNum; i += blockNum){
            uint32_t compSize = compSizeLocal(i);
            uint32_t compSizePrefix = compPrefixLocal(i);
            DataCopy(bindLocal, input[i * DATA_BLOCK_BYTE_NUM / 2 / sizeof(T)], compSize / sizeof(T));


            DataCopy(output[compSizePrefix / sizeof(T)], bindLocal, compSize / sizeof(T));




        }
        queBind.FreeTensor(bindLocal);

    }
# 174 "/root/yjw/HANS/HANS-NPU/hans_merge.cpp"
private:
    TPipe* pipe;
    TQueBind<TPosition::VECIN, TPosition::VECOUT, BUFFER_NUM> queBind;
    TBuf<TPosition::VECCALC> copy;
    TBuf<TPosition::VECCALC> compSize;
    TBuf<TPosition::VECCALC> compPrefix;

    GlobalTensor<T> input;
    GlobalTensor<T> output;
    GlobalTensor<T> compressedsize;
    GlobalTensor<T> compressedsizePrefix;

    uint32_t blockId;
    uint32_t blockNum;
    uint32_t dataBlockNum;
};


__attribute__((cce_kernel)) [aicore] void calcprefix(uint32_t datablockNum,
                                      __attribute__((cce_global)) uint8_t* tilePrefix,
                                      __attribute__((cce_global)) uint8_t* compressedSize,
                                      __attribute__((cce_global)) uint8_t* compressedSizePrefix
                                      )
{
    TPipe pipe;
    PrefixKernel<int32_t> op;
    op.Init(&pipe, datablockNum, tilePrefix, compressedSize, compressedSizePrefix);
    op.Process();
}

__attribute__((cce_kernel)) [aicore] void coalesce(uint32_t dataBlockNum,
                                    __attribute__((cce_global)) uint8_t* finalCompressedExp,
                                    __attribute__((cce_global)) uint8_t* compressedSize,
                                    __attribute__((cce_global)) uint8_t* compressedSizePrefix,
                                    uint32_t totalUncompressedBytes)
{
    TPipe pipe;
    CoalesceKernel<uint32_t> op;
    op.Init(&pipe, dataBlockNum, finalCompressedExp, compressedSize, compressedSizePrefix, totalUncompressedBytes);
    op.Process();
}

extern "C" void merge(uint32_t datablockNum, void* stream, uint8_t* srcDevice, uint8_t* tempBuffer, uint8_t* final, int32_t* histogramDevice, uint32_t* tilePrefix, uint32_t* compressedSize, uint32_t* compressedSizePrefix, uint32_t totalUncompressedSize) {



    calcprefix<<<1, nullptr, stream>>>(datablockNum, reinterpret_cast<uint8_t*>(tilePrefix), reinterpret_cast<uint8_t*>(compressedSize), reinterpret_cast<uint8_t*>(compressedSizePrefix));
    coalesce<<<BLOCK_NUM, nullptr, stream>>>(datablockNum, final + 32 + HISTOGRAM_BINS * sizeof(uint8_t) + TILE_NUM * datablockNum + DATA_BLOCK_BYTE_NUM / 2 * datablockNum, reinterpret_cast<uint8_t*>(compressedSize), reinterpret_cast<uint8_t*>(compressedSizePrefix), totalUncompressedSize);
}
